episode1,iteration0 selected nodes:[19, 5, 1, 18, 15],center node:15
################################################## episode1,iteration0 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:1.8498700874693252,train_acc:0.3585294187068939
node1 epoch1:node_model train_loss:1.4291992748484892,train_acc:0.49147066473960876
node1 epoch2:node_model train_loss:1.2984244525432587,train_acc:0.5452940464019775
node1 epoch3:node_model train_loss:1.1537521104602253,train_acc:0.6005147695541382
node1 epoch4:node_model train_loss:1.0572834514519747,train_acc:0.6337499022483826
node1_model on test-dataset: loss:1.1478568843007089,acc:0.6121000051498413
node1 weight score:5843.9341103805045
node5: train data size:3735
node5 epoch0:node_model train_loss:2.1350077045591256,train_acc:0.2711654603481293
node5 epoch1:node_model train_loss:1.6338645376657184,train_acc:0.4114286005496979
node5 epoch2:node_model train_loss:1.4564323237067776,train_acc:0.46819552779197693
node5 epoch3:node_model train_loss:1.2688976447833211,train_acc:0.5499247312545776
node5 epoch4:node_model train_loss:1.16589135245273,train_acc:0.5842105746269226
node5_model on test-dataset: loss:1.3518191796541215,acc:0.5293998718261719
node5 weight score:2762.9434884594866
node15: train data size:629
node15 epoch0:node_model train_loss:2.785230806895665,train_acc:0.1535467952489853
node15 epoch1:node_model train_loss:2.0379298755100796,train_acc:0.29847288131713867
node15 epoch2:node_model train_loss:1.833595974104745,train_acc:0.3179802894592285
node15 epoch3:node_model train_loss:1.6123143604823522,train_acc:0.4180295765399933
node15 epoch4:node_model train_loss:1.6163068669182914,train_acc:0.4136945903301239
node15_model on test-dataset: loss:1.8494958579540253,acc:0.321399986743927
node15 weight score:340.09267838848854
node18: train data size:472
node18 epoch0:node_model train_loss:2.7801176071166993,train_acc:0.1585555523633957
node18 epoch1:node_model train_loss:2.0491317987442015,train_acc:0.27666667103767395
node18 epoch2:node_model train_loss:1.847714114189148,train_acc:0.3271111249923706
node18 epoch3:node_model train_loss:1.6229448080062867,train_acc:0.40700003504753113
node18 epoch4:node_model train_loss:1.4455488443374633,train_acc:0.48988890647888184
node18_model on test-dataset: loss:2.177095648050308,acc:0.21470002830028534
node18 weight score:216.80260140279012
node19: train data size:4281
node19 epoch0:node_model train_loss:1.9375663496727167,train_acc:0.3196669816970825
node19 epoch1:node_model train_loss:1.5179964997047601,train_acc:0.4557393193244934
node19 epoch2:node_model train_loss:1.3203222557555798,train_acc:0.5316882729530334
node19 epoch3:node_model train_loss:1.1915651227152624,train_acc:0.5834540128707886
node19 epoch4:node_model train_loss:1.1149990322977998,train_acc:0.6141372323036194
node19_model on test-dataset: loss:1.4570050810277462,acc:0.509600043296814
node19 weight score:2938.218991645696
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.336340714097023,acc:0.5319999845325947
total cost energy:9.666820376686506 | all_enery_cp：7.9125 | all_enery_tp: 1.7543203766865054
ef: 23.647414234452402
reward: 13.980593857765896
step 61:loss:253.2791748046875|running q:0.019152499735355377
episode1,iteration1 selected nodes:[5, 0, 11, 19, 4],center node:11
################################################## episode1,iteration1 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:1.3184617964121013,train_acc:0.5520087480545044
node0 epoch1:node_model train_loss:1.1186102915268679,train_acc:0.6105096936225891
node0 epoch2:node_model train_loss:0.9893935322761536,train_acc:0.6564319729804993
node0 epoch3:node_model train_loss:0.9229914270914518,train_acc:0.6794741153717041
node0 epoch4:node_model train_loss:0.8321288640682514,train_acc:0.716598629951477
node0_model on test-dataset: loss:1.203773659169674,acc:0.6156999468803406
node0 weight score:4305.626693622017
node4: train data size:2705
node4 epoch0:node_model train_loss:1.4139909531388963,train_acc:0.5103570818901062
node4 epoch1:node_model train_loss:1.150371668594224,train_acc:0.5907142758369446
node4 epoch2:node_model train_loss:1.0371436199971609,train_acc:0.6378570795059204
node4 epoch3:node_model train_loss:0.9296807327440807,train_acc:0.6639285087585449
node4 epoch4:node_model train_loss:0.9173982505287442,train_acc:0.663214385509491
node4_model on test-dataset: loss:1.3731745009124279,acc:0.554099977016449
node4 weight score:1969.8880209344254
node5: train data size:3735
node5 epoch0:node_model train_loss:1.3075694818245738,train_acc:0.5438345670700073
node5 epoch1:node_model train_loss:1.0888322529039884,train_acc:0.6192104816436768
node5 epoch2:node_model train_loss:0.9541985863133481,train_acc:0.6690601110458374
node5 epoch3:node_model train_loss:0.8684078533398477,train_acc:0.6968043446540833
node5 epoch4:node_model train_loss:0.8399663975364283,train_acc:0.7124060392379761
node5_model on test-dataset: loss:1.1406059446930885,acc:0.6238999962806702
node5 weight score:3274.575253073054
node11: train data size:1682
node11 epoch0:node_model train_loss:1.4019696221632116,train_acc:0.5033860206604004
node11 epoch1:node_model train_loss:1.0613494620603674,train_acc:0.6280918717384338
node11 epoch2:node_model train_loss:0.9535880719914156,train_acc:0.6660544872283936
node11 epoch3:node_model train_loss:0.8726677088176503,train_acc:0.6880200505256653
node11 epoch4:node_model train_loss:0.7771726741510279,train_acc:0.7184792757034302
node11_model on test-dataset: loss:1.2335677298903465,acc:0.5980998873710632
node11 weight score:1363.5246442037808
node19: train data size:4281
node19 epoch0:node_model train_loss:1.3086782333462736,train_acc:0.539882242679596
node19 epoch1:node_model train_loss:1.0450560658477073,train_acc:0.6377059817314148
node19 epoch2:node_model train_loss:0.9373076502666917,train_acc:0.6677058935165405
node19 epoch3:node_model train_loss:0.8702106240183808,train_acc:0.6901004314422607
node19 epoch4:node_model train_loss:0.8524660606716954,train_acc:0.7081567645072937
node19_model on test-dataset: loss:1.1988326869904995,acc:0.5912998914718628
node19 weight score:3570.973703383787
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9358046692609787,acc:0.6756999844312668
total cost energy:10.730163089161124 | all_enery_cp：8.793 | all_enery_tp: 1.9371630891611247
ef: 24.19009879770296
reward: 13.459935708541835
step 62:loss:307.5882263183594|running q:0.1292702853679657
episode1,iteration2 selected nodes:[12, 2, 14, 11, 3],center node:11
################################################## episode1,iteration2 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:1.1011704405148823,train_acc:0.6250473260879517
node2 epoch1:node_model train_loss:0.8792554202179114,train_acc:0.6923388838768005
node2 epoch2:node_model train_loss:0.8140897105137507,train_acc:0.7173862457275391
node2 epoch3:node_model train_loss:0.7273959591984749,train_acc:0.7478976249694824
node2 epoch4:node_model train_loss:0.6824135531981786,train_acc:0.7667045593261719
node2_model on test-dataset: loss:1.0767268577218057,acc:0.6491001844406128
node2 weight score:4446.810224582581
node3: train data size:4247
node3 epoch0:node_model train_loss:1.109553295512532,train_acc:0.6282188296318054
node3 epoch1:node_model train_loss:0.9067784256713335,train_acc:0.6840871572494507
node3 epoch2:node_model train_loss:0.7831082676732263,train_acc:0.7284215092658997
node3 epoch3:node_model train_loss:0.7645046593144883,train_acc:0.7377287745475769
node3 epoch4:node_model train_loss:0.7182995690855869,train_acc:0.7422314286231995
node3_model on test-dataset: loss:1.1413003206253052,acc:0.6257999539375305
node3 weight score:3721.1940829676787
node11: train data size:1682
node11 epoch0:node_model train_loss:1.1753891110420227,train_acc:0.6134433746337891
node11 epoch1:node_model train_loss:0.914937766159282,train_acc:0.6727977395057678
node11 epoch2:node_model train_loss:0.7546892832307255,train_acc:0.737604022026062
node11 epoch3:node_model train_loss:0.683914815678316,train_acc:0.7614060640335083
node11 epoch4:node_model train_loss:0.6064555469681235,train_acc:0.7879340648651123
node11_model on test-dataset: loss:1.1145223805308342,acc:0.6381000280380249
node11 weight score:1509.166643382148
node12: train data size:1336
node12 epoch0:node_model train_loss:1.129794874361583,train_acc:0.6116666793823242
node12 epoch1:node_model train_loss:0.9111496252673013,train_acc:0.6765078902244568
node12 epoch2:node_model train_loss:0.7442901687962669,train_acc:0.7414284944534302
node12 epoch3:node_model train_loss:0.6101834050246647,train_acc:0.7992063164710999
node12 epoch4:node_model train_loss:0.5921181312629155,train_acc:0.7880159020423889
node12_model on test-dataset: loss:1.2450857533514499,acc:0.5905998349189758
node12 weight score:1073.018461904196
node14: train data size:1172
node14 epoch0:node_model train_loss:1.2643900712331135,train_acc:0.5944445133209229
node14 epoch1:node_model train_loss:0.9192076474428177,train_acc:0.6704165935516357
node14 epoch2:node_model train_loss:0.7523724436759949,train_acc:0.7336573600769043
node14 epoch3:node_model train_loss:0.6411416927973429,train_acc:0.7836110591888428
node14 epoch4:node_model train_loss:0.5802835946281751,train_acc:0.8062962889671326
node14_model on test-dataset: loss:1.0433137565851212,acc:0.6410001516342163
node14 weight score:1123.3437617424722
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8477076622843742,acc:0.7038999834656715
total cost energy:8.173055127546398 | all_enery_cp：6.6125 | all_enery_tp: 1.560555127546399
ef: 24.29931462409843
reward: 16.12625949655203
step 63:loss:367.5322265625|running q:0.4173085391521454
episode1,iteration3 selected nodes:[9, 13, 10, 7, 15],center node:7
################################################## episode1,iteration3 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:1.084444710612297,train_acc:0.6494116187095642
node7 epoch1:node_model train_loss:0.7952645868062973,train_acc:0.7307941317558289
node7 epoch2:node_model train_loss:0.6709850236773491,train_acc:0.7727352976799011
node7 epoch3:node_model train_loss:0.5928865775465966,train_acc:0.7996371984481812
node7 epoch4:node_model train_loss:0.5379187494516373,train_acc:0.8222941756248474
node7_model on test-dataset: loss:1.1503046461939812,acc:0.6407999992370605
node7 weight score:1696.0724330335302
node9: train data size:1857
node9 epoch0:node_model train_loss:1.1245014196948002,train_acc:0.6362788081169128
node9 epoch1:node_model train_loss:0.812842983948557,train_acc:0.7211449146270752
node9 epoch2:node_model train_loss:0.7186086899355838,train_acc:0.749713659286499
node9 epoch3:node_model train_loss:0.6124582243593115,train_acc:0.7849768996238708
node9 epoch4:node_model train_loss:0.5381546161676708,train_acc:0.8157709836959839
node9_model on test-dataset: loss:0.958043097704649,acc:0.6805998086929321
node9 weight score:1938.3261613690854
node10: train data size:1975
node10 epoch0:node_model train_loss:1.1125676810741425,train_acc:0.6270000338554382
node10 epoch1:node_model train_loss:0.8264898091554642,train_acc:0.7121665477752686
node10 epoch2:node_model train_loss:0.7056416869163513,train_acc:0.76500004529953
node10 epoch3:node_model train_loss:0.6194546967744827,train_acc:0.7921666502952576
node10 epoch4:node_model train_loss:0.5544522002339363,train_acc:0.8125
node10_model on test-dataset: loss:0.9672371116280556,acc:0.6718999147415161
node10 weight score:2041.89849237244
node13: train data size:1155
node13 epoch0:node_model train_loss:1.1778919051090877,train_acc:0.6209848523139954
node13 epoch1:node_model train_loss:0.9017645219961802,train_acc:0.6864393949508667
node13 epoch2:node_model train_loss:0.715528778731823,train_acc:0.7671212553977966
node13 epoch3:node_model train_loss:0.5874400014678637,train_acc:0.79901522397995
node13 epoch4:node_model train_loss:0.5079382633169492,train_acc:0.8440909385681152
node13_model on test-dataset: loss:1.0097554574906826,acc:0.6561999917030334
node13 weight score:1143.8413047751787
node15: train data size:629
node15 epoch0:node_model train_loss:1.1442308170454842,train_acc:0.6072413921356201
node15 epoch1:node_model train_loss:0.881152365888868,train_acc:0.6886700391769409
node15 epoch2:node_model train_loss:0.7079676176820483,train_acc:0.7455173134803772
node15 epoch3:node_model train_loss:0.5816504274095807,train_acc:0.793448269367218
node15 epoch4:node_model train_loss:0.47587553943906513,train_acc:0.8623645305633545
node15_model on test-dataset: loss:1.1526011164486407,acc:0.6315999627113342
node15 weight score:545.7221852587264
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.784469426870346,acc:0.7316999825835228
total cost energy:5.546742176994632 | all_enery_cp：3.7835 | all_enery_tp: 1.763242176994632
ef: 24.07614766784881
reward: 18.52940549085418
step 64:loss:244.9922332763672|running q:0.7807576656341553
episode1,iteration4 selected nodes:[3, 0, 9, 19, 1],center node:9
################################################## episode1,iteration4 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.9762685745954514,train_acc:0.670085608959198
node0 epoch1:node_model train_loss:0.7935827213984269,train_acc:0.7153615355491638
node0 epoch2:node_model train_loss:0.7248456025352845,train_acc:0.7441728711128235
node0 epoch3:node_model train_loss:0.7149578309976138,train_acc:0.7518303394317627
node0 epoch4:node_model train_loss:0.611116145665829,train_acc:0.7858641743659973
node0_model on test-dataset: loss:0.9221597900986671,acc:0.6881000995635986
node0 weight score:5620.500975699061
node1: train data size:6708
node1 epoch0:node_model train_loss:0.9257354214787483,train_acc:0.6874263286590576
node1 epoch1:node_model train_loss:0.8112133495947894,train_acc:0.7282354235649109
node1 epoch2:node_model train_loss:0.7583946533062879,train_acc:0.741176426410675
node1 epoch3:node_model train_loss:0.6871293778805172,train_acc:0.7555148005485535
node1 epoch4:node_model train_loss:0.6243350562803885,train_acc:0.784852921962738
node1_model on test-dataset: loss:1.1165737774968147,acc:0.6554000973701477
node1 weight score:6007.663922610019
node3: train data size:4247
node3 epoch0:node_model train_loss:0.8875522364017575,train_acc:0.7001087069511414
node3 epoch1:node_model train_loss:0.7142562069172083,train_acc:0.7554031014442444
node3 epoch2:node_model train_loss:0.6622279210146084,train_acc:0.769614040851593
node3 epoch3:node_model train_loss:0.5879184527452602,train_acc:0.8029289841651917
node3 epoch4:node_model train_loss:0.5399663822595463,train_acc:0.8090944290161133
node3_model on test-dataset: loss:0.9462515529990196,acc:0.6878998279571533
node3 weight score:4488.235698572639
node9: train data size:1857
node9 epoch0:node_model train_loss:0.9383717242040133,train_acc:0.6814126968383789
node9 epoch1:node_model train_loss:0.7038851098010415,train_acc:0.7500922679901123
node9 epoch2:node_model train_loss:0.5881290969095732,train_acc:0.799981415271759
node9 epoch3:node_model train_loss:0.5009484165593198,train_acc:0.8272114992141724
node9 epoch4:node_model train_loss:0.4447621985485679,train_acc:0.8589380979537964
node9_model on test-dataset: loss:1.0977005387842655,acc:0.6552999019622803
node9 weight score:1691.7182185741478
node19: train data size:4281
node19 epoch0:node_model train_loss:0.939610784830049,train_acc:0.6777345538139343
node19 epoch1:node_model train_loss:0.7899980697520944,train_acc:0.7242578268051147
node19 epoch2:node_model train_loss:0.7154332614222239,train_acc:0.7496612071990967
node19 epoch3:node_model train_loss:0.6831162149129912,train_acc:0.7596611380577087
node19 epoch4:node_model train_loss:0.5947179558665253,train_acc:0.7849956750869751
node19_model on test-dataset: loss:1.011582299619913,acc:0.6622999310493469
node19 weight score:4231.983894546714
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7448291617631912,acc:0.7448999834060669
total cost energy:12.89445202109687 | all_enery_cp：11.137999999999998 | all_enery_tp: 1.7564520210968708
ef: 24.752867339993575
reward: 11.858415318896705
step 65:loss:256.3442687988281|running q:1.307857871055603
episode1,iteration5 selected nodes:[5, 1, 6, 3, 2],center node:6
################################################## episode1,iteration5 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.785900840864462,train_acc:0.7324998378753662
node1 epoch1:node_model train_loss:0.6935493801446522,train_acc:0.7613970637321472
node1 epoch2:node_model train_loss:0.6515505129800123,train_acc:0.7722058892250061
node1 epoch3:node_model train_loss:0.6046958684044725,train_acc:0.7919853329658508
node1 epoch4:node_model train_loss:0.5446840625475434,train_acc:0.8058087825775146
node1_model on test-dataset: loss:0.9790153485536576,acc:0.6855000257492065
node1 weight score:6851.782262566183
node2: train data size:4788
node2 epoch0:node_model train_loss:0.8549112329880396,train_acc:0.7156060934066772
node2 epoch1:node_model train_loss:0.6725828169534603,train_acc:0.756695032119751
node2 epoch2:node_model train_loss:0.6123440352578958,train_acc:0.7807291746139526
node2 epoch3:node_model train_loss:0.5338664030035337,train_acc:0.8213920593261719
node2 epoch4:node_model train_loss:0.4702751499911149,train_acc:0.8397538065910339
node2_model on test-dataset: loss:0.8824606513977051,acc:0.7052000761032104
node2 weight score:5425.7376716077015
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7726122445838396,train_acc:0.7327263355255127
node3 epoch1:node_model train_loss:0.6286165748917779,train_acc:0.7827857136726379
node3 epoch2:node_model train_loss:0.5763225985127826,train_acc:0.795136034488678
node3 epoch3:node_model train_loss:0.49059036859246186,train_acc:0.8267689347267151
node3 epoch4:node_model train_loss:0.49982989319535187,train_acc:0.8268281817436218
node3_model on test-dataset: loss:0.9274453642964363,acc:0.7044999599456787
node3 weight score:4579.245488193033
node5: train data size:3735
node5 epoch0:node_model train_loss:0.92268894063799,train_acc:0.693157970905304
node5 epoch1:node_model train_loss:0.7114188239762658,train_acc:0.7583457827568054
node5 epoch2:node_model train_loss:0.643188761253106,train_acc:0.7736841440200806
node5 epoch3:node_model train_loss:0.5715372829060805,train_acc:0.7996615171432495
node5 epoch4:node_model train_loss:0.5268972233722085,train_acc:0.8223308324813843
node5_model on test-dataset: loss:1.0385239945352078,acc:0.6713000535964966
node5 weight score:3596.4503657631926
node6: train data size:3007
node6 epoch0:node_model train_loss:0.9692155776485321,train_acc:0.6837325692176819
node6 epoch1:node_model train_loss:0.7973300708878425,train_acc:0.7270045876502991
node6 epoch2:node_model train_loss:0.6878638305971699,train_acc:0.7613362669944763
node6 epoch3:node_model train_loss:0.6765261657776371,train_acc:0.7545620799064636
node6 epoch4:node_model train_loss:0.5766444071646659,train_acc:0.8031336665153503
node6_model on test-dataset: loss:0.9115920913219452,acc:0.7024999260902405
node6 weight score:3298.6244929345526
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6943707056343555,acc:0.7655999818444252
total cost energy:12.302024158061723 | all_enery_cp：11.2425 | all_enery_tp: 1.059524158061724
ef: 25.049505359236715
reward: 12.747481201174992
step 66:loss:241.55209350585938|running q:1.9457981586456299
episode1,iteration6 selected nodes:[8, 7, 19, 12, 0],center node:7
################################################## episode1,iteration6 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.8258048616922818,train_acc:0.725025475025177
node0 epoch1:node_model train_loss:0.6457226626001872,train_acc:0.7804843187332153
node0 epoch2:node_model train_loss:0.5814431447249192,train_acc:0.79706209897995
node0 epoch3:node_model train_loss:0.5403845453491578,train_acc:0.8122544288635254
node0 epoch4:node_model train_loss:0.4760625929786609,train_acc:0.8394832015037537
node0_model on test-dataset: loss:0.9098712664842605,acc:0.7138999700546265
node0 weight score:5696.410240568531
node7: train data size:1951
node7 epoch0:node_model train_loss:0.8764091730117798,train_acc:0.7251960635185242
node7 epoch1:node_model train_loss:0.6097640350461007,train_acc:0.800696074962616
node7 epoch2:node_model train_loss:0.49700732082128524,train_acc:0.8345980644226074
node7 epoch3:node_model train_loss:0.42069211304187776,train_acc:0.8526372909545898
node7 epoch4:node_model train_loss:0.32893204167485235,train_acc:0.896117627620697
node7_model on test-dataset: loss:0.8448308265209198,acc:0.7243998646736145
node7 weight score:2309.338081369938
node8: train data size:1798
node8 epoch0:node_model train_loss:0.9853044185373518,train_acc:0.6830499172210693
node8 epoch1:node_model train_loss:0.7065970566537645,train_acc:0.7592176795005798
node8 epoch2:node_model train_loss:0.5467788245942857,train_acc:0.815861701965332
node8 epoch3:node_model train_loss:0.4611254582802455,train_acc:0.8537187576293945
node8 epoch4:node_model train_loss:0.4056123395760854,train_acc:0.8726643323898315
node8_model on test-dataset: loss:0.8849273148179054,acc:0.7049999833106995
node8 weight score:2031.8052905508753
node12: train data size:1336
node12 epoch0:node_model train_loss:0.9226394551140922,train_acc:0.6842857599258423
node12 epoch1:node_model train_loss:0.6046166632856641,train_acc:0.801111102104187
node12 epoch2:node_model train_loss:0.4934346399136952,train_acc:0.8333333134651184
node12 epoch3:node_model train_loss:0.3710633580173765,train_acc:0.8802381157875061
node12 epoch4:node_model train_loss:0.3218481923852648,train_acc:0.9017460346221924
node12_model on test-dataset: loss:1.0007372818887235,acc:0.6825998425483704
node12 weight score:1335.0157170906277
node19: train data size:4281
node19 epoch0:node_model train_loss:0.8384862569875495,train_acc:0.71945720911026
node19 epoch1:node_model train_loss:0.6443746256273847,train_acc:0.7787855863571167
node19 epoch2:node_model train_loss:0.546163619257683,train_acc:0.8148722052574158
node19 epoch3:node_model train_loss:0.4945187804310821,train_acc:0.8306717276573181
node19 epoch4:node_model train_loss:0.4694022730339405,train_acc:0.8327246308326721
node19_model on test-dataset: loss:0.8698636791110039,acc:0.7147000432014465
node19 weight score:4921.460802197374
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6715385644137859,acc:0.7763999798893928
total cost energy:9.360230076213409 | all_enery_cp：7.2745 | all_enery_tp: 2.0857300762134083
ef: 24.755827473608264
reward: 15.395597397394855
step 67:loss:293.6897277832031|running q:2.665310859680176
episode1,iteration7 selected nodes:[14, 16, 4, 15, 1],center node:14
################################################## episode1,iteration7 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.7014380088623833,train_acc:0.7637500166893005
node1 epoch1:node_model train_loss:0.6175069462727097,train_acc:0.7824264168739319
node1 epoch2:node_model train_loss:0.6020389085306841,train_acc:0.7827205657958984
node1 epoch3:node_model train_loss:0.5211586882086361,train_acc:0.8250000476837158
node1 epoch4:node_model train_loss:0.435927830855636,train_acc:0.8458824157714844
node1_model on test-dataset: loss:0.868315387070179,acc:0.7147000432014465
node1 weight score:7725.303616504779
node4: train data size:2705
node4 epoch0:node_model train_loss:0.8896224222012928,train_acc:0.7128571271896362
node4 epoch1:node_model train_loss:0.7256263047456741,train_acc:0.760357141494751
node4 epoch2:node_model train_loss:0.573568499780127,train_acc:0.7985714673995972
node4 epoch3:node_model train_loss:0.4754957524793489,train_acc:0.8271428942680359
node4 epoch4:node_model train_loss:0.5208112810339246,train_acc:0.8189287781715393
node4_model on test-dataset: loss:0.9573062837123871,acc:0.6947999596595764
node4 weight score:2825.636941930582
node14: train data size:1172
node14 epoch0:node_model train_loss:0.8541400680939356,train_acc:0.7153703570365906
node14 epoch1:node_model train_loss:0.5600244104862213,train_acc:0.8022686243057251
node14 epoch2:node_model train_loss:0.421335369348526,train_acc:0.854722261428833
node14 epoch3:node_model train_loss:0.3189453110098839,train_acc:0.8995370864868164
node14 epoch4:node_model train_loss:0.29567701369524,train_acc:0.9063889384269714
node14_model on test-dataset: loss:0.8325986854732037,acc:0.730499804019928
node14 weight score:1407.64094448924
node15: train data size:629
node15 epoch0:node_model train_loss:0.997978925704956,train_acc:0.6816748380661011
node15 epoch1:node_model train_loss:0.6624427097184318,train_acc:0.7655172944068909
node15 epoch2:node_model train_loss:0.5004697569778987,train_acc:0.8423645496368408
node15 epoch3:node_model train_loss:0.3926980665751866,train_acc:0.8852216601371765
node15 epoch4:node_model train_loss:0.28075398078986574,train_acc:0.9265024662017822
node15_model on test-dataset: loss:0.9633899553120137,acc:0.6953999400138855
node15 weight score:652.902800710939
node16: train data size:877
node16 epoch0:node_model train_loss:0.8751365939776102,train_acc:0.7201443314552307
node16 epoch1:node_model train_loss:0.5974780453575982,train_acc:0.8052526116371155
node16 epoch2:node_model train_loss:0.46214911672804093,train_acc:0.8600144386291504
node16 epoch3:node_model train_loss:0.3673211998409695,train_acc:0.8787878751754761
node16 epoch4:node_model train_loss:0.26092031763659584,train_acc:0.9343434572219849
node16_model on test-dataset: loss:0.9071636791527271,acc:0.7108999490737915
node16 weight score:966.7494633593583
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6815699538588524,acc:0.7730999809503555
total cost energy:7.853147501533542 | all_enery_cp：6.0455 | all_enery_tp: 1.8076475015335425
ef: 24.668999145707478
reward: 16.815851644173936
step 68:loss:280.3244323730469|running q:3.593928575515747
episode1,iteration8 selected nodes:[14, 3, 4, 9, 16],center node:14
################################################## episode1,iteration8 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7111125387424646,train_acc:0.7622908353805542
node3 epoch1:node_model train_loss:0.5401113622410353,train_acc:0.813453733921051
node3 epoch2:node_model train_loss:0.4656938473845637,train_acc:0.841068685054779
node3 epoch3:node_model train_loss:0.4341031943642816,train_acc:0.841855525970459
node3 epoch4:node_model train_loss:0.3806914707948995,train_acc:0.8688322305679321
node3_model on test-dataset: loss:0.8408930893242359,acc:0.7358000874519348
node3 weight score:5050.582593576792
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7416469848581723,train_acc:0.7475000619888306
node4 epoch1:node_model train_loss:0.568493440747261,train_acc:0.7939285635948181
node4 epoch2:node_model train_loss:0.5706302183015006,train_acc:0.8014286756515503
node4 epoch3:node_model train_loss:0.5710683582084519,train_acc:0.7914284467697144
node4 epoch4:node_model train_loss:0.45803702782307354,train_acc:0.8421429395675659
node4_model on test-dataset: loss:1.0038494105637072,acc:0.6934998035430908
node4 weight score:2694.6272733088713
node9: train data size:1857
node9 epoch0:node_model train_loss:0.8391056296072508,train_acc:0.7402307987213135
node9 epoch1:node_model train_loss:0.5542501618987635,train_acc:0.8112927675247192
node9 epoch2:node_model train_loss:0.4744480158153333,train_acc:0.8410341739654541
node9 epoch3:node_model train_loss:0.35479381994197245,train_acc:0.8826223611831665
node9 epoch4:node_model train_loss:0.3363550454378128,train_acc:0.8869436979293823
node9_model on test-dataset: loss:0.8080272082984448,acc:0.7371000647544861
node9 weight score:2298.189938319648
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7943654557069143,train_acc:0.7156944274902344
node14 epoch1:node_model train_loss:0.5207681556542715,train_acc:0.8114351630210876
node14 epoch2:node_model train_loss:0.3626456782221794,train_acc:0.8682407736778259
node14 epoch3:node_model train_loss:0.28431092699368793,train_acc:0.9156943559646606
node14 epoch4:node_model train_loss:0.20605704436699548,train_acc:0.9498610496520996
node14_model on test-dataset: loss:0.8061987075209618,acc:0.7344998717308044
node14 weight score:1453.7358954641181
node16: train data size:877
node16 epoch0:node_model train_loss:0.8415193127261268,train_acc:0.7434776425361633
node16 epoch1:node_model train_loss:0.591163350476159,train_acc:0.8032467365264893
node16 epoch2:node_model train_loss:0.4140276097589069,train_acc:0.8571284413337708
node16 epoch3:node_model train_loss:0.3462918831242455,train_acc:0.8962337970733643
node16 epoch4:node_model train_loss:0.24486775199572244,train_acc:0.9211255311965942
node16_model on test-dataset: loss:1.058692772090435,acc:0.6850000023841858
node16 weight score:828.3800769399088
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6705613574385643,acc:0.7804999822378158
total cost energy:7.052588153981688 | all_enery_cp：5.428999999999999 | all_enery_tp: 1.6235881539816888
ef: 24.850481239725255
reward: 17.797893085743567
step 69:loss:443.31610107421875|running q:4.577664852142334
episode1,iteration9 selected nodes:[3, 9, 0, 13, 10],center node:10
################################################## episode1,iteration9 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.7325477491204555,train_acc:0.7509822845458984
node0 epoch1:node_model train_loss:0.5251326887653425,train_acc:0.818250834941864
node0 epoch2:node_model train_loss:0.46688113246972746,train_acc:0.8407161831855774
node0 epoch3:node_model train_loss:0.4245532934482281,train_acc:0.8501089215278625
node0 epoch4:node_model train_loss:0.3960946761071682,train_acc:0.8660611510276794
node0_model on test-dataset: loss:0.9181040351092815,acc:0.7188999056816101
node0 weight score:5645.329724951127
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5755795032479042,train_acc:0.7920583486557007
node3 epoch1:node_model train_loss:0.42160563448140786,train_acc:0.8584562540054321
node3 epoch2:node_model train_loss:0.3432236878677856,train_acc:0.8867987394332886
node3 epoch3:node_model train_loss:0.317303376835446,train_acc:0.9011281132698059
node3 epoch4:node_model train_loss:0.2638427493877189,train_acc:0.9179909229278564
node3_model on test-dataset: loss:0.8963944748044014,acc:0.7299001216888428
node3 weight score:4737.869452984659
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6953987055703214,train_acc:0.7689288854598999
node9 epoch1:node_model train_loss:0.5176157998411279,train_acc:0.8235364556312561
node9 epoch2:node_model train_loss:0.37611886466804306,train_acc:0.8680054545402527
node9 epoch3:node_model train_loss:0.28916230170350327,train_acc:0.9059095978736877
node9 epoch4:node_model train_loss:0.2344384138521395,train_acc:0.9326130747795105
node9_model on test-dataset: loss:0.900960106998682,acc:0.7162999510765076
node9 weight score:2061.134544775928
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8273141503334045,train_acc:0.7281666398048401
node10 epoch1:node_model train_loss:0.5641114860773087,train_acc:0.8115000128746033
node10 epoch2:node_model train_loss:0.43280205577611924,train_acc:0.8489999771118164
node10 epoch3:node_model train_loss:0.3732694543898106,train_acc:0.8779999613761902
node10 epoch4:node_model train_loss:0.3202523566782475,train_acc:0.9103332757949829
node10_model on test-dataset: loss:0.917511566132307,acc:0.7053999304771423
node10 weight score:2152.5614203703685
node13: train data size:1155
node13 epoch0:node_model train_loss:0.9215634266535441,train_acc:0.7132575511932373
node13 epoch1:node_model train_loss:0.5850154832005501,train_acc:0.8086363077163696
node13 epoch2:node_model train_loss:0.4590347930788994,train_acc:0.8352272510528564
node13 epoch3:node_model train_loss:0.3473787133892377,train_acc:0.8859091997146606
node13 epoch4:node_model train_loss:0.28003625695904094,train_acc:0.9085605144500732
node13_model on test-dataset: loss:0.8594357857108116,acc:0.7184998989105225
node13 weight score:1343.9049422927355
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6568389889597893,acc:0.7859999784827232
total cost energy:8.976526820810989 | all_enery_cp：7.208499999999999 | all_enery_tp: 1.768026820810989
ef: 24.824203310686027
reward: 15.847676489875038
step 70:loss:469.4363708496094|running q:5.893048286437988
episode1,iteration10 selected nodes:[6, 16, 9, 4, 10],center node:9
################################################## episode1,iteration10 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7539378149168832,train_acc:0.7582143545150757
node4 epoch1:node_model train_loss:0.558107930634703,train_acc:0.8128570914268494
node4 epoch2:node_model train_loss:0.4118426686951092,train_acc:0.8585714101791382
node4 epoch3:node_model train_loss:0.35115098527499605,train_acc:0.881428599357605
node4 epoch4:node_model train_loss:0.32572282903960775,train_acc:0.8875001072883606
node4_model on test-dataset: loss:0.8502481983602047,acc:0.7337998151779175
node4 weight score:3181.423971514299
node6: train data size:3007
node6 epoch0:node_model train_loss:0.779386947231908,train_acc:0.749400794506073
node6 epoch1:node_model train_loss:0.6476327526953912,train_acc:0.7953916788101196
node6 epoch2:node_model train_loss:0.47273932060887736,train_acc:0.8386174440383911
node6 epoch3:node_model train_loss:0.408470070169818,train_acc:0.8670967817306519
node6 epoch4:node_model train_loss:0.3623860259690592,train_acc:0.8809677362442017
node6_model on test-dataset: loss:0.8308336490392685,acc:0.7401000261306763
node6 weight score:3619.2563980492773
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6729324930592587,train_acc:0.7687719464302063
node9 epoch1:node_model train_loss:0.420116198690314,train_acc:0.8514313101768494
node9 epoch2:node_model train_loss:0.359426601152671,train_acc:0.879852294921875
node9 epoch3:node_model train_loss:0.2692013186843772,train_acc:0.9192058444023132
node9 epoch4:node_model train_loss:0.21608165926054904,train_acc:0.9323638081550598
node9_model on test-dataset: loss:0.8581297651678323,acc:0.734000027179718
node9 weight score:2164.0083765615677
node10: train data size:1975
node10 epoch0:node_model train_loss:0.730686204135418,train_acc:0.768500030040741
node10 epoch1:node_model train_loss:0.4906740069389343,train_acc:0.8346667289733887
node10 epoch2:node_model train_loss:0.3759359136223793,train_acc:0.8776666522026062
node10 epoch3:node_model train_loss:0.32281485199928284,train_acc:0.8999999165534973
node10 epoch4:node_model train_loss:0.2667726367712021,train_acc:0.9226667284965515
node10_model on test-dataset: loss:0.8796172158420086,acc:0.7291000485420227
node10 weight score:2245.294844655175
node16: train data size:877
node16 epoch0:node_model train_loss:0.8404013713200887,train_acc:0.7452524900436401
node16 epoch1:node_model train_loss:0.5084231297175089,train_acc:0.8415728807449341
node16 epoch2:node_model train_loss:0.37733839948972064,train_acc:0.8768975138664246
node16 epoch3:node_model train_loss:0.2686563713683022,train_acc:0.9171284437179565
node16 epoch4:node_model train_loss:0.18823745681179893,train_acc:0.9575613141059875
node16_model on test-dataset: loss:0.9622486756742,acc:0.6989001035690308
node16 weight score:911.4068142370053
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6393730634450913,acc:0.7908999821543694
total cost energy:6.849016480713451 | all_enery_cp：5.210500000000001 | all_enery_tp: 1.6385164807134505
ef: 24.785879019784687
reward: 17.936862539071235
step 71:loss:449.09039306640625|running q:7.081669807434082
episode1,iteration11 selected nodes:[5, 13, 2, 3, 9],center node:9
################################################## episode1,iteration11 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.7276363881925741,train_acc:0.7570454478263855
node2 epoch1:node_model train_loss:0.5500948373228312,train_acc:0.8098201751708984
node2 epoch2:node_model train_loss:0.478499058013161,train_acc:0.8290152549743652
node2 epoch3:node_model train_loss:0.3909271502246459,train_acc:0.8673393130302429
node2 epoch4:node_model train_loss:0.35648831849296886,train_acc:0.878617525100708
node2_model on test-dataset: loss:0.7801587305963039,acc:0.7497999668121338
node2 weight score:6137.212611003348
node3: train data size:4247
node3 epoch0:node_model train_loss:0.506736274375472,train_acc:0.8215339183807373
node3 epoch1:node_model train_loss:0.3682634608690129,train_acc:0.8719743490219116
node3 epoch2:node_model train_loss:0.3198316298251928,train_acc:0.8907223343849182
node3 epoch3:node_model train_loss:0.26478224646213444,train_acc:0.9106085300445557
node3 epoch4:node_model train_loss:0.2007115722395653,train_acc:0.941771388053894
node3_model on test-dataset: loss:0.8796151521801948,acc:0.7399998903274536
node3 weight score:4828.247887128227
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7705376948180952,train_acc:0.7484209537506104
node5 epoch1:node_model train_loss:0.5520611883778321,train_acc:0.8112781643867493
node5 epoch2:node_model train_loss:0.4576770820115742,train_acc:0.8448870778083801
node5 epoch3:node_model train_loss:0.41167210669893967,train_acc:0.8613908886909485
node5 epoch4:node_model train_loss:0.3558442282833551,train_acc:0.8802255392074585
node5_model on test-dataset: loss:0.7823093625903129,acc:0.7394999265670776
node5 weight score:4774.326089659724
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5602294394844457,train_acc:0.8069530129432678
node9 epoch1:node_model train_loss:0.37415933138445806,train_acc:0.8695844411849976
node9 epoch2:node_model train_loss:0.27137374093658045,train_acc:0.9143306612968445
node9 epoch3:node_model train_loss:0.21969110479480342,train_acc:0.9390766620635986
node9 epoch4:node_model train_loss:0.17758449912071228,train_acc:0.947617769241333
node9_model on test-dataset: loss:0.8790423932671547,acc:0.7327000498771667
node9 weight score:2112.5261013840873
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8566524088382721,train_acc:0.7225757837295532
node13 epoch1:node_model train_loss:0.49330103149016696,train_acc:0.8188636302947998
node13 epoch2:node_model train_loss:0.3876095339655876,train_acc:0.8828787207603455
node13 epoch3:node_model train_loss:0.2862437404692173,train_acc:0.9200758337974548
node13 epoch4:node_model train_loss:0.21778246263662973,train_acc:0.9384848475456238
node13_model on test-dataset: loss:0.8523383036255836,acc:0.7264999151229858
node13 weight score:1355.0957349763435
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6188522289693356,acc:0.7961999806761741
total cost energy:9.572674094652609 | all_enery_cp：7.891 | all_enery_tp: 1.6816740946526076
ef: 25.05977134853305
reward: 15.487097253880442
step 72:loss:435.9090270996094|running q:8.53438949584961
episode1,iteration12 selected nodes:[13, 1, 3, 0, 11],center node:11
################################################## episode1,iteration12 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.622203630896715,train_acc:0.7927919030189514
node0 epoch1:node_model train_loss:0.4458593138708518,train_acc:0.848871648311615
node0 epoch2:node_model train_loss:0.3865082837068118,train_acc:0.8631418347358704
node0 epoch3:node_model train_loss:0.32078005774663043,train_acc:0.8963762521743774
node0 epoch4:node_model train_loss:0.3048677616394483,train_acc:0.8956856727600098
node0_model on test-dataset: loss:0.8588244168460369,acc:0.74180006980896
node0 weight score:6034.993763957186
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5993787523578195,train_acc:0.7982353568077087
node1 epoch1:node_model train_loss:0.505849964259302,train_acc:0.8216912746429443
node1 epoch2:node_model train_loss:0.423413023352623,train_acc:0.8561027646064758
node1 epoch3:node_model train_loss:0.4100634109447984,train_acc:0.859779417514801
node1 epoch4:node_model train_loss:0.3717745185336646,train_acc:0.8724263906478882
node1_model on test-dataset: loss:0.8067104242742061,acc:0.7407998442649841
node1 weight score:8315.251418791519
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4846384906491568,train_acc:0.830024778842926
node3 epoch1:node_model train_loss:0.32163101157476737,train_acc:0.8867985606193542
node3 epoch2:node_model train_loss:0.2357940770858942,train_acc:0.9247945547103882
node3 epoch3:node_model train_loss:0.21129521692908088,train_acc:0.9308410286903381
node3 epoch4:node_model train_loss:0.1825019549145255,train_acc:0.95087069272995
node3_model on test-dataset: loss:0.866783876568079,acc:0.7487000226974487
node3 weight score:4899.721966236219
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7465876831727869,train_acc:0.7553945779800415
node11 epoch1:node_model train_loss:0.5133730032864738,train_acc:0.822109043598175
node11 epoch2:node_model train_loss:0.351977717350511,train_acc:0.8864849209785461
node11 epoch3:node_model train_loss:0.2562297188183841,train_acc:0.920731782913208
node11 epoch4:node_model train_loss:0.18722674119121888,train_acc:0.9509899020195007
node11_model on test-dataset: loss:0.817381277680397,acc:0.7437999844551086
node11 weight score:2057.791199687444
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8289415886004766,train_acc:0.7264394164085388
node13 epoch1:node_model train_loss:0.4692501996954282,train_acc:0.8444696664810181
node13 epoch2:node_model train_loss:0.367424209912618,train_acc:0.8653787970542908
node13 epoch3:node_model train_loss:0.2654941827058792,train_acc:0.9184090495109558
node13 epoch4:node_model train_loss:0.2123445359369119,train_acc:0.9412878751754761
node13_model on test-dataset: loss:0.837929583787918,acc:0.7371001243591309
node13 weight score:1378.3974481229598
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6358539059758186,acc:0.7940999794006348
total cost energy:11.423642503854914 | all_enery_cp：9.487499999999999 | all_enery_tp: 1.9361425038549158
ef: 25.056887771719644
reward: 13.63324526786473
step 73:loss:483.8126525878906|running q:9.631132125854492
episode1,iteration13 selected nodes:[3, 12, 18, 0, 13],center node:3
################################################## episode1,iteration13 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5456803400929158,train_acc:0.8142956495285034
node0 epoch1:node_model train_loss:0.3862640757400256,train_acc:0.8628013134002686
node0 epoch2:node_model train_loss:0.30977900670124936,train_acc:0.8925694227218628
node0 epoch3:node_model train_loss:0.26326474948571277,train_acc:0.9138022065162659
node0 epoch4:node_model train_loss:0.23272919396941477,train_acc:0.924221396446228
node0_model on test-dataset: loss:0.8786521114781499,acc:0.737299919128418
node0 weight score:5898.807881176861
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4345930218696594,train_acc:0.846828281879425
node3 epoch1:node_model train_loss:0.2757195691729701,train_acc:0.908193826675415
node3 epoch2:node_model train_loss:0.22072240467681442,train_acc:0.9292131662368774
node3 epoch3:node_model train_loss:0.18546361025682714,train_acc:0.9441561102867126
node3 epoch4:node_model train_loss:0.1873414114464161,train_acc:0.9419742822647095
node3_model on test-dataset: loss:0.886532552242279,acc:0.7452999949455261
node3 weight score:4790.574231322015
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7371046755995069,train_acc:0.7755555510520935
node12 epoch1:node_model train_loss:0.4831209416900362,train_acc:0.8363492488861084
node12 epoch2:node_model train_loss:0.33549084620816366,train_acc:0.8934921026229858
node12 epoch3:node_model train_loss:0.23438147881201335,train_acc:0.9236508011817932
node12 epoch4:node_model train_loss:0.23063528112002782,train_acc:0.9313493371009827
node12_model on test-dataset: loss:0.8792960745096207,acc:0.7300001382827759
node12 weight score:1519.397207300261
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8457826673984528,train_acc:0.7508333921432495
node13 epoch1:node_model train_loss:0.461073562502861,train_acc:0.8471969962120056
node13 epoch2:node_model train_loss:0.3307787502805392,train_acc:0.8928787708282471
node13 epoch3:node_model train_loss:0.2625116929411888,train_acc:0.9200758337974548
node13 epoch4:node_model train_loss:0.18158511320749918,train_acc:0.956969678401947
node13_model on test-dataset: loss:0.82270576775074,acc:0.7451000213623047
node13 weight score:1403.9040994665022
node18: train data size:472
node18 epoch0:node_model train_loss:0.8068096518516541,train_acc:0.75
node18 epoch1:node_model train_loss:0.4081478238105774,train_acc:0.8685555458068848
node18 epoch2:node_model train_loss:0.2684922605752945,train_acc:0.9101110696792603
node18 epoch3:node_model train_loss:0.20461713671684265,train_acc:0.944444477558136
node18 epoch4:node_model train_loss:0.1349268898367882,train_acc:0.9692221879959106
node18_model on test-dataset: loss:0.9415748603641987,acc:0.7211998105049133
node18 weight score:501.28781031540245
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.662561471760273,acc:0.7925999829173088
total cost energy:8.650994767471039 | all_enery_cp：6.1964999999999995 | all_enery_tp: 2.454494767471039
ef: 24.315995223351116
reward: 15.665000455880078
step 74:loss:647.1646118164062|running q:10.506153106689453
episode1,iteration14 selected nodes:[8, 6, 16, 13, 19],center node:16
################################################## episode1,iteration14 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6474588224964757,train_acc:0.7908755540847778
node6 epoch1:node_model train_loss:0.5036876365061729,train_acc:0.8312442302703857
node6 epoch2:node_model train_loss:0.4685938048266595,train_acc:0.8396773338317871
node6 epoch3:node_model train_loss:0.3064640308580091,train_acc:0.8983870148658752
node6 epoch4:node_model train_loss:0.2696524296076067,train_acc:0.9147465825080872
node6_model on test-dataset: loss:0.771176493242383,acc:0.7572999596595764
node6 weight score:3899.2371089491844
node8: train data size:1798
node8 epoch0:node_model train_loss:0.8584204879071977,train_acc:0.7280272841453552
node8 epoch1:node_model train_loss:0.5306939267449908,train_acc:0.8286282420158386
node8 epoch2:node_model train_loss:0.3567321201165517,train_acc:0.8842970728874207
node8 epoch3:node_model train_loss:0.26897315432627994,train_acc:0.9142969846725464
node8 epoch4:node_model train_loss:0.20246900618076324,train_acc:0.9438435435295105
node8_model on test-dataset: loss:0.8055587933957576,acc:0.7486998438835144
node8 weight score:2231.9910287624066
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7824729159474373,train_acc:0.7561363577842712
node13 epoch1:node_model train_loss:0.47198161234458286,train_acc:0.8368182182312012
node13 epoch2:node_model train_loss:0.3183048702776432,train_acc:0.8845454454421997
node13 epoch3:node_model train_loss:0.2163886638979117,train_acc:0.9299242496490479
node13 epoch4:node_model train_loss:0.15763980274399123,train_acc:0.9659847617149353
node13_model on test-dataset: loss:0.8319090765714645,acc:0.7482998967170715
node13 weight score:1388.3728793536977
node16: train data size:877
node16 epoch0:node_model train_loss:0.8676777415805392,train_acc:0.7528138756752014
node16 epoch1:node_model train_loss:0.49387142724461025,train_acc:0.8325685262680054
node16 epoch2:node_model train_loss:0.34588127334912616,train_acc:0.8884559869766235
node16 epoch3:node_model train_loss:0.22107012901041243,train_acc:0.9331169128417969
node16 epoch4:node_model train_loss:0.14224402606487274,train_acc:0.9604473114013672
node16_model on test-dataset: loss:0.8261176572740078,acc:0.7464000582695007
node16 weight score:1061.5921258648457
node19: train data size:4281
node19 epoch0:node_model train_loss:0.7599786221981049,train_acc:0.7511111497879028
node19 epoch1:node_model train_loss:0.48941164238508356,train_acc:0.8325609564781189
node19 epoch2:node_model train_loss:0.4207984153614488,train_acc:0.8577173948287964
node19 epoch3:node_model train_loss:0.33763579954934675,train_acc:0.8868963718414307
node19 epoch4:node_model train_loss:0.28165258328581966,train_acc:0.906827449798584
node19_model on test-dataset: loss:0.7462974248826504,acc:0.767300009727478
node19 weight score:5736.318868677799
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.604207618534565,acc:0.8056999796628952
total cost energy:7.396010772218997 | all_enery_cp：5.558999999999999 | all_enery_tp: 1.8370107722189972
ef: 25.01052020951849
reward: 17.614509437299493
step 75:loss:340.51177978515625|running q:11.819607734680176
episode1,iteration15 selected nodes:[3, 11, 13, 17, 14],center node:11
################################################## episode1,iteration15 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3824933715337931,train_acc:0.8617119789123535
node3 epoch1:node_model train_loss:0.25261659573676976,train_acc:0.9161304831504822
node3 epoch2:node_model train_loss:0.20502897854461227,train_acc:0.9333695769309998
node3 epoch3:node_model train_loss:0.1848896833699803,train_acc:0.9397079944610596
node3 epoch4:node_model train_loss:0.13168513237736945,train_acc:0.9619741439819336
node3_model on test-dataset: loss:0.8880433468520641,acc:0.7450003027915955
node3 weight score:4782.4242082942965
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7577261994866764,train_acc:0.7664991617202759
node11 epoch1:node_model train_loss:0.4278390249785255,train_acc:0.8567432165145874
node11 epoch2:node_model train_loss:0.2993583933395498,train_acc:0.9026971459388733
node11 epoch3:node_model train_loss:0.2199680366936852,train_acc:0.9371305108070374
node11 epoch4:node_model train_loss:0.17270182379904916,train_acc:0.9564704895019531
node11_model on test-dataset: loss:0.8055518744885921,acc:0.7584999799728394
node11 weight score:2088.0095413691697
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7137443472941717,train_acc:0.7608333826065063
node13 epoch1:node_model train_loss:0.40025872985521954,train_acc:0.854621171951294
node13 epoch2:node_model train_loss:0.276432861884435,train_acc:0.9096969366073608
node13 epoch3:node_model train_loss:0.20746713131666183,train_acc:0.9379545450210571
node13 epoch4:node_model train_loss:0.1304467748850584,train_acc:0.9711363315582275
node13_model on test-dataset: loss:0.834451186656952,acc:0.7387998104095459
node13 weight score:1384.143277004923
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6616742710272471,train_acc:0.7741204500198364
node14 epoch1:node_model train_loss:0.35832344740629196,train_acc:0.8710648417472839
node14 epoch2:node_model train_loss:0.2990492495397727,train_acc:0.8949074745178223
node14 epoch3:node_model train_loss:0.199637354662021,train_acc:0.942361056804657
node14 epoch4:node_model train_loss:0.14480652660131454,train_acc:0.9610185027122498
node14_model on test-dataset: loss:0.828788408190012,acc:0.7494999766349792
node14 weight score:1414.112442233026
node17: train data size:442
node17 epoch0:node_model train_loss:0.8926212072372437,train_acc:0.7258095145225525
node17 epoch1:node_model train_loss:0.5689587235450745,train_acc:0.8066666722297668
node17 epoch2:node_model train_loss:0.38828179240226746,train_acc:0.8619047403335571
node17 epoch3:node_model train_loss:0.25444155037403104,train_acc:0.9221904873847961
node17 epoch4:node_model train_loss:0.15543899238109588,train_acc:0.9409524202346802
node17_model on test-dataset: loss:0.9273116719722748,acc:0.7216998934745789
node17 weight score:476.6466478955472
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.645986275523901,acc:0.8014999800920486
total cost energy:5.780012206452077 | all_enery_cp：4.349 | all_enery_tp: 1.4310122064520765
ef: 24.65505251212938
reward: 18.875040305677302
step 76:loss:249.31881713867188|running q:13.046295166015625
episode1,iteration16 selected nodes:[4, 13, 7, 5, 11],center node:7
################################################## episode1,iteration16 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6960353925824165,train_acc:0.7724999189376831
node4 epoch1:node_model train_loss:0.5738158289875303,train_acc:0.8096429705619812
node4 epoch2:node_model train_loss:0.49995545244642664,train_acc:0.8342856764793396
node4 epoch3:node_model train_loss:0.30284457919853075,train_acc:0.8953571319580078
node4 epoch4:node_model train_loss:0.287282543522971,train_acc:0.9050000309944153
node4_model on test-dataset: loss:0.9394334036111832,acc:0.729699969291687
node4 weight score:2879.3951647897306
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6838852825917696,train_acc:0.7876692414283752
node5 epoch1:node_model train_loss:0.47353045328667287,train_acc:0.838346004486084
node5 epoch2:node_model train_loss:0.37239695771744374,train_acc:0.8669549226760864
node5 epoch3:node_model train_loss:0.3166078497704707,train_acc:0.898496150970459
node5 epoch4:node_model train_loss:0.2712973124886814,train_acc:0.9007893800735474
node5_model on test-dataset: loss:0.7751468746364116,acc:0.7608998417854309
node5 weight score:4818.441668557239
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7687861308455467,train_acc:0.7543333768844604
node7 epoch1:node_model train_loss:0.4555443704128265,train_acc:0.8560979962348938
node7 epoch2:node_model train_loss:0.3306525357067585,train_acc:0.8895784616470337
node7 epoch3:node_model train_loss:0.276877386868,train_acc:0.9126176238059998
node7 epoch4:node_model train_loss:0.1881098449230194,train_acc:0.9460195899009705
node7_model on test-dataset: loss:0.7739368039369583,acc:0.7651000022888184
node7 weight score:2520.8776609090173
node11: train data size:1682
node11 epoch0:node_model train_loss:0.600464733207927,train_acc:0.8004160523414612
node11 epoch1:node_model train_loss:0.3579902683987337,train_acc:0.8892252445220947
node11 epoch2:node_model train_loss:0.23398801684379578,train_acc:0.9233428835868835
node11 epoch3:node_model train_loss:0.1779210874262978,train_acc:0.9530128240585327
node11 epoch4:node_model train_loss:0.1369866299278596,train_acc:0.9628120064735413
node11_model on test-dataset: loss:0.7980155700445175,acc:0.7581998109817505
node11 weight score:2107.728298968113
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6657741715510687,train_acc:0.7883333563804626
node13 epoch1:node_model train_loss:0.38302888721227646,train_acc:0.8618940114974976
node13 epoch2:node_model train_loss:0.2408288506170114,train_acc:0.9176515340805054
node13 epoch3:node_model train_loss:0.16711116023361683,train_acc:0.9528030157089233
node13 epoch4:node_model train_loss:0.13268881229062876,train_acc:0.9681060314178467
node13_model on test-dataset: loss:0.7983128196001053,acc:0.7549999356269836
node13 weight score:1446.8012684282937
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6101834739744664,acc:0.8067999771237373
total cost energy:7.113322955501368 | all_enery_cp：5.614 | all_enery_tp: 1.499322955501368
ef: 25.044022651055787
reward: 17.93069969555442
step 77:loss:441.0743103027344|running q:14.18065071105957
episode1,iteration17 selected nodes:[15, 8, 18, 13, 5],center node:15
################################################## episode1,iteration17 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5278465191000387,train_acc:0.8196994066238403
node5 epoch1:node_model train_loss:0.3579352054941027,train_acc:0.8775188326835632
node5 epoch2:node_model train_loss:0.28043439670612935,train_acc:0.9051503539085388
node5 epoch3:node_model train_loss:0.21837629064133293,train_acc:0.9301127195358276
node5 epoch4:node_model train_loss:0.1932623374618982,train_acc:0.9419170022010803
node5_model on test-dataset: loss:0.7393454110622406,acc:0.7750997543334961
node5 weight score:5051.7659866635395
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7680742426051034,train_acc:0.7597618699073792
node8 epoch1:node_model train_loss:0.49324841962920296,train_acc:0.8270521759986877
node8 epoch2:node_model train_loss:0.3237041135629018,train_acc:0.8943536877632141
node8 epoch3:node_model train_loss:0.24133992360697853,train_acc:0.9265985488891602
node8 epoch4:node_model train_loss:0.18248478944102922,train_acc:0.9477097392082214
node8_model on test-dataset: loss:0.780798367857933,acc:0.7558000683784485
node8 weight score:2302.7712070309394
node13: train data size:1155
node13 epoch0:node_model train_loss:0.637469100455443,train_acc:0.7804545760154724
node13 epoch1:node_model train_loss:0.37761881699164707,train_acc:0.8754545450210571
node13 epoch2:node_model train_loss:0.22737367699543634,train_acc:0.9234848022460938
node13 epoch3:node_model train_loss:0.15635579017301401,train_acc:0.9559848308563232
node13 epoch4:node_model train_loss:0.12286107676724593,train_acc:0.9697727560997009
node13_model on test-dataset: loss:0.8177588585019112,acc:0.7531000971794128
node13 weight score:1412.3968062124034
node15: train data size:629
node15 epoch0:node_model train_loss:0.8144241401127407,train_acc:0.7372413873672485
node15 epoch1:node_model train_loss:0.5251514102731433,train_acc:0.8167979717254639
node15 epoch2:node_model train_loss:0.31191545937742504,train_acc:0.9007881879806519
node15 epoch3:node_model train_loss:0.22494124940463475,train_acc:0.9252216815948486
node15 epoch4:node_model train_loss:0.16406766112361634,train_acc:0.9409359097480774
node15_model on test-dataset: loss:0.8675654312968254,acc:0.7422000169754028
node15 weight score:725.0173615836435
node18: train data size:472
node18 epoch0:node_model train_loss:0.7740792155265808,train_acc:0.7678889036178589
node18 epoch1:node_model train_loss:0.44680984020233155,train_acc:0.8458889126777649
node18 epoch2:node_model train_loss:0.30291964709758756,train_acc:0.891333281993866
node18 epoch3:node_model train_loss:0.16565621346235276,train_acc:0.9448888897895813
node18 epoch4:node_model train_loss:0.115911103785038,train_acc:0.9668889045715332
node18_model on test-dataset: loss:0.8806308266520501,acc:0.744900107383728
node18 weight score:535.9794203371603
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6281452520191669,acc:0.8032999831438065
total cost energy:6.359852558934949 | all_enery_cp：3.8945 | all_enery_tp: 2.465352558934949
ef: 24.633116749243477
reward: 18.273264190308527
step 78:loss:623.7073364257812|running q:15.058544158935547
episode1,iteration18 selected nodes:[10, 7, 2, 1, 0],center node:1
################################################## episode1,iteration18 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.524179593874858,train_acc:0.827372670173645
node0 epoch1:node_model train_loss:0.33097176912885445,train_acc:0.8872983455657959
node0 epoch2:node_model train_loss:0.2541829247314196,train_acc:0.9132643938064575
node0 epoch3:node_model train_loss:0.22626315286526313,train_acc:0.9266079068183899
node0 epoch4:node_model train_loss:0.20408729650080204,train_acc:0.9378799200057983
node0_model on test-dataset: loss:0.774451295286417,acc:0.7633997201919556
node0 weight score:6692.480252206383
node1: train data size:6708
node1 epoch0:node_model train_loss:0.562452119501198,train_acc:0.8082354068756104
node1 epoch1:node_model train_loss:0.43168260310502615,train_acc:0.8552939295768738
node1 epoch2:node_model train_loss:0.3742870252360316,train_acc:0.8726469278335571
node1 epoch3:node_model train_loss:0.33829733252744465,train_acc:0.8857352137565613
node1 epoch4:node_model train_loss:0.24617757626316128,train_acc:0.9172057509422302
node1_model on test-dataset: loss:0.9022077753394843,acc:0.7356001138687134
node1 weight score:7435.094424314734
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6655333004891872,train_acc:0.7864962816238403
node2 epoch1:node_model train_loss:0.42914558822909993,train_acc:0.8510322570800781
node2 epoch2:node_model train_loss:0.3510723349948724,train_acc:0.8867708444595337
node2 epoch3:node_model train_loss:0.30330021151651937,train_acc:0.8972159624099731
node2 epoch4:node_model train_loss:0.2731257400785883,train_acc:0.9029828310012817
node2_model on test-dataset: loss:0.8354147589206695,acc:0.749099850654602
node2 weight score:5731.284908332181
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6476257994771004,train_acc:0.7881960272789001
node7 epoch1:node_model train_loss:0.36515375673770906,train_acc:0.8790980577468872
node7 epoch2:node_model train_loss:0.2675251945853233,train_acc:0.9230390787124634
node7 epoch3:node_model train_loss:0.20492555871605872,train_acc:0.9365783929824829
node7 epoch4:node_model train_loss:0.16722185760736466,train_acc:0.9530588984489441
node7_model on test-dataset: loss:0.7580315108597279,acc:0.7728999853134155
node7 weight score:2573.771633566073
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7121986538171768,train_acc:0.778166651725769
node10 epoch1:node_model train_loss:0.45079026371240616,train_acc:0.8501666188240051
node10 epoch2:node_model train_loss:0.324394154548645,train_acc:0.9035000205039978
node10 epoch3:node_model train_loss:0.23592085018754005,train_acc:0.9293332099914551
node10 epoch4:node_model train_loss:0.19117179661989211,train_acc:0.951166570186615
node10_model on test-dataset: loss:0.7869479241967201,acc:0.7529000043869019
node10 weight score:2509.695926850545
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5968792797625064,acc:0.8096999818086624
total cost energy:11.575296877033646 | all_enery_cp：10.3025 | all_enery_tp: 1.2727968770336455
ef: 25.218445454372088
reward: 13.643148577338442
step 79:loss:342.9830017089844|running q:16.202774047851562
episode1,iteration19 selected nodes:[18, 1, 14, 11, 0],center node:11
################################################## episode1,iteration19 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.40013061234584224,train_acc:0.8559871315956116
node0 epoch1:node_model train_loss:0.2763970371049184,train_acc:0.9064897894859314
node0 epoch2:node_model train_loss:0.20979458666764772,train_acc:0.9340338110923767
node0 epoch3:node_model train_loss:0.18530632856373602,train_acc:0.9380720853805542
node0 epoch4:node_model train_loss:0.14926071665607965,train_acc:0.955305814743042
node0_model on test-dataset: loss:0.8231243898719549,acc:0.7603000998497009
node0 weight score:6296.739671152578
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4388055054142195,train_acc:0.8519117832183838
node1 epoch1:node_model train_loss:0.31299445274121623,train_acc:0.8938971161842346
node1 epoch2:node_model train_loss:0.3124718865489258,train_acc:0.8909556865692139
node1 epoch3:node_model train_loss:0.2336660575340776,train_acc:0.922058641910553
node1 epoch4:node_model train_loss:0.20689615212819157,train_acc:0.9318382143974304
node1_model on test-dataset: loss:0.9092593143880368,acc:0.7441000938415527
node1 weight score:7377.4333612570335
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5887535179362577,train_acc:0.8147918581962585
node11 epoch1:node_model train_loss:0.31564029350000267,train_acc:0.896427571773529
node11 epoch2:node_model train_loss:0.2261506538180744,train_acc:0.93740314245224
node11 epoch3:node_model train_loss:0.17563293874263763,train_acc:0.9534720182418823
node11 epoch4:node_model train_loss:0.11593763793216032,train_acc:0.9731419682502747
node11_model on test-dataset: loss:0.7861875367164611,acc:0.7727001309394836
node11 weight score:2139.4386472023325
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5992037231723467,train_acc:0.8124537467956543
node14 epoch1:node_model train_loss:0.3351445322235425,train_acc:0.8940741419792175
node14 epoch2:node_model train_loss:0.23178469017148018,train_acc:0.929722249507904
node14 epoch3:node_model train_loss:0.15368392132222652,train_acc:0.9610185027122498
node14 epoch4:node_model train_loss:0.11722718738019466,train_acc:0.9696757793426514
node14_model on test-dataset: loss:0.7471805657446384,acc:0.7744000554084778
node14 weight score:1568.5632813963618
node18: train data size:472
node18 epoch0:node_model train_loss:0.6684246063232422,train_acc:0.795555591583252
node18 epoch1:node_model train_loss:0.37890263795852663,train_acc:0.8689999580383301
node18 epoch2:node_model train_loss:0.2092703551054001,train_acc:0.9321112036705017
node18 epoch3:node_model train_loss:0.11783586889505386,train_acc:0.9664444327354431
node18 epoch4:node_model train_loss:0.10245254337787628,train_acc:0.9832221865653992
node18_model on test-dataset: loss:0.8888655605912209,acc:0.7476000189781189
node18 weight score:531.0139361075633
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6474918007105589,acc:0.7987999820709228
total cost energy:9.439870849898476 | all_enery_cp：7.6085 | all_enery_tp: 1.8313708498984762
ef: 24.885967588021256
reward: 15.44609673812278
step 80:loss:242.9561309814453|running q:17.287363052368164
episode1,iteration20 selected nodes:[11, 19, 13, 10, 16],center node:16
################################################## episode1,iteration20 ##################################################
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6822474598884583,train_acc:0.7858332991600037
node10 epoch1:node_model train_loss:0.4416004478931427,train_acc:0.8618332743644714
node10 epoch2:node_model train_loss:0.3004918061196804,train_acc:0.909000039100647
node10 epoch3:node_model train_loss:0.21530879214406012,train_acc:0.9401666522026062
node10 epoch4:node_model train_loss:0.1536591097712517,train_acc:0.9568333029747009
node10_model on test-dataset: loss:0.7447695679962635,acc:0.7699999213218689
node10 weight score:2651.8269339516146
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5562954282059389,train_acc:0.8209325671195984
node11 epoch1:node_model train_loss:0.33777768471661734,train_acc:0.8856958746910095
node11 epoch2:node_model train_loss:0.21981640686007106,train_acc:0.9277904629707336
node11 epoch3:node_model train_loss:0.1711583510041237,train_acc:0.9524245858192444
node11 epoch4:node_model train_loss:0.13157273697502472,train_acc:0.9647058844566345
node11_model on test-dataset: loss:0.7953138896822929,acc:0.7714999318122864
node11 weight score:2114.888249558819
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6834742228190104,train_acc:0.7969697713851929
node13 epoch1:node_model train_loss:0.32316885764400166,train_acc:0.8835605978965759
node13 epoch2:node_model train_loss:0.23679034660259882,train_acc:0.9243940114974976
node13 epoch3:node_model train_loss:0.1651556808501482,train_acc:0.9569696187973022
node13 epoch4:node_model train_loss:0.11323150371511777,train_acc:0.9768182039260864
node13_model on test-dataset: loss:0.8179658818244934,acc:0.7548001408576965
node13 weight score:1412.039335214989
node16: train data size:877
node16 epoch0:node_model train_loss:0.767181932926178,train_acc:0.7628138065338135
node16 epoch1:node_model train_loss:0.4630019896560245,train_acc:0.8423520922660828
node16 epoch2:node_model train_loss:0.25552457405461204,train_acc:0.9160173535346985
node16 epoch3:node_model train_loss:0.1734905739625295,train_acc:0.9531168341636658
node16 epoch4:node_model train_loss:0.11766648540894191,train_acc:0.9707792401313782
node16_model on test-dataset: loss:0.7581623968482017,acc:0.76910001039505
node16 weight score:1156.7442590740777
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6220585433549659,train_acc:0.7984293699264526
node19 epoch1:node_model train_loss:0.42657791043436805,train_acc:0.8531897068023682
node19 epoch2:node_model train_loss:0.31036493459413217,train_acc:0.8931065201759338
node19 epoch3:node_model train_loss:0.2348960780127104,train_acc:0.9231065511703491
node19 epoch4:node_model train_loss:0.20761299791724183,train_acc:0.9398506283760071
node19_model on test-dataset: loss:0.8448686262965203,acc:0.7551000118255615
node19 weight score:5067.059974478818
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6172658516466618,acc:0.8077999776601792
total cost energy:6.419261754766733 | all_enery_cp：4.985 | all_enery_tp: 1.434261754766733
ef: 25.077145500591776
reward: 18.657883745825043
step 81:loss:327.5687255859375|running q:18.38546371459961
episode1,iteration21 selected nodes:[2, 13, 15, 6, 8],center node:6
################################################## episode1,iteration21 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.587640052040418,train_acc:0.8094507455825806
node2 epoch1:node_model train_loss:0.39784000845005113,train_acc:0.866032063961029
node2 epoch2:node_model train_loss:0.294868316501379,train_acc:0.8965623378753662
node2 epoch3:node_model train_loss:0.2345523675903678,train_acc:0.9198011159896851
node2 epoch4:node_model train_loss:0.1985505923318366,train_acc:0.9356058835983276
node2_model on test-dataset: loss:0.7267330081760883,acc:0.7815999388694763
node2 weight score:6588.389334367294
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6472103105437371,train_acc:0.7943317294120789
node6 epoch1:node_model train_loss:0.5095280101222377,train_acc:0.8302764296531677
node6 epoch2:node_model train_loss:0.47537993663741696,train_acc:0.8473272323608398
node6 epoch3:node_model train_loss:0.3148524147127905,train_acc:0.8964515328407288
node6 epoch4:node_model train_loss:0.2240179382985638,train_acc:0.9295852184295654
node6_model on test-dataset: loss:0.7730501388013363,acc:0.7666000127792358
node6 weight score:3889.7865081074115
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6583457572592629,train_acc:0.7836281061172485
node8 epoch1:node_model train_loss:0.37796825336085427,train_acc:0.8738095164299011
node8 epoch2:node_model train_loss:0.2618712642126613,train_acc:0.9143310189247131
node8 epoch3:node_model train_loss:0.19002516071001688,train_acc:0.9443650841712952
node8 epoch4:node_model train_loss:0.13251693877908918,train_acc:0.9671542048454285
node8_model on test-dataset: loss:0.7305034916847944,acc:0.7752999663352966
node8 weight score:2461.3160928953102
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6170434231559435,train_acc:0.7996212840080261
node13 epoch1:node_model train_loss:0.32978638137380284,train_acc:0.8874242305755615
node13 epoch2:node_model train_loss:0.21459118773539862,train_acc:0.9343181848526001
node13 epoch3:node_model train_loss:0.15809151033560434,train_acc:0.9596211314201355
node13 epoch4:node_model train_loss:0.12163216869036357,train_acc:0.9718181490898132
node13_model on test-dataset: loss:0.792056354880333,acc:0.7678002119064331
node13 weight score:1458.2295727865248
node15: train data size:629
node15 epoch0:node_model train_loss:0.7636949505124774,train_acc:0.7818719744682312
node15 epoch1:node_model train_loss:0.46818367498261587,train_acc:0.8566502332687378
node15 epoch2:node_model train_loss:0.2804397089140756,train_acc:0.9222166538238525
node15 epoch3:node_model train_loss:0.18835468696696417,train_acc:0.9536452889442444
node15 epoch4:node_model train_loss:0.13304888776370458,train_acc:0.9630048871040344
node15_model on test-dataset: loss:0.8263497291505337,acc:0.7574999332427979
node15 weight score:761.1789268045092
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.586003760099411,acc:0.8118999803066254
total cost energy:7.812200589049821 | all_enery_cp：5.6885 | all_enery_tp: 2.1237005890498204
ef: 24.779267910275752
reward: 16.967067321225933
step 82:loss:407.4884033203125|running q:19.149024963378906
episode1,iteration22 selected nodes:[1, 19, 5, 4, 2],center node:2
################################################## episode1,iteration22 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.38820244218496713,train_acc:0.8658087849617004
node1 epoch1:node_model train_loss:0.3006099710131393,train_acc:0.8950735330581665
node1 epoch2:node_model train_loss:0.21521991962457404,train_acc:0.9285293221473694
node1 epoch3:node_model train_loss:0.1771007710519959,train_acc:0.9422059059143066
node1 epoch4:node_model train_loss:0.22350801341235638,train_acc:0.9224998950958252
node1_model on test-dataset: loss:0.8447995460778475,acc:0.7615000605583191
node1 weight score:7940.34517554282
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3754872189213832,train_acc:0.8665624856948853
node2 epoch1:node_model train_loss:0.24501478796203932,train_acc:0.9166759848594666
node2 epoch2:node_model train_loss:0.21988023848583302,train_acc:0.9284943342208862
node2 epoch3:node_model train_loss:0.16354506447290382,train_acc:0.9494129419326782
node2 epoch4:node_model train_loss:0.1348006116847197,train_acc:0.9611648917198181
node2_model on test-dataset: loss:0.77467598721385,acc:0.7700000405311584
node2 weight score:6180.648527935162
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6216708549431392,train_acc:0.7924999594688416
node4 epoch1:node_model train_loss:0.42520122123616083,train_acc:0.8560713529586792
node4 epoch2:node_model train_loss:0.32362719997763634,train_acc:0.8907142877578735
node4 epoch3:node_model train_loss:0.24560385090964182,train_acc:0.923214316368103
node4 epoch4:node_model train_loss:0.19741272128054074,train_acc:0.9335713982582092
node4_model on test-dataset: loss:0.7413745248317718,acc:0.7728997468948364
node4 weight score:3648.6282026129807
node5: train data size:3735
node5 epoch0:node_model train_loss:0.49814313806985555,train_acc:0.8278571367263794
node5 epoch1:node_model train_loss:0.3138007662798229,train_acc:0.8949624300003052
node5 epoch2:node_model train_loss:0.2295011317259387,train_acc:0.9254887104034424
node5 epoch3:node_model train_loss:0.1852642464402475,train_acc:0.9424434304237366
node5 epoch4:node_model train_loss:0.15496402116198288,train_acc:0.9501128196716309
node5_model on test-dataset: loss:0.7590938758850098,acc:0.77590012550354
node5 weight score:4920.340051018658
node19: train data size:4281
node19 epoch0:node_model train_loss:0.4740564480077389,train_acc:0.8375394940376282
node19 epoch1:node_model train_loss:0.28707128849833513,train_acc:0.9009444713592529
node19 epoch2:node_model train_loss:0.2523977177780728,train_acc:0.9135716557502747
node19 epoch3:node_model train_loss:0.19834353012401004,train_acc:0.9404938220977783
node19 epoch4:node_model train_loss:0.15590365179056345,train_acc:0.95403653383255
node19_model on test-dataset: loss:0.8111952148377896,acc:0.7673999071121216
node19 weight score:5277.397994582659
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5895524644851684,acc:0.8161999839544296
total cost energy:12.944348547237226 | all_enery_cp：11.108500000000001 | all_enery_tp: 1.8358485472372257
ef: 25.3294525438348
reward: 12.385103996597575
step 83:loss:263.24224853515625|running q:20.41348648071289
episode1,iteration23 selected nodes:[15, 13, 4, 10, 18],center node:10
################################################## episode1,iteration23 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5710128745330232,train_acc:0.8257142901420593
node4 epoch1:node_model train_loss:0.33028171424354824,train_acc:0.8842856287956238
node4 epoch2:node_model train_loss:0.2881939187645912,train_acc:0.8978570103645325
node4 epoch3:node_model train_loss:0.28629337889807566,train_acc:0.9071429371833801
node4 epoch4:node_model train_loss:0.2657619831817491,train_acc:0.910714328289032
node4_model on test-dataset: loss:0.9350543582439422,acc:0.74590003490448
node4 weight score:2892.8799445200857
node10: train data size:1975
node10 epoch0:node_model train_loss:0.606367589533329,train_acc:0.8094999194145203
node10 epoch1:node_model train_loss:0.37350083217024804,train_acc:0.8768333792686462
node10 epoch2:node_model train_loss:0.25427875444293024,train_acc:0.920333206653595
node10 epoch3:node_model train_loss:0.18011586368083954,train_acc:0.9446665644645691
node10 epoch4:node_model train_loss:0.13525910247117282,train_acc:0.9628334045410156
node10_model on test-dataset: loss:0.7462704263627529,acc:0.7760000228881836
node10 weight score:2646.493724300388
node13: train data size:1155
node13 epoch0:node_model train_loss:0.628414568801721,train_acc:0.8121969699859619
node13 epoch1:node_model train_loss:0.2957115725924571,train_acc:0.9032575488090515
node13 epoch2:node_model train_loss:0.19579903843502203,train_acc:0.938636302947998
node13 epoch3:node_model train_loss:0.16329663308958212,train_acc:0.9478030204772949
node13 epoch4:node_model train_loss:0.1010582468782862,train_acc:0.9766666293144226
node13_model on test-dataset: loss:0.8062642568349838,acc:0.7703998684883118
node13 weight score:1432.5328082060707
node15: train data size:629
node15 epoch0:node_model train_loss:0.6596712853227343,train_acc:0.7796551585197449
node15 epoch1:node_model train_loss:0.36988919973373413,train_acc:0.876798152923584
node15 epoch2:node_model train_loss:0.23237435945442744,train_acc:0.9258621335029602
node15 epoch3:node_model train_loss:0.14389762175934656,train_acc:0.9671428799629211
node15 epoch4:node_model train_loss:0.1138680928519794,train_acc:0.9715763926506042
node15_model on test-dataset: loss:0.8727184955775737,acc:0.7531000375747681
node15 weight score:720.7364152214072
node18: train data size:472
node18 epoch0:node_model train_loss:0.6646656513214111,train_acc:0.7918888926506042
node18 epoch1:node_model train_loss:0.3411984384059906,train_acc:0.8617777824401855
node18 epoch2:node_model train_loss:0.222245517373085,train_acc:0.9313332438468933
node18 epoch3:node_model train_loss:0.13463535457849501,train_acc:0.960444450378418
node18 epoch4:node_model train_loss:0.09906787648797036,train_acc:0.9744445085525513
node18_model on test-dataset: loss:0.9625755050778388,acc:0.7440998554229736
node18 weight score:490.3511438947656
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6112920296192169,acc:0.8100999808311462
total cost energy:5.617301014008737 | all_enery_cp：3.4680000000000004 | all_enery_tp: 2.1493010140087367
ef: 24.340269054536137
reward: 18.722968040527398
step 84:loss:242.25845336914062|running q:21.330669403076172
episode1,iteration24 selected nodes:[10, 12, 15, 3, 8],center node:10
################################################## episode1,iteration24 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4516367447930713,train_acc:0.8449975848197937
node3 epoch1:node_model train_loss:0.24454089340775512,train_acc:0.9156355857849121
node3 epoch2:node_model train_loss:0.1866245262844618,train_acc:0.9413061738014221
node3 epoch3:node_model train_loss:0.1567836417708286,train_acc:0.951598048210144
node3 epoch4:node_model train_loss:0.11888458163932313,train_acc:0.9683423638343811
node3_model on test-dataset: loss:0.809528386592865,acc:0.768500030040741
node3 weight score:5246.264455128907
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6453813248210483,train_acc:0.7918934226036072
node8 epoch1:node_model train_loss:0.3851415564616521,train_acc:0.8609409928321838
node8 epoch2:node_model train_loss:0.21888087938229242,train_acc:0.9332652688026428
node8 epoch3:node_model train_loss:0.15415883105662134,train_acc:0.9577323198318481
node8 epoch4:node_model train_loss:0.13209264187349212,train_acc:0.9655101895332336
node8_model on test-dataset: loss:0.7457957303524018,acc:0.7748001217842102
node8 weight score:2410.8478056724903
node10: train data size:1975
node10 epoch0:node_model train_loss:0.42856674939393996,train_acc:0.857499897480011
node10 epoch1:node_model train_loss:0.25935410596430303,train_acc:0.919999897480011
node10 epoch2:node_model train_loss:0.17243945673108102,train_acc:0.9439998865127563
node10 epoch3:node_model train_loss:0.1279830476269126,train_acc:0.9663333892822266
node10 epoch4:node_model train_loss:0.09591875467449426,train_acc:0.9786666035652161
node10_model on test-dataset: loss:0.7875735081732274,acc:0.771399974822998
node10 weight score:2507.7024296830427
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7291922058377948,train_acc:0.7857142686843872
node12 epoch1:node_model train_loss:0.4194933623075485,train_acc:0.8558729887008667
node12 epoch2:node_model train_loss:0.26741775444575716,train_acc:0.910793662071228
node12 epoch3:node_model train_loss:0.18782806023955345,train_acc:0.9403173923492432
node12 epoch4:node_model train_loss:0.14791193870561464,train_acc:0.9657142758369446
node12_model on test-dataset: loss:0.8547163596749305,acc:0.7515000104904175
node12 weight score:1563.0916442363562
node15: train data size:629
node15 epoch0:node_model train_loss:0.631380958216531,train_acc:0.8025123476982117
node15 epoch1:node_model train_loss:0.30350535256522043,train_acc:0.8988670110702515
node15 epoch2:node_model train_loss:0.24501185970646994,train_acc:0.913004994392395
node15 epoch3:node_model train_loss:0.16159548397575105,train_acc:0.9565024375915527
node15 epoch4:node_model train_loss:0.12041010281869344,train_acc:0.9693595767021179
node15_model on test-dataset: loss:0.8233947634696961,acc:0.7599000334739685
node15 weight score:763.9106148179304
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6071750366687775,acc:0.8119999784231186
total cost energy:6.387360726567122 | all_enery_cp：4.9925 | all_enery_tp: 1.3948607265671225
ef: 24.997332789229617
reward: 18.609972062662493
step 85:loss:334.814697265625|running q:22.149417877197266
episode1,iteration25 selected nodes:[14, 8, 12, 10, 7],center node:10
################################################## episode1,iteration25 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6410144746303559,train_acc:0.7981961369514465
node7 epoch1:node_model train_loss:0.3955594927072525,train_acc:0.8671372532844543
node7 epoch2:node_model train_loss:0.25784009918570516,train_acc:0.9121373295783997
node7 epoch3:node_model train_loss:0.17818753346800803,train_acc:0.9445783495903015
node7 epoch4:node_model train_loss:0.11175043173134327,train_acc:0.973519504070282
node7_model on test-dataset: loss:0.7478095524013042,acc:0.7802002429962158
node7 weight score:2608.9530332089366
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5145494805441962,train_acc:0.8259750008583069
node8 epoch1:node_model train_loss:0.3197137001487944,train_acc:0.8893650770187378
node8 epoch2:node_model train_loss:0.19077733241849476,train_acc:0.9377323985099792
node8 epoch3:node_model train_loss:0.12075010066231091,train_acc:0.9671540856361389
node8 epoch4:node_model train_loss:0.08731203795307213,train_acc:0.9805440902709961
node8_model on test-dataset: loss:0.7605417490005493,acc:0.7748998999595642
node8 weight score:2364.104274831468
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3990181669592857,train_acc:0.8663333058357239
node10 epoch1:node_model train_loss:0.21676506772637366,train_acc:0.9263332486152649
node10 epoch2:node_model train_loss:0.13927169777452947,train_acc:0.9589998126029968
node10 epoch3:node_model train_loss:0.10074466969817877,train_acc:0.9781665802001953
node10 epoch4:node_model train_loss:0.07265199720859528,train_acc:0.9848331809043884
node10_model on test-dataset: loss:0.7553443904221058,acc:0.7791000008583069
node10 weight score:2614.701353506206
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5938686643327985,train_acc:0.8092064261436462
node12 epoch1:node_model train_loss:0.3797099196485111,train_acc:0.8767460584640503
node12 epoch2:node_model train_loss:0.24112177320889064,train_acc:0.9203174710273743
node12 epoch3:node_model train_loss:0.14058218683515275,train_acc:0.9640476107597351
node12 epoch4:node_model train_loss:0.10008344666234084,train_acc:0.9785712957382202
node12_model on test-dataset: loss:0.8617494162917138,acc:0.753600001335144
node12 weight score:1550.334673563325
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5639835769931475,train_acc:0.8148148059844971
node14 epoch1:node_model train_loss:0.3589380433162053,train_acc:0.879907488822937
node14 epoch2:node_model train_loss:0.2050274300078551,train_acc:0.9298610091209412
node14 epoch3:node_model train_loss:0.13284161438544592,train_acc:0.9693517684936523
node14 epoch4:node_model train_loss:0.11063033963243167,train_acc:0.9703702926635742
node14_model on test-dataset: loss:0.7335370048880577,acc:0.7822999358177185
node14 weight score:1597.7380720947465
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.589481915384531,acc:0.8179999792575836
total cost energy:5.2215199887160555 | all_enery_cp：4.1160000000000005 | all_enery_tp: 1.1055199887160552
ef: 25.18344852749326
reward: 19.9619285387772
step 86:loss:482.70806884765625|running q:23.254913330078125
episode1,iteration26 selected nodes:[5, 0, 12, 15, 4],center node:5
################################################## episode1,iteration26 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.46881086178697073,train_acc:0.8403313755989075
node0 epoch1:node_model train_loss:0.26941183782540834,train_acc:0.9094532132148743
node0 epoch2:node_model train_loss:0.19002863263281491,train_acc:0.9408387541770935
node0 epoch3:node_model train_loss:0.16756484399621302,train_acc:0.94930499792099
node0 epoch4:node_model train_loss:0.13908071830295599,train_acc:0.957766592502594
node0_model on test-dataset: loss:0.7613075613975525,acc:0.7839999794960022
node0 weight score:6808.023803790191
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4617826427732195,train_acc:0.841071367263794
node4 epoch1:node_model train_loss:0.33770114555954933,train_acc:0.8896430134773254
node4 epoch2:node_model train_loss:0.20177235760326898,train_acc:0.9339286684989929
node4 epoch3:node_model train_loss:0.16163438452141626,train_acc:0.9553571343421936
node4 epoch4:node_model train_loss:0.14793318430227892,train_acc:0.9546428322792053
node4_model on test-dataset: loss:0.9284514041244983,acc:0.7438998222351074
node4 weight score:2913.4535076186708
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4636096009298375,train_acc:0.8374060988426208
node5 epoch1:node_model train_loss:0.2697499504214839,train_acc:0.9034960865974426
node5 epoch2:node_model train_loss:0.21142406428330823,train_acc:0.9285715222358704
node5 epoch3:node_model train_loss:0.15892204230553225,train_acc:0.9482706785202026
node5 epoch4:node_model train_loss:0.12099898773196496,train_acc:0.96695476770401
node5_model on test-dataset: loss:0.7600755675882102,acc:0.7762999534606934
node5 weight score:4913.985081577479
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5264121357883725,train_acc:0.8329364657402039
node12 epoch1:node_model train_loss:0.2870391309261322,train_acc:0.9051588177680969
node12 epoch2:node_model train_loss:0.19642212029014314,train_acc:0.9327778816223145
node12 epoch3:node_model train_loss:0.1521678189081805,train_acc:0.9590475559234619
node12 epoch4:node_model train_loss:0.09739285388163157,train_acc:0.9757142663002014
node12_model on test-dataset: loss:0.8773153294622899,acc:0.7593996524810791
node12 weight score:1522.827602726194
node15: train data size:629
node15 epoch0:node_model train_loss:0.6695328099387032,train_acc:0.8004434108734131
node15 epoch1:node_model train_loss:0.31887112132140566,train_acc:0.8844335079193115
node15 epoch2:node_model train_loss:0.2184766892875944,train_acc:0.9374384880065918
node15 epoch3:node_model train_loss:0.15294638914721354,train_acc:0.9522166848182678
node15 epoch4:node_model train_loss:0.11090627951281411,train_acc:0.9750738739967346
node15_model on test-dataset: loss:0.8260966840386391,acc:0.7618998289108276
node15 weight score:761.412086688124
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5824181375652552,acc:0.822599983215332
total cost energy:9.123061226915837 | all_enery_cp：6.794 | all_enery_tp: 2.3290612269158366
ef: 24.67803363406021
reward: 15.554972407144373
step 87:loss:316.3023681640625|running q:24.213945388793945
episode1,iteration27 selected nodes:[5, 11, 14, 4, 16],center node:11
################################################## episode1,iteration27 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.41542627449546543,train_acc:0.8575000762939453
node4 epoch1:node_model train_loss:0.28745128347405363,train_acc:0.9021427631378174
node4 epoch2:node_model train_loss:0.20543356399450982,train_acc:0.9357141256332397
node4 epoch3:node_model train_loss:0.13048416229763202,train_acc:0.9657142162322998
node4 epoch4:node_model train_loss:0.12142215629241296,train_acc:0.9642856121063232
node4_model on test-dataset: loss:0.8785499287769198,acc:0.7583997249603271
node4 weight score:3078.937134245503
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3534646320499872,train_acc:0.8779699206352234
node5 epoch1:node_model train_loss:0.20910307137589706,train_acc:0.9311277270317078
node5 epoch2:node_model train_loss:0.14088793030302776,train_acc:0.9599999189376831
node5 epoch3:node_model train_loss:0.11516334126262288,train_acc:0.9684584736824036
node5 epoch4:node_model train_loss:0.09159811919457034,train_acc:0.9774057269096375
node5_model on test-dataset: loss:0.7516867245733738,acc:0.7821997404098511
node5 weight score:4968.825280398334
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5785193478359896,train_acc:0.8119798898696899
node11 epoch1:node_model train_loss:0.31297002031522636,train_acc:0.8930846452713013
node11 epoch2:node_model train_loss:0.2071453061173944,train_acc:0.9319081902503967
node11 epoch3:node_model train_loss:0.12166528710547615,train_acc:0.9681779146194458
node11 epoch4:node_model train_loss:0.08894057593801442,train_acc:0.9834719300270081
node11_model on test-dataset: loss:0.7502221341431141,acc:0.781999945640564
node11 weight score:2242.0026328883782
node14: train data size:1172
node14 epoch0:node_model train_loss:0.517175971219937,train_acc:0.8429166674613953
node14 epoch1:node_model train_loss:0.27951931829253834,train_acc:0.9032408595085144
node14 epoch2:node_model train_loss:0.1867799318085114,train_acc:0.9435648322105408
node14 epoch3:node_model train_loss:0.11796095967292786,train_acc:0.9623609781265259
node14 epoch4:node_model train_loss:0.08451200028260548,train_acc:0.9800000190734863
node14_model on test-dataset: loss:0.7391323059797287,acc:0.7811999320983887
node14 weight score:1585.643044578467
node16: train data size:877
node16 epoch0:node_model train_loss:0.6606862545013428,train_acc:0.8031313419342041
node16 epoch1:node_model train_loss:0.3486076560285356,train_acc:0.8877922296524048
node16 epoch2:node_model train_loss:0.191263433959749,train_acc:0.9490042924880981
node16 epoch3:node_model train_loss:0.14174718989266288,train_acc:0.9553390145301819
node16 epoch4:node_model train_loss:0.07326317288809353,train_acc:0.9855554699897766
node16_model on test-dataset: loss:0.7783334881067276,acc:0.7724001407623291
node16 weight score:1126.7663712289648
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.593972497433424,acc:0.8199999785423279
total cost energy:6.473205430228725 | all_enery_cp：5.085500000000001 | all_enery_tp: 1.3877054302287244
ef: 25.132819426785524
reward: 18.6596139965568
step 88:loss:329.6200256347656|running q:25.05833625793457
episode1,iteration28 selected nodes:[9, 7, 2, 11, 17],center node:11
################################################## episode1,iteration28 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.46790083528806764,train_acc:0.8457291126251221
node2 epoch1:node_model train_loss:0.28517150816818077,train_acc:0.8999147415161133
node2 epoch2:node_model train_loss:0.21205386752262712,train_acc:0.9308712482452393
node2 epoch3:node_model train_loss:0.16926892551903924,train_acc:0.9493276476860046
node2 epoch4:node_model train_loss:0.1425485087869068,train_acc:0.9564961791038513
node2_model on test-dataset: loss:0.7356610190868378,acc:0.7845001816749573
node2 weight score:6508.432383631329
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5555645510554313,train_acc:0.8207157254219055
node7 epoch1:node_model train_loss:0.3292450256645679,train_acc:0.8920588493347168
node7 epoch2:node_model train_loss:0.21103270463645457,train_acc:0.9260392189025879
node7 epoch3:node_model train_loss:0.12160227913409472,train_acc:0.9670587778091431
node7 epoch4:node_model train_loss:0.08613091614097357,train_acc:0.9785195589065552
node7_model on test-dataset: loss:0.7361574247479439,acc:0.7804000973701477
node7 weight score:2650.248349621701
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6533571716986204,train_acc:0.8141828179359436
node9 epoch1:node_model train_loss:0.39273124148971156,train_acc:0.8694643974304199
node9 epoch2:node_model train_loss:0.24985081427975706,train_acc:0.9284117817878723
node9 epoch3:node_model train_loss:0.1720974437500301,train_acc:0.9520959258079529
node9 epoch4:node_model train_loss:0.10747753024885529,train_acc:0.9685503244400024
node9_model on test-dataset: loss:0.7626185075938702,acc:0.7749999165534973
node9 weight score:2435.0313839864725
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4557514646474053,train_acc:0.846829354763031
node11 epoch1:node_model train_loss:0.2783835232257843,train_acc:0.9051793217658997
node11 epoch2:node_model train_loss:0.1430408446227803,train_acc:0.9537301659584045
node11 epoch3:node_model train_loss:0.09983148447730962,train_acc:0.9749066829681396
node11 epoch4:node_model train_loss:0.07081532916601967,train_acc:0.9894116520881653
node11_model on test-dataset: loss:0.7615764707326889,acc:0.7826999425888062
node11 weight score:2208.576636278955
node17: train data size:442
node17 epoch0:node_model train_loss:0.7744238018989563,train_acc:0.7786666750907898
node17 epoch1:node_model train_loss:0.3953672856092453,train_acc:0.8761904835700989
node17 epoch2:node_model train_loss:0.24075996577739717,train_acc:0.942476212978363
node17 epoch3:node_model train_loss:0.10314956605434418,train_acc:0.9692381024360657
node17 epoch4:node_model train_loss:0.06421269178390503,train_acc:0.9899999499320984
node17_model on test-dataset: loss:0.8001690444350242,acc:0.768799901008606
node17 weight score:552.3832783509929
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5944439640641213,acc:0.8185999822616578
total cost energy:6.644161925296379 | all_enery_cp：5.36 | all_enery_tp: 1.2841619252963778
ef: 25.166934536669718
reward: 18.52277261137334
step 89:loss:227.19326782226562|running q:26.038928985595703
episode1,iteration29 selected nodes:[12, 9, 16, 0, 4],center node:9
################################################## episode1,iteration29 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.36634874143279517,train_acc:0.8713367581367493
node0 epoch1:node_model train_loss:0.21743564121425152,train_acc:0.9271501898765564
node0 epoch2:node_model train_loss:0.15352185571996066,train_acc:0.9512277841567993
node0 epoch3:node_model train_loss:0.11919997732799786,train_acc:0.9650739431381226
node0 epoch4:node_model train_loss:0.09092655011381094,train_acc:0.9762281775474548
node0_model on test-dataset: loss:0.7609199271351099,acc:0.7892999649047852
node0 weight score:6811.492004834433
node4: train data size:2705
node4 epoch0:node_model train_loss:0.3929823578468391,train_acc:0.8696429133415222
node4 epoch1:node_model train_loss:0.25586035209042685,train_acc:0.9164285659790039
node4 epoch2:node_model train_loss:0.23862396287066595,train_acc:0.9221428036689758
node4 epoch3:node_model train_loss:0.26769840837057146,train_acc:0.9064286351203918
node4 epoch4:node_model train_loss:0.138583017646202,train_acc:0.9510712623596191
node4_model on test-dataset: loss:0.7528080913424492,acc:0.7815999388694763
node4 weight score:3593.2132386838375
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5173901492043546,train_acc:0.8345798850059509
node9 epoch1:node_model train_loss:0.30661186105326604,train_acc:0.9003785848617554
node9 epoch2:node_model train_loss:0.20945324317405098,train_acc:0.9341921806335449
node9 epoch3:node_model train_loss:0.12788636727552666,train_acc:0.9618281126022339
node9 epoch4:node_model train_loss:0.09361226209684421,train_acc:0.9806555509567261
node9_model on test-dataset: loss:0.7698448820412159,acc:0.7758998870849609
node9 weight score:2412.1742487606484
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6313033870288304,train_acc:0.8162698149681091
node12 epoch1:node_model train_loss:0.3286334755165236,train_acc:0.8869841694831848
node12 epoch2:node_model train_loss:0.20192026666232518,train_acc:0.9303175210952759
node12 epoch3:node_model train_loss:0.1514106190630368,train_acc:0.958888828754425
node12 epoch4:node_model train_loss:0.09577156124370438,train_acc:0.9801586270332336
node12_model on test-dataset: loss:0.853495362251997,acc:0.7605000138282776
node12 weight score:1565.3277792569213
node16: train data size:877
node16 epoch0:node_model train_loss:0.6059984332985349,train_acc:0.8075757622718811
node16 epoch1:node_model train_loss:0.2921413779258728,train_acc:0.9050071835517883
node16 epoch2:node_model train_loss:0.18395389368136725,train_acc:0.9457863569259644
node16 epoch3:node_model train_loss:0.11113024834129545,train_acc:0.9745597839355469
node16 epoch4:node_model train_loss:0.08309976963533296,train_acc:0.984112560749054
node16_model on test-dataset: loss:0.7787518803775311,acc:0.7741999626159668
node16 weight score:1126.1610046769185
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.611500156223774,acc:0.8200999850034714
total cost energy:8.085178389446398 | all_enery_cp：5.979 | all_enery_tp: 2.106178389446397
ef: 24.89501419644931
reward: 16.809835807002912
step 90:loss:210.43992614746094|running q:26.85044288635254
episode1,iteration30 selected nodes:[3, 16, 11, 0, 17],center node:11
################################################## episode1,iteration30 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.20427725148888734,train_acc:0.9285705089569092
node0 epoch1:node_model train_loss:0.15639623498114255,train_acc:0.947303056716919
node0 epoch2:node_model train_loss:0.11211819996914038,train_acc:0.9684962034225464
node0 epoch3:node_model train_loss:0.09382623269294317,train_acc:0.9746896028518677
node0 epoch4:node_model train_loss:0.08166559606503981,train_acc:0.9780375957489014
node0_model on test-dataset: loss:0.8350315397977829,acc:0.7788998484611511
node0 weight score:6206.951178461057
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4019826834284982,train_acc:0.8625828623771667
node3 epoch1:node_model train_loss:0.2251683709233306,train_acc:0.9233992695808411
node3 epoch2:node_model train_loss:0.1688565520006557,train_acc:0.9407520890235901
node3 epoch3:node_model train_loss:0.12566038325082424,train_acc:0.9606383442878723
node3 epoch4:node_model train_loss:0.0973893805125425,train_acc:0.9727607369422913
node3_model on test-dataset: loss:0.9255925272405148,acc:0.7551998496055603
node3 weight score:4588.412152226051
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4775632479611565,train_acc:0.8429698944091797
node11 epoch1:node_model train_loss:0.24818714708089828,train_acc:0.9168148636817932
node11 epoch2:node_model train_loss:0.1415210006868138,train_acc:0.9616355895996094
node11 epoch3:node_model train_loss:0.10838931404492434,train_acc:0.973730206489563
node11 epoch4:node_model train_loss:0.06455997246153214,train_acc:0.9894116520881653
node11_model on test-dataset: loss:0.7462399995326996,acc:0.7907999753952026
node11 weight score:2253.9665537270575
node16: train data size:877
node16 epoch0:node_model train_loss:0.5861921575334337,train_acc:0.8055844306945801
node16 epoch1:node_model train_loss:0.3146513882610533,train_acc:0.9006782174110413
node16 epoch2:node_model train_loss:0.17097121477127075,train_acc:0.9510100483894348
node16 epoch3:node_model train_loss:0.10520274854368633,train_acc:0.9718902707099915
node16 epoch4:node_model train_loss:0.08502963640623623,train_acc:0.9830014109611511
node16_model on test-dataset: loss:0.8365707805752755,acc:0.7646997570991516
node16 weight score:1048.3273147514465
node17: train data size:442
node17 epoch0:node_model train_loss:0.7701034486293793,train_acc:0.804190456867218
node17 epoch1:node_model train_loss:0.31166675090789797,train_acc:0.8969523310661316
node17 epoch2:node_model train_loss:0.26659137606620786,train_acc:0.930476188659668
node17 epoch3:node_model train_loss:0.14254750311374664,train_acc:0.9537143707275391
node17 epoch4:node_model train_loss:0.11618658006191254,train_acc:0.977238118648529
node17_model on test-dataset: loss:0.8625714659690857,acc:0.762700080871582
node17 weight score:512.4213093501995
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6460834559798241,acc:0.8150999832153321
total cost energy:7.818523446058874 | all_enery_cp：6.2155 | all_enery_tp: 1.6030234460588741
ef: 24.789460289576006
reward: 16.97093684351713
step 91:loss:294.7395324707031|running q:27.948341369628906
episode1,iteration31 selected nodes:[18, 2, 14, 10, 4],center node:14
################################################## episode1,iteration31 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3744346567740043,train_acc:0.8730207681655884
node2 epoch1:node_model train_loss:0.2129535189208885,train_acc:0.9245927333831787
node2 epoch2:node_model train_loss:0.15247712517157197,train_acc:0.951316237449646
node2 epoch3:node_model train_loss:0.11072185589000583,train_acc:0.9682480692863464
node2 epoch4:node_model train_loss:0.0896027230968078,train_acc:0.9769412875175476
node2_model on test-dataset: loss:0.726330016925931,acc:0.7915999293327332
node2 weight score:6592.044784634401
node4: train data size:2705
node4 epoch0:node_model train_loss:0.47950794973543714,train_acc:0.8592856526374817
node4 epoch1:node_model train_loss:0.2656782455742359,train_acc:0.9103571176528931
node4 epoch2:node_model train_loss:0.20721321659428732,train_acc:0.934999942779541
node4 epoch3:node_model train_loss:0.21622598118015698,train_acc:0.9310714602470398
node4 epoch4:node_model train_loss:0.149883716167616,train_acc:0.9521427750587463
node4_model on test-dataset: loss:0.8579421237111091,acc:0.7575000524520874
node4 weight score:3152.893330728731
node10: train data size:1975
node10 epoch0:node_model train_loss:0.47965362295508385,train_acc:0.8425000309944153
node10 epoch1:node_model train_loss:0.2601950205862522,train_acc:0.9163333773612976
node10 epoch2:node_model train_loss:0.16934434399008752,train_acc:0.9488332867622375
node10 epoch3:node_model train_loss:0.12103732749819755,train_acc:0.9666665196418762
node10 epoch4:node_model train_loss:0.08807240258902312,train_acc:0.9784998893737793
node10_model on test-dataset: loss:0.7591822679713368,acc:0.7811000943183899
node10 weight score:2601.4833108227535
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5435480152567228,train_acc:0.8220371007919312
node14 epoch1:node_model train_loss:0.2948498378197352,train_acc:0.9040277004241943
node14 epoch2:node_model train_loss:0.17315956888099512,train_acc:0.9425462484359741
node14 epoch3:node_model train_loss:0.09566577182461818,train_acc:0.9771758913993835
node14 epoch4:node_model train_loss:0.0716390913973252,train_acc:0.9841666221618652
node14_model on test-dataset: loss:0.8044674886018037,acc:0.7742000222206116
node14 weight score:1456.8643439363627
node18: train data size:472
node18 epoch0:node_model train_loss:0.6006996989250183,train_acc:0.8262222409248352
node18 epoch1:node_model train_loss:0.32506723403930665,train_acc:0.9025554656982422
node18 epoch2:node_model train_loss:0.1784290552139282,train_acc:0.9448888897895813
node18 epoch3:node_model train_loss:0.1029960796236992,train_acc:0.9772221446037292
node18 epoch4:node_model train_loss:0.059304676949977875,train_acc:0.9872221946716309
node18_model on test-dataset: loss:0.9338830032944679,acc:0.7469000220298767
node18 weight score:505.4166296366045
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5997979631274939,acc:0.819099982380867
total cost energy:7.650885277439702 | all_enery_cp：5.556 | all_enery_tp: 2.0948852774397015
ef: 24.819461946063008
reward: 17.168576668623306
step 92:loss:192.981201171875|running q:28.722965240478516
episode1,iteration32 selected nodes:[6, 16, 1, 5, 2],center node:6
################################################## episode1,iteration32 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.44764165561611985,train_acc:0.8469119668006897
node1 epoch1:node_model train_loss:0.26856522481231127,train_acc:0.9049999117851257
node1 epoch2:node_model train_loss:0.25881710177397027,train_acc:0.9094118475914001
node1 epoch3:node_model train_loss:0.25131765303804593,train_acc:0.9138236045837402
node1 epoch4:node_model train_loss:0.1604755679693292,train_acc:0.9516176581382751
node1_model on test-dataset: loss:0.7651035839319229,acc:0.7797001004219055
node1 weight score:8767.440305961058
node2: train data size:4788
node2 epoch0:node_model train_loss:0.21665939890469113,train_acc:0.9250378012657166
node2 epoch1:node_model train_loss:0.13773628029351434,train_acc:0.955302894115448
node2 epoch2:node_model train_loss:0.09903600656737883,train_acc:0.9713163375854492
node2 epoch3:node_model train_loss:0.0821187780238688,train_acc:0.9768182039260864
node2 epoch4:node_model train_loss:0.06652992758123825,train_acc:0.9847917556762695
node2_model on test-dataset: loss:0.7972420506179333,acc:0.7843000888824463
node2 weight score:6005.704285528937
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4472105510924992,train_acc:0.8563910126686096
node5 epoch1:node_model train_loss:0.24428768455982208,train_acc:0.9136841297149658
node5 epoch2:node_model train_loss:0.1637690935872103,train_acc:0.9442857503890991
node5 epoch3:node_model train_loss:0.12332327036481154,train_acc:0.9603381752967834
node5 epoch4:node_model train_loss:0.08327325864842064,train_acc:0.9781951904296875
node5_model on test-dataset: loss:0.7246869772672653,acc:0.7941001057624817
node5 weight score:5153.9493838903745
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5854659272177566,train_acc:0.8229032158851624
node6 epoch1:node_model train_loss:0.364324317824456,train_acc:0.8790322542190552
node6 epoch2:node_model train_loss:0.2710675001144409,train_acc:0.9108755588531494
node6 epoch3:node_model train_loss:0.2539543769051952,train_acc:0.9124885201454163
node6 epoch4:node_model train_loss:0.24203587948314606,train_acc:0.9244240522384644
node6_model on test-dataset: loss:0.7544496956467629,acc:0.7783999443054199
node6 weight score:3985.686543914907
node16: train data size:877
node16 epoch0:node_model train_loss:0.5998121500015259,train_acc:0.823347806930542
node16 epoch1:node_model train_loss:0.3258265968826082,train_acc:0.8784559369087219
node16 epoch2:node_model train_loss:0.19605247262451383,train_acc:0.9351226091384888
node16 epoch3:node_model train_loss:0.11604245007038116,train_acc:0.9671140313148499
node16 epoch4:node_model train_loss:0.08715301752090454,train_acc:0.9774459004402161
node16_model on test-dataset: loss:0.824199226796627,acc:0.7647000551223755
node16 weight score:1064.0631190696345
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5908382201194763,acc:0.8233999818563461
total cost energy:10.787808784984488 | all_enery_cp：9.557500000000001 | all_enery_tp: 1.230308784984488
ef: 25.34331264340283
reward: 14.55550385841834
step 93:loss:141.274658203125|running q:29.598936080932617
episode1,iteration33 selected nodes:[3, 5, 11, 4, 8],center node:3
################################################## episode1,iteration33 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.35935809931089713,train_acc:0.8724096417427063
node3 epoch1:node_model train_loss:0.1886584556033445,train_acc:0.9357247352600098
node3 epoch2:node_model train_loss:0.13896867243009944,train_acc:0.9567144513130188
node3 epoch3:node_model train_loss:0.09749113274521606,train_acc:0.9720929861068726
node3 epoch4:node_model train_loss:0.06941345810543659,train_acc:0.9839237332344055
node3_model on test-dataset: loss:0.7945476432144641,acc:0.7881001234054565
node3 weight score:5345.179784082061
node4: train data size:2705
node4 epoch0:node_model train_loss:0.39391424506902695,train_acc:0.8635715246200562
node4 epoch1:node_model train_loss:0.2410027315574033,train_acc:0.9100000262260437
node4 epoch2:node_model train_loss:0.19594638235867023,train_acc:0.9282143115997314
node4 epoch3:node_model train_loss:0.17726819483297213,train_acc:0.9417856931686401
node4 epoch4:node_model train_loss:0.11570914914565426,train_acc:0.9682140946388245
node4_model on test-dataset: loss:0.7772899475693703,acc:0.7756001353263855
node4 weight score:3480.0398595899615
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3378334923794395,train_acc:0.8775941133499146
node5 epoch1:node_model train_loss:0.1849817029739681,train_acc:0.935413658618927
node5 epoch2:node_model train_loss:0.1257355546480731,train_acc:0.9616163969039917
node5 epoch3:node_model train_loss:0.09287572549165864,train_acc:0.9768418669700623
node5 epoch4:node_model train_loss:0.061089620435316315,train_acc:0.9886841177940369
node5_model on test-dataset: loss:0.7715197686851024,acc:0.786799967288971
node5 weight score:4841.094358950184
node8: train data size:1798
node8 epoch0:node_model train_loss:0.621717760960261,train_acc:0.8104081153869629
node8 epoch1:node_model train_loss:0.3122732813159625,train_acc:0.8864625692367554
node8 epoch2:node_model train_loss:0.2077938061621454,train_acc:0.9254761934280396
node8 epoch3:node_model train_loss:0.16268831160333422,train_acc:0.95383220911026
node8 epoch4:node_model train_loss:0.12081178733044201,train_acc:0.9688547849655151
node8_model on test-dataset: loss:0.8380002975463867,acc:0.7688999176025391
node8 weight score:2145.5839637103154
node11: train data size:1682
node11 epoch0:node_model train_loss:0.47774287357049827,train_acc:0.8487805128097534
node11 epoch1:node_model train_loss:0.24568223997074015,train_acc:0.9141319394111633
node11 epoch2:node_model train_loss:0.141600440530216,train_acc:0.9572022557258606
node11 epoch3:node_model train_loss:0.09004701389109387,train_acc:0.9832710027694702
node11 epoch4:node_model train_loss:0.07314802158404798,train_acc:0.9834001064300537
node11_model on test-dataset: loss:0.8369540589302779,acc:0.7714001536369324
node11 weight score:2009.6682512655314
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.618111417889595,acc:0.8208999818563462
total cost energy:8.3759381783262 | all_enery_cp：7.0835 | all_enery_tp: 1.2924381783262
ef: 25.230215185899667
reward: 16.854277007573465
step 94:loss:158.91506958007812|running q:30.68536376953125
episode1,iteration34 selected nodes:[19, 11, 2, 5, 12],center node:11
################################################## episode1,iteration34 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.25455469110359746,train_acc:0.9070075750350952
node2 epoch1:node_model train_loss:0.1459351220789055,train_acc:0.9530112743377686
node2 epoch2:node_model train_loss:0.09958880107539396,train_acc:0.9709278345108032
node2 epoch3:node_model train_loss:0.09045895786645512,train_acc:0.9760416746139526
node2 epoch4:node_model train_loss:0.06394567980896682,train_acc:0.9851800799369812
node2_model on test-dataset: loss:0.7610114522278308,acc:0.7902000546455383
node2 weight score:6291.626737000239
node5: train data size:3735
node5 epoch0:node_model train_loss:0.2206119608722235,train_acc:0.9211653470993042
node5 epoch1:node_model train_loss:0.1597244408177702,train_acc:0.9481953978538513
node5 epoch2:node_model train_loss:0.11162119809734194,train_acc:0.9674057364463806
node5 epoch3:node_model train_loss:0.08302845585307009,train_acc:0.9792104363441467
node5 epoch4:node_model train_loss:0.06472965388705856,train_acc:0.9829321503639221
node5_model on test-dataset: loss:0.8051643855124712,acc:0.7862998843193054
node5 weight score:4638.804282957382
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4388900022296345,train_acc:0.8521090149879456
node11 epoch1:node_model train_loss:0.2266178481719073,train_acc:0.9281203746795654
node11 epoch2:node_model train_loss:0.13109865828472025,train_acc:0.9598134160041809
node11 epoch3:node_model train_loss:0.09529628758044804,train_acc:0.9773887395858765
node11 epoch4:node_model train_loss:0.05170039350495619,train_acc:0.9928119778633118
node11_model on test-dataset: loss:0.7507159499078989,acc:0.7908000946044922
node11 weight score:2240.52785904756
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6541579706328255,train_acc:0.8007143139839172
node12 epoch1:node_model train_loss:0.3253283106854984,train_acc:0.8863492608070374
node12 epoch2:node_model train_loss:0.19567685148545674,train_acc:0.9376190900802612
node12 epoch3:node_model train_loss:0.13605961310012,train_acc:0.9546032547950745
node12 epoch4:node_model train_loss:0.0811146077300821,train_acc:0.9815872311592102
node12_model on test-dataset: loss:0.7837076726555824,acc:0.7803999781608582
node12 weight score:1704.717264631317
node19: train data size:4281
node19 epoch0:node_model train_loss:0.5497015790883885,train_acc:0.8236549496650696
node19 epoch1:node_model train_loss:0.3366973448631375,train_acc:0.8831610679626465
node19 epoch2:node_model train_loss:0.20978342152612153,train_acc:0.9284553527832031
node19 epoch3:node_model train_loss:0.15055152652568596,train_acc:0.954076886177063
node19 epoch4:node_model train_loss:0.11749911039721134,train_acc:0.968208372592926
node19_model on test-dataset: loss:0.7162355129420758,acc:0.794900119304657
node19 weight score:5977.084244838636
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5979922062903643,acc:0.8217999798059463
total cost energy:9.858213595499958 | all_enery_cp：7.911 | all_enery_tp: 1.9472135954999579
ef: 25.257381817437256
reward: 15.399168221937298
step 95:loss:185.18783569335938|running q:31.469932556152344
episode1,iteration35 selected nodes:[1, 8, 6, 4, 2],center node:6
################################################## episode1,iteration35 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3602679760250099,train_acc:0.8754410743713379
node1 epoch1:node_model train_loss:0.23509891900946112,train_acc:0.9213968515396118
node1 epoch2:node_model train_loss:0.1819891494644039,train_acc:0.9366176128387451
node1 epoch3:node_model train_loss:0.1472693690482308,train_acc:0.9520587921142578
node1 epoch4:node_model train_loss:0.10253260651712909,train_acc:0.9694119691848755
node1_model on test-dataset: loss:0.7359354592859745,acc:0.7976000308990479
node1 weight score:9114.92973379526
node2: train data size:4788
node2 epoch0:node_model train_loss:0.19515869133950522,train_acc:0.9258428812026978
node2 epoch1:node_model train_loss:0.10070751064146559,train_acc:0.9697632789611816
node2 epoch2:node_model train_loss:0.06825528604288895,train_acc:0.9847347736358643
node2 epoch3:node_model train_loss:0.055637843518828355,train_acc:0.9883050322532654
node2 epoch4:node_model train_loss:0.05712303899539014,train_acc:0.987083375453949
node2_model on test-dataset: loss:0.8230422706902027,acc:0.7825999855995178
node2 weight score:5817.441181951428
node4: train data size:2705
node4 epoch0:node_model train_loss:0.3809265030015792,train_acc:0.8742856979370117
node4 epoch1:node_model train_loss:0.19156283419579268,train_acc:0.9367856979370117
node4 epoch2:node_model train_loss:0.12574364851960645,train_acc:0.9628570079803467
node4 epoch3:node_model train_loss:0.09534960772309985,train_acc:0.9639285206794739
node4 epoch4:node_model train_loss:0.20526829401829413,train_acc:0.9328570365905762
node4_model on test-dataset: loss:0.8633682002127171,acc:0.7666000723838806
node4 weight score:3133.0780996260232
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5619710741504547,train_acc:0.8308756351470947
node6 epoch1:node_model train_loss:0.34059106774868503,train_acc:0.8837786912918091
node6 epoch2:node_model train_loss:0.2427931168387013,train_acc:0.9222580194473267
node6 epoch3:node_model train_loss:0.17076035252501887,train_acc:0.9499999284744263
node6 epoch4:node_model train_loss:0.14310254661306257,train_acc:0.9620733857154846
node6_model on test-dataset: loss:0.7961345477402211,acc:0.779999852180481
node6 weight score:3776.9997653476844
node8: train data size:1798
node8 epoch0:node_model train_loss:0.503635103503863,train_acc:0.8386961817741394
node8 epoch1:node_model train_loss:0.30569426922334564,train_acc:0.8920861482620239
node8 epoch2:node_model train_loss:0.1735655644701587,train_acc:0.9460318088531494
node8 epoch3:node_model train_loss:0.12445248415072759,train_acc:0.9621768593788147
node8 epoch4:node_model train_loss:0.09188711891571681,train_acc:0.9794216156005859
node8_model on test-dataset: loss:0.7228663256764412,acc:0.7911999225616455
node8 weight score:2487.3201809718753
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.608711714297533,acc:0.8215999829769135
total cost energy:11.060115546859237 | all_enery_cp：9.503 | all_enery_tp: 1.5571155468592366
ef: 25.271356019758063
reward: 14.211240472898826
step 96:loss:174.31674194335938|running q:32.30406951904297
episode1,iteration36 selected nodes:[16, 7, 10, 12, 18],center node:10
################################################## episode1,iteration36 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5469219192862511,train_acc:0.8340784311294556
node7 epoch1:node_model train_loss:0.30761247128248215,train_acc:0.8935392498970032
node7 epoch2:node_model train_loss:0.21305084191262721,train_acc:0.9305784106254578
node7 epoch3:node_model train_loss:0.12777052521705629,train_acc:0.960019588470459
node7 epoch4:node_model train_loss:0.0850289149209857,train_acc:0.9765195250511169
node7_model on test-dataset: loss:0.7478575745224952,acc:0.7879999876022339
node7 weight score:2608.7855047075072
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4744967848062515,train_acc:0.8448333740234375
node10 epoch1:node_model train_loss:0.25762081816792487,train_acc:0.9179999232292175
node10 epoch2:node_model train_loss:0.15400112904608249,train_acc:0.9551666378974915
node10 epoch3:node_model train_loss:0.12831393033266067,train_acc:0.9684999585151672
node10 epoch4:node_model train_loss:0.08364000637084246,train_acc:0.9776665568351746
node10_model on test-dataset: loss:0.7570320899784565,acc:0.7916997075080872
node10 weight score:2608.8722342750416
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6157009644167764,train_acc:0.8189681768417358
node12 epoch1:node_model train_loss:0.3665894429598536,train_acc:0.8782538175582886
node12 epoch2:node_model train_loss:0.23332829773426056,train_acc:0.9183333516120911
node12 epoch3:node_model train_loss:0.13455982878804207,train_acc:0.9569047689437866
node12 epoch4:node_model train_loss:0.09304561545806271,train_acc:0.9785714745521545
node12_model on test-dataset: loss:0.8130141438543796,acc:0.7767999172210693
node12 weight score:1643.2678448448903
node16: train data size:877
node16 epoch0:node_model train_loss:0.6674833926889632,train_acc:0.8152381777763367
node16 epoch1:node_model train_loss:0.3352544473277198,train_acc:0.8900143504142761
node16 epoch2:node_model train_loss:0.21442267133129966,train_acc:0.9380086064338684
node16 epoch3:node_model train_loss:0.12470006859964794,train_acc:0.9660027623176575
node16 epoch4:node_model train_loss:0.07900612656441,train_acc:0.9867820739746094
node16_model on test-dataset: loss:0.838478144556284,acc:0.7749999165534973
node16 weight score:1045.9425874053063
node18: train data size:472
node18 epoch0:node_model train_loss:0.6691881537437439,train_acc:0.8059999346733093
node18 epoch1:node_model train_loss:0.3497006297111511,train_acc:0.8962222337722778
node18 epoch2:node_model train_loss:0.18928847312927247,train_acc:0.9448888897895813
node18 epoch3:node_model train_loss:0.10872281342744827,train_acc:0.9692221879959106
node18 epoch4:node_model train_loss:0.07420739568769932,train_acc:0.9764444231987
node18_model on test-dataset: loss:0.9602775264531374,acc:0.7436001896858215
node18 weight score:491.52457180099816
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5938182404637337,acc:0.8251999843120575
total cost energy:5.152780685971038 | all_enery_cp：3.3055000000000008 | all_enery_tp: 1.8472806859710378
ef: 24.605346732307883
reward: 19.452566046336845
step 97:loss:116.85086822509766|running q:33.61027526855469
episode1,iteration37 selected nodes:[1, 15, 5, 9, 8],center node:9
################################################## episode1,iteration37 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.2722293916432297,train_acc:0.900955855846405
node1 epoch1:node_model train_loss:0.18785504207891576,train_acc:0.9333823323249817
node1 epoch2:node_model train_loss:0.1380506756112856,train_acc:0.9569117426872253
node1 epoch3:node_model train_loss:0.13175077429589102,train_acc:0.9559558629989624
node1 epoch4:node_model train_loss:0.11208014010780436,train_acc:0.9670588374137878
node1_model on test-dataset: loss:0.8092719769477844,acc:0.7794999480247498
node1 weight score:8288.931522501996
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3032378209264655,train_acc:0.8958646059036255
node5 epoch1:node_model train_loss:0.17190391433082128,train_acc:0.9425563812255859
node5 epoch2:node_model train_loss:0.1091063060846768,train_acc:0.9658268690109253
node5 epoch3:node_model train_loss:0.0706072879072867,train_acc:0.9811276793479919
node5 epoch4:node_model train_loss:0.059115808702221044,train_acc:0.98635333776474
node5_model on test-dataset: loss:0.7272955451905727,acc:0.7988000512123108
node5 weight score:5135.463876684849
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5191608932283189,train_acc:0.8364852070808411
node8 epoch1:node_model train_loss:0.29190780967473984,train_acc:0.8904080986976624
node8 epoch2:node_model train_loss:0.1604147350622548,train_acc:0.9460656046867371
node8 epoch3:node_model train_loss:0.11498966399166319,train_acc:0.9699999690055847
node8 epoch4:node_model train_loss:0.06681241715947787,train_acc:0.9838888645172119
node8_model on test-dataset: loss:0.7589182956516742,acc:0.7846001386642456
node8 weight score:2369.1614898492844
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5945737675616616,train_acc:0.8223453164100647
node9 epoch1:node_model train_loss:0.3504068545605007,train_acc:0.8889288902282715
node9 epoch2:node_model train_loss:0.22164228323258853,train_acc:0.9268327951431274
node9 epoch3:node_model train_loss:0.14426035237939736,train_acc:0.9567034840583801
node9 epoch4:node_model train_loss:0.08307237442778914,train_acc:0.9799999594688416
node9_model on test-dataset: loss:0.7515222628414631,acc:0.7876999378204346
node9 weight score:2470.984682448113
node15: train data size:629
node15 epoch0:node_model train_loss:0.6068330577441624,train_acc:0.7990148067474365
node15 epoch1:node_model train_loss:0.32064708854470936,train_acc:0.8958621025085449
node15 epoch2:node_model train_loss:0.19542701435940607,train_acc:0.9307882189750671
node15 epoch3:node_model train_loss:0.12375409954360553,train_acc:0.9642857313156128
node15 epoch4:node_model train_loss:0.07010599172541074,train_acc:0.9787192940711975
node15_model on test-dataset: loss:0.9009559302031994,acc:0.7627000212669373
node15 weight score:698.1473553963253
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6101582884043455,acc:0.8225999796390533
total cost energy:8.958824412296597 | all_enery_cp：7.363499999999999 | all_enery_tp: 1.595324412296597
ef: 25.08446974185918
reward: 16.125645329562584
step 98:loss:249.88998413085938|running q:34.49104309082031
episode1,iteration38 selected nodes:[15, 16, 3, 10, 6],center node:10
################################################## episode1,iteration38 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3615108309097068,train_acc:0.8763335347175598
node3 epoch1:node_model train_loss:0.1923384290448455,train_acc:0.9317415952682495
node3 epoch2:node_model train_loss:0.12886165134435476,train_acc:0.9575258493423462
node3 epoch3:node_model train_loss:0.09521241668005322,train_acc:0.9729934334754944
node3 epoch4:node_model train_loss:0.06857096715721973,train_acc:0.9827906489372253
node3_model on test-dataset: loss:0.8164993956685066,acc:0.7818997502326965
node3 weight score:5201.473537555751
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5611379531122023,train_acc:0.8337787985801697
node6 epoch1:node_model train_loss:0.3396194875240326,train_acc:0.887327253818512
node6 epoch2:node_model train_loss:0.2476282412967374,train_acc:0.9182950258255005
node6 epoch3:node_model train_loss:0.18984158721662336,train_acc:0.9370967149734497
node6 epoch4:node_model train_loss:0.13827045225808698,train_acc:0.956774115562439
node6_model on test-dataset: loss:0.7777681966125966,acc:0.7821999192237854
node6 weight score:3866.1904833552558
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3791219711303711,train_acc:0.8736667037010193
node10 epoch1:node_model train_loss:0.20897980742156505,train_acc:0.9298334121704102
node10 epoch2:node_model train_loss:0.1116239869967103,train_acc:0.9704999327659607
node10 epoch3:node_model train_loss:0.08085625730454922,train_acc:0.9821667075157166
node10 epoch4:node_model train_loss:0.056660393439233306,train_acc:0.9874998927116394
node10_model on test-dataset: loss:0.7734713227301836,acc:0.7883999347686768
node10 weight score:2553.423691299484
node15: train data size:629
node15 epoch0:node_model train_loss:0.638702426637922,train_acc:0.8023645281791687
node15 epoch1:node_model train_loss:0.32469356060028076,train_acc:0.8907881379127502
node15 epoch2:node_model train_loss:0.25762141389506205,train_acc:0.9201477766036987
node15 epoch3:node_model train_loss:0.165533575628485,train_acc:0.9493595957756042
node15 epoch4:node_model train_loss:0.0885368814425809,train_acc:0.9750738739967346
node15_model on test-dataset: loss:0.863696119338274,acc:0.773099958896637
node15 weight score:728.2654001987554
node16: train data size:877
node16 epoch0:node_model train_loss:0.5906583368778229,train_acc:0.8267965316772461
node16 epoch1:node_model train_loss:0.3053329437971115,train_acc:0.8961182832717896
node16 epoch2:node_model train_loss:0.19520852963129678,train_acc:0.933232307434082
node16 epoch3:node_model train_loss:0.09314213196436565,train_acc:0.9752236604690552
node16 epoch4:node_model train_loss:0.06699062614805168,train_acc:0.9855554699897766
node16_model on test-dataset: loss:0.7860022638738156,acc:0.7788999080657959
node16 weight score:1115.772867723945
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6186719307303429,acc:0.8217999792098999
total cost energy:7.35444811943907 | all_enery_cp：5.3675 | all_enery_tp: 1.9869481194390706
ef: 24.7519493737399
reward: 17.39750125430083
step 99:loss:208.83921813964844|running q:35.39481735229492
episode1,iteration39 selected nodes:[16, 12, 11, 3, 5],center node:11
################################################## episode1,iteration39 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1910378174033276,train_acc:0.9324689507484436
node3 epoch1:node_model train_loss:0.11837111500113509,train_acc:0.9586886167526245
node3 epoch2:node_model train_loss:0.11131137912703115,train_acc:0.9651162028312683
node3 epoch3:node_model train_loss:0.06747829030419505,train_acc:0.9813359975814819
node3 epoch4:node_model train_loss:0.062323562064489656,train_acc:0.986481785774231
node3_model on test-dataset: loss:0.8853672748804092,acc:0.7777999043464661
node3 weight score:4796.879352214212
node5: train data size:3735
node5 epoch0:node_model train_loss:0.27021240717486333,train_acc:0.9074060320854187
node5 epoch1:node_model train_loss:0.13235504709576307,train_acc:0.957669198513031
node5 epoch2:node_model train_loss:0.09818274782676446,train_acc:0.9695110321044922
node5 epoch3:node_model train_loss:0.06936995665493764,train_acc:0.9816163778305054
node5 epoch4:node_model train_loss:0.0515931671191203,train_acc:0.9895110726356506
node5_model on test-dataset: loss:0.7627661999315023,acc:0.7965001463890076
node5 weight score:4896.651163010906
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4870154945289387,train_acc:0.8445336222648621
node11 epoch1:node_model train_loss:0.22930302777711084,train_acc:0.917073130607605
node11 epoch2:node_model train_loss:0.13906857217935956,train_acc:0.9550358057022095
node11 epoch3:node_model train_loss:0.07705467238145716,train_acc:0.9805881381034851
node11 epoch4:node_model train_loss:0.06778174200478722,train_acc:0.9876468777656555
node11_model on test-dataset: loss:0.7815390609204769,acc:0.7856999039649963
node11 weight score:2152.163703780823
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5727629171950477,train_acc:0.8320635557174683
node12 epoch1:node_model train_loss:0.32098062549318584,train_acc:0.890396773815155
node12 epoch2:node_model train_loss:0.17426218571407454,train_acc:0.9423015713691711
node12 epoch3:node_model train_loss:0.12198874354362488,train_acc:0.9634920358657837
node12 epoch4:node_model train_loss:0.08069374678390366,train_acc:0.9760317802429199
node12_model on test-dataset: loss:0.802299140393734,acc:0.7850999236106873
node12 weight score:1665.214298178543
node16: train data size:877
node16 epoch0:node_model train_loss:0.49362139569388497,train_acc:0.8356854319572449
node16 epoch1:node_model train_loss:0.25804245554738575,train_acc:0.9136796593666077
node16 epoch2:node_model train_loss:0.15206368764241537,train_acc:0.9527848362922668
node16 epoch3:node_model train_loss:0.1049221112496323,train_acc:0.9701154828071594
node16 epoch4:node_model train_loss:0.07053892914619711,train_acc:0.9848917126655579
node16_model on test-dataset: loss:0.7851871663331985,acc:0.7838999032974243
node16 weight score:1116.9311440679357
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6440116765350103,acc:0.821899985074997
total cost energy:7.562496489063195 | all_enery_cp：5.9384999999999994 | all_enery_tp: 1.6239964890631948
ef: 24.993289817554228
reward: 17.430793328491035
step 100:loss:165.07627868652344|running q:36.5644645690918
episode1,iteration40 selected nodes:[5, 10, 1, 8, 6],center node:6
################################################## episode1,iteration40 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.2633564133635339,train_acc:0.9088969826698303
node1 epoch1:node_model train_loss:0.14196273528368158,train_acc:0.9535295963287354
node1 epoch2:node_model train_loss:0.1100075226277113,train_acc:0.9657352566719055
node1 epoch3:node_model train_loss:0.11076558069052066,train_acc:0.9663234949111938
node1 epoch4:node_model train_loss:0.09881960556787603,train_acc:0.9683824181556702
node1_model on test-dataset: loss:0.7980814188718796,acc:0.7870998978614807
node1 weight score:8405.157470627533
node5: train data size:3735
node5 epoch0:node_model train_loss:0.1758000334038546,train_acc:0.9373683929443359
node5 epoch1:node_model train_loss:0.09858707897365093,train_acc:0.9674060344696045
node5 epoch2:node_model train_loss:0.07712099887430668,train_acc:0.9816916584968567
node5 epoch3:node_model train_loss:0.0642218193235366,train_acc:0.9840601682662964
node5 epoch4:node_model train_loss:0.05945776770577619,train_acc:0.9863532185554504
node5_model on test-dataset: loss:0.8074214164912701,acc:0.7909001111984253
node5 weight score:4625.83716967877
node6: train data size:3007
node6 epoch0:node_model train_loss:0.48993449441848264,train_acc:0.8536865711212158
node6 epoch1:node_model train_loss:0.25653473137607496,train_acc:0.9083870649337769
node6 epoch2:node_model train_loss:0.1728512844251048,train_acc:0.9451612830162048
node6 epoch3:node_model train_loss:0.11558843079593874,train_acc:0.9661288261413574
node6 epoch4:node_model train_loss:0.0843280213734796,train_acc:0.9787094593048096
node6_model on test-dataset: loss:0.7945537249743938,acc:0.787800133228302
node6 weight score:3784.514382708239
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5535468127992418,train_acc:0.823684811592102
node8 epoch1:node_model train_loss:0.24676298018958834,train_acc:0.9149318337440491
node8 epoch2:node_model train_loss:0.14882010594010353,train_acc:0.9543650150299072
node8 epoch3:node_model train_loss:0.09451128728687763,train_acc:0.9788547158241272
node8 epoch4:node_model train_loss:0.053558152065508895,train_acc:0.9900000095367432
node8_model on test-dataset: loss:0.7231182813644409,acc:0.7952001094818115
node8 weight score:2486.4535254279303
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3939907103776932,train_acc:0.8738333582878113
node10 epoch1:node_model train_loss:0.20916634872555734,train_acc:0.9299999475479126
node10 epoch2:node_model train_loss:0.11653140559792519,train_acc:0.9659997820854187
node10 epoch3:node_model train_loss:0.06930079655721784,train_acc:0.9809998869895935
node10 epoch4:node_model train_loss:0.0444979795254767,train_acc:0.9924999475479126
node10_model on test-dataset: loss:0.7555338636040687,acc:0.7921999096870422
node10 weight score:2614.045637317697
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6347225829958916,acc:0.8219999837875366
total cost energy:10.145008749109257 | all_enery_cp：8.6115 | all_enery_tp: 1.5335087491092576
ef: 25.24373402463229
reward: 15.098725275523032
step 101:loss:214.43421936035156|running q:37.411800384521484
episode1,iteration41 selected nodes:[9, 0, 12, 13, 18],center node:9
################################################## episode1,iteration41 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.39204940744317496,train_acc:0.8703011870384216
node0 epoch1:node_model train_loss:0.18625684010867888,train_acc:0.934837818145752
node0 epoch2:node_model train_loss:0.1399254695727275,train_acc:0.9548818469047546
node0 epoch3:node_model train_loss:0.10445495186230311,train_acc:0.9643441438674927
node0 epoch4:node_model train_loss:0.07534569046961573,train_acc:0.9814204573631287
node0_model on test-dataset: loss:0.7669621585309505,acc:0.7908000946044922
node0 weight score:6757.830151526103
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5591188371181488,train_acc:0.829713761806488
node9 epoch1:node_model train_loss:0.30018367814390284,train_acc:0.9060388207435608
node9 epoch2:node_model train_loss:0.1715533945121263,train_acc:0.9398522973060608
node9 epoch3:node_model train_loss:0.12441433338742507,train_acc:0.9609140157699585
node9 epoch4:node_model train_loss:0.09015115546552759,train_acc:0.9786795377731323
node9_model on test-dataset: loss:0.7839691193401813,acc:0.7861998081207275
node9 weight score:2368.7157493689583
node12: train data size:1336
node12 epoch0:node_model train_loss:0.513675434248788,train_acc:0.8419048190116882
node12 epoch1:node_model train_loss:0.3003933908683913,train_acc:0.9007936716079712
node12 epoch2:node_model train_loss:0.1533558964729309,train_acc:0.9431746602058411
node12 epoch3:node_model train_loss:0.1159979695720332,train_acc:0.9630158543586731
node12 epoch4:node_model train_loss:0.08168692540909563,train_acc:0.9787300825119019
node12_model on test-dataset: loss:0.8011119960248471,acc:0.7843001484870911
node12 weight score:1667.681930403353
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6859074085950851,train_acc:0.8087121844291687
node13 epoch1:node_model train_loss:0.31591808671752614,train_acc:0.8975757956504822
node13 epoch2:node_model train_loss:0.22255183073381582,train_acc:0.9303029775619507
node13 epoch3:node_model train_loss:0.13412275413672128,train_acc:0.9583333730697632
node13 epoch4:node_model train_loss:0.08304842747747898,train_acc:0.9811362624168396
node13_model on test-dataset: loss:0.7967155183851719,acc:0.7837998867034912
node13 weight score:1449.7018990429851
node18: train data size:472
node18 epoch0:node_model train_loss:0.6619760870933533,train_acc:0.8107777833938599
node18 epoch1:node_model train_loss:0.2981224149465561,train_acc:0.9001111388206482
node18 epoch2:node_model train_loss:0.16782640665769577,train_acc:0.9552222490310669
node18 epoch3:node_model train_loss:0.08144191950559616,train_acc:0.9744443893432617
node18 epoch4:node_model train_loss:0.056761538982391356,train_acc:0.9892222285270691
node18_model on test-dataset: loss:0.8554709228873253,acc:0.7724002599716187
node18 weight score:551.7428908126285
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6212985280156136,acc:0.8237999802827836
total cost energy:7.335689378374448 | all_enery_cp：5.001499999999999 | all_enery_tp: 2.3341893783744485
ef: 24.526186112101662
reward: 17.190496733727215
step 102:loss:128.09912109375|running q:38.23661804199219
episode1,iteration42 selected nodes:[9, 17, 1, 10, 15],center node:9
################################################## episode1,iteration42 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.20327163082273567,train_acc:0.928896963596344
node1 epoch1:node_model train_loss:0.1550118430134128,train_acc:0.9515441656112671
node1 epoch2:node_model train_loss:0.12960395041634054,train_acc:0.9599998593330383
node1 epoch3:node_model train_loss:0.07490971019518945,train_acc:0.9792647957801819
node1 epoch4:node_model train_loss:0.057916497921242434,train_acc:0.9855884313583374
node1_model on test-dataset: loss:0.8161637677252292,acc:0.7938000559806824
node1 weight score:8218.938729289835
node9: train data size:1857
node9 epoch0:node_model train_loss:0.44894896682940033,train_acc:0.8611634969711304
node9 epoch1:node_model train_loss:0.2680987903946324,train_acc:0.9089381694793701
node9 epoch2:node_model train_loss:0.1708084237026541,train_acc:0.9402585625648499
node9 epoch3:node_model train_loss:0.10227032809665329,train_acc:0.9717081189155579
node9 epoch4:node_model train_loss:0.05999651374785524,train_acc:0.9847368001937866
node9_model on test-dataset: loss:0.7847458653151989,acc:0.790600061416626
node9 weight score:2366.3711808843013
node10: train data size:1975
node10 epoch0:node_model train_loss:0.40500076562166215,train_acc:0.8663334250450134
node10 epoch1:node_model train_loss:0.18152266517281532,train_acc:0.9401665925979614
node10 epoch2:node_model train_loss:0.11089639905840158,train_acc:0.9683332443237305
node10 epoch3:node_model train_loss:0.0863913333043456,train_acc:0.9764999747276306
node10 epoch4:node_model train_loss:0.046834496594965455,train_acc:0.9874998927116394
node10_model on test-dataset: loss:0.7836024686694145,acc:0.7889999747276306
node10 weight score:2520.4106405555635
node15: train data size:629
node15 epoch0:node_model train_loss:0.7415782340935299,train_acc:0.7977339625358582
node15 epoch1:node_model train_loss:0.33436818633760723,train_acc:0.8945813775062561
node15 epoch2:node_model train_loss:0.19262456681047166,train_acc:0.935862123966217
node15 epoch3:node_model train_loss:0.1914229957120759,train_acc:0.9472906589508057
node15 epoch4:node_model train_loss:0.06664226789559637,train_acc:0.9836452603340149
node15_model on test-dataset: loss:0.8699865940213204,acc:0.7790997624397278
node15 weight score:722.9996465722384
node17: train data size:442
node17 epoch0:node_model train_loss:0.6624483168125153,train_acc:0.8219047784805298
node17 epoch1:node_model train_loss:0.3849108010530472,train_acc:0.9064761996269226
node17 epoch2:node_model train_loss:0.24240694046020508,train_acc:0.9337143301963806
node17 epoch3:node_model train_loss:0.14539460092782974,train_acc:0.958476185798645
node17 epoch4:node_model train_loss:0.08108659610152244,train_acc:0.9799999594688416
node17_model on test-dataset: loss:0.8424144479632377,acc:0.774199903011322
node17 weight score:524.6823592219403
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6599206183105707,acc:0.8174999809265137
total cost energy:7.6346267864660335 | all_enery_cp：5.805499999999999 | all_enery_tp: 1.829126786466034
ef: 24.594630772733204
reward: 16.96000398626717
step 103:loss:129.82305908203125|running q:39.356422424316406
episode1,iteration43 selected nodes:[5, 12, 0, 18, 4],center node:5
################################################## episode1,iteration43 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.2653594601612825,train_acc:0.9045713543891907
node0 epoch1:node_model train_loss:0.13657786718641335,train_acc:0.9534173607826233
node0 epoch2:node_model train_loss:0.09543235284777787,train_acc:0.9718442559242249
node0 epoch3:node_model train_loss:0.0629808260486103,train_acc:0.9839599132537842
node0 epoch4:node_model train_loss:0.05303715058387472,train_acc:0.9861146211624146
node0_model on test-dataset: loss:0.7895157793909311,acc:0.8005998134613037
node0 weight score:6564.783295399625
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5577633668269429,train_acc:0.8371428847312927
node4 epoch1:node_model train_loss:0.3442842084914446,train_acc:0.8882142901420593
node4 epoch2:node_model train_loss:0.24360068475029298,train_acc:0.9182144403457642
node4 epoch3:node_model train_loss:0.14553421150360787,train_acc:0.9524999260902405
node4 epoch4:node_model train_loss:0.09789396802495633,train_acc:0.9696426391601562
node4_model on test-dataset: loss:0.7819466349482537,acc:0.7865996956825256
node4 weight score:3459.315353635363
node5: train data size:3735
node5 epoch0:node_model train_loss:0.28611282788609205,train_acc:0.8930074572563171
node5 epoch1:node_model train_loss:0.14730162487218254,train_acc:0.9495112895965576
node5 epoch2:node_model train_loss:0.08841053190592088,train_acc:0.9710898995399475
node5 epoch3:node_model train_loss:0.05677257738027133,train_acc:0.9868420362472534
node5 epoch4:node_model train_loss:0.04330608211947899,train_acc:0.9923684000968933
node5_model on test-dataset: loss:0.7502573969960212,acc:0.8025000095367432
node5 weight score:4978.291470306966
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5254472919872829,train_acc:0.8388890027999878
node12 epoch1:node_model train_loss:0.2736980633011886,train_acc:0.9094444513320923
node12 epoch2:node_model train_loss:0.1432954440159457,train_acc:0.9517460465431213
node12 epoch3:node_model train_loss:0.11268838495016098,train_acc:0.9626190066337585
node12 epoch4:node_model train_loss:0.08236698154360056,train_acc:0.9817459583282471
node12_model on test-dataset: loss:0.8249605844914913,acc:0.784899890422821
node12 weight score:1619.471311860936
node18: train data size:472
node18 epoch0:node_model train_loss:0.6482464432716369,train_acc:0.8027777671813965
node18 epoch1:node_model train_loss:0.23869725912809373,train_acc:0.9292222261428833
node18 epoch2:node_model train_loss:0.1819415032863617,train_acc:0.9505555033683777
node18 epoch3:node_model train_loss:0.0909307949244976,train_acc:0.9732222557067871
node18 epoch4:node_model train_loss:0.06290930584073066,train_acc:0.9852222800254822
node18_model on test-dataset: loss:0.9181842289119959,acc:0.7671999335289001
node18 weight score:514.0580562566373
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6358799750357866,acc:0.8245999836921691
total cost energy:9.144561226915837 | all_enery_cp：6.7155 | all_enery_tp: 2.4290612269158363
ef: 24.6642109358348
reward: 15.519649708918962
step 104:loss:114.3670425415039|running q:40.14828872680664
episode1,iteration44 selected nodes:[2, 17, 5, 9, 14],center node:9
################################################## episode1,iteration44 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.36431798587242764,train_acc:0.8736743927001953
node2 epoch1:node_model train_loss:0.17128738605727753,train_acc:0.940577507019043
node2 epoch2:node_model train_loss:0.1194443351123482,train_acc:0.9617611765861511
node2 epoch3:node_model train_loss:0.07934188524571557,train_acc:0.9785417318344116
node2 epoch4:node_model train_loss:0.061712746935275696,train_acc:0.9854828715324402
node2_model on test-dataset: loss:0.7627510644495488,acc:0.7913001179695129
node2 weight score:6277.277375489912
node5: train data size:3735
node5 epoch0:node_model train_loss:0.17833172333867928,train_acc:0.9408646821975708
node5 epoch1:node_model train_loss:0.10684404482966975,train_acc:0.9674811959266663
node5 epoch2:node_model train_loss:0.06300298046124608,train_acc:0.9821426868438721
node5 epoch3:node_model train_loss:0.04516315514123753,train_acc:0.988947331905365
node5 epoch4:node_model train_loss:0.04014139850378821,train_acc:0.9915788769721985
node5_model on test-dataset: loss:0.8172093835473061,acc:0.7911999225616455
node5 weight score:4570.432101241029
node9: train data size:1857
node9 epoch0:node_model train_loss:0.47980590870505885,train_acc:0.8468328714370728
node9 epoch1:node_model train_loss:0.2667627307145219,train_acc:0.9095937013626099
node9 epoch2:node_model train_loss:0.1686984422175508,train_acc:0.9492059350013733
node9 epoch3:node_model train_loss:0.09915944580969058,train_acc:0.968153178691864
node9 epoch4:node_model train_loss:0.06697854478108256,train_acc:0.9831578135490417
node9_model on test-dataset: loss:0.8195317690074444,acc:0.7866999506950378
node9 weight score:2265.9280216178313
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5696227836112181,train_acc:0.835972249507904
node14 epoch1:node_model train_loss:0.33114274467031163,train_acc:0.8932408094406128
node14 epoch2:node_model train_loss:0.1520351984848579,train_acc:0.9456944465637207
node14 epoch3:node_model train_loss:0.1084689541409413,train_acc:0.9698610305786133
node14 epoch4:node_model train_loss:0.07785616768524051,train_acc:0.9805091619491577
node14_model on test-dataset: loss:0.7915383574366569,acc:0.7937999367713928
node14 weight score:1480.6610304969201
node17: train data size:442
node17 epoch0:node_model train_loss:0.7768767356872559,train_acc:0.8051428198814392
node17 epoch1:node_model train_loss:0.3895363867282867,train_acc:0.8946666717529297
node17 epoch2:node_model train_loss:0.16569675952196122,train_acc:0.9504761695861816
node17 epoch3:node_model train_loss:0.13227370232343674,train_acc:0.9639999270439148
node17 epoch4:node_model train_loss:0.064842239767313,train_acc:0.9860000014305115
node17_model on test-dataset: loss:0.8586055894196033,acc:0.7736998796463013
node17 weight score:514.7881698496528
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.656298375055194,acc:0.8212999826669694
total cost energy:7.429690062720235 | all_enery_cp：5.997 | all_enery_tp: 1.4326900627202352
ef: 24.907510359172367
reward: 17.47782029645213
step 105:loss:164.90040588378906|running q:41.15520095825195
episode1,iteration45 selected nodes:[0, 7, 8, 16, 17],center node:7
################################################## episode1,iteration45 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.18741837416130763,train_acc:0.9353405237197876
node0 epoch1:node_model train_loss:0.10531517247167918,train_acc:0.9680721163749695
node0 epoch2:node_model train_loss:0.07736689547220102,train_acc:0.9752271175384521
node0 epoch3:node_model train_loss:0.04918211401225282,train_acc:0.9898079037666321
node0 epoch4:node_model train_loss:0.047257496133589975,train_acc:0.9894232153892517
node0_model on test-dataset: loss:0.8504968991875649,acc:0.7847999930381775
node0 weight score:6094.084534524521
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5492005705833435,train_acc:0.8321568369865417
node7 epoch1:node_model train_loss:0.28333461508154867,train_acc:0.9030391573905945
node7 epoch2:node_model train_loss:0.17785632200539112,train_acc:0.9480589032173157
node7 epoch3:node_model train_loss:0.12495245058089495,train_acc:0.9675195813179016
node7 epoch4:node_model train_loss:0.0927800815552473,train_acc:0.9740586280822754
node7_model on test-dataset: loss:0.7971926268190146,acc:0.7886999249458313
node7 weight score:2447.338239673574
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5721518248319626,train_acc:0.8336847424507141
node8 epoch1:node_model train_loss:0.27487892740302616,train_acc:0.9043196439743042
node8 epoch2:node_model train_loss:0.16533063848813376,train_acc:0.9504875540733337
node8 epoch3:node_model train_loss:0.11471808474096987,train_acc:0.966054379940033
node8 epoch4:node_model train_loss:0.06519149109307262,train_acc:0.9849658608436584
node8_model on test-dataset: loss:0.7905389139056206,acc:0.7870000004768372
node8 weight score:2274.3978422479736
node16: train data size:877
node16 epoch0:node_model train_loss:0.600437343120575,train_acc:0.8176911473274231
node16 epoch1:node_model train_loss:0.31830265124638873,train_acc:0.8963491916656494
node16 epoch2:node_model train_loss:0.16944491035408443,train_acc:0.9578931927680969
node16 epoch3:node_model train_loss:0.10034500973092185,train_acc:0.9745597839355469
node16 epoch4:node_model train_loss:0.04514314917226633,train_acc:0.9900000095367432
node16_model on test-dataset: loss:0.8042962943017483,acc:0.7874998450279236
node16 weight score:1090.3941821109217
node17: train data size:442
node17 epoch0:node_model train_loss:0.6466319739818573,train_acc:0.8071428537368774
node17 epoch1:node_model train_loss:0.378622955083847,train_acc:0.8861904144287109
node17 epoch2:node_model train_loss:0.16111161112785338,train_acc:0.9517143368721008
node17 epoch3:node_model train_loss:0.12585859894752502,train_acc:0.9552380442619324
node17 epoch4:node_model train_loss:0.09414152130484581,train_acc:0.974476158618927
node17_model on test-dataset: loss:0.9169396936893464,acc:0.7653000354766846
node17 weight score:482.0382442182145
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6542944851517677,acc:0.820599980354309
total cost energy:7.021132027572687 | all_enery_cp：5.125500000000001 | all_enery_tp: 1.8956320275726868
ef: 24.579733341739974
reward: 17.558601314167287
step 106:loss:158.7974090576172|running q:42.095008850097656
episode1,iteration46 selected nodes:[3, 12, 1, 14, 8],center node:3
################################################## episode1,iteration46 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.2056225679047844,train_acc:0.9306617379188538
node1 epoch1:node_model train_loss:0.1461928583243314,train_acc:0.9492645263671875
node1 epoch2:node_model train_loss:0.13643359080614412,train_acc:0.9511763453483582
node1 epoch3:node_model train_loss:0.06838098030044314,train_acc:0.9802941679954529
node1 epoch4:node_model train_loss:0.06371306060977719,train_acc:0.9826472997665405
node1_model on test-dataset: loss:0.7957399015128612,acc:0.7962998151779175
node1 weight score:8429.890203126357
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3283042503997337,train_acc:0.8868876695632935
node3 epoch1:node_model train_loss:0.16614237150480582,train_acc:0.941073477268219
node3 epoch2:node_model train_loss:0.10213448514425477,train_acc:0.968109667301178
node3 epoch3:node_model train_loss:0.06631822723808677,train_acc:0.9813655614852905
node3 epoch4:node_model train_loss:0.052928454567526664,train_acc:0.9883722066879272
node3_model on test-dataset: loss:0.768915757983923,acc:0.7974000573158264
node3 weight score:5523.36189745353
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4309351411130693,train_acc:0.8665080070495605
node8 epoch1:node_model train_loss:0.2256788176794847,train_acc:0.9137754440307617
node8 epoch2:node_model train_loss:0.15362078116999733,train_acc:0.9505101442337036
node8 epoch3:node_model train_loss:0.1060236141913467,train_acc:0.9632992148399353
node8 epoch4:node_model train_loss:0.06256957755734523,train_acc:0.9827550053596497
node8_model on test-dataset: loss:0.7862704136967659,acc:0.7876999974250793
node8 weight score:2286.745079910153
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5401721043246133,train_acc:0.8330159783363342
node12 epoch1:node_model train_loss:0.2570393000330244,train_acc:0.921349287033081
node12 epoch2:node_model train_loss:0.1169847452214786,train_acc:0.9674602746963501
node12 epoch3:node_model train_loss:0.10233855194279126,train_acc:0.9730159044265747
node12 epoch4:node_model train_loss:0.06428277066775731,train_acc:0.9835713505744934
node12_model on test-dataset: loss:0.8310732929408551,acc:0.7888999581336975
node12 weight score:1607.5597800434662
node14: train data size:1172
node14 epoch0:node_model train_loss:0.49632469316323596,train_acc:0.8545833826065063
node14 epoch1:node_model train_loss:0.2335836204389731,train_acc:0.9100462794303894
node14 epoch2:node_model train_loss:0.1618394088000059,train_acc:0.9530091285705566
node14 epoch3:node_model train_loss:0.09468514084195097,train_acc:0.96833336353302
node14 epoch4:node_model train_loss:0.050695950320611395,train_acc:0.9871758222579956
node14_model on test-dataset: loss:0.768659470975399,acc:0.7921001315116882
node14 weight score:1524.73240004807
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6494930092990399,acc:0.8236999827623367
total cost energy:9.326789298534637 | all_enery_cp：7.6305000000000005 | all_enery_tp: 1.6962892985346365
ef: 25.028866381962686
reward: 15.70207708342805
step 107:loss:102.65953063964844|running q:42.936187744140625
episode1,iteration47 selected nodes:[11, 1, 16, 14, 0],center node:11
################################################## episode1,iteration47 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.16196512717467088,train_acc:0.9439548850059509
node0 epoch1:node_model train_loss:0.09060634249964586,train_acc:0.9701133966445923
node0 epoch2:node_model train_loss:0.06356455833436205,train_acc:0.9810751080513
node0 epoch3:node_model train_loss:0.053746804397983045,train_acc:0.9864599704742432
node0 epoch4:node_model train_loss:0.04203324062893024,train_acc:0.9901530742645264
node0_model on test-dataset: loss:0.8665518254041672,acc:0.7905998826026917
node0 weight score:5981.177176082462
node1: train data size:6708
node1 epoch0:node_model train_loss:0.13900263108970487,train_acc:0.9497792720794678
node1 epoch1:node_model train_loss:0.11946863752296742,train_acc:0.961103081703186
node1 epoch2:node_model train_loss:0.11006934212192017,train_acc:0.9644117951393127
node1 epoch3:node_model train_loss:0.0518216068363365,train_acc:0.9852943420410156
node1 epoch4:node_model train_loss:0.04603888447779943,train_acc:0.9895589351654053
node1_model on test-dataset: loss:0.8026616044342518,acc:0.7928000688552856
node1 weight score:8357.195564036065
node11: train data size:1682
node11 epoch0:node_model train_loss:0.537319094819181,train_acc:0.8451219797134399
node11 epoch1:node_model train_loss:0.27943720361765695,train_acc:0.9127545952796936
node11 epoch2:node_model train_loss:0.14970167003133716,train_acc:0.9562122225761414
node11 epoch3:node_model train_loss:0.09848800520686542,train_acc:0.9734002351760864
node11 epoch4:node_model train_loss:0.06335179332424612,train_acc:0.9882351160049438
node11_model on test-dataset: loss:0.863121389746666,acc:0.7822999358177185
node11 weight score:1948.7409534523092
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4754680668314298,train_acc:0.8451389670372009
node14 epoch1:node_model train_loss:0.23656273633241653,train_acc:0.9133796095848083
node14 epoch2:node_model train_loss:0.12789650509754816,train_acc:0.9580091834068298
node14 epoch3:node_model train_loss:0.09523596552511056,train_acc:0.9760185480117798
node14 epoch4:node_model train_loss:0.057950674556195736,train_acc:0.9848610162734985
node14_model on test-dataset: loss:0.8082940080761909,acc:0.7867001295089722
node14 weight score:1449.967447846682
node16: train data size:877
node16 epoch0:node_model train_loss:0.5730217397212982,train_acc:0.8370130658149719
node16 epoch1:node_model train_loss:0.24394885864522722,train_acc:0.9040115475654602
node16 epoch2:node_model train_loss:0.16396230790350172,train_acc:0.9531167149543762
node16 epoch3:node_model train_loss:0.08781515599952804,train_acc:0.9741125106811523
node16 epoch4:node_model train_loss:0.04625054531627231,train_acc:0.9904472827911377
node16_model on test-dataset: loss:0.7856789824366569,acc:0.7900999188423157
node16 weight score:1116.2319720964479
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6559857827425003,acc:0.8233999806642532
total cost energy:9.392913190966077 | all_enery_cp：7.811000000000001 | all_enery_tp: 1.5819131909660762
ef: 25.04345458630331
reward: 15.650541395337232
step 108:loss:55.5058708190918|running q:43.85026550292969
episode1,iteration48 selected nodes:[7, 13, 3, 10, 2],center node:7
################################################## episode1,iteration48 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3039530549819271,train_acc:0.8965244293212891
node2 epoch1:node_model train_loss:0.16077668437113365,train_acc:0.941969633102417
node2 epoch2:node_model train_loss:0.10162242129445076,train_acc:0.968636155128479
node2 epoch3:node_model train_loss:0.07039954575399558,train_acc:0.9822632074356079
node2 epoch4:node_model train_loss:0.05691671057138592,train_acc:0.9845267534255981
node2_model on test-dataset: loss:0.7926305714249611,acc:0.7933999300003052
node2 weight score:6040.645128527298
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2840985437465269,train_acc:0.9005193710327148
node3 epoch1:node_model train_loss:0.13480248874010042,train_acc:0.9536317586898804
node3 epoch2:node_model train_loss:0.09988960424481436,train_acc:0.9664522409439087
node3 epoch3:node_model train_loss:0.06272013983580955,train_acc:0.9818011522293091
node3 epoch4:node_model train_loss:0.045863301471568814,train_acc:0.9888370633125305
node3_model on test-dataset: loss:0.8532987323403358,acc:0.7910000681877136
node3 weight score:4977.154938870923
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5582964897155762,train_acc:0.84063720703125
node7 epoch1:node_model train_loss:0.2557147353887558,train_acc:0.9131175875663757
node7 epoch2:node_model train_loss:0.14677598252892493,train_acc:0.9460588693618774
node7 epoch3:node_model train_loss:0.11422054637223482,train_acc:0.9665783047676086
node7 epoch4:node_model train_loss:0.0650075115263462,train_acc:0.9820391535758972
node7_model on test-dataset: loss:0.8481768502295017,acc:0.7848998308181763
node7 weight score:2300.22783511728
node10: train data size:1975
node10 epoch0:node_model train_loss:0.45730952024459837,train_acc:0.8618332743644714
node10 epoch1:node_model train_loss:0.23260200209915638,train_acc:0.9201666116714478
node10 epoch2:node_model train_loss:0.14996813274919987,train_acc:0.952833354473114
node10 epoch3:node_model train_loss:0.07918156664818525,train_acc:0.9789999127388
node10 epoch4:node_model train_loss:0.062368853110820056,train_acc:0.985333263874054
node10_model on test-dataset: loss:0.8006639422476292,acc:0.7909999489784241
node10 weight score:2466.702814736189
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7071741496523222,train_acc:0.8127273321151733
node13 epoch1:node_model train_loss:0.33631839354832965,train_acc:0.8971211314201355
node13 epoch2:node_model train_loss:0.19774881874521574,train_acc:0.9364393949508667
node13 epoch3:node_model train_loss:0.11766923653582732,train_acc:0.9581060409545898
node13 epoch4:node_model train_loss:0.07061791668335597,train_acc:0.9808332324028015
node13_model on test-dataset: loss:0.84926533639431,acc:0.780299723148346
node13 weight score:1359.9989903081816
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6447536310553551,acc:0.8238999813795089
total cost energy:8.593240315813112 | all_enery_cp：7.058 | all_enery_tp: 1.5352403158131132
ef: 25.041158756952765
reward: 16.447918441139652
step 109:loss:105.14667510986328|running q:44.793575286865234
episode1,iteration49 selected nodes:[9, 10, 17, 14, 8],center node:10
################################################## episode1,iteration49 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.48445357879002887,train_acc:0.8465192914009094
node8 epoch1:node_model train_loss:0.25111281209521824,train_acc:0.9126530885696411
node8 epoch2:node_model train_loss:0.14832151391439968,train_acc:0.954433023929596
node8 epoch3:node_model train_loss:0.08993807207379076,train_acc:0.9782991409301758
node8 epoch4:node_model train_loss:0.06396786734047863,train_acc:0.9872108101844788
node8_model on test-dataset: loss:0.8004566496610641,acc:0.7927998304367065
node8 weight score:2246.217831735577
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5653500258922577,train_acc:0.8395845293998718
node9 epoch1:node_model train_loss:0.25520642376259756,train_acc:0.913933515548706
node9 epoch2:node_model train_loss:0.16924717669424258,train_acc:0.9426131248474121
node9 epoch3:node_model train_loss:0.1011645325312489,train_acc:0.9684116840362549
node9 epoch4:node_model train_loss:0.07923856740327258,train_acc:0.9768419861793518
node9_model on test-dataset: loss:0.826493462100625,acc:0.7870001196861267
node9 weight score:2246.8417297339874
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3777698062360287,train_acc:0.8806666731834412
node10 epoch1:node_model train_loss:0.1854463718831539,train_acc:0.9393333792686462
node10 epoch2:node_model train_loss:0.09851198550313711,train_acc:0.9723332524299622
node10 epoch3:node_model train_loss:0.07621006909757852,train_acc:0.9806666374206543
node10 epoch4:node_model train_loss:0.04329094551503658,train_acc:0.9864999055862427
node10_model on test-dataset: loss:0.8020718365907669,acc:0.7918998599052429
node10 weight score:2462.3729570094415
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5178894723455111,train_acc:0.8510648608207703
node14 epoch1:node_model train_loss:0.24393612829347452,train_acc:0.9163889288902283
node14 epoch2:node_model train_loss:0.12843871427079043,train_acc:0.959861159324646
node14 epoch3:node_model train_loss:0.08457803819328547,train_acc:0.9774999618530273
node14 epoch4:node_model train_loss:0.043964858477314316,train_acc:0.9880092144012451
node14_model on test-dataset: loss:0.8010115233063698,acc:0.7956000566482544
node14 weight score:1463.1499871091553
node17: train data size:442
node17 epoch0:node_model train_loss:0.725477010011673,train_acc:0.8306666612625122
node17 epoch1:node_model train_loss:0.31146426796913146,train_acc:0.9137142300605774
node17 epoch2:node_model train_loss:0.16867555305361748,train_acc:0.9639999270439148
node17 epoch3:node_model train_loss:0.11686752885580062,train_acc:0.9632380604743958
node17 epoch4:node_model train_loss:0.08380754739046097,train_acc:0.9792380332946777
node17_model on test-dataset: loss:0.8789686293900013,acc:0.7820996642112732
node17 weight score:502.8620877024305
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6353218913078308,acc:0.8222999805212021
total cost energy:5.099995987511004 | all_enery_cp：3.622 | all_enery_tp: 1.477995987511004
ef: 24.704517328712072
reward: 19.604521341201067
step 110:loss:166.9000244140625|running q:45.999542236328125
episode1,iteration50 selected nodes:[12, 14, 3, 7, 2],center node:7
################################################## episode1,iteration50 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.21440308339272937,train_acc:0.9210795760154724
node2 epoch1:node_model train_loss:0.10372824229610463,train_acc:0.9695550203323364
node2 epoch2:node_model train_loss:0.0643396689556539,train_acc:0.9815813899040222
node2 epoch3:node_model train_loss:0.04972713126335293,train_acc:0.9872633218765259
node2 epoch4:node_model train_loss:0.041775725316256285,train_acc:0.9914583563804626
node2_model on test-dataset: loss:0.8064194369316101,acc:0.79010009765625
node2 weight score:5937.356889881184
node3: train data size:4247
node3 epoch0:node_model train_loss:0.19838541295639303,train_acc:0.927674412727356
node3 epoch1:node_model train_loss:0.10364478462657263,train_acc:0.9673230051994324
node3 epoch2:node_model train_loss:0.07425285312671993,train_acc:0.9783125519752502
node3 epoch3:node_model train_loss:0.06435705535113811,train_acc:0.9820632934570312
node3 epoch4:node_model train_loss:0.042833138863707695,train_acc:0.9892725348472595
node3_model on test-dataset: loss:0.8510020552575588,acc:0.793999969959259
node3 weight score:4990.587242136131
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4346810981631279,train_acc:0.8576175570487976
node7 epoch1:node_model train_loss:0.22282208725810052,train_acc:0.9220784306526184
node7 epoch2:node_model train_loss:0.12893721051514148,train_acc:0.9585587382316589
node7 epoch3:node_model train_loss:0.08999454434961081,train_acc:0.9764998555183411
node7 epoch4:node_model train_loss:0.05733587397262454,train_acc:0.988019585609436
node7_model on test-dataset: loss:0.8054707805812359,acc:0.791999876499176
node7 weight score:2422.185940242474
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5530899805682046,train_acc:0.8268253803253174
node12 epoch1:node_model train_loss:0.2659563049674034,train_acc:0.9160318970680237
node12 epoch2:node_model train_loss:0.17200543146048272,train_acc:0.9373016357421875
node12 epoch3:node_model train_loss:0.1158975176513195,train_acc:0.9730158448219299
node12 epoch4:node_model train_loss:0.05598111330930676,train_acc:0.98714280128479
node12_model on test-dataset: loss:0.8166784031689167,acc:0.7905998229980469
node12 weight score:1635.8948575301922
node14: train data size:1172
node14 epoch0:node_model train_loss:0.38096727927525836,train_acc:0.877731442451477
node14 epoch1:node_model train_loss:0.19465971613923708,train_acc:0.9348609447479248
node14 epoch2:node_model train_loss:0.1284152598430713,train_acc:0.9562036991119385
node14 epoch3:node_model train_loss:0.06450467510148883,train_acc:0.9833332896232605
node14 epoch4:node_model train_loss:0.03625334744962553,train_acc:0.9933333396911621
node14_model on test-dataset: loss:0.8319486825168133,acc:0.7842998504638672
node14 weight score:1408.740736813793
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.668598311021924,acc:0.8241999781131745
total cost energy:8.321433841025195 | all_enery_cp：6.747 | all_enery_tp: 1.5744338410251952
ef: 24.992884378807997
reward: 16.671450537782803
step 111:loss:105.65633392333984|running q:47.02553176879883
episode1,iteration51 selected nodes:[5, 4, 9, 8, 2],center node:9
################################################## episode1,iteration51 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.1322925831967344,train_acc:0.9568463563919067
node2 epoch1:node_model train_loss:0.0745466243242845,train_acc:0.9802083373069763
node2 epoch2:node_model train_loss:0.05537615871677796,train_acc:0.9850946664810181
node2 epoch3:node_model train_loss:0.04394743934972212,train_acc:0.989318311214447
node2 epoch4:node_model train_loss:0.03911171624592195,train_acc:0.9908334612846375
node2_model on test-dataset: loss:0.8444148440659046,acc:0.7861998081207275
node2 weight score:5670.198757929826
node4: train data size:2705
node4 epoch0:node_model train_loss:0.49402962092842373,train_acc:0.8542858362197876
node4 epoch1:node_model train_loss:0.2936207375356129,train_acc:0.897142767906189
node4 epoch2:node_model train_loss:0.17645769561308303,train_acc:0.9407143592834473
node4 epoch3:node_model train_loss:0.11398857539253575,train_acc:0.9682141542434692
node4 epoch4:node_model train_loss:0.09340294335769224,train_acc:0.9732142090797424
node4_model on test-dataset: loss:0.8310633844137192,acc:0.7923998832702637
node4 weight score:3254.8660556237423
node5: train data size:3735
node5 epoch0:node_model train_loss:0.27307075614991944,train_acc:0.9032331109046936
node5 epoch1:node_model train_loss:0.13852042920495333,train_acc:0.953721821308136
node5 epoch2:node_model train_loss:0.08447159772836849,train_acc:0.9744735360145569
node5 epoch3:node_model train_loss:0.06260683787006296,train_acc:0.981052577495575
node5 epoch4:node_model train_loss:0.04464391913068922,train_acc:0.9906014204025269
node5_model on test-dataset: loss:0.8018248927593231,acc:0.7953999638557434
node5 weight score:4658.124278415366
node8: train data size:1798
node8 epoch0:node_model train_loss:0.39294887002971435,train_acc:0.8843650817871094
node8 epoch1:node_model train_loss:0.19258172561724982,train_acc:0.937709629535675
node8 epoch2:node_model train_loss:0.12250620230204529,train_acc:0.9571767449378967
node8 epoch3:node_model train_loss:0.08114610860745113,train_acc:0.9788435101509094
node8 epoch4:node_model train_loss:0.04156488811390267,train_acc:0.9927777051925659
node8_model on test-dataset: loss:0.7907300531864166,acc:0.7933001518249512
node8 weight score:2273.8480632607457
node9: train data size:1857
node9 epoch0:node_model train_loss:0.41547974787260356,train_acc:0.870120108127594
node9 epoch1:node_model train_loss:0.2047605079255606,train_acc:0.9367035627365112
node9 epoch2:node_model train_loss:0.09970703799473613,train_acc:0.9690766930580139
node9 epoch3:node_model train_loss:0.07625176728163895,train_acc:0.9776269197463989
node9 epoch4:node_model train_loss:0.06287723956139464,train_acc:0.981181800365448
node9_model on test-dataset: loss:0.8037781444191933,acc:0.7923000454902649
node9 weight score:2310.3390069679745
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6512399261444807,acc:0.824399978518486
total cost energy:9.09474224396787 | all_enery_cp：7.4415 | all_enery_tp: 1.6532422439678705
ef: 25.134865428556427
reward: 16.04012318458856
step 112:loss:126.82567596435547|running q:48.075984954833984
episode1,iteration52 selected nodes:[5, 10, 15, 12, 7],center node:7
################################################## episode1,iteration52 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.1775922724290898,train_acc:0.9332329630851746
node5 epoch1:node_model train_loss:0.10139671095499866,train_acc:0.965112566947937
node5 epoch2:node_model train_loss:0.07653001029240458,train_acc:0.9779321551322937
node5 epoch3:node_model train_loss:0.05499536906810183,train_acc:0.9839848279953003
node5 epoch4:node_model train_loss:0.047169800431124474,train_acc:0.9879323244094849
node5_model on test-dataset: loss:0.8479584438353777,acc:0.7976999282836914
node5 weight score:4404.696983859638
node7: train data size:1951
node7 epoch0:node_model train_loss:0.40624946132302286,train_acc:0.8691176772117615
node7 epoch1:node_model train_loss:0.23486893270164727,train_acc:0.9289999008178711
node7 epoch2:node_model train_loss:0.14339870922267436,train_acc:0.9549999237060547
node7 epoch3:node_model train_loss:0.08033016510307789,train_acc:0.9815391898155212
node7 epoch4:node_model train_loss:0.050381010537967084,train_acc:0.9860588312149048
node7_model on test-dataset: loss:0.8464948435127735,acc:0.7847999930381775
node7 weight score:2304.798446147368
node10: train data size:1975
node10 epoch0:node_model train_loss:0.32613749876618386,train_acc:0.8938332796096802
node10 epoch1:node_model train_loss:0.16404504738748074,train_acc:0.9411665797233582
node10 epoch2:node_model train_loss:0.08512120600789785,train_acc:0.9743332266807556
node10 epoch3:node_model train_loss:0.06413500411435961,train_acc:0.9854999780654907
node10 epoch4:node_model train_loss:0.0370938200969249,train_acc:0.9944999814033508
node10_model on test-dataset: loss:0.7795819044113159,acc:0.7967999577522278
node10 weight score:2533.409240035372
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5096323426280703,train_acc:0.84634929895401
node12 epoch1:node_model train_loss:0.25293733179569244,train_acc:0.9167461395263672
node12 epoch2:node_model train_loss:0.16006913594901562,train_acc:0.9537301063537598
node12 epoch3:node_model train_loss:0.09627238794096879,train_acc:0.9742856621742249
node12 epoch4:node_model train_loss:0.05574019931788955,train_acc:0.9815872311592102
node12_model on test-dataset: loss:0.8489954015240073,acc:0.7900999784469604
node12 weight score:1573.6245421374306
node15: train data size:629
node15 epoch0:node_model train_loss:0.7353227393967765,train_acc:0.7999507784843445
node15 epoch1:node_model train_loss:0.3723255395889282,train_acc:0.8902956247329712
node15 epoch2:node_model train_loss:0.22586458708558763,train_acc:0.9280787706375122
node15 epoch3:node_model train_loss:0.138448992477996,train_acc:0.9587192535400391
node15 epoch4:node_model train_loss:0.06940139564020294,train_acc:0.9857142567634583
node15_model on test-dataset: loss:0.9246363365650176,acc:0.7751998901367188
node15 weight score:680.267446915083
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6700108159333468,acc:0.8223999798297882
total cost energy:6.708056670473574 | all_enery_cp：4.813000000000001 | all_enery_tp: 1.8950566704735732
ef: 24.644789021856376
reward: 17.9367323513828
step 113:loss:73.00908660888672|running q:49.00503921508789
episode1,iteration53 selected nodes:[10, 13, 19, 2, 9],center node:9
################################################## episode1,iteration53 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.12011994048953056,train_acc:0.9626514911651611
node2 epoch1:node_model train_loss:0.07310430825843166,train_acc:0.9763730764389038
node2 epoch2:node_model train_loss:0.04452011724545931,train_acc:0.9899717569351196
node2 epoch3:node_model train_loss:0.046379622033176325,train_acc:0.9874716997146606
node2 epoch4:node_model train_loss:0.04073322383919731,train_acc:0.9893466830253601
node2_model on test-dataset: loss:0.8514266705513001,acc:0.7890998721122742
node2 weight score:5623.502487771217
node9: train data size:1857
node9 epoch0:node_model train_loss:0.46922461139528376,train_acc:0.8594552278518677
node9 epoch1:node_model train_loss:0.21276047433677472,train_acc:0.9265651702880859
node9 epoch2:node_model train_loss:0.12591492149390673,train_acc:0.9585503339767456
node9 epoch3:node_model train_loss:0.07918346026226093,train_acc:0.976047933101654
node9 epoch4:node_model train_loss:0.04892012202426007,train_acc:0.9894735217094421
node9_model on test-dataset: loss:0.7982971107959748,acc:0.7927001714706421
node9 weight score:2326.201579445029
node10: train data size:1975
node10 epoch0:node_model train_loss:0.24023816734552383,train_acc:0.919999897480011
node10 epoch1:node_model train_loss:0.11122526675462723,train_acc:0.9633333086967468
node10 epoch2:node_model train_loss:0.06815889114513993,train_acc:0.9781665205955505
node10 epoch3:node_model train_loss:0.04618768906220794,train_acc:0.9898332953453064
node10 epoch4:node_model train_loss:0.033197160717099904,train_acc:0.994999885559082
node10_model on test-dataset: loss:0.8756554202735424,acc:0.7805999517440796
node10 weight score:2255.4534058420354
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6604607403278351,train_acc:0.8253788352012634
node13 epoch1:node_model train_loss:0.33644936854640645,train_acc:0.8850758075714111
node13 epoch2:node_model train_loss:0.17596354459722838,train_acc:0.9462878704071045
node13 epoch3:node_model train_loss:0.1127034816890955,train_acc:0.965833306312561
node13 epoch4:node_model train_loss:0.10722808012117942,train_acc:0.9694696664810181
node13_model on test-dataset: loss:0.8812012422084808,acc:0.7761998772621155
node13 weight score:1310.7108168678021
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6095129216826239,train_acc:0.8263624310493469
node19 epoch1:node_model train_loss:0.3185720776402673,train_acc:0.8952139019966125
node19 epoch2:node_model train_loss:0.20319347239510957,train_acc:0.9311914443969727
node19 epoch3:node_model train_loss:0.14237482819792835,train_acc:0.9525722861289978
node19 epoch4:node_model train_loss:0.1215642292139142,train_acc:0.9629141092300415
node19_model on test-dataset: loss:0.8237950006127357,acc:0.7807996869087219
node19 weight score:5196.681209300624
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6643270567804574,acc:0.8227999800443649
total cost energy:9.171309233436347 | all_enery_cp：7.028 | all_enery_tp: 2.143309233436347
ef: 24.855722169791182
reward: 15.684412936354835
step 114:loss:100.62792205810547|running q:49.91999816894531
episode1,iteration54 selected nodes:[1, 17, 19, 8, 12],center node:17
################################################## episode1,iteration54 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.18783008307218552,train_acc:0.9306616187095642
node1 epoch1:node_model train_loss:0.1042721384917112,train_acc:0.9631617069244385
node1 epoch2:node_model train_loss:0.10355653838418863,train_acc:0.9670588374137878
node1 epoch3:node_model train_loss:0.06797986620051019,train_acc:0.9769120216369629
node1 epoch4:node_model train_loss:0.13383294784409158,train_acc:0.9606617093086243
node1_model on test-dataset: loss:1.0102684373036026,acc:0.7545998096466064
node1 weight score:6639.819430470967
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3764529898762703,train_acc:0.8687868118286133
node8 epoch1:node_model train_loss:0.19628582729233635,train_acc:0.934965968132019
node8 epoch2:node_model train_loss:0.11361137156685193,train_acc:0.9644102454185486
node8 epoch3:node_model train_loss:0.07535363816552693,train_acc:0.9777662754058838
node8 epoch4:node_model train_loss:0.0508297270991736,train_acc:0.9871881604194641
node8_model on test-dataset: loss:0.8746300802752376,acc:0.7781999707221985
node8 weight score:2055.726232779676
node12: train data size:1336
node12 epoch0:node_model train_loss:0.4954774039132254,train_acc:0.8507936596870422
node12 epoch1:node_model train_loss:0.2403790642108236,train_acc:0.9188888669013977
node12 epoch2:node_model train_loss:0.13252481073141098,train_acc:0.9581745862960815
node12 epoch3:node_model train_loss:0.07648645075304168,train_acc:0.9757142066955566
node12 epoch4:node_model train_loss:0.05047836620360613,train_acc:0.9892856478691101
node12_model on test-dataset: loss:0.8847627572715282,acc:0.7876001596450806
node12 weight score:1510.009309297803
node17: train data size:442
node17 epoch0:node_model train_loss:0.8241231381893158,train_acc:0.8066666722297668
node17 epoch1:node_model train_loss:0.39457727074623106,train_acc:0.8957142233848572
node17 epoch2:node_model train_loss:0.16930150538682937,train_acc:0.9389523863792419
node17 epoch3:node_model train_loss:0.11950476467609406,train_acc:0.9764761924743652
node17 epoch4:node_model train_loss:0.08133265003561974,train_acc:0.984000027179718
node17_model on test-dataset: loss:0.8397487947344779,acc:0.7773000001907349
node17 weight score:526.3478825709502
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3549561048316401,train_acc:0.8788804411888123
node19 epoch1:node_model train_loss:0.2262003676489342,train_acc:0.9161297082901001
node19 epoch2:node_model train_loss:0.15614286169063213,train_acc:0.9496036767959595
node19 epoch3:node_model train_loss:0.11006100605740103,train_acc:0.9673469066619873
node19 epoch4:node_model train_loss:0.08556151156162106,train_acc:0.9758827090263367
node19_model on test-dataset: loss:0.7898293715715409,acc:0.7955000996589661
node19 weight score:5420.158016511845
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6720825795084238,acc:0.8218999797105789
total cost energy:9.388355788604262 | all_enery_cp：7.282500000000001 | all_enery_tp: 2.1058557886042606
ef: 24.857364798263518
reward: 15.469009009659256
step 115:loss:85.76106262207031|running q:50.82243347167969
episode1,iteration55 selected nodes:[10, 3, 12, 11, 1],center node:10
################################################## episode1,iteration55 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.11256899873671286,train_acc:0.9602941870689392
node1 epoch1:node_model train_loss:0.07436712157419499,train_acc:0.9770588874816895
node1 epoch2:node_model train_loss:0.08024486652849351,train_acc:0.9726471900939941
node1 epoch3:node_model train_loss:0.06591747555991306,train_acc:0.9805882573127747
node1 epoch4:node_model train_loss:0.04313803969553726,train_acc:0.988235592842102
node1_model on test-dataset: loss:0.8689494419097901,acc:0.7903998494148254
node1 weight score:7719.666618643609
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2746705359151197,train_acc:0.9026421904563904
node3 epoch1:node_model train_loss:0.11715049461223358,train_acc:0.9599700570106506
node3 epoch2:node_model train_loss:0.06347710132425607,train_acc:0.9797378182411194
node3 epoch3:node_model train_loss:0.052076320770348225,train_acc:0.985319197177887
node3 epoch4:node_model train_loss:0.03421973924390798,train_acc:0.9934587478637695
node3_model on test-dataset: loss:0.8076247090101242,acc:0.8029998540878296
node3 weight score:5258.630589949868
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3054006062448025,train_acc:0.893666684627533
node10 epoch1:node_model train_loss:0.1388706114143133,train_acc:0.9593332409858704
node10 epoch2:node_model train_loss:0.0835891792550683,train_acc:0.9728332757949829
node10 epoch3:node_model train_loss:0.04580493588000536,train_acc:0.9898332953453064
node10 epoch4:node_model train_loss:0.031527474103495476,train_acc:0.9944999814033508
node10_model on test-dataset: loss:0.7792990958690643,acc:0.8027001023292542
node10 weight score:2534.328617175547
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5051400310852948,train_acc:0.8502153158187866
node11 epoch1:node_model train_loss:0.24133300080018885,train_acc:0.9200717210769653
node11 epoch2:node_model train_loss:0.1567214350490009,train_acc:0.9546484351158142
node11 epoch3:node_model train_loss:0.07921167605501764,train_acc:0.9766713380813599
node11 epoch4:node_model train_loss:0.05196249627453439,train_acc:0.9876469969749451
node11_model on test-dataset: loss:0.8140923000872136,acc:0.7960999011993408
node11 weight score:2066.1047891250264
node12: train data size:1336
node12 epoch0:node_model train_loss:0.47290902052606854,train_acc:0.864762008190155
node12 epoch1:node_model train_loss:0.2060964336352689,train_acc:0.9321427941322327
node12 epoch2:node_model train_loss:0.14445912359016283,train_acc:0.9599206447601318
node12 epoch3:node_model train_loss:0.10579719154962472,train_acc:0.9644443988800049
node12 epoch4:node_model train_loss:0.0683701972344092,train_acc:0.9830158352851868
node12_model on test-dataset: loss:0.9573826521635056,acc:0.7677997350692749
node12 weight score:1395.4712851553033
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6733323965221644,acc:0.8271999830007553
total cost energy:9.408261754766734 | all_enery_cp：7.974 | all_enery_tp: 1.434261754766733
ef: 25.137742145489703
reward: 15.72948039072297
step 116:loss:113.77045440673828|running q:51.56439971923828
episode1,iteration56 selected nodes:[7, 9, 8, 3, 19],center node:7
################################################## episode1,iteration56 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1747894403199817,train_acc:0.936249315738678
node3 epoch1:node_model train_loss:0.09201949964775595,train_acc:0.969302237033844
node3 epoch2:node_model train_loss:0.05189914932084638,train_acc:0.9865115880966187
node3 epoch3:node_model train_loss:0.03833830469222956,train_acc:0.990232527256012
node3 epoch4:node_model train_loss:0.02687370977472774,train_acc:0.9953488111495972
node3_model on test-dataset: loss:0.8447613690793514,acc:0.7943999171257019
node3 weight score:5027.455273704714
node7: train data size:1951
node7 epoch0:node_model train_loss:0.429665369540453,train_acc:0.8722156882286072
node7 epoch1:node_model train_loss:0.2208011943846941,train_acc:0.9251176714897156
node7 epoch2:node_model train_loss:0.11682432740926743,train_acc:0.963039219379425
node7 epoch3:node_model train_loss:0.07782834162935615,train_acc:0.9755392074584961
node7 epoch4:node_model train_loss:0.0590582394041121,train_acc:0.9860195517539978
node7_model on test-dataset: loss:0.830218185633421,acc:0.7896996140480042
node7 weight score:2349.9846591670002
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3991648670699861,train_acc:0.8732199668884277
node8 epoch1:node_model train_loss:0.18735717402564156,train_acc:0.9376983642578125
node8 epoch2:node_model train_loss:0.10495755003972186,train_acc:0.9693536758422852
node8 epoch3:node_model train_loss:0.07825598389738136,train_acc:0.9788773655891418
node8 epoch4:node_model train_loss:0.05118569224658939,train_acc:0.9861111044883728
node8_model on test-dataset: loss:0.9340558454394341,acc:0.7785999178886414
node8 weight score:1924.9384378662244
node9: train data size:1857
node9 epoch0:node_model train_loss:0.47458153335671677,train_acc:0.8518189787864685
node9 epoch1:node_model train_loss:0.19571211620381004,train_acc:0.9285410642623901
node9 epoch2:node_model train_loss:0.12621791425504184,train_acc:0.9642105102539062
node9 epoch3:node_model train_loss:0.08641101214054384,train_acc:0.9743397235870361
node9 epoch4:node_model train_loss:0.05913829636809073,train_acc:0.9843397736549377
node9_model on test-dataset: loss:0.8267082729935646,acc:0.7887002229690552
node9 weight score:2246.2579130552085
node19: train data size:4281
node19 epoch0:node_model train_loss:0.349315105829128,train_acc:0.88283371925354
node19 epoch1:node_model train_loss:0.19355074264282404,train_acc:0.9356645941734314
node19 epoch2:node_model train_loss:0.12118736515904582,train_acc:0.9619837999343872
node19 epoch3:node_model train_loss:0.08318454391041467,train_acc:0.9765804409980774
node19 epoch4:node_model train_loss:0.050573697660204975,train_acc:0.9906977415084839
node19_model on test-dataset: loss:0.794714334756136,acc:0.7993999123573303
node19 weight score:5386.8412997906435
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6680682147294283,acc:0.8241999828815461
total cost energy:8.614213595499958 | all_enery_cp：7.067 | all_enery_tp: 1.547213595499958
ef: 25.088299642466435
reward: 16.474086046966477
step 117:loss:86.61353302001953|running q:52.37664031982422
episode1,iteration57 selected nodes:[11, 9, 7, 6, 19],center node:9
################################################## episode1,iteration57 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.544169892226496,train_acc:0.8378801941871643
node6 epoch1:node_model train_loss:0.32424904838685065,train_acc:0.8980644941329956
node6 epoch2:node_model train_loss:0.21793274557398212,train_acc:0.9293549060821533
node6 epoch3:node_model train_loss:0.16424593521702674,train_acc:0.948064386844635
node6 epoch4:node_model train_loss:0.1178879062494924,train_acc:0.966128945350647
node6_model on test-dataset: loss:0.8302210132777691,acc:0.7886000871658325
node6 weight score:3621.927115682316
node7: train data size:1951
node7 epoch0:node_model train_loss:0.37380324080586436,train_acc:0.8820980191230774
node7 epoch1:node_model train_loss:0.14780000708997248,train_acc:0.9505195617675781
node7 epoch2:node_model train_loss:0.10876723267138004,train_acc:0.9690195322036743
node7 epoch3:node_model train_loss:0.0605351977981627,train_acc:0.9840194582939148
node7 epoch4:node_model train_loss:0.051760177593678236,train_acc:0.9885194897651672
node7_model on test-dataset: loss:0.8338209586590528,acc:0.7918999791145325
node7 weight score:2339.8308470652855
node9: train data size:1857
node9 epoch0:node_model train_loss:0.34011111918248627,train_acc:0.8822160363197327
node9 epoch1:node_model train_loss:0.19567900817645223,train_acc:0.9319666624069214
node9 epoch2:node_model train_loss:0.11623349197601017,train_acc:0.9640718698501587
node9 epoch3:node_model train_loss:0.08280165356240775,train_acc:0.9735456705093384
node9 epoch4:node_model train_loss:0.047604736038728765,train_acc:0.9885501861572266
node9_model on test-dataset: loss:0.8245267686247826,acc:0.7887001037597656
node9 weight score:2252.200984447438
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4918931705110213,train_acc:0.8550501465797424
node11 epoch1:node_model train_loss:0.21240495232974782,train_acc:0.930530846118927
node11 epoch2:node_model train_loss:0.13770059561904738,train_acc:0.9558247923851013
node11 epoch3:node_model train_loss:0.10017191027017201,train_acc:0.9718363285064697
node11 epoch4:node_model train_loss:0.05621775393100346,train_acc:0.9847057461738586
node11_model on test-dataset: loss:0.8351879509538412,acc:0.790600061416626
node11 weight score:2013.917942756528
node19: train data size:4281
node19 epoch0:node_model train_loss:0.23387994929108508,train_acc:0.9196726083755493
node19 epoch1:node_model train_loss:0.13184374796096668,train_acc:0.9547743797302246
node19 epoch2:node_model train_loss:0.09837258225956629,train_acc:0.9682081341743469
node19 epoch3:node_model train_loss:0.07348031715251678,train_acc:0.9777976870536804
node19 epoch4:node_model train_loss:0.05534277427508388,train_acc:0.9862791299819946
node19_model on test-dataset: loss:0.8380342721939087,acc:0.7917998433113098
node19 weight score:5108.382964807244
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.669644707068801,acc:0.8202999794483185
total cost energy:7.568669127533633 | all_enery_cp：6.388999999999999 | all_enery_tp: 1.179669127533634
ef: 25.21020101646214
reward: 17.641531888928505
step 118:loss:81.13273620605469|running q:53.063079833984375
episode1,iteration58 selected nodes:[2, 8, 0, 1, 12],center node:1
################################################## episode1,iteration58 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.2825663806154178,train_acc:0.9063413143157959
node0 epoch1:node_model train_loss:0.1350467033111132,train_acc:0.9538413882255554
node0 epoch2:node_model train_loss:0.07969385012984276,train_acc:0.9771501421928406
node0 epoch3:node_model train_loss:0.05420368705661251,train_acc:0.9854981899261475
node0 epoch4:node_model train_loss:0.04430356061157699,train_acc:0.9889204502105713
node0_model on test-dataset: loss:0.8386312963627279,acc:0.7984999418258667
node0 weight score:6180.308345848125
node1: train data size:6708
node1 epoch0:node_model train_loss:0.11596832577796544,train_acc:0.9576470851898193
node1 epoch1:node_model train_loss:0.08234089078819927,train_acc:0.9713234901428223
node1 epoch2:node_model train_loss:0.12029227209003533,train_acc:0.9597058296203613
node1 epoch3:node_model train_loss:0.13871498420998893,train_acc:0.9588236212730408
node1 epoch4:node_model train_loss:0.04530374998884166,train_acc:0.9877944588661194
node1_model on test-dataset: loss:0.8366919067502022,acc:0.7986001372337341
node1 weight score:8017.2880194987965
node2: train data size:4788
node2 epoch0:node_model train_loss:0.20239866090317568,train_acc:0.9262878894805908
node2 epoch1:node_model train_loss:0.0927422046661377,train_acc:0.969886064529419
node2 epoch2:node_model train_loss:0.057395569320457675,train_acc:0.9855682253837585
node2 epoch3:node_model train_loss:0.04244238244912898,train_acc:0.9889017343521118
node2 epoch4:node_model train_loss:0.032367051423837744,train_acc:0.9928598403930664
node2_model on test-dataset: loss:0.9667025884985924,acc:0.7775999903678894
node2 weight score:4952.919395236492
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3529556161827511,train_acc:0.8787528872489929
node8 epoch1:node_model train_loss:0.19844325797425377,train_acc:0.9326984882354736
node8 epoch2:node_model train_loss:0.10129186966352993,train_acc:0.966020405292511
node8 epoch3:node_model train_loss:0.07085957585109605,train_acc:0.9816552400588989
node8 epoch4:node_model train_loss:0.04299931310945087,train_acc:0.9927777051925659
node8_model on test-dataset: loss:0.8662257716059685,acc:0.7904999852180481
node8 weight score:2075.671330658446
node12: train data size:1336
node12 epoch0:node_model train_loss:0.4587353861757687,train_acc:0.8623810410499573
node12 epoch1:node_model train_loss:0.22497943522674696,train_acc:0.9249206781387329
node12 epoch2:node_model train_loss:0.13466663445745194,train_acc:0.9546030759811401
node12 epoch3:node_model train_loss:0.0872570187119501,train_acc:0.9753173589706421
node12 epoch4:node_model train_loss:0.06513598641114575,train_acc:0.9821428060531616
node12_model on test-dataset: loss:0.89613933339715,acc:0.7864001393318176
node12 weight score:1490.8395940343273
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.686867279857397,acc:0.8267999798059463
total cost energy:11.770359536165277 | all_enery_cp：9.9065 | all_enery_tp: 1.8638595361652772
ef: 24.819469475704572
reward: 13.049109939539296
step 119:loss:59.398643493652344|running q:53.75991439819336
episode1,iteration59 selected nodes:[7, 18, 13, 5, 0],center node:5
################################################## episode1,iteration59 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.16555134292978507,train_acc:0.9376136064529419
node0 epoch1:node_model train_loss:0.0872998615153707,train_acc:0.9760358333587646
node0 epoch2:node_model train_loss:0.06901355044772992,train_acc:0.9789990782737732
node0 epoch3:node_model train_loss:0.05595436312544804,train_acc:0.9858828186988831
node0 epoch4:node_model train_loss:0.04373796807172207,train_acc:0.9896156191825867
node0_model on test-dataset: loss:0.9192678701877594,acc:0.7911000847816467
node0 weight score:5638.182479869963
node5: train data size:3735
node5 epoch0:node_model train_loss:0.2584739125480777,train_acc:0.9129698872566223
node5 epoch1:node_model train_loss:0.11768182906273164,train_acc:0.9615786671638489
node5 epoch2:node_model train_loss:0.06855873616510316,train_acc:0.9802631735801697
node5 epoch3:node_model train_loss:0.04950345332097066,train_acc:0.9866164326667786
node5 epoch4:node_model train_loss:0.03446478215291312,train_acc:0.9934210777282715
node5_model on test-dataset: loss:0.8047053004801273,acc:0.8019999265670776
node5 weight score:4641.450724596337
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3427377551794052,train_acc:0.8900784850120544
node7 epoch1:node_model train_loss:0.180781040340662,train_acc:0.93953937292099
node7 epoch2:node_model train_loss:0.11333542335778475,train_acc:0.9645196199417114
node7 epoch3:node_model train_loss:0.06588449776172638,train_acc:0.9835195541381836
node7 epoch4:node_model train_loss:0.04943582685664296,train_acc:0.9840194582939148
node7_model on test-dataset: loss:0.8145164933800697,acc:0.7962000966072083
node7 weight score:2395.2860572580503
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6517045175035795,train_acc:0.8228031396865845
node13 epoch1:node_model train_loss:0.3142429826160272,train_acc:0.9014394283294678
node13 epoch2:node_model train_loss:0.13893134519457817,train_acc:0.9624242186546326
node13 epoch3:node_model train_loss:0.097792472379903,train_acc:0.9721211791038513
node13 epoch4:node_model train_loss:0.07206002778063218,train_acc:0.9841666221618652
node13_model on test-dataset: loss:0.9271126805245876,acc:0.7740000486373901
node13 weight score:1245.8032602320432
node18: train data size:472
node18 epoch0:node_model train_loss:0.6087711691856384,train_acc:0.8342222571372986
node18 epoch1:node_model train_loss:0.33328097462654116,train_acc:0.8953332901000977
node18 epoch2:node_model train_loss:0.16699845641851424,train_acc:0.9464444518089294
node18 epoch3:node_model train_loss:0.07876087874174117,train_acc:0.9739999771118164
node18 epoch4:node_model train_loss:0.05767422914505005,train_acc:0.9828888773918152
node18_model on test-dataset: loss:0.9441367828845978,acc:0.7738998532295227
node18 weight score:499.92756193431006
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6962318760156632,acc:0.8248999834060669
total cost energy:8.396683298050515 | all_enery_cp：6.248 | all_enery_tp: 2.148683298050514
ef: 24.4583737955909
reward: 16.061690497540386
step 120:loss:77.87267303466797|running q:54.58107376098633
episode1_cost time: 25491.204924106598
episode2,iteration0 selected nodes:[15, 8, 16, 17, 1],center node:16
################################################## episode2,iteration0 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:1.8170682822956759,train_acc:0.35249999165534973
node1 epoch1:node_model train_loss:1.4938992051517261,train_acc:0.47374993562698364
node1 epoch2:node_model train_loss:1.2772716941202389,train_acc:0.5475001335144043
node1 epoch3:node_model train_loss:1.1524776334271711,train_acc:0.5919118523597717
node1 epoch4:node_model train_loss:1.1334736785467934,train_acc:0.6119117736816406
node1_model on test-dataset: loss:1.1655384626984597,acc:0.5959998965263367
node1 weight score:5755.279825317483
node8: train data size:1798
node8 epoch0:node_model train_loss:2.34426928891076,train_acc:0.22705213725566864
node8 epoch1:node_model train_loss:1.7994075549973383,train_acc:0.35541948676109314
node8 epoch2:node_model train_loss:1.5891518659061856,train_acc:0.42878684401512146
node8 epoch3:node_model train_loss:1.4904279708862305,train_acc:0.47826528549194336
node8 epoch4:node_model train_loss:1.3796968195173476,train_acc:0.5089341998100281
node8_model on test-dataset: loss:1.519997301697731,acc:0.45590004324913025
node8 weight score:1182.8968367192226
node15: train data size:629
node15 epoch0:node_model train_loss:2.5741104739052907,train_acc:0.21384236216545105
node15 epoch1:node_model train_loss:1.9252263307571411,train_acc:0.29768475890159607
node15 epoch2:node_model train_loss:1.8426933288574219,train_acc:0.33926111459732056
node15 epoch3:node_model train_loss:1.593872002192906,train_acc:0.40083739161491394
node15 epoch4:node_model train_loss:1.5451067856379919,train_acc:0.44325125217437744
node15_model on test-dataset: loss:2.050498114824295,acc:0.239299938082695
node15 weight score:306.75473215633673
node16: train data size:877
node16 epoch0:node_model train_loss:2.700395451651679,train_acc:0.17441558837890625
node16 epoch1:node_model train_loss:1.9454473521974351,train_acc:0.2989610433578491
node16 epoch2:node_model train_loss:1.7220277388890584,train_acc:0.3829581141471863
node16 epoch3:node_model train_loss:1.5661960177951388,train_acc:0.4620634615421295
node16 epoch4:node_model train_loss:1.458170043097602,train_acc:0.48828282952308655
node16_model on test-dataset: loss:1.699486914873123,acc:0.36700010299682617
node16 weight score:516.0381008673276
node17: train data size:442
node17 epoch0:node_model train_loss:2.938521385192871,train_acc:0.14533333480358124
node17 epoch1:node_model train_loss:2.2633139371871946,train_acc:0.2191428691148758
node17 epoch2:node_model train_loss:1.8654796838760377,train_acc:0.3169523775577545
node17 epoch3:node_model train_loss:1.660510516166687,train_acc:0.41819047927856445
node17 epoch4:node_model train_loss:1.537113070487976,train_acc:0.4329524040222168
node17_model on test-dataset: loss:2.1984815299510956,acc:0.18459996581077576
node17 weight score:201.04785688594444
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.277064284682274,acc:0.534699985831976
total cost energy:7.216320964314385 | all_enery_cp：5.227 | all_enery_tp: 1.989320964314385
ef: 23.118598544205124
reward: 15.90227757989074
step 121:loss:85.95271301269531|running q:0.9087892174720764
episode2,iteration1 selected nodes:[7, 11, 16, 9, 8],center node:11
################################################## episode2,iteration1 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:1.3778224050998689,train_acc:0.5244118571281433
node7 epoch1:node_model train_loss:1.097833237051964,train_acc:0.6014901995658875
node7 epoch2:node_model train_loss:0.9529714912176133,train_acc:0.6734117865562439
node7 epoch3:node_model train_loss:0.847054049372673,train_acc:0.7127745151519775
node7 epoch4:node_model train_loss:0.801681637763977,train_acc:0.7302352786064148
node7_model on test-dataset: loss:1.777816015034914,acc:0.47140005230903625
node7 weight score:1097.413896320247
node8: train data size:1798
node8 epoch0:node_model train_loss:1.3446967833571963,train_acc:0.550578236579895
node8 epoch1:node_model train_loss:1.112281517850028,train_acc:0.6135261654853821
node8 epoch2:node_model train_loss:0.9492420885297987,train_acc:0.6562358140945435
node8 epoch3:node_model train_loss:0.8373113804393344,train_acc:0.7118480801582336
node8 epoch4:node_model train_loss:0.7971418466832902,train_acc:0.7102721333503723
node8_model on test-dataset: loss:1.4977117791771888,acc:0.5127999186515808
node8 weight score:1200.498003018834
node9: train data size:1857
node9 epoch0:node_model train_loss:1.3455989737259715,train_acc:0.5363896489143372
node9 epoch1:node_model train_loss:1.0588265406458002,train_acc:0.6190211772918701
node9 epoch2:node_model train_loss:0.8874821129598116,train_acc:0.6874515414237976
node9 epoch3:node_model train_loss:0.8126540936921772,train_acc:0.7169344425201416
node9 epoch4:node_model train_loss:0.7198543987776104,train_acc:0.7432501912117004
node9_model on test-dataset: loss:1.2005817595124244,acc:0.5871000289916992
node9 weight score:1546.7501361624531
node11: train data size:1682
node11 epoch0:node_model train_loss:1.420338848057915,train_acc:0.5266571044921875
node11 epoch1:node_model train_loss:1.0462695886107052,train_acc:0.6314204335212708
node11 epoch2:node_model train_loss:0.9409414985600639,train_acc:0.6697847247123718
node11 epoch3:node_model train_loss:0.8313776184530819,train_acc:0.7132998108863831
node11 epoch4:node_model train_loss:0.7675879772971658,train_acc:0.7296413779258728
node11_model on test-dataset: loss:1.3876439711451531,acc:0.5499001741409302
node11 weight score:1212.1264783876295
node16: train data size:877
node16 epoch0:node_model train_loss:1.4095501634809706,train_acc:0.5247185826301575
node16 epoch1:node_model train_loss:1.1074348820580378,train_acc:0.6044877171516418
node16 epoch2:node_model train_loss:0.8918725119696723,train_acc:0.6982539296150208
node16 epoch3:node_model train_loss:0.7813609970940484,train_acc:0.7342568635940552
node16 epoch4:node_model train_loss:0.6607113944159614,train_acc:0.7715872526168823
node16_model on test-dataset: loss:1.2667385923862458,acc:0.5665000677108765
node16 weight score:692.3291082084526
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.0035224172472954,acc:0.6460999849438668
total cost energy:5.269548159266774 | all_enery_cp：4.0825 | all_enery_tp: 1.187048159266775
ef: 23.67254279843253
reward: 18.402994639165755
step 122:loss:81.79263305664062|running q:2.0890750885009766
episode2,iteration2 selected nodes:[13, 8, 14, 12, 3],center node:12
################################################## episode2,iteration2 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:1.1465873052907545,train_acc:0.6047551035881042
node3 epoch1:node_model train_loss:0.9575897663138634,train_acc:0.6658632755279541
node3 epoch2:node_model train_loss:0.9178736570269562,train_acc:0.6789708137512207
node3 epoch3:node_model train_loss:0.8409426531126333,train_acc:0.7092972993850708
node3 epoch4:node_model train_loss:0.7166249779767768,train_acc:0.7438890933990479
node3_model on test-dataset: loss:1.109577307999134,acc:0.6343000531196594
node3 weight score:3827.5836837889933
node8: train data size:1798
node8 epoch0:node_model train_loss:1.1159369846185048,train_acc:0.6273696422576904
node8 epoch1:node_model train_loss:0.919994486702813,train_acc:0.681836724281311
node8 epoch2:node_model train_loss:0.7810708946651883,train_acc:0.7201927304267883
node8 epoch3:node_model train_loss:0.7379683057467142,train_acc:0.7546259164810181
node8 epoch4:node_model train_loss:0.6431756450070275,train_acc:0.7908616065979004
node8_model on test-dataset: loss:1.0600538691878318,acc:0.6319000124931335
node8 weight score:1696.1402172679686
node12: train data size:1336
node12 epoch0:node_model train_loss:1.298503088099616,train_acc:0.5632539987564087
node12 epoch1:node_model train_loss:1.0119721634047372,train_acc:0.638888955116272
node12 epoch2:node_model train_loss:0.8205095529556274,train_acc:0.7090476155281067
node12 epoch3:node_model train_loss:0.7654825491564614,train_acc:0.7322222590446472
node12 epoch4:node_model train_loss:0.6574203159127917,train_acc:0.7692064046859741
node12_model on test-dataset: loss:1.1388348272442819,acc:0.6087998747825623
node12 weight score:1173.1288577052146
node13: train data size:1155
node13 epoch0:node_model train_loss:1.4403667747974396,train_acc:0.5299242734909058
node13 epoch1:node_model train_loss:0.9479852169752121,train_acc:0.6659091711044312
node13 epoch2:node_model train_loss:0.8687708924214045,train_acc:0.7023484706878662
node13 epoch3:node_model train_loss:0.7398782024780909,train_acc:0.7453030943870544
node13 epoch4:node_model train_loss:0.6896999776363373,train_acc:0.7628030776977539
node13_model on test-dataset: loss:1.156780418753624,acc:0.6083999276161194
node13 weight score:998.4608844299576
node14: train data size:1172
node14 epoch0:node_model train_loss:1.2871622443199158,train_acc:0.5670832991600037
node14 epoch1:node_model train_loss:0.933942382534345,train_acc:0.6742591857910156
node14 epoch2:node_model train_loss:0.7865004887183508,train_acc:0.7138888835906982
node14 epoch3:node_model train_loss:0.7103612621625265,train_acc:0.7348148226737976
node14 epoch4:node_model train_loss:0.5853370527426401,train_acc:0.8023148775100708
node14_model on test-dataset: loss:1.1346837228536606,acc:0.6140000224113464
node14 weight score:1032.8869414399383
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9051175206899643,acc:0.6882999867200852
total cost energy:6.140387347175357 | all_enery_cp：4.854 | all_enery_tp: 1.2863873471753577
ef: 24.29413282843434
reward: 18.15374548125898
step 123:loss:42.07350540161133|running q:3.3378806114196777
episode2,iteration3 selected nodes:[18, 5, 4, 2, 12],center node:5
################################################## episode2,iteration3 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:1.0618967153131962,train_acc:0.6357197761535645
node2 epoch1:node_model train_loss:0.9249913307527701,train_acc:0.6844412088394165
node2 epoch2:node_model train_loss:0.8002124366660913,train_acc:0.7218751311302185
node2 epoch3:node_model train_loss:0.7422794091204802,train_acc:0.7415245771408081
node2 epoch4:node_model train_loss:0.6682608438034853,train_acc:0.7626703977584839
node2_model on test-dataset: loss:1.1023680537194014,acc:0.6389997601509094
node2 weight score:4343.376954589021
node4: train data size:2705
node4 epoch0:node_model train_loss:1.1237624734640121,train_acc:0.6182142496109009
node4 epoch1:node_model train_loss:0.8971649812800544,train_acc:0.6889285445213318
node4 epoch2:node_model train_loss:0.8742370711905616,train_acc:0.6874999403953552
node4 epoch3:node_model train_loss:0.8583496702568871,train_acc:0.6878570914268494
node4 epoch4:node_model train_loss:0.788252762385777,train_acc:0.7403571605682373
node4_model on test-dataset: loss:1.372271096408367,acc:0.5808000564575195
node4 weight score:1971.184853400886
node5: train data size:3735
node5 epoch0:node_model train_loss:1.0847861531533693,train_acc:0.6327818632125854
node5 epoch1:node_model train_loss:0.9144946995534395,train_acc:0.6919547915458679
node5 epoch2:node_model train_loss:0.7839356425561403,train_acc:0.7223683595657349
node5 epoch3:node_model train_loss:0.7553983722862444,train_acc:0.7394735813140869
node5 epoch4:node_model train_loss:0.7090258159135517,train_acc:0.7566916346549988
node5_model on test-dataset: loss:0.9590210857987403,acc:0.6830001473426819
node5 weight score:3894.5963288067114
node12: train data size:1336
node12 epoch0:node_model train_loss:1.1619573448385512,train_acc:0.6174602508544922
node12 epoch1:node_model train_loss:0.8576336375304631,train_acc:0.7044444680213928
node12 epoch2:node_model train_loss:0.6948453060218266,train_acc:0.7643651366233826
node12 epoch3:node_model train_loss:0.5794997960329056,train_acc:0.8077778220176697
node12 epoch4:node_model train_loss:0.5195130790982928,train_acc:0.8161112070083618
node12_model on test-dataset: loss:1.2276665884256364,acc:0.6003000140190125
node12 weight score:1088.2433492902098
node18: train data size:472
node18 epoch0:node_model train_loss:1.063383197784424,train_acc:0.6451111435890198
node18 epoch1:node_model train_loss:0.9086888670921326,train_acc:0.6781111359596252
node18 epoch2:node_model train_loss:0.6394766449928284,train_acc:0.7927777171134949
node18 epoch3:node_model train_loss:0.6205583453178406,train_acc:0.8028888702392578
node18 epoch4:node_model train_loss:0.42022088170051575,train_acc:0.8842222094535828
node18_model on test-dataset: loss:1.375196180343628,acc:0.5666999220848083
node18 weight score:343.223757269351
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8879904234409333,acc:0.6930999845266342
total cost energy:8.847061226915837 | all_enery_cp：6.518 | all_enery_tp: 2.3290612269158366
ef: 23.819399659876588
reward: 14.97233843296075
step 124:loss:38.59857940673828|running q:4.581662178039551
episode2,iteration4 selected nodes:[13, 18, 15, 6, 19],center node:15
################################################## episode2,iteration4 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:1.083647083851599,train_acc:0.6412441730499268
node6 epoch1:node_model train_loss:0.8810745842995182,train_acc:0.6958523988723755
node6 epoch2:node_model train_loss:0.7900809328402242,train_acc:0.7299998998641968
node6 epoch3:node_model train_loss:0.6838786486656435,train_acc:0.7589401006698608
node6 epoch4:node_model train_loss:0.6735650310593266,train_acc:0.7716589570045471
node6_model on test-dataset: loss:1.104012605547905,acc:0.6480998992919922
node6 weight score:2723.700784655145
node13: train data size:1155
node13 epoch0:node_model train_loss:1.1169769416252773,train_acc:0.6503788232803345
node13 epoch1:node_model train_loss:0.8060569912195206,train_acc:0.7259091138839722
node13 epoch2:node_model train_loss:0.5977182959516844,train_acc:0.7924999594688416
node13 epoch3:node_model train_loss:0.5208409105738004,train_acc:0.8289394378662109
node13 epoch4:node_model train_loss:0.44582576553026837,train_acc:0.8630303740501404
node13_model on test-dataset: loss:1.072766894698143,acc:0.6508998870849609
node13 weight score:1076.655148204397
node15: train data size:629
node15 epoch0:node_model train_loss:1.1131280064582825,train_acc:0.6015270948410034
node15 epoch1:node_model train_loss:0.8748630455562046,train_acc:0.7147290706634521
node15 epoch2:node_model train_loss:0.6481772150312152,train_acc:0.7769458293914795
node15 epoch3:node_model train_loss:0.5730977058410645,train_acc:0.7905911207199097
node15 epoch4:node_model train_loss:0.48923572472163607,train_acc:0.8361576795578003
node15_model on test-dataset: loss:1.1372276890277861,acc:0.6306001543998718
node15 weight score:553.0994417993207
node18: train data size:472
node18 epoch0:node_model train_loss:1.0935574173927307,train_acc:0.675777792930603
node18 epoch1:node_model train_loss:0.7676559329032898,train_acc:0.7362222075462341
node18 epoch2:node_model train_loss:0.5758222699165344,train_acc:0.8170000314712524
node18 epoch3:node_model train_loss:0.4259291708469391,train_acc:0.8577777743339539
node18 epoch4:node_model train_loss:0.375934773683548,train_acc:0.8865556120872498
node18_model on test-dataset: loss:1.0904549163579942,acc:0.644599974155426
node18 weight score:432.84687236445393
node19: train data size:4281
node19 epoch0:node_model train_loss:1.0415513473887776,train_acc:0.64737868309021
node19 epoch1:node_model train_loss:0.8228034862252169,train_acc:0.7127818465232849
node19 epoch2:node_model train_loss:0.7424828333910122,train_acc:0.7320153713226318
node19 epoch3:node_model train_loss:0.6872007347816644,train_acc:0.7545046210289001
node19 epoch4:node_model train_loss:0.6187273561954498,train_acc:0.7869393825531006
node19_model on test-dataset: loss:0.9021059104800224,acc:0.7006000876426697
node19 weight score:4745.562522389443
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7704742312431335,acc:0.7357999840378762
total cost energy:6.7796845219426976 | all_enery_cp：4.772 | all_enery_tp: 2.0076845219426973
ef: 24.21530068227189
reward: 17.435616160329193
step 125:loss:50.67047119140625|running q:5.7926716804504395
episode2,iteration5 selected nodes:[12, 11, 0, 14, 2],center node:11
################################################## episode2,iteration5 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.9722985648191892,train_acc:0.6722404360771179
node0 epoch1:node_model train_loss:0.8192661507771566,train_acc:0.7144439816474915
node0 epoch2:node_model train_loss:0.7430575810945951,train_acc:0.7354448437690735
node0 epoch3:node_model train_loss:0.6457419641889058,train_acc:0.775637149810791
node0 epoch4:node_model train_loss:0.6125693527551798,train_acc:0.7850210070610046
node0_model on test-dataset: loss:0.9274310058355332,acc:0.6911999583244324
node0 weight score:5588.555878968675
node2: train data size:4788
node2 epoch0:node_model train_loss:0.864476335545381,train_acc:0.7031817436218262
node2 epoch1:node_model train_loss:0.6924383950730165,train_acc:0.7665246725082397
node2 epoch2:node_model train_loss:0.6190305724740028,train_acc:0.786392092704773
node2 epoch3:node_model train_loss:0.5608085170388222,train_acc:0.8073201179504395
node2 epoch4:node_model train_loss:0.547014756128192,train_acc:0.814933717250824
node2_model on test-dataset: loss:0.9886551649868488,acc:0.6853001117706299
node2 weight score:4842.942382305453
node11: train data size:1682
node11 epoch0:node_model train_loss:0.9975850722369026,train_acc:0.6625251173973083
node11 epoch1:node_model train_loss:0.7097605887581321,train_acc:0.7453227639198303
node11 epoch2:node_model train_loss:0.6223307532422683,train_acc:0.780545175075531
node11 epoch3:node_model train_loss:0.5384398663745207,train_acc:0.8142755031585693
node11 epoch4:node_model train_loss:0.433090238886721,train_acc:0.8494977951049805
node11_model on test-dataset: loss:0.898050824701786,acc:0.7038999199867249
node11 weight score:1872.9452206210472
node12: train data size:1336
node12 epoch0:node_model train_loss:1.0219895499093192,train_acc:0.6672222018241882
node12 epoch1:node_model train_loss:0.7776141549859729,train_acc:0.7225397229194641
node12 epoch2:node_model train_loss:0.6217808787311826,train_acc:0.7761905193328857
node12 epoch3:node_model train_loss:0.5025349727698735,train_acc:0.8263491988182068
node12 epoch4:node_model train_loss:0.4047175794839859,train_acc:0.8710318207740784
node12_model on test-dataset: loss:1.0037556508183478,acc:0.6724998950958252
node12 weight score:1331.0012241632494
node14: train data size:1172
node14 epoch0:node_model train_loss:0.9169310728708903,train_acc:0.6978240013122559
node14 epoch1:node_model train_loss:0.6417917236685753,train_acc:0.7757870554924011
node14 epoch2:node_model train_loss:0.5535778477787971,train_acc:0.8098148703575134
node14 epoch3:node_model train_loss:0.4582030028104782,train_acc:0.8379167318344116
node14 epoch4:node_model train_loss:0.3707763801018397,train_acc:0.8857407569885254
node14_model on test-dataset: loss:0.9569130632281303,acc:0.6855999827384949
node14 weight score:1224.7716590327207
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7145294006168842,acc:0.7633999800682068
total cost energy:8.846185424949239 | all_enery_cp：7.080500000000001 | all_enery_tp: 1.7656854249492382
ef: 24.67539774192072
reward: 15.82921231697148
step 126:loss:66.29905700683594|running q:6.943714141845703
episode2,iteration6 selected nodes:[14, 0, 19, 11, 16],center node:14
################################################## episode2,iteration6 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.8333013464625065,train_acc:0.7223238945007324
node0 epoch1:node_model train_loss:0.6701972077672298,train_acc:0.7627479434013367
node0 epoch2:node_model train_loss:0.6677129818842962,train_acc:0.7720180749893188
node0 epoch3:node_model train_loss:0.5847804385882157,train_acc:0.795903742313385
node0 epoch4:node_model train_loss:0.5344597955162709,train_acc:0.8158689141273499
node0_model on test-dataset: loss:0.8004624766111373,acc:0.7309999465942383
node0 weight score:6475.006825982536
node11: train data size:1682
node11 epoch0:node_model train_loss:0.8588591340710136,train_acc:0.7060545086860657
node11 epoch1:node_model train_loss:0.6599428005078259,train_acc:0.7698995471000671
node11 epoch2:node_model train_loss:0.4803614020347595,train_acc:0.8374030590057373
node11 epoch3:node_model train_loss:0.39168787353179035,train_acc:0.8773458003997803
node11 epoch4:node_model train_loss:0.335618992062176,train_acc:0.8961549401283264
node11_model on test-dataset: loss:0.9520625303685665,acc:0.6966999769210815
node11 weight score:1766.6906808619565
node14: train data size:1172
node14 epoch0:node_model train_loss:0.8392501026391983,train_acc:0.7113425731658936
node14 epoch1:node_model train_loss:0.5889813179771105,train_acc:0.7904166579246521
node14 epoch2:node_model train_loss:0.48175812512636185,train_acc:0.8346759080886841
node14 epoch3:node_model train_loss:0.38726706802845,train_acc:0.8834259510040283
node14 epoch4:node_model train_loss:0.28364266579349834,train_acc:0.9118980765342712
node14_model on test-dataset: loss:1.0557235760986805,acc:0.6796000003814697
node14 weight score:1110.1390804693472
node16: train data size:877
node16 epoch0:node_model train_loss:0.9708604878849454,train_acc:0.6894804835319519
node16 epoch1:node_model train_loss:0.6883492668469747,train_acc:0.7561327815055847
node16 epoch2:node_model train_loss:0.5433099435435401,train_acc:0.8113564252853394
node16 epoch3:node_model train_loss:0.4176381660832299,train_acc:0.8813419342041016
node16 epoch4:node_model train_loss:0.3491814997461107,train_acc:0.8917893171310425
node16_model on test-dataset: loss:1.175627050101757,acc:0.6299998760223389
node16 weight score:745.9848766869484
node19: train data size:4281
node19 epoch0:node_model train_loss:0.8273194044135338,train_acc:0.7165288925170898
node19 epoch1:node_model train_loss:0.6719350184119025,train_acc:0.7651190757751465
node19 epoch2:node_model train_loss:0.5956714368143747,train_acc:0.7938616275787354
node19 epoch3:node_model train_loss:0.5181335169215535,train_acc:0.8243124485015869
node19 epoch4:node_model train_loss:0.47879993638326956,train_acc:0.8358024954795837
node19_model on test-dataset: loss:1.0392821052670478,acc:0.6682999134063721
node19 weight score:4119.189562010191
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6959338180720807,acc:0.7671999832987786
total cost energy:7.960031611330107 | all_enery_cp：6.5975 | all_enery_tp: 1.3625316113301074
ef: 24.719859604113594
reward: 16.759827992783485
step 127:loss:45.779544830322266|running q:8.368289947509766
episode2,iteration7 selected nodes:[10, 17, 14, 3, 18],center node:14
################################################## episode2,iteration7 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.8406794015751329,train_acc:0.7113259434700012
node3 epoch1:node_model train_loss:0.6277413499909777,train_acc:0.7804009318351746
node3 epoch2:node_model train_loss:0.5839274352373078,train_acc:0.7995002269744873
node3 epoch3:node_model train_loss:0.5052253479181334,train_acc:0.8276397585868835
node3 epoch4:node_model train_loss:0.45651359682859377,train_acc:0.8423206210136414
node3_model on test-dataset: loss:1.1719652572274208,acc:0.6592999696731567
node3 weight score:3623.8275612771567
node10: train data size:1975
node10 epoch0:node_model train_loss:0.91658636033535,train_acc:0.7095000147819519
node10 epoch1:node_model train_loss:0.6498704046010971,train_acc:0.7788333296775818
node10 epoch2:node_model train_loss:0.5292293220758438,train_acc:0.8176667094230652
node10 epoch3:node_model train_loss:0.49762801378965377,train_acc:0.8295000195503235
node10 epoch4:node_model train_loss:0.4182691991329193,train_acc:0.8588333129882812
node10_model on test-dataset: loss:0.8864358025789261,acc:0.7095000147819519
node10 weight score:2228.0237263139547
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7617451498905817,train_acc:0.727546215057373
node14 epoch1:node_model train_loss:0.5911431709925333,train_acc:0.7899999618530273
node14 epoch2:node_model train_loss:0.40342305848995846,train_acc:0.8700925707817078
node14 epoch3:node_model train_loss:0.3572592536608378,train_acc:0.8927314281463623
node14 epoch4:node_model train_loss:0.2924041785299778,train_acc:0.9105555415153503
node14_model on test-dataset: loss:0.9687374550104141,acc:0.690800130367279
node14 weight score:1209.8221184060658
node17: train data size:442
node17 epoch0:node_model train_loss:1.0228219866752624,train_acc:0.6765714287757874
node17 epoch1:node_model train_loss:0.7220880508422851,train_acc:0.7606666684150696
node17 epoch2:node_model train_loss:0.49946138858795164,train_acc:0.8206666111946106
node17 epoch3:node_model train_loss:0.3995814919471741,train_acc:0.8689523935317993
node17 epoch4:node_model train_loss:0.31124134063720704,train_acc:0.9009523391723633
node17_model on test-dataset: loss:0.9885400614142418,acc:0.690099835395813
node17 weight score:447.1240137376512
node18: train data size:472
node18 epoch0:node_model train_loss:0.8360423564910888,train_acc:0.7192222476005554
node18 epoch1:node_model train_loss:0.6484406352043152,train_acc:0.7680000066757202
node18 epoch2:node_model train_loss:0.43229901790618896,train_acc:0.863777756690979
node18 epoch3:node_model train_loss:0.3168820977210999,train_acc:0.9005555510520935
node18 epoch4:node_model train_loss:0.24731799066066742,train_acc:0.9348889589309692
node18_model on test-dataset: loss:0.9522792626917362,acc:0.6907000541687012
node18 weight score:495.6529229312765
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.756490418612957,acc:0.7506999826431274
total cost energy:5.787600942675316 | all_enery_cp：4.154 | all_enery_tp: 1.633600942675316
ef: 24.14542540058649
reward: 18.357824457911175
step 128:loss:51.25434875488281|running q:9.631004333496094
episode2,iteration8 selected nodes:[10, 7, 1, 17, 18],center node:7
################################################## episode2,iteration8 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.837412194294088,train_acc:0.7219117283821106
node1 epoch1:node_model train_loss:0.6414211002342841,train_acc:0.7779412269592285
node1 epoch2:node_model train_loss:0.6344864697140806,train_acc:0.7813971042633057
node1 epoch3:node_model train_loss:0.5923584704013432,train_acc:0.7942647337913513
node1 epoch4:node_model train_loss:0.5221374679137679,train_acc:0.8192646503448486
node1_model on test-dataset: loss:0.9606503063440323,acc:0.6939998269081116
node1 weight score:6982.769854650628
node7: train data size:1951
node7 epoch0:node_model train_loss:0.9320535242557526,train_acc:0.704352855682373
node7 epoch1:node_model train_loss:0.6238525360822678,train_acc:0.7861765027046204
node7 epoch2:node_model train_loss:0.5367521360516548,train_acc:0.8166372179985046
node7 epoch3:node_model train_loss:0.4350820228457451,train_acc:0.8491175770759583
node7 epoch4:node_model train_loss:0.3752857558429241,train_acc:0.8756372332572937
node7_model on test-dataset: loss:0.9051192283630372,acc:0.7115997672080994
node7 weight score:2155.517128421304
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8281464338302612,train_acc:0.7339999675750732
node10 epoch1:node_model train_loss:0.5660079330205917,train_acc:0.7923333048820496
node10 epoch2:node_model train_loss:0.4628517642617226,train_acc:0.8425000309944153
node10 epoch3:node_model train_loss:0.36433065980672835,train_acc:0.8861665725708008
node10 epoch4:node_model train_loss:0.3086523063480854,train_acc:0.9073331952095032
node10_model on test-dataset: loss:0.9023570071160794,acc:0.7082000374794006
node10 weight score:2188.7124324684673
node17: train data size:442
node17 epoch0:node_model train_loss:0.8824888110160828,train_acc:0.7022857069969177
node17 epoch1:node_model train_loss:0.5804357886314392,train_acc:0.793904721736908
node17 epoch2:node_model train_loss:0.47229082584381105,train_acc:0.8368571400642395
node17 epoch3:node_model train_loss:0.3153307557106018,train_acc:0.8977142572402954
node17 epoch4:node_model train_loss:0.28452732861042024,train_acc:0.9134286046028137
node17_model on test-dataset: loss:0.9617324855923652,acc:0.6935999393463135
node17 weight score:459.5872621769207
node18: train data size:472
node18 epoch0:node_model train_loss:0.8116236925125122,train_acc:0.7365555763244629
node18 epoch1:node_model train_loss:0.5276013970375061,train_acc:0.8325554728507996
node18 epoch2:node_model train_loss:0.45856339037418364,train_acc:0.8498889207839966
node18 epoch3:node_model train_loss:0.2568075150251389,train_acc:0.913777768611908
node18 epoch4:node_model train_loss:0.1817575752735138,train_acc:0.9636666178703308
node18_model on test-dataset: loss:1.1200615987181664,acc:0.6597999930381775
node18 weight score:421.4053946141637
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7621602511405945,acc:0.7430999821424484
total cost energy:7.710409629369485 | all_enery_cp：5.774 | all_enery_tp: 1.9364096293694852
ef: 24.173430973791934
reward: 16.46302134442245
step 129:loss:35.391204833984375|running q:10.753872871398926
episode2,iteration9 selected nodes:[14, 9, 15, 4, 12],center node:14
################################################## episode2,iteration9 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.841208951813834,train_acc:0.7267857789993286
node4 epoch1:node_model train_loss:0.6682428941130638,train_acc:0.7721428275108337
node4 epoch2:node_model train_loss:0.5260980767863137,train_acc:0.8132142424583435
node4 epoch3:node_model train_loss:0.4727172298090799,train_acc:0.8310714364051819
node4 epoch4:node_model train_loss:0.46882812146629604,train_acc:0.8307144045829773
node4_model on test-dataset: loss:0.9530857336521149,acc:0.698199987411499
node4 weight score:2838.149711500508
node9: train data size:1857
node9 epoch0:node_model train_loss:0.841527173393651,train_acc:0.7249583005905151
node9 epoch1:node_model train_loss:0.6016648301952764,train_acc:0.8002400398254395
node9 epoch2:node_model train_loss:0.4843173434859828,train_acc:0.8428808450698853
node9 epoch3:node_model train_loss:0.36883988035352605,train_acc:0.8798521757125854
node9 epoch4:node_model train_loss:0.3134271094673558,train_acc:0.9049862623214722
node9_model on test-dataset: loss:0.8456123940646648,acc:0.7264999151229858
node9 weight score:2196.041606100198
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8711306537900653,train_acc:0.732698380947113
node12 epoch1:node_model train_loss:0.5979259674038205,train_acc:0.7975396513938904
node12 epoch2:node_model train_loss:0.4441848929439272,train_acc:0.8411112427711487
node12 epoch3:node_model train_loss:0.3796080340232168,train_acc:0.8747619390487671
node12 epoch4:node_model train_loss:0.33734679967164993,train_acc:0.8879364132881165
node12_model on test-dataset: loss:0.859467346072197,acc:0.7260000705718994
node12 weight score:1554.451144776562
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7239274233579636,train_acc:0.7580091953277588
node14 epoch1:node_model train_loss:0.5237674017747244,train_acc:0.8315741419792175
node14 epoch2:node_model train_loss:0.3572835524876912,train_acc:0.8772684931755066
node14 epoch3:node_model train_loss:0.26519757757584256,train_acc:0.9213889837265015
node14 epoch4:node_model train_loss:0.2271193377673626,train_acc:0.9396759271621704
node14_model on test-dataset: loss:0.9272266470268369,acc:0.7069999575614929
node14 weight score:1263.98438155119
node15: train data size:629
node15 epoch0:node_model train_loss:0.9482250809669495,train_acc:0.7140886783599854
node15 epoch1:node_model train_loss:0.6146472479615893,train_acc:0.7888670563697815
node15 epoch2:node_model train_loss:0.4243995526007244,train_acc:0.8472906947135925
node15 epoch3:node_model train_loss:0.3484811399664198,train_acc:0.8758621215820312
node15 epoch4:node_model train_loss:0.2993559496743338,train_acc:0.9150739312171936
node15_model on test-dataset: loss:1.006548637598753,acc:0.6906999945640564
node15 weight score:624.9077059013838
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6566216130554676,acc:0.7821999812126159
total cost energy:5.743977360306145 | all_enery_cp：3.8495 | all_enery_tp: 1.8944773603061453
ef: 24.398401137073083
reward: 18.65442377676694
step 130:loss:39.621788024902344|running q:11.710803985595703
episode2,iteration10 selected nodes:[1, 12, 8, 5, 16],center node:8
################################################## episode2,iteration10 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.6467254766646553,train_acc:0.7786031365394592
node1 epoch1:node_model train_loss:0.5123267804875093,train_acc:0.822132408618927
node1 epoch2:node_model train_loss:0.4876924398190835,train_acc:0.833823561668396
node1 epoch3:node_model train_loss:0.46814123453462825,train_acc:0.8358088731765747
node1 epoch4:node_model train_loss:0.4026099005166222,train_acc:0.8618380427360535
node1_model on test-dataset: loss:0.8128186821937561,acc:0.741100013256073
node1 weight score:8252.76306629106
node5: train data size:3735
node5 epoch0:node_model train_loss:0.8027741681588324,train_acc:0.7381955981254578
node5 epoch1:node_model train_loss:0.5854261627322749,train_acc:0.8104134798049927
node5 epoch2:node_model train_loss:0.5265147176228071,train_acc:0.8215789794921875
node5 epoch3:node_model train_loss:0.446364501589223,train_acc:0.8465790152549744
node5 epoch4:node_model train_loss:0.415274977684021,train_acc:0.8580827116966248
node5_model on test-dataset: loss:0.8189853338897228,acc:0.7328999638557434
node5 weight score:4560.521227237167
node8: train data size:1798
node8 epoch0:node_model train_loss:0.8708391338586807,train_acc:0.7224602699279785
node8 epoch1:node_model train_loss:0.6096905635462867,train_acc:0.780351459980011
node8 epoch2:node_model train_loss:0.4651789151959949,train_acc:0.8425282835960388
node8 epoch3:node_model train_loss:0.41402190426985425,train_acc:0.8559864163398743
node8 epoch4:node_model train_loss:0.33925436105993056,train_acc:0.8898865580558777
node8_model on test-dataset: loss:0.8584924502670765,acc:0.7232999205589294
node8 weight score:2094.369029617725
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7332706813301358,train_acc:0.758809506893158
node12 epoch1:node_model train_loss:0.4873693266085216,train_acc:0.8327777981758118
node12 epoch2:node_model train_loss:0.3563063932316644,train_acc:0.8770634531974792
node12 epoch3:node_model train_loss:0.2901740436043058,train_acc:0.906349241733551
node12 epoch4:node_model train_loss:0.25701155087777544,train_acc:0.9211905002593994
node12_model on test-dataset: loss:0.8781208869814873,acc:0.726699948310852
node12 weight score:1521.4306137193225
node16: train data size:877
node16 epoch0:node_model train_loss:0.8318656815422906,train_acc:0.7395814657211304
node16 epoch1:node_model train_loss:0.5397301051351759,train_acc:0.8220202326774597
node16 epoch2:node_model train_loss:0.4380015366607242,train_acc:0.8639104962348938
node16 epoch3:node_model train_loss:0.32068490485350293,train_acc:0.9006782174110413
node16 epoch4:node_model train_loss:0.2709312058157391,train_acc:0.9158008694648743
node16_model on test-dataset: loss:0.8936700481176376,acc:0.7127999663352966
node16 weight score:981.3465292333001
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6398862469196319,acc:0.786899983882904
total cost energy:9.56171297616256 | all_enery_cp：7.227 | all_enery_tp: 2.334712976162561
ef: 24.77945625766084
reward: 15.21774328149828
step 131:loss:131.17449951171875|running q:13.073358535766602
episode2,iteration11 selected nodes:[17, 16, 13, 3, 18],center node:16
################################################## episode2,iteration11 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7010457792947459,train_acc:0.7672587633132935
node3 epoch1:node_model train_loss:0.5019542422405509,train_acc:0.8246462345123291
node3 epoch2:node_model train_loss:0.43053245232548826,train_acc:0.8531025052070618
node3 epoch3:node_model train_loss:0.40042869157569355,train_acc:0.8668283224105835
node3 epoch4:node_model train_loss:0.32543017940465796,train_acc:0.893077552318573
node3_model on test-dataset: loss:0.9629719027876854,acc:0.7072001695632935
node3 weight score:4410.305210053852
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8856493880351385,train_acc:0.7429546117782593
node13 epoch1:node_model train_loss:0.5671486159165701,train_acc:0.807121217250824
node13 epoch2:node_model train_loss:0.4595703383286794,train_acc:0.8390151858329773
node13 epoch3:node_model train_loss:0.35332726935545605,train_acc:0.8875758051872253
node13 epoch4:node_model train_loss:0.298466220498085,train_acc:0.9018939733505249
node13_model on test-dataset: loss:1.0117197130620479,acc:0.6792001128196716
node13 weight score:1141.6205349051697
node16: train data size:877
node16 epoch0:node_model train_loss:0.7701422439681159,train_acc:0.7576912045478821
node16 epoch1:node_model train_loss:0.46434299813376534,train_acc:0.8475757837295532
node16 epoch2:node_model train_loss:0.38771960967116886,train_acc:0.8831168413162231
node16 epoch3:node_model train_loss:0.27186668250295853,train_acc:0.9298990368843079
node16 epoch4:node_model train_loss:0.22122053470876482,train_acc:0.9371140003204346
node16_model on test-dataset: loss:0.9972863171994686,acc:0.6894998550415039
node16 weight score:879.3863756827119
node17: train data size:442
node17 epoch0:node_model train_loss:0.8398172736167908,train_acc:0.7393333315849304
node17 epoch1:node_model train_loss:0.546212238073349,train_acc:0.7879047393798828
node17 epoch2:node_model train_loss:0.4747302234172821,train_acc:0.8346666693687439
node17 epoch3:node_model train_loss:0.2924053430557251,train_acc:0.9037141799926758
node17 epoch4:node_model train_loss:0.2149142861366272,train_acc:0.9377142786979675
node17_model on test-dataset: loss:0.891877281665802,acc:0.7183998823165894
node17 weight score:495.5838758158021
node18: train data size:472
node18 epoch0:node_model train_loss:0.7623484075069428,train_acc:0.7516666650772095
node18 epoch1:node_model train_loss:0.47514209151268005,train_acc:0.8333333134651184
node18 epoch2:node_model train_loss:0.34720988273620607,train_acc:0.8841111063957214
node18 epoch3:node_model train_loss:0.24614374935626984,train_acc:0.9236666560173035
node18 epoch4:node_model train_loss:0.17792632728815078,train_acc:0.965222179889679
node18_model on test-dataset: loss:0.9373664961755276,acc:0.7086001634597778
node18 weight score:503.538372585076
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7135329772531986,acc:0.7659999763965607
total cost energy:5.261891799889058 | all_enery_cp：3.5965000000000003 | all_enery_tp: 1.6653917998890586
ef: 24.22237627781043
reward: 18.96048447792137
step 132:loss:62.95378875732422|running q:14.140472412109375
episode2,iteration12 selected nodes:[18, 15, 14, 3, 0],center node:15
################################################## episode2,iteration12 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.7414731876208231,train_acc:0.7585657835006714
node0 epoch1:node_model train_loss:0.5565356669517664,train_acc:0.8054448366165161
node0 epoch2:node_model train_loss:0.45532266337137955,train_acc:0.843797504901886
node0 epoch3:node_model train_loss:0.3986210668316254,train_acc:0.868141770362854
node0 epoch4:node_model train_loss:0.35766672887481177,train_acc:0.8843791484832764
node0_model on test-dataset: loss:0.8321937905251979,acc:0.7378001809120178
node0 weight score:6228.116646639488
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4713719857986583,train_acc:0.8292379975318909
node3 epoch1:node_model train_loss:0.3519266019033831,train_acc:0.8824395537376404
node3 epoch2:node_model train_loss:0.2990205935960592,train_acc:0.8966500759124756
node3 epoch3:node_model train_loss:0.2910965223645055,train_acc:0.8995892405509949
node3 epoch4:node_model train_loss:0.2339329794049263,train_acc:0.9251707196235657
node3_model on test-dataset: loss:0.8623287096619606,acc:0.7443998456001282
node3 weight score:4925.036070832961
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6988223642110825,train_acc:0.7643980979919434
node14 epoch1:node_model train_loss:0.44341713190078735,train_acc:0.8346296548843384
node14 epoch2:node_model train_loss:0.3404619519909223,train_acc:0.8961111307144165
node14 epoch3:node_model train_loss:0.2363955577214559,train_acc:0.9290277361869812
node14 epoch4:node_model train_loss:0.19977983087301254,train_acc:0.9410648345947266
node14_model on test-dataset: loss:0.8041391667723655,acc:0.7447000741958618
node14 weight score:1457.4591668058472
node15: train data size:629
node15 epoch0:node_model train_loss:0.8828833784375872,train_acc:0.7021675705909729
node15 epoch1:node_model train_loss:0.49182051845959257,train_acc:0.8331528306007385
node15 epoch2:node_model train_loss:0.3780081272125244,train_acc:0.8667980432510376
node15 epoch3:node_model train_loss:0.28798426049096243,train_acc:0.9075862765312195
node15 epoch4:node_model train_loss:0.234527445265225,train_acc:0.9245812892913818
node15_model on test-dataset: loss:0.9117704713344574,acc:0.7146998643875122
node15 weight score:689.8666054400758
node18: train data size:472
node18 epoch0:node_model train_loss:0.710090160369873,train_acc:0.7684444785118103
node18 epoch1:node_model train_loss:0.4862566351890564,train_acc:0.8345555663108826
node18 epoch2:node_model train_loss:0.2795038640499115,train_acc:0.9121111035346985
node18 epoch3:node_model train_loss:0.2393743485212326,train_acc:0.9402222037315369
node18 epoch4:node_model train_loss:0.14467824697494508,train_acc:0.9628888964653015
node18_model on test-dataset: loss:0.9814889939129352,acc:0.7083998918533325
node18 weight score:480.90197946923655
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6517590001225472,acc:0.7868999856710434
total cost energy:7.912338699985623 | all_enery_cp：5.8515 | all_enery_tp: 2.060838699985623
ef: 24.75262960956896
reward: 16.840290909583338
step 133:loss:68.25137329101562|running q:15.249775886535645
episode2,iteration13 selected nodes:[5, 11, 8, 4, 17],center node:11
################################################## episode2,iteration13 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7312333871211324,train_acc:0.7599999904632568
node4 epoch1:node_model train_loss:0.6166294036167008,train_acc:0.791071355342865
node4 epoch2:node_model train_loss:0.48742563410529066,train_acc:0.837857186794281
node4 epoch3:node_model train_loss:0.34743013525647776,train_acc:0.8853572010993958
node4 epoch4:node_model train_loss:0.2907392994633743,train_acc:0.9099999666213989
node4_model on test-dataset: loss:0.7941432165354491,acc:0.7539000511169434
node4 weight score:3406.186621855069
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7171198749228528,train_acc:0.76093989610672
node5 epoch1:node_model train_loss:0.5049584931448886,train_acc:0.8290979266166687
node5 epoch2:node_model train_loss:0.4322694202786998,train_acc:0.8538721799850464
node5 epoch3:node_model train_loss:0.3429797993678796,train_acc:0.883571445941925
node5 epoch4:node_model train_loss:0.2932701902954202,train_acc:0.9027819633483887
node5_model on test-dataset: loss:0.8054124242812395,acc:0.7457000017166138
node5 weight score:4637.375693990768
node8: train data size:1798
node8 epoch0:node_model train_loss:0.8541341026624044,train_acc:0.7269274592399597
node8 epoch1:node_model train_loss:0.5129023674461577,train_acc:0.8181406259536743
node8 epoch2:node_model train_loss:0.3923317376110289,train_acc:0.8680952191352844
node8 epoch3:node_model train_loss:0.29486314124531215,train_acc:0.9054761528968811
node8 epoch4:node_model train_loss:0.2541506083475219,train_acc:0.9238207936286926
node8_model on test-dataset: loss:0.743562287837267,acc:0.7600999474525452
node8 weight score:2418.089283723199
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7626774626619676,train_acc:0.7537016272544861
node11 epoch1:node_model train_loss:0.48524202143444733,train_acc:0.8374747633934021
node11 epoch2:node_model train_loss:0.37614258510224957,train_acc:0.8855667114257812
node11 epoch3:node_model train_loss:0.3003573750748354,train_acc:0.9036728143692017
node11 epoch4:node_model train_loss:0.21749704988563762,train_acc:0.9381060600280762
node11_model on test-dataset: loss:0.7875065314769745,acc:0.7552998661994934
node11 weight score:2135.8553012194025
node17: train data size:442
node17 epoch0:node_model train_loss:0.8847198367118836,train_acc:0.7485713958740234
node17 epoch1:node_model train_loss:0.5214724779129029,train_acc:0.8279047012329102
node17 epoch2:node_model train_loss:0.3741686701774597,train_acc:0.8809523582458496
node17 epoch3:node_model train_loss:0.24200113117694855,train_acc:0.9269523620605469
node17 epoch4:node_model train_loss:0.20206613838672638,train_acc:0.9512380957603455
node17_model on test-dataset: loss:0.942311257570982,acc:0.7138999700546265
node17 weight score:469.0594497823934
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.629673410281539,acc:0.7916999816894531
total cost energy:6.860246387258243 | all_enery_cp：5.181 | all_enery_tp: 1.6792463872582433
ef: 24.87292904081763
reward: 18.012682653559388
step 134:loss:56.69890213012695|running q:16.419431686401367
episode2,iteration14 selected nodes:[8, 12, 14, 2, 5],center node:14
################################################## episode2,iteration14 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.718515494838357,train_acc:0.7618464827537537
node2 epoch1:node_model train_loss:0.5236611446986595,train_acc:0.827698826789856
node2 epoch2:node_model train_loss:0.42229869589209557,train_acc:0.8568278551101685
node2 epoch3:node_model train_loss:0.3740649002914627,train_acc:0.8749622106552124
node2 epoch4:node_model train_loss:0.33680766603598994,train_acc:0.886382520198822
node2_model on test-dataset: loss:0.792905927747488,acc:0.7476999759674072
node2 weight score:6038.547364126663
node5: train data size:3735
node5 epoch0:node_model train_loss:0.526961895980333,train_acc:0.8162781596183777
node5 epoch1:node_model train_loss:0.3767742016597798,train_acc:0.8706390857696533
node5 epoch2:node_model train_loss:0.3189222495022573,train_acc:0.8917669057846069
node5 epoch3:node_model train_loss:0.27447254524419185,train_acc:0.9116916656494141
node5 epoch4:node_model train_loss:0.25032644169895274,train_acc:0.9173683524131775
node5_model on test-dataset: loss:0.793031357973814,acc:0.752399742603302
node5 weight score:4709.775928082948
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6619307978285683,train_acc:0.769727885723114
node8 epoch1:node_model train_loss:0.43701554669274223,train_acc:0.8470861911773682
node8 epoch2:node_model train_loss:0.3327777178751098,train_acc:0.8893083930015564
node8 epoch3:node_model train_loss:0.24544990062713623,train_acc:0.9293311238288879
node8 epoch4:node_model train_loss:0.1956727686855528,train_acc:0.9465987086296082
node8_model on test-dataset: loss:0.8274034293740988,acc:0.7441999912261963
node8 weight score:2173.0632677702615
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7741783729621342,train_acc:0.7595238089561462
node12 epoch1:node_model train_loss:0.4892606075320925,train_acc:0.827380895614624
node12 epoch2:node_model train_loss:0.3473132870026997,train_acc:0.8890476822853088
node12 epoch3:node_model train_loss:0.2449712881020137,train_acc:0.9219047427177429
node12 epoch4:node_model train_loss:0.20843423104711942,train_acc:0.9342063069343567
node12_model on test-dataset: loss:0.8573928509652614,acc:0.7411998510360718
node12 weight score:1558.2121993388655
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6501822421948115,train_acc:0.7838425040245056
node14 epoch1:node_model train_loss:0.4204680596788724,train_acc:0.848287045955658
node14 epoch2:node_model train_loss:0.30060190831621486,train_acc:0.8959259390830994
node14 epoch3:node_model train_loss:0.18559085950255394,train_acc:0.9495371580123901
node14 epoch4:node_model train_loss:0.16230929270386696,train_acc:0.9585184454917908
node14_model on test-dataset: loss:0.8033967898786067,acc:0.746999979019165
node14 weight score:1458.8059284840922
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6323782957345248,acc:0.7921999812126159
total cost energy:8.724699754355212 | all_enery_cp：6.4145 | all_enery_tp: 2.3101997543552115
ef: 24.90766080697827
reward: 16.182961052623057
step 135:loss:93.45570373535156|running q:17.677888870239258
episode2,iteration15 selected nodes:[18, 16, 14, 2, 5],center node:14
################################################## episode2,iteration15 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5493972226977348,train_acc:0.8048388957977295
node2 epoch1:node_model train_loss:0.4029093384742737,train_acc:0.8659090399742126
node2 epoch2:node_model train_loss:0.3209536401554942,train_acc:0.8922158479690552
node2 epoch3:node_model train_loss:0.28527475117395323,train_acc:0.9076040983200073
node2 epoch4:node_model train_loss:0.26660805009305477,train_acc:0.9146307706832886
node2_model on test-dataset: loss:0.8253069366514683,acc:0.7445998787879944
node2 weight score:5801.477956100106
node5: train data size:3735
node5 epoch0:node_model train_loss:0.44107941145959656,train_acc:0.8424060344696045
node5 epoch1:node_model train_loss:0.33460664278582525,train_acc:0.88018798828125
node5 epoch2:node_model train_loss:0.2873279248413287,train_acc:0.9016916155815125
node5 epoch3:node_model train_loss:0.23203386484008087,train_acc:0.9230076670646667
node5 epoch4:node_model train_loss:0.20599659806803652,train_acc:0.9359399080276489
node5_model on test-dataset: loss:0.8459948244690895,acc:0.7427000999450684
node5 weight score:4414.92062595528
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5715843687454859,train_acc:0.8062036037445068
node14 epoch1:node_model train_loss:0.33192109192411107,train_acc:0.8835648894309998
node14 epoch2:node_model train_loss:0.255232618500789,train_acc:0.917546272277832
node14 epoch3:node_model train_loss:0.17158141545951366,train_acc:0.957870364189148
node14 epoch4:node_model train_loss:0.10874556098133326,train_acc:0.9774999618530273
node14_model on test-dataset: loss:0.7984554503858089,acc:0.7529998421669006
node14 weight score:1467.8339279088102
node16: train data size:877
node16 epoch0:node_model train_loss:0.6858469578954909,train_acc:0.7806926369667053
node16 epoch1:node_model train_loss:0.4553554587894016,train_acc:0.8577921986579895
node16 epoch2:node_model train_loss:0.28437639276186627,train_acc:0.9111254811286926
node16 epoch3:node_model train_loss:0.2156576696369383,train_acc:0.9316738247871399
node16 epoch4:node_model train_loss:0.16045381956630284,train_acc:0.957114040851593
node16_model on test-dataset: loss:0.7934151731431485,acc:0.7445000410079956
node16 weight score:1105.348157794521
node18: train data size:472
node18 epoch0:node_model train_loss:0.7518659949302673,train_acc:0.7540000081062317
node18 epoch1:node_model train_loss:0.488872104883194,train_acc:0.8184444308280945
node18 epoch2:node_model train_loss:0.31785829067230226,train_acc:0.8977777361869812
node18 epoch3:node_model train_loss:0.21904781758785247,train_acc:0.9453333020210266
node18 epoch4:node_model train_loss:0.18255552053451538,train_acc:0.9432222247123718
node18_model on test-dataset: loss:0.9390243110060692,acc:0.7236000895500183
node18 weight score:502.64939306448827
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6371387906372548,acc:0.793899981379509
total cost energy:7.31663280042979 | all_enery_cp：5.522 | all_enery_tp: 1.7946328004297905
ef: 24.82030119078729
reward: 17.5036683903575
step 136:loss:51.837005615234375|running q:18.80135154724121
episode2,iteration16 selected nodes:[18, 9, 10, 12, 3],center node:10
################################################## episode2,iteration16 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5307448963786281,train_acc:0.8084807991981506
node3 epoch1:node_model train_loss:0.3519111611815386,train_acc:0.8750519156455994
node3 epoch2:node_model train_loss:0.2951925795438678,train_acc:0.8963630795478821
node3 epoch3:node_model train_loss:0.23098044547923777,train_acc:0.919678270816803
node3 epoch4:node_model train_loss:0.18957665770552878,train_acc:0.9424986839294434
node3_model on test-dataset: loss:0.8271669044345618,acc:0.7613999247550964
node3 weight score:5134.393043569825
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7505781838768407,train_acc:0.7704985737800598
node9 epoch1:node_model train_loss:0.46002895267386185,train_acc:0.848005473613739
node9 epoch2:node_model train_loss:0.37815624789187785,train_acc:0.8766942620277405
node9 epoch3:node_model train_loss:0.25853778421878815,train_acc:0.9197229146957397
node9 epoch4:node_model train_loss:0.2164821354182143,train_acc:0.9262972474098206
node9_model on test-dataset: loss:0.8085590010881424,acc:0.7519998550415039
node9 weight score:2296.6784087504893
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7902283355593681,train_acc:0.7576667070388794
node10 epoch1:node_model train_loss:0.5616785779595375,train_acc:0.8098332285881042
node10 epoch2:node_model train_loss:0.403404226899147,train_acc:0.8709999322891235
node10 epoch3:node_model train_loss:0.2665181592106819,train_acc:0.9156665802001953
node10 epoch4:node_model train_loss:0.24318761378526688,train_acc:0.9203333258628845
node10_model on test-dataset: loss:0.7706667253375054,acc:0.7575000524520874
node10 weight score:2562.716067876253
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6931745090654918,train_acc:0.7761110663414001
node12 epoch1:node_model train_loss:0.5274388534682137,train_acc:0.8189682364463806
node12 epoch2:node_model train_loss:0.30740736637796673,train_acc:0.8943650126457214
node12 epoch3:node_model train_loss:0.23825749435595103,train_acc:0.9296032190322876
node12 epoch4:node_model train_loss:0.16981323010155133,train_acc:0.9551587104797363
node12_model on test-dataset: loss:0.8480601823329925,acc:0.7489998936653137
node12 weight score:1575.359895243162
node18: train data size:472
node18 epoch0:node_model train_loss:0.615417218208313,train_acc:0.8011110424995422
node18 epoch1:node_model train_loss:0.3613243043422699,train_acc:0.8633333444595337
node18 epoch2:node_model train_loss:0.23068115413188933,train_acc:0.9373332858085632
node18 epoch3:node_model train_loss:0.19432688653469085,train_acc:0.9388889670372009
node18 epoch4:node_model train_loss:0.10536273419857026,train_acc:0.9852222800254822
node18_model on test-dataset: loss:0.9422301355004311,acc:0.7236000895500183
node18 weight score:500.93918907541035
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6104732951521874,acc:0.8054999816417694
total cost energy:6.750946122204221 | all_enery_cp：4.9435 | all_enery_tp: 1.8074461222042206
ef: 24.746966369228534
reward: 17.996020247024312
step 137:loss:73.3506088256836|running q:19.855310440063477
episode2,iteration17 selected nodes:[18, 12, 10, 11, 19],center node:11
################################################## episode2,iteration17 ##################################################
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6673562094569206,train_acc:0.7858332991600037
node10 epoch1:node_model train_loss:0.41401703655719757,train_acc:0.8715001344680786
node10 epoch2:node_model train_loss:0.3100110165774822,train_acc:0.9005001187324524
node10 epoch3:node_model train_loss:0.22524330243468285,train_acc:0.9338333010673523
node10 epoch4:node_model train_loss:0.20405373387038708,train_acc:0.9409999847412109
node10_model on test-dataset: loss:0.747100617736578,acc:0.7636998295783997
node10 weight score:2643.5528938303864
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7376538883237278,train_acc:0.7600287795066833
node11 epoch1:node_model train_loss:0.4546003166367026,train_acc:0.8478623032569885
node11 epoch2:node_model train_loss:0.34152303460766287,train_acc:0.88968425989151
node11 epoch3:node_model train_loss:0.22513300794012406,train_acc:0.9298134446144104
node11 epoch4:node_model train_loss:0.16877185597139246,train_acc:0.9543183445930481
node11_model on test-dataset: loss:0.7898079842329025,acc:0.7562999725341797
node11 weight score:2129.631547892789
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6045868758644376,train_acc:0.7986508011817932
node12 epoch1:node_model train_loss:0.40624505494322094,train_acc:0.8610318303108215
node12 epoch2:node_model train_loss:0.2602226361632347,train_acc:0.9129365682601929
node12 epoch3:node_model train_loss:0.18049997623477662,train_acc:0.9460317492485046
node12 epoch4:node_model train_loss:0.14051385915705136,train_acc:0.9649205803871155
node12_model on test-dataset: loss:0.7953997656702996,acc:0.7601998448371887
node12 weight score:1679.6585285314052
node18: train data size:472
node18 epoch0:node_model train_loss:0.6363662362098694,train_acc:0.8234444856643677
node18 epoch1:node_model train_loss:0.3680445343255997,train_acc:0.8701111078262329
node18 epoch2:node_model train_loss:0.2685471147298813,train_acc:0.9088889360427856
node18 epoch3:node_model train_loss:0.16398701965808868,train_acc:0.9576666951179504
node18 epoch4:node_model train_loss:0.10779612436890602,train_acc:0.9692221879959106
node18_model on test-dataset: loss:0.9424846568703651,acc:0.727100133895874
node18 weight score:500.80390864646364
node19: train data size:4281
node19 epoch0:node_model train_loss:0.7138629417086757,train_acc:0.7694429159164429
node19 epoch1:node_model train_loss:0.49112436313961827,train_acc:0.8301522731781006
node19 epoch2:node_model train_loss:0.3875741473464079,train_acc:0.8651595711708069
node19 epoch3:node_model train_loss:0.3544526141743327,train_acc:0.8811368942260742
node19 epoch4:node_model train_loss:0.3081600288319033,train_acc:0.8968961834907532
node19_model on test-dataset: loss:0.7913576850295067,acc:0.7514000535011292
node19 weight score:5409.690309433689
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6072496727854013,acc:0.8058999836444855
total cost energy:6.754913190966075 | all_enery_cp：4.872999999999999 | all_enery_tp: 1.8819131909660762
ef: 24.763333992867665
reward: 18.00842080190159
step 138:loss:54.52794647216797|running q:21.198572158813477
episode2,iteration18 selected nodes:[1, 3, 6, 7, 13],center node:7
################################################## episode2,iteration18 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5907853613881504,train_acc:0.7995588779449463
node1 epoch1:node_model train_loss:0.4559517314328867,train_acc:0.8426471948623657
node1 epoch2:node_model train_loss:0.3829706337522058,train_acc:0.8661762475967407
node1 epoch3:node_model train_loss:0.3426384139148628,train_acc:0.8812500834465027
node1 epoch4:node_model train_loss:0.3113953696892542,train_acc:0.8937497735023499
node1_model on test-dataset: loss:0.8226867882162332,acc:0.7419000267982483
node1 weight score:8153.7713940252115
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4182834160882373,train_acc:0.8506927490234375
node3 epoch1:node_model train_loss:0.2547835004191066,train_acc:0.9178177714347839
node3 epoch2:node_model train_loss:0.19834593544865764,train_acc:0.9359871745109558
node3 epoch3:node_model train_loss:0.1748602147712264,train_acc:0.9489806294441223
node3 epoch4:node_model train_loss:0.15419097175431806,train_acc:0.9529638290405273
node3_model on test-dataset: loss:0.8147062654793262,acc:0.7616001963615417
node3 weight score:5212.9217362791605
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7372227224611467,train_acc:0.7727187275886536
node6 epoch1:node_model train_loss:0.5162983661697756,train_acc:0.824562132358551
node6 epoch2:node_model train_loss:0.4434479186611791,train_acc:0.8533639311790466
node6 epoch3:node_model train_loss:0.3757662691416279,train_acc:0.8716129064559937
node6 epoch4:node_model train_loss:0.2932221644347714,train_acc:0.9025805592536926
node6_model on test-dataset: loss:0.8386925084143877,acc:0.7409000396728516
node6 weight score:3585.342625374064
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7403812065720559,train_acc:0.7686765193939209
node7 epoch1:node_model train_loss:0.4601869285106659,train_acc:0.8490980267524719
node7 epoch2:node_model train_loss:0.3360834687948227,train_acc:0.8855392336845398
node7 epoch3:node_model train_loss:0.2866809077560902,train_acc:0.9025980234146118
node7 epoch4:node_model train_loss:0.215386825799942,train_acc:0.9371176958084106
node7_model on test-dataset: loss:0.8157899973541498,acc:0.7476000189781189
node7 weight score:2391.5468519198257
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8862662216027578,train_acc:0.7308332920074463
node13 epoch1:node_model train_loss:0.5251613855361938,train_acc:0.8260606527328491
node13 epoch2:node_model train_loss:0.3464184155066808,train_acc:0.8824242949485779
node13 epoch3:node_model train_loss:0.276480865975221,train_acc:0.9017423987388611
node13 epoch4:node_model train_loss:0.2088506929576397,train_acc:0.9324241876602173
node13_model on test-dataset: loss:0.8975305278599263,acc:0.7319996356964111
node13 weight score:1286.8643061690443
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6036989887803793,acc:0.8035999828577042
total cost energy:9.901257114780908 | all_enery_cp：8.534 | all_enery_tp: 1.3672571147809078
ef: 25.139183182572644
reward: 15.237926067791737
step 139:loss:55.46875762939453|running q:22.39243507385254
episode2,iteration19 selected nodes:[3, 19, 0, 1, 9],center node:9
################################################## episode2,iteration19 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.6134980378242639,train_acc:0.7934081554412842
node0 epoch1:node_model train_loss:0.4195036698992436,train_acc:0.8611446022987366
node0 epoch2:node_model train_loss:0.332577638901197,train_acc:0.8922196626663208
node0 epoch3:node_model train_loss:0.30939962829534823,train_acc:0.8945667147636414
node0 epoch4:node_model train_loss:0.27074455985656154,train_acc:0.9090731739997864
node0_model on test-dataset: loss:0.6929232849180699,acc:0.7834999561309814
node0 weight score:7479.904504310069
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4295575986890232,train_acc:0.8475002646446228
node1 epoch1:node_model train_loss:0.3304039417382549,train_acc:0.8886029720306396
node1 epoch2:node_model train_loss:0.29835915784625444,train_acc:0.8940439820289612
node1 epoch3:node_model train_loss:0.25712525318650636,train_acc:0.9123528003692627
node1 epoch4:node_model train_loss:0.2639373619766796,train_acc:0.9097058176994324
node1_model on test-dataset: loss:0.780473880469799,acc:0.770099937915802
node1 weight score:8594.778336415538
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3240043112011843,train_acc:0.88400799036026
node3 epoch1:node_model train_loss:0.24524867084137228,train_acc:0.9180751442909241
node3 epoch2:node_model train_loss:0.19993736317684485,train_acc:0.9363926649093628
node3 epoch3:node_model train_loss:0.1390315852192945,train_acc:0.9622957110404968
node3 epoch4:node_model train_loss:0.11553774462189785,train_acc:0.9697674512863159
node3_model on test-dataset: loss:0.7912649168074131,acc:0.7630999088287354
node3 weight score:5367.3553695970095
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7040098638910997,train_acc:0.7750968933105469
node9 epoch1:node_model train_loss:0.46028953633810343,train_acc:0.8485318422317505
node9 epoch2:node_model train_loss:0.30114243140346125,train_acc:0.8943306803703308
node9 epoch3:node_model train_loss:0.21314772806669535,train_acc:0.9369713664054871
node9 epoch4:node_model train_loss:0.176947840734532,train_acc:0.9513111114501953
node9_model on test-dataset: loss:0.8125730853527784,acc:0.7594001889228821
node9 weight score:2285.332893094513
node19: train data size:4281
node19 epoch0:node_model train_loss:0.5669615927130677,train_acc:0.811863362789154
node19 epoch1:node_model train_loss:0.37379485680613406,train_acc:0.8775251507759094
node19 epoch2:node_model train_loss:0.2937258790398753,train_acc:0.9059372544288635
node19 epoch3:node_model train_loss:0.24897208497967832,train_acc:0.9194402694702148
node19 epoch4:node_model train_loss:0.20978854769884153,train_acc:0.9393855333328247
node19_model on test-dataset: loss:0.8111101734638214,acc:0.7550000548362732
node19 weight score:5277.951306809678
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5951346750557422,acc:0.8092999809980392
total cost energy:12.89445202109687 | all_enery_cp：11.137999999999998 | all_enery_tp: 1.7564520210968708
ef: 25.43406564443412
reward: 12.539613623337251
step 140:loss:61.20551300048828|running q:23.59395980834961
episode2,iteration20 selected nodes:[1, 9, 13, 14, 11],center node:11
################################################## episode2,iteration20 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.385618854971493,train_acc:0.8657354116439819
node1 epoch1:node_model train_loss:0.30241699889302254,train_acc:0.8919851779937744
node1 epoch2:node_model train_loss:0.25607281068668647,train_acc:0.9122057557106018
node1 epoch3:node_model train_loss:0.23543929768835797,train_acc:0.92463219165802
node1 epoch4:node_model train_loss:0.2035698169732795,train_acc:0.9324999451637268
node1_model on test-dataset: loss:0.8512276144325733,acc:0.7597000598907471
node1 weight score:7880.383444176138
node9: train data size:1857
node9 epoch0:node_model train_loss:0.67011706138912,train_acc:0.8015604615211487
node9 epoch1:node_model train_loss:0.3833297930265728,train_acc:0.874468982219696
node9 epoch2:node_model train_loss:0.25669740062010915,train_acc:0.9189382195472717
node9 epoch3:node_model train_loss:0.18602289612355985,train_acc:0.9436749219894409
node9 epoch4:node_model train_loss:0.14932475788028618,train_acc:0.9576268792152405
node9_model on test-dataset: loss:0.748208786547184,acc:0.7714999914169312
node9 weight score:2481.9275493537557
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6016160775633419,train_acc:0.8057101964950562
node11 epoch1:node_model train_loss:0.35127149609958425,train_acc:0.8800143599510193
node11 epoch2:node_model train_loss:0.26646772465285135,train_acc:0.9174605011940002
node11 epoch3:node_model train_loss:0.1850795219926273,train_acc:0.9526255130767822
node11 epoch4:node_model train_loss:0.12179575027788386,train_acc:0.9710473418235779
node11_model on test-dataset: loss:0.7679727480560541,acc:0.770799994468689
node11 weight score:2190.1818837420924
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7295411949356397,train_acc:0.7783333659172058
node13 epoch1:node_model train_loss:0.4483296622832616,train_acc:0.8551515340805054
node13 epoch2:node_model train_loss:0.3372594987352689,train_acc:0.8823485374450684
node13 epoch3:node_model train_loss:0.22276715810100237,train_acc:0.9317424297332764
node13 epoch4:node_model train_loss:0.13834336151679358,train_acc:0.9679545164108276
node13_model on test-dataset: loss:0.8190388892591,acc:0.7625001072883606
node13 weight score:1410.1894490563318
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6021327152848244,train_acc:0.8006018996238708
node14 epoch1:node_model train_loss:0.3651608241101106,train_acc:0.8652777671813965
node14 epoch2:node_model train_loss:0.25966542835036915,train_acc:0.9115740656852722
node14 epoch3:node_model train_loss:0.17181512837608656,train_acc:0.9506944417953491
node14 epoch4:node_model train_loss:0.1022702536235253,train_acc:0.9730091094970703
node14_model on test-dataset: loss:0.7221263574063778,acc:0.7774999737739563
node14 weight score:1622.9846590967943
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6402842123806477,acc:0.802299981713295
total cost energy:7.720508749109257 | all_enery_cp：6.287 | all_enery_tp: 1.4335087491092573
ef: 25.159992922347595
reward: 17.439484173238338
step 141:loss:82.46817016601562|running q:24.933063507080078
episode2,iteration21 selected nodes:[12, 19, 15, 9, 16],center node:16
################################################## episode2,iteration21 ##################################################
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5570259533430401,train_acc:0.805632472038269
node9 epoch1:node_model train_loss:0.32963161484191295,train_acc:0.892880916595459
node9 epoch2:node_model train_loss:0.22999618320088638,train_acc:0.9220960736274719
node9 epoch3:node_model train_loss:0.16273922473192215,train_acc:0.9540718793869019
node9 epoch4:node_model train_loss:0.12547212937160543,train_acc:0.9717081189155579
node9_model on test-dataset: loss:0.770565721988678,acc:0.765500009059906
node9 weight score:2409.9177357739836
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6556602865457535,train_acc:0.7940475940704346
node12 epoch1:node_model train_loss:0.3920675665140152,train_acc:0.8732540607452393
node12 epoch2:node_model train_loss:0.2609967514872551,train_acc:0.921746015548706
node12 epoch3:node_model train_loss:0.1965279658990247,train_acc:0.9315873384475708
node12 epoch4:node_model train_loss:0.1287810674735478,train_acc:0.9657142162322998
node12_model on test-dataset: loss:0.8257434611022473,acc:0.7620998024940491
node12 weight score:1617.9359122222227
node15: train data size:629
node15 epoch0:node_model train_loss:0.8199839081083026,train_acc:0.7408867478370667
node15 epoch1:node_model train_loss:0.4768220952578953,train_acc:0.8339409232139587
node15 epoch2:node_model train_loss:0.4017407170363835,train_acc:0.8817241787910461
node15 epoch3:node_model train_loss:0.25179105358464376,train_acc:0.9122167229652405
node15 epoch4:node_model train_loss:0.16333028035504477,train_acc:0.9466502070426941
node15_model on test-dataset: loss:0.8656851182878017,acc:0.7454999089241028
node15 weight score:726.5921369239543
node16: train data size:877
node16 epoch0:node_model train_loss:0.7539926899804009,train_acc:0.7673592567443848
node16 epoch1:node_model train_loss:0.43606217371092904,train_acc:0.8578932285308838
node16 epoch2:node_model train_loss:0.29859472148948246,train_acc:0.9017893671989441
node16 epoch3:node_model train_loss:0.18913642234272426,train_acc:0.9437806010246277
node16 epoch4:node_model train_loss:0.12272728068961038,train_acc:0.9693360924720764
node16_model on test-dataset: loss:0.7739873743057251,acc:0.7664999961853027
node16 weight score:1133.0934187223381
node19: train data size:4281
node19 epoch0:node_model train_loss:0.5762440402840459,train_acc:0.8102898597717285
node19 epoch1:node_model train_loss:0.34602930968584017,train_acc:0.8809875845909119
node19 epoch2:node_model train_loss:0.26840867621954095,train_acc:0.9118890166282654
node19 epoch3:node_model train_loss:0.19910568605328716,train_acc:0.9400689005851746
node19 epoch4:node_model train_loss:0.16964524481878723,train_acc:0.9510536193847656
node19_model on test-dataset: loss:0.84089887753129,acc:0.7545999884605408
node19 weight score:5090.980752130571
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6274511189758778,acc:0.804899984896183
total cost energy:6.213606797749979 | all_enery_cp：4.49 | all_enery_tp: 1.723606797749979
ef: 24.771986967009543
reward: 18.558380169259564
step 142:loss:94.10364532470703|running q:26.172748565673828
episode2,iteration22 selected nodes:[9, 17, 2, 16, 6],center node:9
################################################## episode2,iteration22 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5775604366014401,train_acc:0.8139394521713257
node2 epoch1:node_model train_loss:0.3449505539610982,train_acc:0.8826895952224731
node2 epoch2:node_model train_loss:0.29866376612335443,train_acc:0.8964395523071289
node2 epoch3:node_model train_loss:0.21446820814162493,train_acc:0.934109628200531
node2 epoch4:node_model train_loss:0.19117449751744667,train_acc:0.9409564733505249
node2_model on test-dataset: loss:0.6995283387601375,acc:0.7872999310493469
node2 weight score:6844.611911629452
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7242837875120102,train_acc:0.7798156142234802
node6 epoch1:node_model train_loss:0.4907299347462193,train_acc:0.8353917598724365
node6 epoch2:node_model train_loss:0.36179398913537303,train_acc:0.876681923866272
node6 epoch3:node_model train_loss:0.3451396212462456,train_acc:0.892488420009613
node6 epoch4:node_model train_loss:0.27277541929675686,train_acc:0.9103226661682129
node6_model on test-dataset: loss:0.8693591852486133,acc:0.7491000294685364
node6 weight score:3458.869534046596
node9: train data size:1857
node9 epoch0:node_model train_loss:0.47932626542292145,train_acc:0.8333979845046997
node9 epoch1:node_model train_loss:0.2675052598903054,train_acc:0.9159095287322998
node9 epoch2:node_model train_loss:0.2013910075551585,train_acc:0.9377561807632446
node9 epoch3:node_model train_loss:0.14679252277863652,train_acc:0.9626222848892212
node9 epoch4:node_model train_loss:0.10755057064326186,train_acc:0.9727514982223511
node9_model on test-dataset: loss:0.8274026411026716,acc:0.7627000212669373
node9 weight score:2244.372821345112
node16: train data size:877
node16 epoch0:node_model train_loss:0.6408596899774339,train_acc:0.8169119954109192
node16 epoch1:node_model train_loss:0.35941239529185826,train_acc:0.8672438263893127
node16 epoch2:node_model train_loss:0.26444106300671893,train_acc:0.9068975448608398
node16 epoch3:node_model train_loss:0.17421582010057238,train_acc:0.9448917508125305
node16 epoch4:node_model train_loss:0.115070348398553,train_acc:0.9696680903434753
node16_model on test-dataset: loss:0.7960594077408314,acc:0.7630001902580261
node16 weight score:1101.676572718201
node17: train data size:442
node17 epoch0:node_model train_loss:0.7463469266891479,train_acc:0.7540952563285828
node17 epoch1:node_model train_loss:0.404950875043869,train_acc:0.8654285669326782
node17 epoch2:node_model train_loss:0.26321440041065214,train_acc:0.9261904954910278
node17 epoch3:node_model train_loss:0.1685890182852745,train_acc:0.9444762468338013
node17 epoch4:node_model train_loss:0.1357615262269974,train_acc:0.9589523673057556
node17_model on test-dataset: loss:0.8545218896865845,acc:0.744700014591217
node17 weight score:517.2483061400728
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5981564328074456,acc:0.8117999804019927
total cost energy:6.934028137423857 | all_enery_cp：5.4855 | all_enery_tp: 1.4485281374238572
ef: 24.845258001279873
reward: 17.911229863856015
step 143:loss:57.123008728027344|running q:27.67438316345215
episode2,iteration23 selected nodes:[19, 6, 18, 1, 10],center node:6
################################################## episode2,iteration23 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.36194487836431055,train_acc:0.8719116449356079
node1 epoch1:node_model train_loss:0.2399709480266799,train_acc:0.9161761999130249
node1 epoch2:node_model train_loss:0.19996484369039536,train_acc:0.9333087801933289
node1 epoch3:node_model train_loss:0.18415862880647182,train_acc:0.9415439367294312
node1 epoch4:node_model train_loss:0.21313948217121995,train_acc:0.9272057414054871
node1_model on test-dataset: loss:0.9644266214221716,acc:0.7287998795509338
node1 weight score:6955.428076122772
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5577119435033491,train_acc:0.8186174631118774
node6 epoch1:node_model train_loss:0.39616018269331227,train_acc:0.872258186340332
node6 epoch2:node_model train_loss:0.3054668533225213,train_acc:0.8969122171401978
node6 epoch3:node_model train_loss:0.266533124831415,train_acc:0.9141014218330383
node6 epoch4:node_model train_loss:0.25238068546018294,train_acc:0.9185252785682678
node6_model on test-dataset: loss:0.902349898442626,acc:0.7345000505447388
node6 weight score:3332.4101938613935
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6719184517860413,train_acc:0.7884999513626099
node10 epoch1:node_model train_loss:0.40212335586547854,train_acc:0.8651666045188904
node10 epoch2:node_model train_loss:0.2740864686667919,train_acc:0.9224998354911804
node10 epoch3:node_model train_loss:0.20734439119696618,train_acc:0.940666675567627
node10 epoch4:node_model train_loss:0.17368996441364287,train_acc:0.9521664977073669
node10_model on test-dataset: loss:0.7762578424811363,acc:0.7651999592781067
node10 weight score:2544.2577091232342
node18: train data size:472
node18 epoch0:node_model train_loss:0.6572849333286286,train_acc:0.8012221455574036
node18 epoch1:node_model train_loss:0.3205236613750458,train_acc:0.875
node18 epoch2:node_model train_loss:0.2405553936958313,train_acc:0.9253333210945129
node18 epoch3:node_model train_loss:0.1599864885210991,train_acc:0.9556666612625122
node18 epoch4:node_model train_loss:0.06577063724398613,train_acc:0.9892222285270691
node18_model on test-dataset: loss:0.876995072066784,acc:0.7499000430107117
node18 weight score:538.2014278457164
node19: train data size:4281
node19 epoch0:node_model train_loss:0.43016628401224005,train_acc:0.851234495639801
node19 epoch1:node_model train_loss:0.25087363099636034,train_acc:0.915951669216156
node19 epoch2:node_model train_loss:0.2003072760132856,train_acc:0.9376887679100037
node19 epoch3:node_model train_loss:0.15478371066409488,train_acc:0.9535715579986572
node19 epoch4:node_model train_loss:0.15413032818672268,train_acc:0.9530921578407288
node19_model on test-dataset: loss:0.8121299712359905,acc:0.7662997245788574
node19 weight score:5271.3237432731285
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6116639207303524,acc:0.8123999816179276
total cost energy:10.315475270070987 | all_enery_cp：8.2215 | all_enery_tp: 2.0939752700709855
ef: 24.660228184697967
reward: 14.34475291462698
step 144:loss:99.48542785644531|running q:28.960617065429688
episode2,iteration24 selected nodes:[19, 17, 6, 7, 4],center node:7
################################################## episode2,iteration24 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6922706557171685,train_acc:0.7932142615318298
node4 epoch1:node_model train_loss:0.4816125695194517,train_acc:0.8475000858306885
node4 epoch2:node_model train_loss:0.4259246231189796,train_acc:0.8628571629524231
node4 epoch3:node_model train_loss:0.38335488949503216,train_acc:0.8642857670783997
node4 epoch4:node_model train_loss:0.25531083026102613,train_acc:0.914642870426178
node4_model on test-dataset: loss:0.7639766984432935,acc:0.765999972820282
node4 weight score:3540.683905035069
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5589773385755478,train_acc:0.8086634874343872
node6 epoch1:node_model train_loss:0.40394212449750594,train_acc:0.8621658682823181
node6 epoch2:node_model train_loss:0.31175010963793726,train_acc:0.8949769139289856
node6 epoch3:node_model train_loss:0.3277781985459789,train_acc:0.8983871340751648
node6 epoch4:node_model train_loss:0.21764458187164798,train_acc:0.9266819953918457
node6_model on test-dataset: loss:0.7840873370319605,acc:0.7657998204231262
node6 weight score:3835.0319638913265
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7380931600928307,train_acc:0.776774525642395
node7 epoch1:node_model train_loss:0.42831370383501055,train_acc:0.8612157106399536
node7 epoch2:node_model train_loss:0.28821960985660555,train_acc:0.9085980653762817
node7 epoch3:node_model train_loss:0.21845487393438817,train_acc:0.9325000643730164
node7 epoch4:node_model train_loss:0.1620871987193823,train_acc:0.9525391459465027
node7_model on test-dataset: loss:0.7436679762601852,acc:0.7813000082969666
node7 weight score:2623.4826055188487
node17: train data size:442
node17 epoch0:node_model train_loss:0.7776153683662415,train_acc:0.7768570780754089
node17 epoch1:node_model train_loss:0.3770004689693451,train_acc:0.872952401638031
node17 epoch2:node_model train_loss:0.31744916141033175,train_acc:0.8986666798591614
node17 epoch3:node_model train_loss:0.15214388072490692,train_acc:0.9564762115478516
node17 epoch4:node_model train_loss:0.1361495390534401,train_acc:0.9572381377220154
node17_model on test-dataset: loss:0.8248842363059521,acc:0.7615000605583191
node17 weight score:535.8327636122517
node19: train data size:4281
node19 epoch0:node_model train_loss:0.33875432645165643,train_acc:0.8814929723739624
node19 epoch1:node_model train_loss:0.2163588496488194,train_acc:0.9264712929725647
node19 epoch2:node_model train_loss:0.16106457731058432,train_acc:0.9521073698997498
node19 epoch3:node_model train_loss:0.12080815867629162,train_acc:0.9675106406211853
node19 epoch4:node_model train_loss:0.10959895119764083,train_acc:0.972572386264801
node19_model on test-dataset: loss:0.7587909869849682,acc:0.7802998423576355
node19 weight score:5641.869860645575
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6103327092900872,acc:0.8136999791860581
total cost energy:7.947744246730289 | all_enery_cp：6.1930000000000005 | all_enery_tp: 1.7547442467302883
ef: 25.05641323448804
reward: 17.108668987757753
step 145:loss:46.49604797363281|running q:30.10501480102539
episode2,iteration25 selected nodes:[16, 4, 15, 19, 5],center node:16
################################################## episode2,iteration25 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5637910046747753,train_acc:0.8128572106361389
node4 epoch1:node_model train_loss:0.4152325107050793,train_acc:0.8653572201728821
node4 epoch2:node_model train_loss:0.2862405228827681,train_acc:0.904285728931427
node4 epoch3:node_model train_loss:0.2367109041661024,train_acc:0.9225001335144043
node4 epoch4:node_model train_loss:0.16580364666879177,train_acc:0.9532143473625183
node4_model on test-dataset: loss:0.7413687840104103,acc:0.777999997138977
node4 weight score:3648.656455923853
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6134987598971317,train_acc:0.8058646321296692
node5 epoch1:node_model train_loss:0.3463077172636986,train_acc:0.8811654448509216
node5 epoch2:node_model train_loss:0.2497865177298847,train_acc:0.9148497581481934
node5 epoch3:node_model train_loss:0.21121145863282054,train_acc:0.932105302810669
node5 epoch4:node_model train_loss:0.1639235282414838,train_acc:0.9509397745132446
node5_model on test-dataset: loss:0.7205213350057602,acc:0.7770999670028687
node5 weight score:5183.746571460151
node15: train data size:629
node15 epoch0:node_model train_loss:0.7448383484567914,train_acc:0.7761576771736145
node15 epoch1:node_model train_loss:0.44279518723487854,train_acc:0.8525123000144958
node15 epoch2:node_model train_loss:0.3299832216330937,train_acc:0.878866970539093
node15 epoch3:node_model train_loss:0.20349857530423573,train_acc:0.9287192821502686
node15 epoch4:node_model train_loss:0.12994918014322007,train_acc:0.9665024280548096
node15_model on test-dataset: loss:0.844526293873787,acc:0.753000020980835
node15 weight score:744.7962302213447
node16: train data size:877
node16 epoch0:node_model train_loss:0.6657057934337192,train_acc:0.7842568159103394
node16 epoch1:node_model train_loss:0.3487326039208306,train_acc:0.8836796283721924
node16 epoch2:node_model train_loss:0.23142257663938734,train_acc:0.9364501237869263
node16 epoch3:node_model train_loss:0.1572099799911181,train_acc:0.9561182856559753
node16 epoch4:node_model train_loss:0.12270825645989841,train_acc:0.9748916625976562
node16_model on test-dataset: loss:0.7863976456224918,acc:0.7661998867988586
node16 weight score:1115.2118840663488
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2689644489870515,train_acc:0.9037495255470276
node19 epoch1:node_model train_loss:0.17116864164208256,train_acc:0.946347713470459
node19 epoch2:node_model train_loss:0.13227649864762328,train_acc:0.9594399333000183
node19 epoch3:node_model train_loss:0.11434315812102584,train_acc:0.9717512726783752
node19 epoch4:node_model train_loss:0.08510990605451339,train_acc:0.9812861680984497
node19_model on test-dataset: loss:0.784047135412693,acc:0.7806001305580139
node19 weight score:5460.130911321604
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5941109228879213,acc:0.8122999805212021
total cost energy:8.176669110970202 | all_enery_cp：6.1135 | all_enery_tp: 2.0631691109702026
ef: 25.021435128924512
reward: 16.84476601795431
step 146:loss:41.083091735839844|running q:31.240877151489258
episode2,iteration26 selected nodes:[6, 3, 15, 17, 2],center node:6
################################################## episode2,iteration26 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.46466108163197833,train_acc:0.8429639339447021
node2 epoch1:node_model train_loss:0.3000346580520272,train_acc:0.8968276977539062
node2 epoch2:node_model train_loss:0.2200403194874525,train_acc:0.9320927858352661
node2 epoch3:node_model train_loss:0.17539730590457717,train_acc:0.9406628608703613
node2 epoch4:node_model train_loss:0.13334763267387947,train_acc:0.9600946307182312
node2_model on test-dataset: loss:0.7857783199846744,acc:0.7711999416351318
node2 weight score:6093.321587306435
node3: train data size:4247
node3 epoch0:node_model train_loss:0.48370768996172175,train_acc:0.8391242623329163
node3 epoch1:node_model train_loss:0.260237357297609,train_acc:0.9115931391716003
node3 epoch2:node_model train_loss:0.20157089049732962,train_acc:0.9291538000106812
node3 epoch3:node_model train_loss:0.15532172991092816,train_acc:0.9533991813659668
node3 epoch4:node_model train_loss:0.13337362782899723,train_acc:0.9618010520935059
node3_model on test-dataset: loss:0.7661011970043182,acc:0.7747998833656311
node3 weight score:5543.654045453816
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5220627534774042,train_acc:0.829815685749054
node6 epoch1:node_model train_loss:0.33954085577880183,train_acc:0.890967607498169
node6 epoch2:node_model train_loss:0.25805896280273316,train_acc:0.911336362361908
node6 epoch3:node_model train_loss:0.27649699728335103,train_acc:0.9041014313697815
node6 epoch4:node_model train_loss:0.18280328569873686,train_acc:0.938617467880249
node6_model on test-dataset: loss:0.797501385807991,acc:0.7683000564575195
node6 weight score:3770.5263633535237
node15: train data size:629
node15 epoch0:node_model train_loss:0.6890296765736171,train_acc:0.7840887308120728
node15 epoch1:node_model train_loss:0.3870850397007806,train_acc:0.8653694987297058
node15 epoch2:node_model train_loss:0.2791851099048342,train_acc:0.9066502451896667
node15 epoch3:node_model train_loss:0.16325507419449942,train_acc:0.9431527256965637
node15 epoch4:node_model train_loss:0.13610088612352098,train_acc:0.9636452794075012
node15_model on test-dataset: loss:0.8977793183922768,acc:0.7465998530387878
node15 weight score:700.6176095996499
node17: train data size:442
node17 epoch0:node_model train_loss:0.7033480644226074,train_acc:0.7986666560173035
node17 epoch1:node_model train_loss:0.38445481061935427,train_acc:0.8661904335021973
node17 epoch2:node_model train_loss:0.2147444263100624,train_acc:0.9380000233650208
node17 epoch3:node_model train_loss:0.1664259761571884,train_acc:0.9472380876541138
node17 epoch4:node_model train_loss:0.1128526747226715,train_acc:0.9719999432563782
node17_model on test-dataset: loss:0.9274745081365109,acc:0.7413000464439392
node17 weight score:476.56296331860364
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.607743925601244,acc:0.8104999828338623
total cost energy:8.37124626476848 | all_enery_cp：6.5565 | all_enery_tp: 1.8147462647684802
ef: 24.68365814752581
reward: 16.31241188275733
step 147:loss:50.165096282958984|running q:32.30022430419922
episode2,iteration27 selected nodes:[10, 14, 11, 13, 1],center node:11
################################################## episode2,iteration27 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.34798622503876686,train_acc:0.8798531293869019
node1 epoch1:node_model train_loss:0.24738367844153852,train_acc:0.9160292744636536
node1 epoch2:node_model train_loss:0.16866863738088048,train_acc:0.9472793340682983
node1 epoch3:node_model train_loss:0.16616214603623924,train_acc:0.9472057223320007
node1 epoch4:node_model train_loss:0.11379965843961519,train_acc:0.9672794342041016
node1_model on test-dataset: loss:0.8336659675836563,acc:0.7697999477386475
node1 weight score:8046.388194834005
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6776179909706116,train_acc:0.7878333330154419
node10 epoch1:node_model train_loss:0.3682524789124727,train_acc:0.8736667037010193
node10 epoch2:node_model train_loss:0.26216495372354987,train_acc:0.9220000505447388
node10 epoch3:node_model train_loss:0.17676764242351056,train_acc:0.9488332867622375
node10 epoch4:node_model train_loss:0.1290929105132818,train_acc:0.9708332419395447
node10_model on test-dataset: loss:0.7729120391607285,acc:0.7705000638961792
node10 weight score:2555.2713632777236
node11: train data size:1682
node11 epoch0:node_model train_loss:0.654423887238783,train_acc:0.7959684133529663
node11 epoch1:node_model train_loss:0.38724186315256004,train_acc:0.8766140937805176
node11 epoch2:node_model train_loss:0.2111465523348135,train_acc:0.9379769563674927
node11 epoch3:node_model train_loss:0.1594843978390974,train_acc:0.9572595953941345
node11 epoch4:node_model train_loss:0.09805699437856674,train_acc:0.9763413667678833
node11_model on test-dataset: loss:0.7356869071722031,acc:0.7787999510765076
node11 weight score:2286.298673528374
node13: train data size:1155
node13 epoch0:node_model train_loss:0.757394736011823,train_acc:0.773863673210144
node13 epoch1:node_model train_loss:0.4102327898144722,train_acc:0.8646969795227051
node13 epoch2:node_model train_loss:0.2960258337358634,train_acc:0.9026515483856201
node13 epoch3:node_model train_loss:0.2066846564412117,train_acc:0.9404544830322266
node13 epoch4:node_model train_loss:0.13817632384598255,train_acc:0.9593181014060974
node13_model on test-dataset: loss:0.7574502375721931,acc:0.7792998552322388
node13 weight score:1524.852647352845
node14: train data size:1172
node14 epoch0:node_model train_loss:0.575559047361215,train_acc:0.8169907927513123
node14 epoch1:node_model train_loss:0.3425400381286939,train_acc:0.8884259462356567
node14 epoch2:node_model train_loss:0.2076917470743259,train_acc:0.9408332705497742
node14 epoch3:node_model train_loss:0.14416136095921198,train_acc:0.958194375038147
node14 epoch4:node_model train_loss:0.0863597138474385,train_acc:0.9781942367553711
node14_model on test-dataset: loss:0.7615047144889832,acc:0.7851999998092651
node14 weight score:1539.0581012836992
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6244735431671142,acc:0.8120999783277512
total cost energy:7.872129717376117 | all_enery_cp：6.346 | all_enery_tp: 1.5261297173761166
ef: 25.150510318661752
reward: 17.278380601285633
step 148:loss:63.1612434387207|running q:33.47163772583008
episode2,iteration28 selected nodes:[4, 17, 11, 18, 6],center node:11
################################################## episode2,iteration28 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6695737721664565,train_acc:0.7985716462135315
node4 epoch1:node_model train_loss:0.4556629141526563,train_acc:0.8457143902778625
node4 epoch2:node_model train_loss:0.2837319379406316,train_acc:0.9003571271896362
node4 epoch3:node_model train_loss:0.25414365875933853,train_acc:0.9239285588264465
node4 epoch4:node_model train_loss:0.2379491270652839,train_acc:0.9185713529586792
node4_model on test-dataset: loss:0.8567532747983932,acc:0.7585000991821289
node4 weight score:3157.26835200779
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5103711115737115,train_acc:0.829400897026062
node6 epoch1:node_model train_loss:0.33332355320453644,train_acc:0.8867740631103516
node6 epoch2:node_model train_loss:0.22471735746629776,train_acc:0.9260368347167969
node6 epoch3:node_model train_loss:0.20795737086765229,train_acc:0.937649667263031
node6 epoch4:node_model train_loss:0.2101936032695155,train_acc:0.9295852780342102
node6_model on test-dataset: loss:0.8254991760849952,acc:0.7609000205993652
node6 weight score:3642.6444593936126
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5309725175885593,train_acc:0.8321807384490967
node11 epoch1:node_model train_loss:0.30134888168643503,train_acc:0.8956384658813477
node11 epoch2:node_model train_loss:0.21366068720817566,train_acc:0.9372596144676208
node11 epoch3:node_model train_loss:0.15546498360002742,train_acc:0.9621663689613342
node11 epoch4:node_model train_loss:0.09088006812859983,train_acc:0.9816353917121887
node11_model on test-dataset: loss:0.7565623138844967,acc:0.7850998640060425
node11 weight score:2223.2140950345943
node17: train data size:442
node17 epoch0:node_model train_loss:0.8020033299922943,train_acc:0.7816190123558044
node17 epoch1:node_model train_loss:0.41413567066192625,train_acc:0.8649523854255676
node17 epoch2:node_model train_loss:0.26665709614753724,train_acc:0.915238082408905
node17 epoch3:node_model train_loss:0.18925065100193023,train_acc:0.9489523768424988
node17 epoch4:node_model train_loss:0.11812178790569305,train_acc:0.9604762196540833
node17_model on test-dataset: loss:0.8898452359437943,acc:0.751500129699707
node17 weight score:496.71558844859425
node18: train data size:472
node18 epoch0:node_model train_loss:0.6422841548919678,train_acc:0.7998888492584229
node18 epoch1:node_model train_loss:0.28275197744369507,train_acc:0.8973332643508911
node18 epoch2:node_model train_loss:0.1933373838663101,train_acc:0.9348889589309692
node18 epoch3:node_model train_loss:0.10770784467458724,train_acc:0.9668889045715332
node18 epoch4:node_model train_loss:0.06489198058843612,train_acc:0.9892221689224243
node18_model on test-dataset: loss:0.8937683053314686,acc:0.748999834060669
node18 weight score:528.1010718151961
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6186333148181439,acc:0.8096999806165696
total cost energy:5.7873473336821855 | all_enery_cp：4.154000000000001 | all_enery_tp: 1.6333473336821847
ef: 24.55611290109496
reward: 18.768765567412775
step 149:loss:91.32002258300781|running q:34.74814224243164
episode2,iteration29 selected nodes:[0, 18, 9, 7, 2],center node:9
################################################## episode2,iteration29 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.6107608068447846,train_acc:0.8071802258491516
node0 epoch1:node_model train_loss:0.393152330070734,train_acc:0.8629889488220215
node0 epoch2:node_model train_loss:0.3236770893518741,train_acc:0.8912973999977112
node0 epoch3:node_model train_loss:0.23648188105569437,train_acc:0.9216867685317993
node0 epoch4:node_model train_loss:0.19551382743968412,train_acc:0.9388019442558289
node0_model on test-dataset: loss:0.7217552411556244,acc:0.783700168132782
node0 weight score:7181.104763023737
node2: train data size:4788
node2 epoch0:node_model train_loss:0.37803388262788457,train_acc:0.8631155490875244
node2 epoch1:node_model train_loss:0.25225342887764174,train_acc:0.9110795259475708
node2 epoch2:node_model train_loss:0.18037496786564589,train_acc:0.9411076903343201
node2 epoch3:node_model train_loss:0.1474292976781726,train_acc:0.9565812349319458
node2 epoch4:node_model train_loss:0.11709258883881073,train_acc:0.9691097736358643
node2_model on test-dataset: loss:0.7625739122927189,acc:0.7766001224517822
node2 weight score:6278.735638365368
node7: train data size:1951
node7 epoch0:node_model train_loss:0.700076374411583,train_acc:0.7887157797813416
node7 epoch1:node_model train_loss:0.40261111706495284,train_acc:0.8661176562309265
node7 epoch2:node_model train_loss:0.2704498864710331,train_acc:0.9106371998786926
node7 epoch3:node_model train_loss:0.21270742490887642,train_acc:0.9310784339904785
node7 epoch4:node_model train_loss:0.14451544135808944,train_acc:0.958617627620697
node7_model on test-dataset: loss:0.7542371922731399,acc:0.7743000388145447
node7 weight score:2586.719429891842
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5411346837093955,train_acc:0.8307573199272156
node9 epoch1:node_model train_loss:0.30165159545446696,train_acc:0.9035457968711853
node9 epoch2:node_model train_loss:0.2033585686432688,train_acc:0.9328901171684265
node9 epoch3:node_model train_loss:0.1291065678784722,train_acc:0.9680240154266357
node9 epoch4:node_model train_loss:0.0915921044192816,train_acc:0.9797322154045105
node9_model on test-dataset: loss:0.7327662314474582,acc:0.7872999310493469
node9 weight score:2534.2325018605243
node18: train data size:472
node18 epoch0:node_model train_loss:0.5924436032772065,train_acc:0.8266666531562805
node18 epoch1:node_model train_loss:0.26405436396598814,train_acc:0.9065555930137634
node18 epoch2:node_model train_loss:0.13911956697702407,train_acc:0.9564444422721863
node18 epoch3:node_model train_loss:0.10348093584179878,train_acc:0.9712222218513489
node18 epoch4:node_model train_loss:0.07431706711649895,train_acc:0.984000027179718
node18_model on test-dataset: loss:0.9088632571697235,acc:0.7475998997688293
node18 weight score:519.3300491317557
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5709377019107342,acc:0.8201999813318253
total cost energy:8.531021118484448 | all_enery_cp：7.125500000000001 | all_enery_tp: 1.4055211184844474
ef: 25.201834506038885
reward: 16.67081338755444
step 150:loss:47.734615325927734|running q:36.14260482788086
episode2,iteration30 selected nodes:[0, 12, 17, 11, 19],center node:11
################################################## episode2,iteration30 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.3992962653820331,train_acc:0.8640292286872864
node0 epoch1:node_model train_loss:0.2772889810685928,train_acc:0.9051876664161682
node0 epoch2:node_model train_loss:0.20142353942187932,train_acc:0.9351089000701904
node0 epoch3:node_model train_loss:0.16912682311466107,train_acc:0.9478011131286621
node0 epoch4:node_model train_loss:0.1420351663747659,train_acc:0.9591127038002014
node0_model on test-dataset: loss:0.7807568262517453,acc:0.7808999419212341
node0 weight score:6638.430591612666
node11: train data size:1682
node11 epoch0:node_model train_loss:0.546532120774774,train_acc:0.8386512994766235
node11 epoch1:node_model train_loss:0.27145946464117837,train_acc:0.9115781784057617
node11 epoch2:node_model train_loss:0.18961902751642115,train_acc:0.9322955012321472
node11 epoch3:node_model train_loss:0.11585120111703873,train_acc:0.969741702079773
node11 epoch4:node_model train_loss:0.07585519026307498,train_acc:0.9857531189918518
node11_model on test-dataset: loss:0.7304240909218788,acc:0.79010009765625
node11 weight score:2302.7718018954215
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6097331855978284,train_acc:0.8032540678977966
node12 epoch1:node_model train_loss:0.36663159834487097,train_acc:0.8643651604652405
node12 epoch2:node_model train_loss:0.24598375920738494,train_acc:0.9197618961334229
node12 epoch3:node_model train_loss:0.17023713886737823,train_acc:0.948888897895813
node12 epoch4:node_model train_loss:0.09306579163031918,train_acc:0.9835714101791382
node12_model on test-dataset: loss:0.7710040359199047,acc:0.7833001017570496
node12 weight score:1732.8054559480793
node17: train data size:442
node17 epoch0:node_model train_loss:0.6437796473503112,train_acc:0.7748571634292603
node17 epoch1:node_model train_loss:0.2990992397069931,train_acc:0.9104762077331543
node17 epoch2:node_model train_loss:0.19771199822425842,train_acc:0.949999988079071
node17 epoch3:node_model train_loss:0.12071560174226761,train_acc:0.968000054359436
node17 epoch4:node_model train_loss:0.09745580181479455,train_acc:0.9799999594688416
node17_model on test-dataset: loss:0.8120463000237942,acc:0.7644999623298645
node17 weight score:544.3039393037673
node19: train data size:4281
node19 epoch0:node_model train_loss:0.36658845322076666,train_acc:0.8691673874855042
node19 epoch1:node_model train_loss:0.21059389291114586,train_acc:0.9285098314285278
node19 epoch2:node_model train_loss:0.14631410844104234,train_acc:0.9567441940307617
node19 epoch3:node_model train_loss:0.10776975811567417,train_acc:0.9714640974998474
node19 epoch4:node_model train_loss:0.07847130142672118,train_acc:0.9830233454704285
node19_model on test-dataset: loss:0.7274902740120888,acc:0.7936000823974609
node19 weight score:5884.614754215755
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6036342982202768,acc:0.8210999828577041
total cost energy:8.388240552495636 | all_enery_cp：6.462 | all_enery_tp: 1.9262405524956372
ef: 24.987426591120038
reward: 16.599186038624403
step 151:loss:75.16276550292969|running q:37.18255615234375
episode2,iteration31 selected nodes:[11, 18, 4, 10, 2],center node:11
################################################## episode2,iteration31 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3440960394218564,train_acc:0.8774528503417969
node2 epoch1:node_model train_loss:0.2126176243958374,train_acc:0.9269411563873291
node2 epoch2:node_model train_loss:0.13979477885489663,train_acc:0.957802951335907
node2 epoch3:node_model train_loss:0.11012119085838397,train_acc:0.9688162207603455
node2 epoch4:node_model train_loss:0.08539473381824791,train_acc:0.9766096472740173
node2_model on test-dataset: loss:0.7545212305337191,acc:0.7922000885009766
node2 weight score:6345.745893211188
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5023029569004264,train_acc:0.8400000333786011
node4 epoch1:node_model train_loss:0.3197014895933015,train_acc:0.9039286375045776
node4 epoch2:node_model train_loss:0.2503967710903713,train_acc:0.9164284467697144
node4 epoch3:node_model train_loss:0.16787003832204,train_acc:0.9553570747375488
node4 epoch4:node_model train_loss:0.11642953114850181,train_acc:0.9689285159111023
node4_model on test-dataset: loss:0.7594265606254339,acc:0.7836999893188477
node4 weight score:3561.8980692119435
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5857017546892166,train_acc:0.8201665878295898
node10 epoch1:node_model train_loss:0.361645495891571,train_acc:0.8833332061767578
node10 epoch2:node_model train_loss:0.23336852267384528,train_acc:0.9281665682792664
node10 epoch3:node_model train_loss:0.16046225875616074,train_acc:0.9533331990242004
node10 epoch4:node_model train_loss:0.1184661727398634,train_acc:0.9718331694602966
node10_model on test-dataset: loss:0.7293304124474526,acc:0.7864999771118164
node10 weight score:2707.9633130509233
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4071175894316505,train_acc:0.861061692237854
node11 epoch1:node_model train_loss:0.2214021301444839,train_acc:0.9300717115402222
node11 epoch2:node_model train_loss:0.14882224228452234,train_acc:0.956671416759491
node11 epoch3:node_model train_loss:0.11057162766947466,train_acc:0.9707889556884766
node11 epoch4:node_model train_loss:0.06926855905091062,train_acc:0.9857531189918518
node11_model on test-dataset: loss:0.8068197897076607,acc:0.7746001482009888
node11 weight score:2084.7282397590275
node18: train data size:472
node18 epoch0:node_model train_loss:0.5846705079078675,train_acc:0.8170000314712524
node18 epoch1:node_model train_loss:0.3201085925102234,train_acc:0.9053333401679993
node18 epoch2:node_model train_loss:0.15002589523792267,train_acc:0.9568888545036316
node18 epoch3:node_model train_loss:0.09109118282794952,train_acc:0.9796666502952576
node18 epoch4:node_model train_loss:0.07396675832569599,train_acc:0.978444516658783
node18_model on test-dataset: loss:0.8560846331343055,acc:0.7610996961593628
node18 weight score:551.3473571788211
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5976688849925995,acc:0.8196999782323837
total cost energy:7.617177259678005 | all_enery_cp：5.811 | all_enery_tp: 1.8061772596780048
ef: 24.974822277785897
reward: 17.357645018107892
step 152:loss:67.61356353759766|running q:38.29818344116211
episode2,iteration32 selected nodes:[12, 6, 14, 11, 8],center node:11
################################################## episode2,iteration32 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4290773127708704,train_acc:0.8570966124534607
node6 epoch1:node_model train_loss:0.2248328197747469,train_acc:0.9280646443367004
node6 epoch2:node_model train_loss:0.1707513551077535,train_acc:0.9496773481369019
node6 epoch3:node_model train_loss:0.1334707787440669,train_acc:0.9664513468742371
node6 epoch4:node_model train_loss:0.10781930651395552,train_acc:0.9636863470077515
node6_model on test-dataset: loss:0.7738374377787113,acc:0.7814997434616089
node6 weight score:3885.829055559199
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6661010897821851,train_acc:0.7957823872566223
node8 epoch1:node_model train_loss:0.42696786092387307,train_acc:0.8632426857948303
node8 epoch2:node_model train_loss:0.26083335859907997,train_acc:0.9121201634407043
node8 epoch3:node_model train_loss:0.15245981555846003,train_acc:0.9605100750923157
node8 epoch4:node_model train_loss:0.11818796727392408,train_acc:0.9782878160476685
node8_model on test-dataset: loss:0.7517369040846824,acc:0.7804999351501465
node8 weight score:2391.794243744427
node11: train data size:1682
node11 epoch0:node_model train_loss:0.3774433188578662,train_acc:0.8613200187683105
node11 epoch1:node_model train_loss:0.21437959110035615,train_acc:0.9243902564048767
node11 epoch2:node_model train_loss:0.14897842573768952,train_acc:0.9506599307060242
node11 epoch3:node_model train_loss:0.0948607932557078,train_acc:0.980789065361023
node11 epoch4:node_model train_loss:0.06695666229900192,train_acc:0.9863413572311401
node11_model on test-dataset: loss:0.8126932372897864,acc:0.7763999700546265
node11 weight score:2069.661617474787
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6057978336300168,train_acc:0.8217460513114929
node12 epoch1:node_model train_loss:0.3352655608739172,train_acc:0.8845237493515015
node12 epoch2:node_model train_loss:0.2239229226750987,train_acc:0.934206485748291
node12 epoch3:node_model train_loss:0.1276783442923001,train_acc:0.9603174924850464
node12 epoch4:node_model train_loss:0.08313412857907158,train_acc:0.9780157208442688
node12_model on test-dataset: loss:0.7723403383791446,acc:0.7809000015258789
node12 weight score:1729.8073577301006
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5826267997423807,train_acc:0.8179166316986084
node14 epoch1:node_model train_loss:0.2805035288135211,train_acc:0.9035648107528687
node14 epoch2:node_model train_loss:0.1831602118909359,train_acc:0.9480555057525635
node14 epoch3:node_model train_loss:0.1164610752214988,train_acc:0.9695371389389038
node14 epoch4:node_model train_loss:0.09243070241063833,train_acc:0.9751851558685303
node14_model on test-dataset: loss:0.7736089213192463,acc:0.7795000076293945
node14 weight score:1514.977358329027
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5882566718757153,acc:0.8182999831438065
total cost energy:5.9275563079745766 | all_enery_cp：4.4975 | all_enery_tp: 1.430056307974577
ef: 25.073633772796324
reward: 19.146077464821747
step 153:loss:93.93978118896484|running q:39.34420394897461
episode2,iteration33 selected nodes:[17, 1, 10, 13, 3],center node:10
################################################## episode2,iteration33 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.32999442912199917,train_acc:0.8859557509422302
node1 epoch1:node_model train_loss:0.21625538641477332,train_acc:0.9212498664855957
node1 epoch2:node_model train_loss:0.17146775992039373,train_acc:0.9462500214576721
node1 epoch3:node_model train_loss:0.1529502049636315,train_acc:0.9531615972518921
node1 epoch4:node_model train_loss:0.14993088944431612,train_acc:0.9479411840438843
node1_model on test-dataset: loss:0.8282677440345287,acc:0.7741999626159668
node1 weight score:8098.830418440583
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4466301434954932,train_acc:0.8536319136619568
node3 epoch1:node_model train_loss:0.26079133187615594,train_acc:0.9063631296157837
node3 epoch2:node_model train_loss:0.1737961557715438,train_acc:0.9413061738014221
node3 epoch3:node_model train_loss:0.12615760874956153,train_acc:0.9620928764343262
node3 epoch4:node_model train_loss:0.09970520549388819,train_acc:0.969445526599884
node3_model on test-dataset: loss:0.7370826573669911,acc:0.7982999086380005
node3 weight score:5761.904662322603
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5377155758440495,train_acc:0.825666606426239
node10 epoch1:node_model train_loss:0.2852652184665203,train_acc:0.9081665873527527
node10 epoch2:node_model train_loss:0.20489950142800809,train_acc:0.9344999194145203
node10 epoch3:node_model train_loss:0.1438874512910843,train_acc:0.9561665654182434
node10 epoch4:node_model train_loss:0.11373180299997329,train_acc:0.9714999198913574
node10_model on test-dataset: loss:0.7769629577547312,acc:0.7773001790046692
node10 weight score:2541.948725209961
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7209006696939468,train_acc:0.7962878942489624
node13 epoch1:node_model train_loss:0.38128233328461647,train_acc:0.8755303621292114
node13 epoch2:node_model train_loss:0.2453598336627086,train_acc:0.9294697046279907
node13 epoch3:node_model train_loss:0.1425221065680186,train_acc:0.9586362838745117
node13 epoch4:node_model train_loss:0.09838664097090562,train_acc:0.9768182039260864
node13_model on test-dataset: loss:0.7737838995456695,acc:0.7804997563362122
node13 weight score:1492.664813364769
node17: train data size:442
node17 epoch0:node_model train_loss:0.6715683460235595,train_acc:0.7900952100753784
node17 epoch1:node_model train_loss:0.2794219046831131,train_acc:0.907714307308197
node17 epoch2:node_model train_loss:0.2663720339536667,train_acc:0.9189524054527283
node17 epoch3:node_model train_loss:0.14980678707361222,train_acc:0.9664761424064636
node17 epoch4:node_model train_loss:0.08121216148138047,train_acc:0.977238118648529
node17_model on test-dataset: loss:0.9486455789953471,acc:0.7451998591423035
node17 weight score:465.927433581776
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6319961276650429,acc:0.816099979877472
total cost energy:8.853080466036301 | all_enery_cp：7.2635 | all_enery_tp: 1.589580466036301
ef: 24.993482560263043
reward: 16.140402094226744
step 154:loss:115.36756896972656|running q:40.38066101074219
episode2,iteration34 selected nodes:[0, 19, 13, 9, 17],center node:17
################################################## episode2,iteration34 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.4230035027632347,train_acc:0.8615685701370239
node0 epoch1:node_model train_loss:0.23841580304388815,train_acc:0.9199166297912598
node0 epoch2:node_model train_loss:0.18689934766063324,train_acc:0.941149115562439
node0 epoch3:node_model train_loss:0.15886221969356903,train_acc:0.9478796720504761
node0 epoch4:node_model train_loss:0.12322642768804844,train_acc:0.9639596343040466
node0_model on test-dataset: loss:0.7491726142168045,acc:0.7926000952720642
node0 weight score:6918.298802764407
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5651378678648096,train_acc:0.8205078840255737
node9 epoch1:node_model train_loss:0.28729245537205744,train_acc:0.8955125212669373
node9 epoch2:node_model train_loss:0.18616950433505208,train_acc:0.944330632686615
node9 epoch3:node_model train_loss:0.12944905283419708,train_acc:0.9619665741920471
node9 epoch4:node_model train_loss:0.09204218634649326,train_acc:0.9764449596405029
node9_model on test-dataset: loss:0.7964828763902188,acc:0.7824000120162964
node9 weight score:2331.5002180790198
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6475217615564665,train_acc:0.8149242401123047
node13 epoch1:node_model train_loss:0.3321380130946636,train_acc:0.8835605978965759
node13 epoch2:node_model train_loss:0.21585795159141222,train_acc:0.9249242544174194
node13 epoch3:node_model train_loss:0.1222033283362786,train_acc:0.9691665172576904
node13 epoch4:node_model train_loss:0.10743561604370673,train_acc:0.9693180918693542
node13_model on test-dataset: loss:0.8029594737291336,acc:0.7746001482009888
node13 weight score:1438.4287598425199
node17: train data size:442
node17 epoch0:node_model train_loss:0.6034187436103821,train_acc:0.8259047865867615
node17 epoch1:node_model train_loss:0.3033051908016205,train_acc:0.9029523134231567
node17 epoch2:node_model train_loss:0.2730383202433586,train_acc:0.9301905035972595
node17 epoch3:node_model train_loss:0.14147531241178513,train_acc:0.9597143530845642
node17 epoch4:node_model train_loss:0.11174095571041107,train_acc:0.981238067150116
node17_model on test-dataset: loss:0.8496297094225883,acc:0.7687997221946716
node17 weight score:520.2266294341154
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3445726664260376,train_acc:0.8759517073631287
node19 epoch1:node_model train_loss:0.17423457686984262,train_acc:0.9461842775344849
node19 epoch2:node_model train_loss:0.12692677402912184,train_acc:0.9625723958015442
node19 epoch3:node_model train_loss:0.08573122842367305,train_acc:0.9791931509971619
node19 epoch4:node_model train_loss:0.06410636934776638,train_acc:0.9871001243591309
node19_model on test-dataset: loss:0.7607238879799842,acc:0.7892001867294312
node19 weight score:5627.534599140443
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6077590625733137,acc:0.8238999819755555
total cost energy:8.590037710699544 | all_enery_cp：6.458999999999999 | all_enery_tp: 2.1310377106995446
ef: 25.052539630366812
reward: 16.462501919667268
step 155:loss:67.49760437011719|running q:41.52046585083008
episode2,iteration35 selected nodes:[19, 8, 17, 7, 15],center node:17
################################################## episode2,iteration35 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7069285735487938,train_acc:0.7886568307876587
node7 epoch1:node_model train_loss:0.3733485542237759,train_acc:0.8835588693618774
node7 epoch2:node_model train_loss:0.23900107964873313,train_acc:0.9210979342460632
node7 epoch3:node_model train_loss:0.16169867999851703,train_acc:0.9490978121757507
node7 epoch4:node_model train_loss:0.12262187339365482,train_acc:0.970519483089447
node7_model on test-dataset: loss:0.743347297757864,acc:0.7834998965263367
node7 weight score:2624.6143705435434
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6288797557353973,train_acc:0.8103060722351074
node8 epoch1:node_model train_loss:0.3835790571239259,train_acc:0.8787642121315002
node8 epoch2:node_model train_loss:0.19260002424319586,train_acc:0.9371313452720642
node8 epoch3:node_model train_loss:0.14666942010323206,train_acc:0.9605215191841125
node8 epoch4:node_model train_loss:0.10089066852298048,train_acc:0.9766552448272705
node8_model on test-dataset: loss:0.7663387331366539,acc:0.7846000790596008
node8 weight score:2346.2209624205175
node15: train data size:629
node15 epoch0:node_model train_loss:0.7584678360394069,train_acc:0.7804433703422546
node15 epoch1:node_model train_loss:0.34704228384154184,train_acc:0.881576418876648
node15 epoch2:node_model train_loss:0.2519161786351885,train_acc:0.9137930870056152
node15 epoch3:node_model train_loss:0.13490964046546391,train_acc:0.964285671710968
node15 epoch4:node_model train_loss:0.08477378104414258,train_acc:0.9857142567634583
node15_model on test-dataset: loss:0.8515045914053917,acc:0.7595999240875244
node15 weight score:738.6924349542822
node17: train data size:442
node17 epoch0:node_model train_loss:0.6487143993377685,train_acc:0.813428521156311
node17 epoch1:node_model train_loss:0.34522525668144227,train_acc:0.8841904997825623
node17 epoch2:node_model train_loss:0.19677582383155823,train_acc:0.9344761967658997
node17 epoch3:node_model train_loss:0.11610708087682724,train_acc:0.9739999771118164
node17 epoch4:node_model train_loss:0.10085748992860318,train_acc:0.9644762277603149
node17_model on test-dataset: loss:0.899299955368042,acc:0.7547000050544739
node17 weight score:491.4934081355645
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2258469585415929,train_acc:0.9217113256454468
node19 epoch1:node_model train_loss:0.12720723640780116,train_acc:0.9607120752334595
node19 epoch2:node_model train_loss:0.08770564120522766,train_acc:0.9773324728012085
node19 epoch3:node_model train_loss:0.0660353473625904,train_acc:0.985116183757782
node19 epoch4:node_model train_loss:0.05697848899073379,train_acc:0.9862790703773499
node19_model on test-dataset: loss:0.8772462577000261,acc:0.7696001529693604
node19 weight score:4880.043616514219
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6231474534422159,acc:0.8181999832391739
total cost energy:6.510375673901519 | all_enery_cp：4.5504999999999995 | all_enery_tp: 1.95987567390152
ef: 24.687979528674287
reward: 18.17760385477277
step 156:loss:93.9416732788086|running q:42.62779235839844
episode2,iteration36 selected nodes:[7, 13, 10, 2, 3],center node:7
################################################## episode2,iteration36 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.32198029197752476,train_acc:0.8911079168319702
node2 epoch1:node_model train_loss:0.1812588331910471,train_acc:0.9413160085678101
node2 epoch2:node_model train_loss:0.1253354283204923,train_acc:0.962329626083374
node2 epoch3:node_model train_loss:0.10350104258395731,train_acc:0.9697348475456238
node2 epoch4:node_model train_loss:0.07144359169372667,train_acc:0.9837216138839722
node2_model on test-dataset: loss:0.7891239915788174,acc:0.7847001552581787
node2 weight score:6067.487557209539
node3: train data size:4247
node3 epoch0:node_model train_loss:0.39087229100770726,train_acc:0.8646164536476135
node3 epoch1:node_model train_loss:0.24004005554110505,train_acc:0.9161304831504822
node3 epoch2:node_model train_loss:0.15839348430203837,train_acc:0.9513952732086182
node3 epoch3:node_model train_loss:0.11629874025319897,train_acc:0.9655219912528992
node3 epoch4:node_model train_loss:0.08746328204870224,train_acc:0.9749975204467773
node3_model on test-dataset: loss:0.8000934985280037,acc:0.7810001373291016
node3 weight score:5308.129622117349
node7: train data size:1951
node7 epoch0:node_model train_loss:0.436888562887907,train_acc:0.8540393114089966
node7 epoch1:node_model train_loss:0.2735129863023758,train_acc:0.9145979881286621
node7 epoch2:node_model train_loss:0.17064285650849342,train_acc:0.9410980343818665
node7 epoch3:node_model train_loss:0.1193788604810834,train_acc:0.9645391702651978
node7 epoch4:node_model train_loss:0.0835425827652216,train_acc:0.9785195589065552
node7_model on test-dataset: loss:0.7747388251125813,acc:0.7831001281738281
node7 weight score:2518.2680107925275
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5808205381035805,train_acc:0.8098333477973938
node10 epoch1:node_model train_loss:0.3208701364696026,train_acc:0.8914999961853027
node10 epoch2:node_model train_loss:0.19071026146411896,train_acc:0.9421665072441101
node10 epoch3:node_model train_loss:0.14124747402966023,train_acc:0.9578332304954529
node10 epoch4:node_model train_loss:0.09606674667447805,train_acc:0.9763332605361938
node10_model on test-dataset: loss:0.7811291847378016,acc:0.7754001617431641
node10 weight score:2528.3909993235497
node13: train data size:1155
node13 epoch0:node_model train_loss:0.639491672317187,train_acc:0.8165151476860046
node13 epoch1:node_model train_loss:0.31682295848925907,train_acc:0.8965908885002136
node13 epoch2:node_model train_loss:0.2202583116789659,train_acc:0.9369696974754333
node13 epoch3:node_model train_loss:0.15153922388950983,train_acc:0.9574999809265137
node13 epoch4:node_model train_loss:0.08962422392020623,train_acc:0.9751514792442322
node13_model on test-dataset: loss:0.8507362748682499,acc:0.7683000564575195
node13 weight score:1357.6475273478497
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.611478525698185,acc:0.8234999793767929
total cost energy:8.593240315813112 | all_enery_cp：7.058 | all_enery_tp: 1.5352403158131132
ef: 25.147429961463494
reward: 16.554189645650382
step 157:loss:124.87238311767578|running q:43.654869079589844
episode2,iteration37 selected nodes:[14, 5, 12, 16, 1],center node:14
################################################## episode2,iteration37 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.27874523488914266,train_acc:0.9047792553901672
node1 epoch1:node_model train_loss:0.19900642203934052,train_acc:0.9346322417259216
node1 epoch2:node_model train_loss:0.16357691257315524,train_acc:0.9441176056861877
node1 epoch3:node_model train_loss:0.11122631928061738,train_acc:0.965441107749939
node1 epoch4:node_model train_loss:0.08550843481412705,train_acc:0.975073516368866
node1_model on test-dataset: loss:0.751221128553152,acc:0.7967000603675842
node1 weight score:8929.46130644591
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5243531230248903,train_acc:0.8316165804862976
node5 epoch1:node_model train_loss:0.31239537384949234,train_acc:0.8983458280563354
node5 epoch2:node_model train_loss:0.21626367537598862,train_acc:0.925902247428894
node5 epoch3:node_model train_loss:0.1731975084464801,train_acc:0.9430075287818909
node5 epoch4:node_model train_loss:0.13389218128041216,train_acc:0.95714271068573
node5_model on test-dataset: loss:0.7831817060709,acc:0.7783999443054199
node5 weight score:4769.0082276537205
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5900539308786392,train_acc:0.8257142901420593
node12 epoch1:node_model train_loss:0.29759834706783295,train_acc:0.8990476131439209
node12 epoch2:node_model train_loss:0.17621610632964543,train_acc:0.9458730220794678
node12 epoch3:node_model train_loss:0.11492106265255383,train_acc:0.9687301516532898
node12 epoch4:node_model train_loss:0.07694431394338608,train_acc:0.9781745672225952
node12_model on test-dataset: loss:0.7771849107742309,acc:0.7805998921394348
node12 weight score:1719.0246252582003
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5284481992324194,train_acc:0.8469444513320923
node14 epoch1:node_model train_loss:0.2967960213621457,train_acc:0.8995833396911621
node14 epoch2:node_model train_loss:0.18091343777875105,train_acc:0.941064715385437
node14 epoch3:node_model train_loss:0.11723640871544679,train_acc:0.9648610949516296
node14 epoch4:node_model train_loss:0.06424650518844525,train_acc:0.9841665625572205
node14_model on test-dataset: loss:0.761706548333168,acc:0.7932999134063721
node14 weight score:1538.650288046849
node16: train data size:877
node16 epoch0:node_model train_loss:0.6475766367382474,train_acc:0.8048051595687866
node16 epoch1:node_model train_loss:0.3389857957760493,train_acc:0.8954545259475708
node16 epoch2:node_model train_loss:0.2071892966826757,train_acc:0.9531168341636658
node16 epoch3:node_model train_loss:0.14185618940326902,train_acc:0.9624530076980591
node16 epoch4:node_model train_loss:0.08878576941788197,train_acc:0.9704473614692688
node16_model on test-dataset: loss:0.7545457053184509,acc:0.7857998013496399
node16 weight score:1162.2887703401188
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6155705626308918,acc:0.8221999818086624
total cost energy:8.830443655149935 | all_enery_cp：6.914 | all_enery_tp: 1.916443655149935
ef: 25.176852605791524
reward: 16.34640895064159
step 158:loss:53.7913818359375|running q:44.80616760253906
episode2,iteration38 selected nodes:[4, 12, 11, 0, 15],center node:11
################################################## episode2,iteration38 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.36469566363554734,train_acc:0.8705283403396606
node0 epoch1:node_model train_loss:0.20867545229311174,train_acc:0.9320366382598877
node0 epoch2:node_model train_loss:0.16526899279023594,train_acc:0.9462627172470093
node0 epoch3:node_model train_loss:0.12630606607462352,train_acc:0.9651875495910645
node0 epoch4:node_model train_loss:0.1002031872765376,train_acc:0.9721153974533081
node0_model on test-dataset: loss:0.7358869232982397,acc:0.7937998175621033
node0 weight score:7043.2016603445445
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5654686731951577,train_acc:0.8250000476837158
node4 epoch1:node_model train_loss:0.34005700211439815,train_acc:0.891785740852356
node4 epoch2:node_model train_loss:0.27845933474600315,train_acc:0.9121427536010742
node4 epoch3:node_model train_loss:0.2843617019908769,train_acc:0.8971428871154785
node4 epoch4:node_model train_loss:0.18827945805553878,train_acc:0.9417855739593506
node4_model on test-dataset: loss:0.8136010424792767,acc:0.7744998931884766
node4 weight score:3324.725336827355
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4750102986307705,train_acc:0.8538163304328918
node11 epoch1:node_model train_loss:0.2646890931269702,train_acc:0.9090387225151062
node11 epoch2:node_model train_loss:0.17876358154942007,train_acc:0.9392969608306885
node11 epoch3:node_model train_loss:0.09810746724114698,train_acc:0.9726828336715698
node11 epoch4:node_model train_loss:0.06021645314553205,train_acc:0.9841893315315247
node11_model on test-dataset: loss:0.7620030270516872,acc:0.7910001277923584
node11 weight score:2207.3403126860658
node12: train data size:1336
node12 epoch0:node_model train_loss:0.49085008246558054,train_acc:0.8440476655960083
node12 epoch1:node_model train_loss:0.30216224598033087,train_acc:0.8982540369033813
node12 epoch2:node_model train_loss:0.13800039514899254,train_acc:0.9587302207946777
node12 epoch3:node_model train_loss:0.11598355935088225,train_acc:0.9678571224212646
node12 epoch4:node_model train_loss:0.06788310461810657,train_acc:0.9871427416801453
node12_model on test-dataset: loss:0.7696565972268581,acc:0.7874998450279236
node12 weight score:1735.8390804596856
node15: train data size:629
node15 epoch0:node_model train_loss:0.7115816729409354,train_acc:0.7996551990509033
node15 epoch1:node_model train_loss:0.3599370079381125,train_acc:0.8923645615577698
node15 epoch2:node_model train_loss:0.24794942779200418,train_acc:0.9187191724777222
node15 epoch3:node_model train_loss:0.14890325388738088,train_acc:0.9474384784698486
node15 epoch4:node_model train_loss:0.14679434044020517,train_acc:0.9530049562454224
node15_model on test-dataset: loss:0.9535762689262629,acc:0.7542998194694519
node15 weight score:659.6221199047462
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6130969393253326,acc:0.8194999802112579
total cost energy:7.757449493661166 | all_enery_cp：5.7675 | all_enery_tp: 1.9899494936611666
ef: 24.86187510067058
reward: 17.104425607009414
step 159:loss:63.847389221191406|running q:45.8876838684082
episode2,iteration39 selected nodes:[13, 7, 16, 6, 11],center node:11
################################################## episode2,iteration39 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4747980746530717,train_acc:0.842718780040741
node6 epoch1:node_model train_loss:0.28520589778500216,train_acc:0.8992626070976257
node6 epoch2:node_model train_loss:0.20581930082651875,train_acc:0.9296773076057434
node6 epoch3:node_model train_loss:0.12606424549894948,train_acc:0.9657142162322998
node6 epoch4:node_model train_loss:0.14956960995351115,train_acc:0.9545159339904785
node6_model on test-dataset: loss:0.8435329470038414,acc:0.7696999311447144
node6 weight score:3564.768881500851
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5136858329176903,train_acc:0.8351176381111145
node7 epoch1:node_model train_loss:0.26602699160575866,train_acc:0.9156175851821899
node7 epoch2:node_model train_loss:0.18390242643654348,train_acc:0.9485196471214294
node7 epoch3:node_model train_loss:0.12730311788618565,train_acc:0.9675586819648743
node7 epoch4:node_model train_loss:0.07792841922491789,train_acc:0.9804998636245728
node7_model on test-dataset: loss:0.7249333097040653,acc:0.7941997647285461
node7 weight score:2691.282044684143
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4464357951108147,train_acc:0.8517789840698242
node11 epoch1:node_model train_loss:0.23090317231767318,train_acc:0.924332857131958
node11 epoch2:node_model train_loss:0.1244378013207632,train_acc:0.9632710814476013
node11 epoch3:node_model train_loss:0.0856018561650725,train_acc:0.9766713380813599
node11 epoch4:node_model train_loss:0.05289474414551959,train_acc:0.9899998903274536
node11_model on test-dataset: loss:0.7435110278427601,acc:0.7932999134063721
node11 weight score:2262.23947865332
node13: train data size:1155
node13 epoch0:node_model train_loss:0.651948461929957,train_acc:0.8200758695602417
node13 epoch1:node_model train_loss:0.35017553592721623,train_acc:0.8842424154281616
node13 epoch2:node_model train_loss:0.20521280666192374,train_acc:0.9336363673210144
node13 epoch3:node_model train_loss:0.11849943672617276,train_acc:0.9711362719535828
node13 epoch4:node_model train_loss:0.094911294678847,train_acc:0.9694697260856628
node13_model on test-dataset: loss:0.8029622490704059,acc:0.7780998349189758
node13 weight score:1438.423788088606
node16: train data size:877
node16 epoch0:node_model train_loss:0.5986614326635996,train_acc:0.8237950801849365
node16 epoch1:node_model train_loss:0.33742809957928127,train_acc:0.8906782269477844
node16 epoch2:node_model train_loss:0.20328069726626077,train_acc:0.9426695704460144
node16 epoch3:node_model train_loss:0.13792788237333298,train_acc:0.9553390145301819
node16 epoch4:node_model train_loss:0.06419364818268353,train_acc:0.9863346815109253
node16_model on test-dataset: loss:0.8059894570708275,acc:0.7800001502037048
node16 weight score:1088.1035630258032
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.597324175387621,acc:0.827099979519844
total cost energy:5.6449724298507356 | all_enery_cp：4.336 | all_enery_tp: 1.3089724298507357
ef: 25.05265259872807
reward: 19.407680168877334
step 160:loss:51.95054626464844|running q:46.82662582397461
episode2,iteration40 selected nodes:[9, 14, 11, 1, 17],center node:11
################################################## episode2,iteration40 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.22373251299209454,train_acc:0.9185292720794678
node1 epoch1:node_model train_loss:0.17664756998419762,train_acc:0.938088059425354
node1 epoch2:node_model train_loss:0.1267181248678004,train_acc:0.9576470851898193
node1 epoch3:node_model train_loss:0.07922153240617584,train_acc:0.9792648553848267
node1 epoch4:node_model train_loss:0.05071965598172563,train_acc:0.9889708161354065
node1_model on test-dataset: loss:0.7967541401088237,acc:0.7915998697280884
node1 weight score:8419.159264216432
node9: train data size:1857
node9 epoch0:node_model train_loss:0.566652426594182,train_acc:0.82957524061203
node9 epoch1:node_model train_loss:0.2697969243714684,train_acc:0.9063065052032471
node9 epoch2:node_model train_loss:0.1670905504571764,train_acc:0.9430193901062012
node9 epoch3:node_model train_loss:0.11837307932345491,train_acc:0.9653831124305725
node9 epoch4:node_model train_loss:0.0901792368998653,train_acc:0.9723637104034424
node9_model on test-dataset: loss:0.7719166246056557,acc:0.7851998805999756
node9 weight score:2405.700228245083
node11: train data size:1682
node11 epoch0:node_model train_loss:0.3027689790024477,train_acc:0.8866857886314392
node11 epoch1:node_model train_loss:0.18089723192593632,train_acc:0.9433427453041077
node11 epoch2:node_model train_loss:0.09849240871913292,train_acc:0.9730128049850464
node11 epoch3:node_model train_loss:0.0694545597276267,train_acc:0.9845767617225647
node11 epoch4:node_model train_loss:0.04530419606496306,train_acc:0.9923527836799622
node11_model on test-dataset: loss:0.7676883129775525,acc:0.7898999452590942
node11 weight score:2190.9933648412625
node14: train data size:1172
node14 epoch0:node_model train_loss:0.538106178243955,train_acc:0.8348147869110107
node14 epoch1:node_model train_loss:0.25629706618686515,train_acc:0.9157407283782959
node14 epoch2:node_model train_loss:0.15026405888299146,train_acc:0.9560184478759766
node14 epoch3:node_model train_loss:0.09719802346080542,train_acc:0.9751851558685303
node14 epoch4:node_model train_loss:0.05963109744091829,train_acc:0.9850000143051147
node14_model on test-dataset: loss:0.7515662034600973,acc:0.7898999452590942
node14 weight score:1559.41019514221
node17: train data size:442
node17 epoch0:node_model train_loss:0.6273826122283935,train_acc:0.8114285469055176
node17 epoch1:node_model train_loss:0.3826546609401703,train_acc:0.8929523825645447
node17 epoch2:node_model train_loss:0.17509885430335997,train_acc:0.949238121509552
node17 epoch3:node_model train_loss:0.10765525102615356,train_acc:0.9692381024360657
node17 epoch4:node_model train_loss:0.08507256880402565,train_acc:0.977238118648529
node17_model on test-dataset: loss:0.831249610632658,acc:0.7751999497413635
node17 weight score:531.729572376698
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.645937974601984,acc:0.8203999871015548
total cost energy:7.214661925296378 | all_enery_cp：5.9305 | all_enery_tp: 1.2841619252963778
ef: 25.06205933750411
reward: 17.847397412207734
step 161:loss:57.439292907714844|running q:47.72357177734375
episode2,iteration41 selected nodes:[5, 2, 10, 3, 15],center node:5
################################################## episode2,iteration41 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3037459633002679,train_acc:0.895520806312561
node2 epoch1:node_model train_loss:0.16977669841920337,train_acc:0.9439961910247803
node2 epoch2:node_model train_loss:0.12012379957983892,train_acc:0.9632481336593628
node2 epoch3:node_model train_loss:0.08540137709739308,train_acc:0.9761077761650085
node2 epoch4:node_model train_loss:0.07613625978895773,train_acc:0.980596661567688
node2_model on test-dataset: loss:0.763135282844305,acc:0.7915998697280884
node2 weight score:6274.11693265511
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3729798163092414,train_acc:0.8760713934898376
node3 epoch1:node_model train_loss:0.21883095904838207,train_acc:0.9180504679679871
node3 epoch2:node_model train_loss:0.1536644288273745,train_acc:0.9468280076980591
node3 epoch3:node_model train_loss:0.09795040143437164,train_acc:0.9718307852745056
node3 epoch4:node_model train_loss:0.08267086176851461,train_acc:0.9750864505767822
node3_model on test-dataset: loss:0.7616162623465061,acc:0.7920999526977539
node3 weight score:5576.298997234093
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5033859194893586,train_acc:0.84308260679245
node5 epoch1:node_model train_loss:0.2953314000838681,train_acc:0.9028195738792419
node5 epoch2:node_model train_loss:0.19436693975799962,train_acc:0.9339474439620972
node5 epoch3:node_model train_loss:0.12749050714467702,train_acc:0.9597744345664978
node5 epoch4:node_model train_loss:0.09097400729201342,train_acc:0.9782329201698303
node5_model on test-dataset: loss:0.7483670240640641,acc:0.7979000210762024
node5 weight score:4990.866620120163
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5399893611669541,train_acc:0.8409999012947083
node10 epoch1:node_model train_loss:0.2728100448846817,train_acc:0.9129999279975891
node10 epoch2:node_model train_loss:0.20896779149770736,train_acc:0.9381665587425232
node10 epoch3:node_model train_loss:0.1325576201081276,train_acc:0.9618334174156189
node10 epoch4:node_model train_loss:0.08525169398635626,train_acc:0.9793333411216736
node10_model on test-dataset: loss:0.7077288217842579,acc:0.8000999093055725
node10 weight score:2790.616884897834
node15: train data size:629
node15 epoch0:node_model train_loss:0.7170959115028381,train_acc:0.8126600980758667
node15 epoch1:node_model train_loss:0.328933732850211,train_acc:0.8915764093399048
node15 epoch2:node_model train_loss:0.2815306990274361,train_acc:0.9040886759757996
node15 epoch3:node_model train_loss:0.14366460591554642,train_acc:0.9600000381469727
node15 epoch4:node_model train_loss:0.08033867712531771,train_acc:0.9785714745521545
node15_model on test-dataset: loss:0.8540751212835311,acc:0.7707999348640442
node15 weight score:736.4691750471773
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6113825109601021,acc:0.8252999812364579
total cost energy:9.60238303421637 | all_enery_cp：7.686999999999999 | all_enery_tp: 1.9153830342163696
ef: 25.063151424102756
reward: 15.460768389886386
step 162:loss:73.13221740722656|running q:48.7374382019043
episode2,iteration42 selected nodes:[17, 6, 1, 9, 5],center node:6
################################################## episode2,iteration42 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.13324098195880651,train_acc:0.9564706087112427
node1 epoch1:node_model train_loss:0.11717957559534732,train_acc:0.9611764550209045
node1 epoch2:node_model train_loss:0.13229342889698112,train_acc:0.9549264311790466
node1 epoch3:node_model train_loss:0.10693285345812054,train_acc:0.9679412841796875
node1 epoch4:node_model train_loss:0.06668769127196249,train_acc:0.9829414486885071
node1_model on test-dataset: loss:0.790305197685957,acc:0.7959001660346985
node1 weight score:8487.860157874797
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3721622236465153,train_acc:0.8799247741699219
node5 epoch1:node_model train_loss:0.21892926077309408,train_acc:0.9243984222412109
node5 epoch2:node_model train_loss:0.14377055885760406,train_acc:0.9510901570320129
node5 epoch3:node_model train_loss:0.10837932342761442,train_acc:0.9666916728019714
node5 epoch4:node_model train_loss:0.08045906573534012,train_acc:0.9813156127929688
node5_model on test-dataset: loss:0.7902125765383243,acc:0.7857999801635742
node5 weight score:4726.576254154134
node6: train data size:3007
node6 epoch0:node_model train_loss:0.42088580708349904,train_acc:0.868617594242096
node6 epoch1:node_model train_loss:0.23826660071649858,train_acc:0.9232257604598999
node6 epoch2:node_model train_loss:0.1424795896295578,train_acc:0.9574192762374878
node6 epoch3:node_model train_loss:0.13736801594495773,train_acc:0.9614284634590149
node6 epoch4:node_model train_loss:0.1521915307929439,train_acc:0.9466818571090698
node6_model on test-dataset: loss:0.8287775914371014,acc:0.7759001851081848
node6 weight score:3628.2351635326654
node9: train data size:1857
node9 epoch0:node_model train_loss:0.47316671045202957,train_acc:0.8552539348602295
node9 epoch1:node_model train_loss:0.27524782560373606,train_acc:0.911301851272583
node9 epoch2:node_model train_loss:0.1392223460501746,train_acc:0.9589474201202393
node9 epoch3:node_model train_loss:0.10544918536355621,train_acc:0.9719668030738831
node9 epoch4:node_model train_loss:0.06996531176723932,train_acc:0.9832870960235596
node9_model on test-dataset: loss:0.757604047358036,acc:0.7853999733924866
node9 weight score:2451.1484679574323
node17: train data size:442
node17 epoch0:node_model train_loss:0.7752385914325715,train_acc:0.7980952262878418
node17 epoch1:node_model train_loss:0.30066777765750885,train_acc:0.892666757106781
node17 epoch2:node_model train_loss:0.20011137053370476,train_acc:0.9300000071525574
node17 epoch3:node_model train_loss:0.0896414875984192,train_acc:0.9739999771118164
node17 epoch4:node_model train_loss:0.11557416394352912,train_acc:0.9732381105422974
node17_model on test-dataset: loss:0.9002315518260002,acc:0.761999785900116
node17 weight score:490.9847906390991
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6266184010356665,acc:0.824699981212616
total cost energy:9.038419221493264 | all_enery_cp：7.874499999999999 | all_enery_tp: 1.1639192214932639
ef: 25.125159444869585
reward: 16.08674022337632
step 163:loss:80.55473327636719|running q:49.76111602783203
episode2,iteration43 selected nodes:[12, 13, 5, 10, 3],center node:10
################################################## episode2,iteration43 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.32955492859662966,train_acc:0.8904898166656494
node3 epoch1:node_model train_loss:0.16894521806822266,train_acc:0.9434585571289062
node3 epoch2:node_model train_loss:0.12407156859719476,train_acc:0.9618011116981506
node3 epoch3:node_model train_loss:0.08427739481246749,train_acc:0.9764224886894226
node3 epoch4:node_model train_loss:0.05553326513185058,train_acc:0.9864820241928101
node3_model on test-dataset: loss:0.7790481454133987,acc:0.7969999313354492
node3 weight score:5451.524434021144
node5: train data size:3735
node5 epoch0:node_model train_loss:0.30012055035484464,train_acc:0.8948120474815369
node5 epoch1:node_model train_loss:0.16850420352267592,train_acc:0.9434962272644043
node5 epoch2:node_model train_loss:0.11756914461913862,train_acc:0.9645112752914429
node5 epoch3:node_model train_loss:0.08890862204134464,train_acc:0.9727064371109009
node5 epoch4:node_model train_loss:0.0706186016629401,train_acc:0.9787591695785522
node5_model on test-dataset: loss:0.7820398925244808,acc:0.7904001474380493
node5 weight score:4775.971194951644
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5454765766859054,train_acc:0.8399999737739563
node10 epoch1:node_model train_loss:0.28244211450219153,train_acc:0.9083333015441895
node10 epoch2:node_model train_loss:0.16129444986581803,train_acc:0.9518332481384277
node10 epoch3:node_model train_loss:0.11548479981720447,train_acc:0.9686665534973145
node10 epoch4:node_model train_loss:0.09172338731586933,train_acc:0.9758333563804626
node10_model on test-dataset: loss:0.7386502370983362,acc:0.7907999753952026
node10 weight score:2673.7959331854504
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6650431390319552,train_acc:0.8142856955528259
node12 epoch1:node_model train_loss:0.3253429202096803,train_acc:0.902222216129303
node12 epoch2:node_model train_loss:0.1909532600215503,train_acc:0.9435713291168213
node12 epoch3:node_model train_loss:0.11010687345904964,train_acc:0.9692857265472412
node12 epoch4:node_model train_loss:0.07455035458718028,train_acc:0.9788888692855835
node12_model on test-dataset: loss:0.7744164031744003,acc:0.790600061416626
node12 weight score:1725.170069388535
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6215771610538164,train_acc:0.8243181705474854
node13 epoch1:node_model train_loss:0.3198118445773919,train_acc:0.8985607028007507
node13 epoch2:node_model train_loss:0.21731295312444368,train_acc:0.9343181848526001
node13 epoch3:node_model train_loss:0.12486168400694926,train_acc:0.9636363387107849
node13 epoch4:node_model train_loss:0.09232324765374263,train_acc:0.9754545092582703
node13_model on test-dataset: loss:0.8233596006035805,acc:0.777899980545044
node13 weight score:1402.7892541160675
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6370897477865219,acc:0.8233999806642532
total cost energy:7.661163089161125 | all_enery_cp：6.224 | all_enery_tp: 1.4371630891611247
ef: 25.229849332408712
reward: 17.568686243247587
step 164:loss:52.96251678466797|running q:50.84995651245117
episode2,iteration44 selected nodes:[15, 7, 17, 12, 2],center node:7
################################################## episode2,iteration44 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.297169307867686,train_acc:0.8978978395462036
node2 epoch1:node_model train_loss:0.14956606226041913,train_acc:0.9475662708282471
node2 epoch2:node_model train_loss:0.10400748446894188,train_acc:0.9657764434814453
node2 epoch3:node_model train_loss:0.07410241349134594,train_acc:0.9824432134628296
node2 epoch4:node_model train_loss:0.057078845255697765,train_acc:0.986458420753479
node2_model on test-dataset: loss:0.7259995794296265,acc:0.8029000163078308
node2 weight score:6595.0451428107435
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6019378930330277,train_acc:0.8207157254219055
node7 epoch1:node_model train_loss:0.3086023546755314,train_acc:0.9020783305168152
node7 epoch2:node_model train_loss:0.14724818766117095,train_acc:0.9605196118354797
node7 epoch3:node_model train_loss:0.10413361247628927,train_acc:0.9705195426940918
node7 epoch4:node_model train_loss:0.08517487784847617,train_acc:0.9794998168945312
node7_model on test-dataset: loss:0.7058767279982567,acc:0.7981999516487122
node7 weight score:2763.9386915796126
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5072179032223565,train_acc:0.8365873098373413
node12 epoch1:node_model train_loss:0.3214034321052687,train_acc:0.8938094973564148
node12 epoch2:node_model train_loss:0.17110527466450418,train_acc:0.9423015117645264
node12 epoch3:node_model train_loss:0.11031366246087211,train_acc:0.9644443988800049
node12 epoch4:node_model train_loss:0.06785049715212413,train_acc:0.9864284992218018
node12_model on test-dataset: loss:0.8057636423408985,acc:0.7814998626708984
node12 weight score:1658.0544588964858
node15: train data size:629
node15 epoch0:node_model train_loss:0.6430725625583104,train_acc:0.8174384236335754
node15 epoch1:node_model train_loss:0.29675737023353577,train_acc:0.9215763211250305
node15 epoch2:node_model train_loss:0.1810658574104309,train_acc:0.9337931275367737
node15 epoch3:node_model train_loss:0.16020061182124273,train_acc:0.950788140296936
node15 epoch4:node_model train_loss:0.12051997067672866,train_acc:0.9679310321807861
node15_model on test-dataset: loss:0.8677935056388378,acc:0.7734000086784363
node15 weight score:724.8268118081309
node17: train data size:442
node17 epoch0:node_model train_loss:0.6311112701892853,train_acc:0.8319048285484314
node17 epoch1:node_model train_loss:0.27923480570316317,train_acc:0.9069523811340332
node17 epoch2:node_model train_loss:0.21595627665519715,train_acc:0.9504761695861816
node17 epoch3:node_model train_loss:0.11332811564207076,train_acc:0.9632380604743958
node17 epoch4:node_model train_loss:0.12124797031283378,train_acc:0.9689523577690125
node17_model on test-dataset: loss:0.839202840179205,acc:0.781000018119812
node17 weight score:526.6903051777261
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.619914947450161,acc:0.8227999806404114
total cost energy:6.702655947731952 | all_enery_cp：4.573 | all_enery_tp: 2.1296559477319517
ef: 24.567711185267683
reward: 17.86505523753573
step 165:loss:54.009010314941406|running q:51.848052978515625
episode2,iteration45 selected nodes:[17, 11, 6, 13, 2],center node:11
################################################## episode2,iteration45 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.1263442748847107,train_acc:0.957717776298523
node2 epoch1:node_model train_loss:0.09752331177393596,train_acc:0.9703881740570068
node2 epoch2:node_model train_loss:0.0701816026121378,train_acc:0.9790813326835632
node2 epoch3:node_model train_loss:0.05631700829447558,train_acc:0.9857765436172485
node2 epoch4:node_model train_loss:0.05362804672525575,train_acc:0.9858335256576538
node2_model on test-dataset: loss:0.8969094633683562,acc:0.7815998792648315
node2 weight score:5338.331454346126
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4011148017260336,train_acc:0.8790321946144104
node6 epoch1:node_model train_loss:0.2369791319774043,train_acc:0.9202303290367126
node6 epoch2:node_model train_loss:0.16488995601332956,train_acc:0.9496772289276123
node6 epoch3:node_model train_loss:0.09854746477738503,train_acc:0.9719353318214417
node6 epoch4:node_model train_loss:0.06617664659936581,train_acc:0.9816127419471741
node6_model on test-dataset: loss:0.7718562553077937,acc:0.7899000644683838
node6 weight score:3895.8031101281886
node11: train data size:1682
node11 epoch0:node_model train_loss:0.41090591690119577,train_acc:0.8645337224006653
node11 epoch1:node_model train_loss:0.24907612055540085,train_acc:0.9153084754943848
node11 epoch2:node_model train_loss:0.11982735684689354,train_acc:0.9578479528427124
node11 epoch3:node_model train_loss:0.0733498862999327,train_acc:0.9791533350944519
node11 epoch4:node_model train_loss:0.05462394479443045,train_acc:0.98856520652771
node11_model on test-dataset: loss:0.7916504599153995,acc:0.7963001132011414
node11 weight score:2124.675074627947
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6691946561137835,train_acc:0.8144696950912476
node13 epoch1:node_model train_loss:0.36544309680660564,train_acc:0.8921970129013062
node13 epoch2:node_model train_loss:0.18852924865980944,train_acc:0.9450756907463074
node13 epoch3:node_model train_loss:0.12673373644550642,train_acc:0.9619697332382202
node13 epoch4:node_model train_loss:0.08438627018282811,train_acc:0.9766666293144226
node13_model on test-dataset: loss:0.8513426649570465,acc:0.7732000350952148
node13 weight score:1356.6805089678833
node17: train data size:442
node17 epoch0:node_model train_loss:0.6407905220985413,train_acc:0.7996190786361694
node17 epoch1:node_model train_loss:0.35855686068534853,train_acc:0.9001904726028442
node17 epoch2:node_model train_loss:0.18650550544261932,train_acc:0.9417142868041992
node17 epoch3:node_model train_loss:0.14520332962274551,train_acc:0.9552380442619324
node17 epoch4:node_model train_loss:0.07722239345312118,train_acc:0.9779999852180481
node17_model on test-dataset: loss:0.9381781936436892,acc:0.7606000900268555
node17 weight score:471.1258511385389
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6660335297137499,acc:0.8183999794721604
total cost energy:7.190299791380298 | all_enery_cp：5.537000000000001 | all_enery_tp: 1.6532997913802965
ef: 24.74562300390132
reward: 17.55532321252102
step 166:loss:51.14824295043945|running q:52.744056701660156
episode2,iteration46 selected nodes:[10, 1, 15, 3, 11],center node:11
################################################## episode2,iteration46 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.18371620620874798,train_acc:0.9372794032096863
node1 epoch1:node_model train_loss:0.11019437965553473,train_acc:0.9652942419052124
node1 epoch2:node_model train_loss:0.08910890408408116,train_acc:0.9731616973876953
node1 epoch3:node_model train_loss:0.09133454723119297,train_acc:0.9726471900939941
node1 epoch4:node_model train_loss:0.04838785141713314,train_acc:0.987205982208252
node1_model on test-dataset: loss:0.8008891323208809,acc:0.7984999418258667
node1 weight score:8375.691127885602
node3: train data size:4247
node3 epoch0:node_model train_loss:0.30757151855978854,train_acc:0.8898219466209412
node3 epoch1:node_model train_loss:0.1483075902905575,train_acc:0.9480502009391785
node3 epoch2:node_model train_loss:0.1043917802703935,train_acc:0.9667440056800842
node3 epoch3:node_model train_loss:0.06172243016230505,train_acc:0.9831963181495667
node3 epoch4:node_model train_loss:0.050914441213704816,train_acc:0.9889805912971497
node3_model on test-dataset: loss:0.7974352829158307,acc:0.7927002310752869
node3 weight score:5325.8240398779435
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5154483109712601,train_acc:0.830833375453949
node10 epoch1:node_model train_loss:0.2486819989979267,train_acc:0.9201664924621582
node10 epoch2:node_model train_loss:0.14941321834921836,train_acc:0.9521666765213013
node10 epoch3:node_model train_loss:0.1165231816470623,train_acc:0.9726664423942566
node10 epoch4:node_model train_loss:0.07610656339675188,train_acc:0.9828330874443054
node10_model on test-dataset: loss:0.7533571704477072,acc:0.7865999937057495
node10 weight score:2621.5984628198225
node11: train data size:1682
node11 epoch0:node_model train_loss:0.31781118669930625,train_acc:0.8881204724311829
node11 epoch1:node_model train_loss:0.19635916983380036,train_acc:0.9352366328239441
node11 epoch2:node_model train_loss:0.09525501925279112,train_acc:0.9752939939498901
node11 epoch3:node_model train_loss:0.07243870658909574,train_acc:0.9791534543037415
node11 epoch4:node_model train_loss:0.0389232935712618,train_acc:0.9941176176071167
node11_model on test-dataset: loss:0.7763049755990505,acc:0.7972002029418945
node11 weight score:2166.674248998665
node15: train data size:629
node15 epoch0:node_model train_loss:0.7505911844117301,train_acc:0.7920197248458862
node15 epoch1:node_model train_loss:0.41020999210221426,train_acc:0.8660098910331726
node15 epoch2:node_model train_loss:0.2924192079475948,train_acc:0.9074384570121765
node15 epoch3:node_model train_loss:0.17291371737207686,train_acc:0.9450738430023193
node15 epoch4:node_model train_loss:0.07221474338855062,train_acc:0.9814285039901733
node15_model on test-dataset: loss:0.9062339049577713,acc:0.7680001854896545
node15 weight score:694.0812924333372
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6508173255622387,acc:0.8221999788284302
total cost energy:9.297282893563237 | all_enery_cp：7.6205 | all_enery_tp: 1.676782893563237
ef: 25.05703498565113
reward: 15.759752092087892
step 167:loss:46.213905334472656|running q:53.71840286254883
episode2,iteration47 selected nodes:[16, 4, 19, 0, 15],center node:16
################################################## episode2,iteration47 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.39266926136154395,train_acc:0.8699167370796204
node0 epoch1:node_model train_loss:0.22375716956762168,train_acc:0.9199211597442627
node0 epoch2:node_model train_loss:0.14641047591486803,train_acc:0.9501087665557861
node0 epoch3:node_model train_loss:0.10344902468988529,train_acc:0.9714990258216858
node0 epoch4:node_model train_loss:0.08513168385252357,train_acc:0.9768049120903015
node0_model on test-dataset: loss:0.7560788791254163,acc:0.7920998930931091
node0 weight score:6855.10486153953
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5930403987211841,train_acc:0.8296429514884949
node4 epoch1:node_model train_loss:0.32191363690487507,train_acc:0.8946429491043091
node4 epoch2:node_model train_loss:0.18896596396474966,train_acc:0.9367856979370117
node4 epoch3:node_model train_loss:0.15862271429172584,train_acc:0.9510713815689087
node4 epoch4:node_model train_loss:0.1909014679757612,train_acc:0.9375000596046448
node4_model on test-dataset: loss:0.828222069144249,acc:0.780799925327301
node4 weight score:3266.0322645047486
node15: train data size:629
node15 epoch0:node_model train_loss:0.574001703943525,train_acc:0.8291625380516052
node15 epoch1:node_model train_loss:0.2863555614437376,train_acc:0.8939409255981445
node15 epoch2:node_model train_loss:0.2142974947180067,train_acc:0.942364513874054
node15 epoch3:node_model train_loss:0.12029846012592316,train_acc:0.9593595266342163
node15 epoch4:node_model train_loss:0.08495831436344556,train_acc:0.9814286231994629
node15_model on test-dataset: loss:0.8975116176903248,acc:0.76910001039505
node15 weight score:700.8265827451702
node16: train data size:877
node16 epoch0:node_model train_loss:0.6060081985261705,train_acc:0.8229148983955383
node16 epoch1:node_model train_loss:0.3157547149393294,train_acc:0.898787796497345
node16 epoch2:node_model train_loss:0.16602267490492928,train_acc:0.9413419365882874
node16 epoch3:node_model train_loss:0.12748493750890097,train_acc:0.9712265133857727
node16 epoch4:node_model train_loss:0.06609252302183045,train_acc:0.9866665601730347
node16_model on test-dataset: loss:0.8357272127270698,acc:0.7770998477935791
node16 weight score:1049.3854772758357
node19: train data size:4281
node19 epoch0:node_model train_loss:0.39601854425530103,train_acc:0.8731209635734558
node19 epoch1:node_model train_loss:0.18462320313204167,train_acc:0.9372379183769226
node19 epoch2:node_model train_loss:0.12537993620647941,train_acc:0.9590293765068054
node19 epoch3:node_model train_loss:0.08462059727415096,train_acc:0.9763479232788086
node19 epoch4:node_model train_loss:0.058584897280778994,train_acc:0.9872093796730042
node19_model on test-dataset: loss:0.7698347514867783,acc:0.8005999326705933
node19 weight score:5560.933683147097
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6055735769122839,acc:0.8278999775648117
total cost energy:9.053794856487917 | all_enery_cp：6.8374999999999995 | all_enery_tp: 2.216294856487918
ef: 24.920338826772156
reward: 15.866543970284239
step 168:loss:50.07951736450195|running q:54.53874969482422
episode2,iteration48 selected nodes:[1, 10, 2, 13, 4],center node:10
################################################## episode2,iteration48 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.1231896391905406,train_acc:0.9573529362678528
node1 epoch1:node_model train_loss:0.08217658609261408,train_acc:0.9777942895889282
node1 epoch2:node_model train_loss:0.09059678319403354,train_acc:0.9698529839515686
node1 epoch3:node_model train_loss:0.08847600544857628,train_acc:0.9726470708847046
node1 epoch4:node_model train_loss:0.07645049776114962,train_acc:0.9757354855537415
node1_model on test-dataset: loss:0.8641759787499904,acc:0.7811999320983887
node1 weight score:7762.307868940026
node2: train data size:4788
node2 epoch0:node_model train_loss:0.1757973039833208,train_acc:0.9398863315582275
node2 epoch1:node_model train_loss:0.10320048624028762,train_acc:0.9680966734886169
node2 epoch2:node_model train_loss:0.06861469227199753,train_acc:0.9792898893356323
node2 epoch3:node_model train_loss:0.04849917649213845,train_acc:0.9887499809265137
node2 epoch4:node_model train_loss:0.031490133221571646,train_acc:0.9937500953674316
node2_model on test-dataset: loss:0.7544931749999523,acc:0.8079000115394592
node2 weight score:6345.981857291556
node4: train data size:2705
node4 epoch0:node_model train_loss:0.42359634462211815,train_acc:0.867142915725708
node4 epoch1:node_model train_loss:0.25804391742816996,train_acc:0.9153571724891663
node4 epoch2:node_model train_loss:0.14979604099478042,train_acc:0.9474999904632568
node4 epoch3:node_model train_loss:0.15262610598334245,train_acc:0.9528570175170898
node4 epoch4:node_model train_loss:0.18334299299333776,train_acc:0.9414284825325012
node4_model on test-dataset: loss:0.8280268488824367,acc:0.7770997285842896
node4 weight score:3266.8022826202537
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4745080038905144,train_acc:0.8498333096504211
node10 epoch1:node_model train_loss:0.27353668212890625,train_acc:0.9063333868980408
node10 epoch2:node_model train_loss:0.14126428700983523,train_acc:0.9553333520889282
node10 epoch3:node_model train_loss:0.09121449030935765,train_acc:0.9738330841064453
node10 epoch4:node_model train_loss:0.06260090293362737,train_acc:0.9858331680297852
node10_model on test-dataset: loss:0.7325895872712135,acc:0.7997998595237732
node10 weight score:2695.9160139807327
node13: train data size:1155
node13 epoch0:node_model train_loss:0.610083095729351,train_acc:0.8181061148643494
node13 epoch1:node_model train_loss:0.3169264222184817,train_acc:0.8936364054679871
node13 epoch2:node_model train_loss:0.18906076376636824,train_acc:0.9375756978988647
node13 epoch3:node_model train_loss:0.11621267938365538,train_acc:0.9617424011230469
node13 epoch4:node_model train_loss:0.07800801796838641,train_acc:0.9761363863945007
node13_model on test-dataset: loss:0.8045367956161499,acc:0.7910001277923584
node13 weight score:1435.6086710930977
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6495200208574533,acc:0.8220999807119369
total cost energy:10.547174094652608 | all_enery_cp：8.6655 | all_enery_tp: 1.8816740946526078
ef: 25.23309427062805
reward: 14.685920175975443
step 169:loss:37.22273254394531|running q:55.44217300415039
episode2,iteration49 selected nodes:[13, 3, 2, 18, 8],center node:3
################################################## episode2,iteration49 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.1204426990977178,train_acc:0.9568182826042175
node2 epoch1:node_model train_loss:0.06841750167465459,train_acc:0.9830682277679443
node2 epoch2:node_model train_loss:0.05276024714112282,train_acc:0.9880683422088623
node2 epoch3:node_model train_loss:0.05066660213439415,train_acc:0.9870550036430359
node2 epoch4:node_model train_loss:0.05335076397750527,train_acc:0.9874433279037476
node2_model on test-dataset: loss:0.9025657537579537,acc:0.7823999524116516
node2 weight score:5304.876658641789
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2780378076226212,train_acc:0.9004896879196167
node3 epoch1:node_model train_loss:0.15246086091149685,train_acc:0.9473824501037598
node3 epoch2:node_model train_loss:0.11169286098244578,train_acc:0.964794397354126
node3 epoch3:node_model train_loss:0.06742827516309051,train_acc:0.9809302091598511
node3 epoch4:node_model train_loss:0.0543298598292262,train_acc:0.9853190779685974
node3_model on test-dataset: loss:0.7917718873172999,acc:0.8010998964309692
node3 weight score:5363.918658933175
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6784311599201627,train_acc:0.8164852261543274
node8 epoch1:node_model train_loss:0.3554710066980786,train_acc:0.8764399290084839
node8 epoch2:node_model train_loss:0.2081836387515068,train_acc:0.9305214285850525
node8 epoch3:node_model train_loss:0.13930421281192037,train_acc:0.9598637819290161
node8 epoch4:node_model train_loss:0.10518527030944824,train_acc:0.9716325402259827
node8_model on test-dataset: loss:0.7601602382957935,acc:0.7951000332832336
node8 weight score:2365.2907761012916
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6083287075161934,train_acc:0.8252272605895996
node13 epoch1:node_model train_loss:0.2701429712275664,train_acc:0.9169696569442749
node13 epoch2:node_model train_loss:0.17145532617966333,train_acc:0.9479544162750244
node13 epoch3:node_model train_loss:0.0830947154512008,train_acc:0.9801514148712158
node13 epoch4:node_model train_loss:0.05189796723425388,train_acc:0.9899999499320984
node13_model on test-dataset: loss:0.828202237188816,acc:0.781000018119812
node13 weight score:1394.586911429315
node18: train data size:472
node18 epoch0:node_model train_loss:0.5471664309501648,train_acc:0.8483332991600037
node18 epoch1:node_model train_loss:0.23037331998348237,train_acc:0.9229999780654907
node18 epoch2:node_model train_loss:0.1953943684697151,train_acc:0.9477777481079102
node18 epoch3:node_model train_loss:0.12887423634529113,train_acc:0.9624444246292114
node18 epoch4:node_model train_loss:0.052336746081709865,train_acc:0.9899999499320984
node18_model on test-dataset: loss:0.9319977748394013,acc:0.7681000232696533
node18 weight score:506.438977369161
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6751895532011986,acc:0.8186999821662903
total cost energy:8.475561243479268 | all_enery_cp：6.23 | all_enery_tp: 2.2455612434792678
ef: 24.60612987657041
reward: 16.130568633091144
step 170:loss:74.51351928710938|running q:56.447654724121094
episode2,iteration50 selected nodes:[16, 17, 4, 15, 5],center node:16
################################################## episode2,iteration50 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4136637265660933,train_acc:0.8714285492897034
node4 epoch1:node_model train_loss:0.2404525088412421,train_acc:0.920714259147644
node4 epoch2:node_model train_loss:0.17614217767758028,train_acc:0.9382143020629883
node4 epoch3:node_model train_loss:0.14775471262899892,train_acc:0.9503570795059204
node4 epoch4:node_model train_loss:0.10464986506849527,train_acc:0.9667854905128479
node4_model on test-dataset: loss:0.8089978249371051,acc:0.792100191116333
node4 weight score:3343.6431058517346
node5: train data size:3735
node5 epoch0:node_model train_loss:0.408790794642348,train_acc:0.8682708740234375
node5 epoch1:node_model train_loss:0.23147643847685112,train_acc:0.9224060773849487
node5 epoch2:node_model train_loss:0.12853851835978658,train_acc:0.9584585428237915
node5 epoch3:node_model train_loss:0.10921635331684038,train_acc:0.9697742462158203
node5 epoch4:node_model train_loss:0.06967150135652016,train_acc:0.9810523986816406
node5_model on test-dataset: loss:0.7579839485883713,acc:0.7985000610351562
node5 weight score:4927.544978961447
node15: train data size:629
node15 epoch0:node_model train_loss:0.6344949432781765,train_acc:0.8031527400016785
node15 epoch1:node_model train_loss:0.35050153732299805,train_acc:0.8865024447441101
node15 epoch2:node_model train_loss:0.20584345289639064,train_acc:0.9252216815948486
node15 epoch3:node_model train_loss:0.12133675068616867,train_acc:0.9642857313156128
node15 epoch4:node_model train_loss:0.11382106319069862,train_acc:0.9736452698707581
node15_model on test-dataset: loss:0.9033729047328234,acc:0.7717999815940857
node15 weight score:696.279461897332
node16: train data size:877
node16 epoch0:node_model train_loss:0.6369627879725562,train_acc:0.8190186619758606
node16 epoch1:node_model train_loss:0.33698135283258224,train_acc:0.8962337970733643
node16 epoch2:node_model train_loss:0.1675426024529669,train_acc:0.9375612735748291
node16 epoch3:node_model train_loss:0.10761879467301899,train_acc:0.9616738557815552
node16 epoch4:node_model train_loss:0.05518160512049993,train_acc:0.9866666793823242
node16_model on test-dataset: loss:0.8145407715439796,acc:0.7870001196861267
node16 weight score:1076.6802972153591
node17: train data size:442
node17 epoch0:node_model train_loss:0.7231231987476349,train_acc:0.8183809518814087
node17 epoch1:node_model train_loss:0.3359078526496887,train_acc:0.9009523391723633
node17 epoch2:node_model train_loss:0.15485165268182755,train_acc:0.9592380523681641
node17 epoch3:node_model train_loss:0.11559026837348937,train_acc:0.9732381105422974
node17 epoch4:node_model train_loss:0.06116125546395779,train_acc:0.9899999499320984
node17_model on test-dataset: loss:0.9123255035281181,acc:0.7741002440452576
node17 weight score:484.4762075495102
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6462819036841393,acc:0.8222999811172486
total cost energy:6.1335623132202235 | all_enery_cp：4.194 | all_enery_tp: 1.9395623132202235
ef: 24.689957278215445
reward: 18.55639496499522
step 171:loss:57.28125|running q:57.242820739746094
episode2,iteration51 selected nodes:[11, 6, 7, 1, 0],center node:6
################################################## episode2,iteration51 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.35948459574809444,train_acc:0.8793002963066101
node0 epoch1:node_model train_loss:0.18327262109288803,train_acc:0.9368442893028259
node0 epoch2:node_model train_loss:0.13638820957679015,train_acc:0.9532297849655151
node0 epoch3:node_model train_loss:0.08929197955876589,train_acc:0.9742261171340942
node0 epoch4:node_model train_loss:0.07403662936905256,train_acc:0.9818836450576782
node0_model on test-dataset: loss:0.7438827143609523,acc:0.8027999401092529
node0 weight score:6967.49621941755
node1: train data size:6708
node1 epoch0:node_model train_loss:0.14206221151877851,train_acc:0.9488971829414368
node1 epoch1:node_model train_loss:0.09292483239379876,train_acc:0.9726470708847046
node1 epoch2:node_model train_loss:0.1197514093535788,train_acc:0.9581618905067444
node1 epoch3:node_model train_loss:0.09622188827351612,train_acc:0.9698532223701477
node1 epoch4:node_model train_loss:0.0564390195752768,train_acc:0.9848530888557434
node1_model on test-dataset: loss:0.8203862328827382,acc:0.7975997924804688
node1 weight score:8176.636480635344
node6: train data size:3007
node6 epoch0:node_model train_loss:0.40719978559401726,train_acc:0.8670046329498291
node6 epoch1:node_model train_loss:0.25938376760290516,train_acc:0.9145622253417969
node6 epoch2:node_model train_loss:0.22762217829304357,train_acc:0.9231334924697876
node6 epoch3:node_model train_loss:0.16298279286392273,train_acc:0.941428542137146
node6 epoch4:node_model train_loss:0.13842773854529725,train_acc:0.9554837942123413
node6_model on test-dataset: loss:0.8607181161642075,acc:0.7772002220153809
node6 weight score:3493.594410909699
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5618129760026932,train_acc:0.8300784230232239
node7 epoch1:node_model train_loss:0.2614544402807951,train_acc:0.9136175513267517
node7 epoch2:node_model train_loss:0.17685528360307218,train_acc:0.9466372728347778
node7 epoch3:node_model train_loss:0.11149793770164251,train_acc:0.9699999094009399
node7 epoch4:node_model train_loss:0.07502448773011565,train_acc:0.982539176940918
node7_model on test-dataset: loss:0.7428703586757183,acc:0.7951999306678772
node7 weight score:2626.2994306004625
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4115808299359153,train_acc:0.8634863495826721
node11 epoch1:node_model train_loss:0.22339989025803172,train_acc:0.9284361004829407
node11 epoch2:node_model train_loss:0.1366724902216126,train_acc:0.9612481594085693
node11 epoch3:node_model train_loss:0.09130525128806338,train_acc:0.975494921207428
node11 epoch4:node_model train_loss:0.050117320983725434,train_acc:0.9878478646278381
node11_model on test-dataset: loss:0.7773273388296366,acc:0.7964999079704285
node11 weight score:2163.824576828162
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6508924780786037,acc:0.825499981045723
total cost energy:10.254792222699216 | all_enery_cp：9.2655 | all_enery_tp: 0.9892922226992171
ef: 25.403508338203473
reward: 15.148716115504257
step 172:loss:40.527381896972656|running q:58.19670486450195
episode2,iteration52 selected nodes:[1, 14, 7, 9, 13],center node:7
################################################## episode2,iteration52 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.08995045529788032,train_acc:0.9720587134361267
node1 epoch1:node_model train_loss:0.09084284461706,train_acc:0.971323549747467
node1 epoch2:node_model train_loss:0.0548162829295239,train_acc:0.9833089113235474
node1 epoch3:node_model train_loss:0.061530609204269504,train_acc:0.9811031222343445
node1 epoch4:node_model train_loss:0.09081782671787283,train_acc:0.9705884456634521
node1_model on test-dataset: loss:0.8723305305838585,acc:0.7880997061729431
node1 weight score:7689.745761288759
node7: train data size:1951
node7 epoch0:node_model train_loss:0.46680596470832825,train_acc:0.8501569628715515
node7 epoch1:node_model train_loss:0.25761141553521155,train_acc:0.9176176190376282
node7 epoch2:node_model train_loss:0.18248108662664891,train_acc:0.9345195889472961
node7 epoch3:node_model train_loss:0.08811034699901939,train_acc:0.9740195274353027
node7 epoch4:node_model train_loss:0.06466908417642117,train_acc:0.9820782542228699
node7_model on test-dataset: loss:0.7947694849967957,acc:0.790999710559845
node7 weight score:2454.7998342033297
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5481185175870594,train_acc:0.8390489220619202
node9 epoch1:node_model train_loss:0.2656468047123206,train_acc:0.9019668102264404
node9 epoch2:node_model train_loss:0.18700096520938372,train_acc:0.9417082071304321
node9 epoch3:node_model train_loss:0.10437252999920595,train_acc:0.9680238962173462
node9 epoch4:node_model train_loss:0.0720416808402852,train_acc:0.9817081093788147
node9_model on test-dataset: loss:0.7786352053284645,acc:0.7951000928878784
node9 weight score:2384.94225189398
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5429743404189745,train_acc:0.8365909457206726
node13 epoch1:node_model train_loss:0.25984759752949077,train_acc:0.9100757837295532
node13 epoch2:node_model train_loss:0.20082915760576725,train_acc:0.9361364245414734
node13 epoch3:node_model train_loss:0.09814845367024343,train_acc:0.9750000238418579
node13 epoch4:node_model train_loss:0.05795604425172011,train_acc:0.9869696497917175
node13_model on test-dataset: loss:0.8430366890132427,acc:0.7868002653121948
node13 weight score:1370.0471344276891
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5406667627394199,train_acc:0.8443056344985962
node14 epoch1:node_model train_loss:0.3024062340458234,train_acc:0.9084258079528809
node14 epoch2:node_model train_loss:0.12657252699136734,train_acc:0.9568518996238708
node14 epoch3:node_model train_loss:0.08634593430906534,train_acc:0.9735184907913208
node14 epoch4:node_model train_loss:0.05229759976888696,train_acc:0.9860185384750366
node14_model on test-dataset: loss:0.7995710706710816,acc:0.7920000553131104
node14 weight score:1465.7858982020673
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6980701987445355,acc:0.8182999795675278
total cost energy:7.988757114780907 | all_enery_cp：6.421499999999999 | all_enery_tp: 1.567257114780908
ef: 24.94359779503156
reward: 16.954840680250655
step 173:loss:29.227615356445312|running q:59.21391677856445
episode2,iteration53 selected nodes:[13, 19, 0, 5, 17],center node:17
################################################## episode2,iteration53 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.30888605776887673,train_acc:0.8939110040664673
node0 epoch1:node_model train_loss:0.18482623406900808,train_acc:0.9350301623344421
node0 epoch2:node_model train_loss:0.10544177459982726,train_acc:0.9677663445472717
node0 epoch3:node_model train_loss:0.07095484922711666,train_acc:0.9821155667304993
node0 epoch4:node_model train_loss:0.05328642293954125,train_acc:0.9885751008987427
node0_model on test-dataset: loss:0.7556575033068657,acc:0.8034002780914307
node0 weight score:6858.927460282533
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3589437384354441,train_acc:0.8780074715614319
node5 epoch1:node_model train_loss:0.18074807603108256,train_acc:0.9366540312767029
node5 epoch2:node_model train_loss:0.10739560395871338,train_acc:0.9681577086448669
node5 epoch3:node_model train_loss:0.07303378216334079,train_acc:0.9797742962837219
node5 epoch4:node_model train_loss:0.06941071887941737,train_acc:0.9808269143104553
node5_model on test-dataset: loss:0.8165542417764664,acc:0.7872999906539917
node5 weight score:4574.099072554282
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5749030609925588,train_acc:0.8350757956504822
node13 epoch1:node_model train_loss:0.26913026968638104,train_acc:0.9081060886383057
node13 epoch2:node_model train_loss:0.15471600741147995,train_acc:0.948257565498352
node13 epoch3:node_model train_loss:0.09311684096852939,train_acc:0.9751514792442322
node13 epoch4:node_model train_loss:0.057053109320501484,train_acc:0.9844695925712585
node13_model on test-dataset: loss:0.8563698518276215,acc:0.7811001539230347
node13 weight score:1348.716325703266
node17: train data size:442
node17 epoch0:node_model train_loss:0.7435867011547088,train_acc:0.8199047446250916
node17 epoch1:node_model train_loss:0.3494682013988495,train_acc:0.8977142572402954
node17 epoch2:node_model train_loss:0.16421453803777694,train_acc:0.9472380876541138
node17 epoch3:node_model train_loss:0.13152562379837035,train_acc:0.9712381362915039
node17 epoch4:node_model train_loss:0.07294121533632278,train_acc:0.9880000352859497
node17_model on test-dataset: loss:0.9567898322641849,acc:0.7641999125480652
node17 weight score:461.9614309174188
node19: train data size:4281
node19 epoch0:node_model train_loss:0.36903107859367545,train_acc:0.8729974627494812
node19 epoch1:node_model train_loss:0.1732350677944893,train_acc:0.939603865146637
node19 epoch2:node_model train_loss:0.10629274003034414,train_acc:0.9666493535041809
node19 epoch3:node_model train_loss:0.08120663091540337,train_acc:0.9771000742912292
node19 epoch4:node_model train_loss:0.049937407507799396,train_acc:0.9879069328308105
node19_model on test-dataset: loss:0.7815330857038498,acc:0.7993998527526855
node19 weight score:5477.695158797436
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6390391093492508,acc:0.8248999798297882
total cost energy:9.744377253340971 | all_enery_cp：7.398 | all_enery_tp: 2.346377253340972
ef: 24.98184580541384
reward: 15.237468552072869
step 174:loss:36.4646110534668|running q:60.201271057128906
episode2,iteration54 selected nodes:[0, 11, 2, 9, 19],center node:9
################################################## episode2,iteration54 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.16727195723125568,train_acc:0.9444184899330139
node0 epoch1:node_model train_loss:0.13068781241487998,train_acc:0.9531856775283813
node0 epoch2:node_model train_loss:0.0907646118878172,train_acc:0.9716519117355347
node0 epoch3:node_model train_loss:0.06778463336209264,train_acc:0.9802664518356323
node0 epoch4:node_model train_loss:0.0490804564458533,train_acc:0.9901531934738159
node0_model on test-dataset: loss:0.7813905994594097,acc:0.8028998374938965
node0 weight score:6633.046268518921
node2: train data size:4788
node2 epoch0:node_model train_loss:0.20245481670523682,train_acc:0.9290246367454529
node2 epoch1:node_model train_loss:0.10130129700216155,train_acc:0.9674431085586548
node2 epoch2:node_model train_loss:0.06807643768843263,train_acc:0.9795548915863037
node2 epoch3:node_model train_loss:0.043792646184253194,train_acc:0.9903882741928101
node2 epoch4:node_model train_loss:0.03028187423478812,train_acc:0.9939301013946533
node2_model on test-dataset: loss:0.8410290601849556,acc:0.7930999398231506
node2 weight score:5693.025635698062
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5209926194266269,train_acc:0.8419391512870789
node9 epoch1:node_model train_loss:0.26948708727171544,train_acc:0.9120866656303406
node9 epoch2:node_model train_loss:0.14902793459202113,train_acc:0.9489381909370422
node9 epoch3:node_model train_loss:0.09393219453723807,train_acc:0.9719666242599487
node9 epoch4:node_model train_loss:0.055989096627423636,train_acc:0.9848660230636597
node9_model on test-dataset: loss:0.7882920249551535,acc:0.7975001335144043
node9 weight score:2355.7259761769706
node11: train data size:1682
node11 epoch0:node_model train_loss:0.41928377484574036,train_acc:0.8689669966697693
node11 epoch1:node_model train_loss:0.23893527598942027,train_acc:0.9219654202461243
node11 epoch2:node_model train_loss:0.15408381600590312,train_acc:0.951248049736023
node11 epoch3:node_model train_loss:0.0893856978372616,train_acc:0.9747774600982666
node11 epoch4:node_model train_loss:0.05487108778427629,train_acc:0.9820947051048279
node11_model on test-dataset: loss:0.7930751414597035,acc:0.7945999503135681
node11 weight score:2120.8583046799017
node19: train data size:4281
node19 epoch0:node_model train_loss:0.21930959630151128,train_acc:0.9221618175506592
node19 epoch1:node_model train_loss:0.11803643786629965,train_acc:0.9612861275672913
node19 epoch2:node_model train_loss:0.0776431474748046,train_acc:0.9744185209274292
node19 epoch3:node_model train_loss:0.05450408450912598,train_acc:0.9859373569488525
node19 epoch4:node_model train_loss:0.04185847507053336,train_acc:0.9899455904960632
node19_model on test-dataset: loss:0.8194731190800667,acc:0.795499861240387
node19 weight score:5224.088381087855
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.657156144157052,acc:0.8241999781131745
total cost energy:10.394960169804673 | all_enery_cp：8.8955 | all_enery_tp: 1.499460169804673
ef: 25.309859865089674
reward: 14.914899695285001
step 175:loss:38.94227981567383|running q:61.163814544677734
episode2,iteration55 selected nodes:[1, 6, 17, 3, 14],center node:6
################################################## episode2,iteration55 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.10824219055254669,train_acc:0.963382363319397
node1 epoch1:node_model train_loss:0.06679508073584121,train_acc:0.9792647957801819
node1 epoch2:node_model train_loss:0.047097075158757544,train_acc:0.9872060418128967
node1 epoch3:node_model train_loss:0.03024175576865673,train_acc:0.9939706325531006
node1 epoch4:node_model train_loss:0.026146706179989612,train_acc:0.9951472878456116
node1_model on test-dataset: loss:0.8407660360634327,acc:0.8010000586509705
node1 weight score:7978.438367239071
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3068284815134004,train_acc:0.8956653475761414
node3 epoch1:node_model train_loss:0.1472968641928462,train_acc:0.947382390499115
node3 epoch2:node_model train_loss:0.09268289990723133,train_acc:0.9704651236534119
node3 epoch3:node_model train_loss:0.06688407942825972,train_acc:0.9809302091598511
node3 epoch4:node_model train_loss:0.046265761086414024,train_acc:0.9883720874786377
node3_model on test-dataset: loss:0.7789868005365134,acc:0.8067001104354858
node3 weight score:5451.953739235317
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3947511714312338,train_acc:0.8736865520477295
node6 epoch1:node_model train_loss:0.26086939486765093,train_acc:0.919907808303833
node6 epoch2:node_model train_loss:0.2023629690370252,train_acc:0.9345160722732544
node6 epoch3:node_model train_loss:0.12120431373196264,train_acc:0.9608753323554993
node6 epoch4:node_model train_loss:0.08791747728302594,train_acc:0.9764513969421387
node6_model on test-dataset: loss:0.8621832396835089,acc:0.7838999032974243
node6 weight score:3487.6576829582223
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5517319738864899,train_acc:0.8550925254821777
node14 epoch1:node_model train_loss:0.2446499764919281,train_acc:0.9192129373550415
node14 epoch2:node_model train_loss:0.1393361445516348,train_acc:0.9580091834068298
node14 epoch3:node_model train_loss:0.0903654689900577,train_acc:0.9758332967758179
node14 epoch4:node_model train_loss:0.04517039485896627,train_acc:0.9883332252502441
node14_model on test-dataset: loss:0.8212195965647697,acc:0.7945998907089233
node14 weight score:1427.145680525129
node17: train data size:442
node17 epoch0:node_model train_loss:0.6195076644420624,train_acc:0.8314286470413208
node17 epoch1:node_model train_loss:0.3126688927412033,train_acc:0.9181904792785645
node17 epoch2:node_model train_loss:0.1806911423802376,train_acc:0.9457142949104309
node17 epoch3:node_model train_loss:0.08785304352641106,train_acc:0.9724761843681335
node17 epoch4:node_model train_loss:0.0708074253052473,train_acc:0.9852380752563477
node17_model on test-dataset: loss:0.9224560575187206,acc:0.774399995803833
node17 weight score:479.15561548689806
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.677320067062974,acc:0.8256999802589416
total cost energy:9.511443379554988 | all_enery_cp：7.788 | all_enery_tp: 1.723443379554988
ef: 24.753584662091043
reward: 15.242141282536055
step 176:loss:33.93820571899414|running q:62.071136474609375
episode2,iteration56 selected nodes:[16, 5, 6, 0, 13],center node:6
################################################## episode2,iteration56 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.19479335371691447,train_acc:0.9281858801841736
node0 epoch1:node_model train_loss:0.11655586563910429,train_acc:0.9606903791427612
node0 epoch2:node_model train_loss:0.0663213785021351,train_acc:0.9801136255264282
node0 epoch3:node_model train_loss:0.04819860513536976,train_acc:0.9874605536460876
node0 epoch4:node_model train_loss:0.034900455843084134,train_acc:0.9924213290214539
node0_model on test-dataset: loss:0.7709811639785766,acc:0.8046000599861145
node0 weight score:6722.602629166205
node5: train data size:3735
node5 epoch0:node_model train_loss:0.31271821025170776,train_acc:0.9006014466285706
node5 epoch1:node_model train_loss:0.1631556204275081,train_acc:0.9442856311798096
node5 epoch2:node_model train_loss:0.1058378686246119,train_acc:0.9679322838783264
node5 epoch3:node_model train_loss:0.06944167339488079,train_acc:0.9799999594688416
node5 epoch4:node_model train_loss:0.04883303253078147,train_acc:0.9873683452606201
node5_model on test-dataset: loss:0.7745438723266125,acc:0.8006001114845276
node5 weight score:4822.192949226524
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3046859782940197,train_acc:0.8948386907577515
node6 epoch1:node_model train_loss:0.21006253578009143,train_acc:0.9382027387619019
node6 epoch2:node_model train_loss:0.16953742263778562,train_acc:0.9432259202003479
node6 epoch3:node_model train_loss:0.1038857251405716,train_acc:0.9667739868164062
node6 epoch4:node_model train_loss:0.09920354680188241,train_acc:0.9706449508666992
node6_model on test-dataset: loss:0.8724476939439774,acc:0.781000018119812
node6 weight score:3446.6249620153035
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6396939555803934,train_acc:0.8301515579223633
node13 epoch1:node_model train_loss:0.27618271484971046,train_acc:0.9096969962120056
node13 epoch2:node_model train_loss:0.1402037190273404,train_acc:0.9536363482475281
node13 epoch3:node_model train_loss:0.11453897630174954,train_acc:0.970151424407959
node13 epoch4:node_model train_loss:0.06358603807166219,train_acc:0.9843181371688843
node13_model on test-dataset: loss:0.8427848613262177,acc:0.7909998893737793
node13 weight score:1370.4565103156651
node16: train data size:877
node16 epoch0:node_model train_loss:0.6062309576405419,train_acc:0.840245246887207
node16 epoch1:node_model train_loss:0.30080488158596885,train_acc:0.916118323802948
node16 epoch2:node_model train_loss:0.1626044168240494,train_acc:0.9556710124015808
node16 epoch3:node_model train_loss:0.09340270857016246,train_acc:0.9771139621734619
node16 epoch4:node_model train_loss:0.0572611755794949,train_acc:0.9867820739746094
node16_model on test-dataset: loss:0.8270680454373359,acc:0.7896000742912292
node16 weight score:1060.372244869237
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6530863092839718,acc:0.8254999828338623
total cost energy:8.806015212545539 | all_enery_cp：6.9784999999999995 | all_enery_tp: 1.8275152125455398
ef: 24.862496202099294
reward: 16.056480989553755
step 177:loss:45.666141510009766|running q:63.00970458984375
episode2,iteration57 selected nodes:[11, 1, 7, 5, 6],center node:6
################################################## episode2,iteration57 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.08862898490556023,train_acc:0.9704412817955017
node1 epoch1:node_model train_loss:0.06633080711916965,train_acc:0.9798532128334045
node1 epoch2:node_model train_loss:0.046904379118453056,train_acc:0.9883824586868286
node1 epoch3:node_model train_loss:0.04143650010775994,train_acc:0.989853024482727
node1 epoch4:node_model train_loss:0.052384971079471356,train_acc:0.982720673084259
node1_model on test-dataset: loss:0.8933893913775682,acc:0.7908998727798462
node1 weight score:7508.484054927662
node5: train data size:3735
node5 epoch0:node_model train_loss:0.1882882498596844,train_acc:0.9303382039070129
node5 epoch1:node_model train_loss:0.11641824461127583,train_acc:0.9608644247055054
node5 epoch2:node_model train_loss:0.09635262000129412,train_acc:0.9690600037574768
node5 epoch3:node_model train_loss:0.07771549810116228,train_acc:0.9757893085479736
node5 epoch4:node_model train_loss:0.05942374315897101,train_acc:0.9816539287567139
node5_model on test-dataset: loss:0.8976277828216552,acc:0.7890999913215637
node5 weight score:4160.967464998894
node6: train data size:3007
node6 epoch0:node_model train_loss:0.24547417726247542,train_acc:0.906682014465332
node6 epoch1:node_model train_loss:0.15783052110383589,train_acc:0.9453916549682617
node6 epoch2:node_model train_loss:0.12844939998561336,train_acc:0.959262490272522
node6 epoch3:node_model train_loss:0.11985255777835846,train_acc:0.9579721093177795
node6 epoch4:node_model train_loss:0.11205175879501528,train_acc:0.9667739868164062
node6_model on test-dataset: loss:0.8957146281003951,acc:0.7798999547958374
node6 weight score:3357.0960054288225
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5230165854096412,train_acc:0.8416765332221985
node7 epoch1:node_model train_loss:0.25251791104674337,train_acc:0.9170392155647278
node7 epoch2:node_model train_loss:0.15276051107794048,train_acc:0.9520587921142578
node7 epoch3:node_model train_loss:0.09207124086096882,train_acc:0.9735391736030579
node7 epoch4:node_model train_loss:0.05762576498091221,train_acc:0.9870196580886841
node7_model on test-dataset: loss:0.7874972064048051,acc:0.7920998334884644
node7 weight score:2477.4691060898926
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4016364514827728,train_acc:0.8649211525917053
node11 epoch1:node_model train_loss:0.2713861000888488,train_acc:0.9132137298583984
node11 epoch2:node_model train_loss:0.1313189224285238,train_acc:0.9609180688858032
node11 epoch3:node_model train_loss:0.06464960645226871,train_acc:0.9803298711776733
node11 epoch4:node_model train_loss:0.03577434572884265,train_acc:0.9911763668060303
node11_model on test-dataset: loss:0.7885588228702545,acc:0.8039001822471619
node11 weight score:2133.0051116259565
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6884537091851235,acc:0.8258999806642532
total cost energy:9.4479495102246 | all_enery_cp：8.541500000000001 | all_enery_tp: 0.9064495102245982
ef: 25.205353324844275
reward: 15.757403814619675
step 178:loss:57.53452682495117|running q:64.0631332397461
episode2,iteration58 selected nodes:[6, 15, 14, 10, 3],center node:14
################################################## episode2,iteration58 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2691589742898941,train_acc:0.9072042107582092
node3 epoch1:node_model train_loss:0.13168968190980512,train_acc:0.9516525268554688
node3 epoch2:node_model train_loss:0.09581744857132435,train_acc:0.9703759551048279
node3 epoch3:node_model train_loss:0.07747089204400084,train_acc:0.9748538732528687
node3 epoch4:node_model train_loss:0.05263789457290671,train_acc:0.9883720278739929
node3_model on test-dataset: loss:0.8165941655635833,acc:0.8020999431610107
node3 weight score:5200.869880167313
node6: train data size:3007
node6 epoch0:node_model train_loss:0.21695688126548643,train_acc:0.9280645251274109
node6 epoch1:node_model train_loss:0.15297731432703235,train_acc:0.9495851993560791
node6 epoch2:node_model train_loss:0.17058425228441915,train_acc:0.9425804615020752
node6 epoch3:node_model train_loss:0.07397107407450676,train_acc:0.9790319204330444
node6 epoch4:node_model train_loss:0.06175148958760884,train_acc:0.9838707447052002
node6_model on test-dataset: loss:0.9602935659885407,acc:0.7818999886512756
node6 weight score:3131.3341112564353
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5480730310082436,train_acc:0.8386666178703308
node10 epoch1:node_model train_loss:0.2637135021388531,train_acc:0.9169999957084656
node10 epoch2:node_model train_loss:0.17802223227918149,train_acc:0.9436665773391724
node10 epoch3:node_model train_loss:0.11320639196783304,train_acc:0.9701665043830872
node10 epoch4:node_model train_loss:0.07628161637112499,train_acc:0.9811665415763855
node10_model on test-dataset: loss:0.7846797628700733,acc:0.7930999398231506
node10 weight score:2516.9503451652276
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5596700335542361,train_acc:0.8487500548362732
node14 epoch1:node_model train_loss:0.2710362523794174,train_acc:0.9082407355308533
node14 epoch2:node_model train_loss:0.12889524611334005,train_acc:0.9623610377311707
node14 epoch3:node_model train_loss:0.06103682362784942,train_acc:0.9849998950958252
node14 epoch4:node_model train_loss:0.03395277075469494,train_acc:0.9933332800865173
node14_model on test-dataset: loss:0.8144322785735131,acc:0.8000999093055725
node14 weight score:1439.0392311718915
node15: train data size:629
node15 epoch0:node_model train_loss:0.7464202557291303,train_acc:0.7956650853157043
node15 epoch1:node_model train_loss:0.3820370158978871,train_acc:0.8672906756401062
node15 epoch2:node_model train_loss:0.20236856117844582,train_acc:0.9385714530944824
node15 epoch3:node_model train_loss:0.17412314883300237,train_acc:0.9557142853736877
node15 epoch4:node_model train_loss:0.08600907293813569,train_acc:0.9765024185180664
node15_model on test-dataset: loss:0.9075860533118248,acc:0.7832999229431152
node15 weight score:693.0472297417408
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6671015083789825,acc:0.8225999826192856
total cost energy:7.337304707487102 | all_enery_cp：5.515 | all_enery_tp: 1.8223047074871028
ef: 24.785973536619405
reward: 17.448668829132302
step 179:loss:50.617183685302734|running q:64.83263397216797
episode2,iteration59 selected nodes:[5, 14, 4, 3, 19],center node:14
################################################## episode2,iteration59 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.14878059507802474,train_acc:0.9437754154205322
node3 epoch1:node_model train_loss:0.09620212035816769,train_acc:0.9648243188858032
node3 epoch2:node_model train_loss:0.06925320166141488,train_acc:0.9788073897361755
node3 epoch3:node_model train_loss:0.04337293695831715,train_acc:0.9888371229171753
node3 epoch4:node_model train_loss:0.033774145164115484,train_acc:0.9904354214668274
node3_model on test-dataset: loss:0.9121653106063604,acc:0.7854999303817749
node3 weight score:4655.9542997494755
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4852096715143749,train_acc:0.8596428632736206
node4 epoch1:node_model train_loss:0.27426485078675406,train_acc:0.903571367263794
node4 epoch2:node_model train_loss:0.20832123953316892,train_acc:0.9267857670783997
node4 epoch3:node_model train_loss:0.15739236266485282,train_acc:0.9557140469551086
node4 epoch4:node_model train_loss:0.14476136144782817,train_acc:0.9485713839530945
node4_model on test-dataset: loss:0.8799548229575157,acc:0.7872998118400574
node4 weight score:3074.021449088185
node5: train data size:3735
node5 epoch0:node_model train_loss:0.21925414471249832,train_acc:0.9256012439727783
node5 epoch1:node_model train_loss:0.11551386902206823,train_acc:0.9606013894081116
node5 epoch2:node_model train_loss:0.07432985943006842,train_acc:0.9802630543708801
node5 epoch3:node_model train_loss:0.05187039106692139,train_acc:0.9853007793426514
node5 epoch4:node_model train_loss:0.037520769444343294,train_acc:0.9923684000968933
node5_model on test-dataset: loss:0.820086018294096,acc:0.7953001260757446
node5 weight score:4554.400290556556
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4662874949475129,train_acc:0.8659259080886841
node14 epoch1:node_model train_loss:0.22120804463823637,train_acc:0.9227315187454224
node14 epoch2:node_model train_loss:0.10997319966554642,train_acc:0.9671758413314819
node14 epoch3:node_model train_loss:0.06151365007584294,train_acc:0.9858332872390747
node14 epoch4:node_model train_loss:0.04369311135572692,train_acc:0.9860185384750366
node14_model on test-dataset: loss:0.8271690142154694,acc:0.7951998710632324
node14 weight score:1416.8809274264058
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2926454147280649,train_acc:0.8981682658195496
node19 epoch1:node_model train_loss:0.1548956638678562,train_acc:0.9471144676208496
node19 epoch2:node_model train_loss:0.09570323849140211,train_acc:0.9686188697814941
node19 epoch3:node_model train_loss:0.06695011946870837,train_acc:0.9811081886291504
node19 epoch4:node_model train_loss:0.04251595794461494,train_acc:0.9909301400184631
node19_model on test-dataset: loss:0.7964746131002903,acc:0.8035998940467834
node19 weight score:5374.935910808429
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6859254723787308,acc:0.8227999800443649
total cost energy:10.057297095147218 | all_enery_cp：8.07 | all_enery_tp: 1.9872970951472186
ef: 25.07564014513446
reward: 15.018343049987243
step 180:loss:28.238771438598633|running q:65.68672943115234
episode2_cost time: 26912.82109451294
episode3,iteration0 selected nodes:[8, 2, 14, 7, 1],center node:7
################################################## episode3,iteration0 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:1.8425029568812425,train_acc:0.3559558689594269
node1 epoch1:node_model train_loss:1.467852450468961,train_acc:0.4733823835849762
node1 epoch2:node_model train_loss:1.2865522644099068,train_acc:0.5483087301254272
node1 epoch3:node_model train_loss:1.1558087485678055,train_acc:0.5892646908760071
node1 epoch4:node_model train_loss:1.040274450884146,train_acc:0.6364705562591553
node1_model on test-dataset: loss:1.370445328950882,acc:0.550399899482727
node1 weight score:4894.759286118462
node2: train data size:4788
node2 epoch0:node_model train_loss:1.9488189791639645,train_acc:0.3204261064529419
node2 epoch1:node_model train_loss:1.5104393164316814,train_acc:0.458816260099411
node2 epoch2:node_model train_loss:1.3201734994848568,train_acc:0.5246590971946716
node2 epoch3:node_model train_loss:1.1864162956674893,train_acc:0.5853787064552307
node2 epoch4:node_model train_loss:1.0994693736235301,train_acc:0.6193939447402954
node2_model on test-dataset: loss:1.3426741111278533,acc:0.539400041103363
node2 weight score:3566.017963940673
node7: train data size:1951
node7 epoch0:node_model train_loss:2.2726968705654143,train_acc:0.24572549760341644
node7 epoch1:node_model train_loss:1.692896169424057,train_acc:0.39349016547203064
node7 epoch2:node_model train_loss:1.489610457420349,train_acc:0.46908822655677795
node7 epoch3:node_model train_loss:1.3486168026924132,train_acc:0.5125097632408142
node7 epoch4:node_model train_loss:1.2743610620498658,train_acc:0.5483137965202332
node7_model on test-dataset: loss:1.4407502621412278,acc:0.49599990248680115
node7 weight score:1354.155575235117
node8: train data size:1798
node8 epoch0:node_model train_loss:2.34541079070833,train_acc:0.23861677944660187
node8 epoch1:node_model train_loss:1.7195708950360615,train_acc:0.3787301778793335
node8 epoch2:node_model train_loss:1.5376437173949347,train_acc:0.4594556987285614
node8 epoch3:node_model train_loss:1.4102986852327983,train_acc:0.5044216513633728
node8 epoch4:node_model train_loss:1.2943502068519592,train_acc:0.5488775372505188
node8_model on test-dataset: loss:1.5297764611244202,acc:0.46709999442100525
node8 weight score:1175.3351196673725
node14: train data size:1172
node14 epoch0:node_model train_loss:2.29677806297938,train_acc:0.2336110919713974
node14 epoch1:node_model train_loss:1.7516041199366252,train_acc:0.38222217559814453
node14 epoch2:node_model train_loss:1.5927474200725555,train_acc:0.43976855278015137
node14 epoch3:node_model train_loss:1.4274474879105885,train_acc:0.4766203761100769
node14 epoch4:node_model train_loss:1.2990586360295613,train_acc:0.540601909160614
node14_model on test-dataset: loss:1.6388022017478943,acc:0.43800002336502075
node14 weight score:715.1564714460245
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.4104884895682335,acc:0.47369998902082444
total cost energy:9.781365690108165 | all_enery_cp：8.2085 | all_enery_tp: 1.572865690108165
ef: 23.706685851797246
reward: 13.925320161689081
step 181:loss:26.86808204650879|running q:1.2954717874526978
episode3,iteration1 selected nodes:[18, 7, 5, 10, 15],center node:15
################################################## episode3,iteration1 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:1.3760133573883457,train_acc:0.5179322957992554
node5 epoch1:node_model train_loss:1.1263428706871836,train_acc:0.6088345646858215
node5 epoch2:node_model train_loss:1.016283261148553,train_acc:0.6492481827735901
node5 epoch3:node_model train_loss:0.9241139637796503,train_acc:0.6813157796859741
node5 epoch4:node_model train_loss:0.8789399548580772,train_acc:0.697857141494751
node5_model on test-dataset: loss:1.3054893556237221,acc:0.565099835395813
node5 weight score:2860.9961344461008
node7: train data size:1951
node7 epoch0:node_model train_loss:1.4820795476436615,train_acc:0.5013334155082703
node7 epoch1:node_model train_loss:1.1381008595228195,train_acc:0.6012746095657349
node7 epoch2:node_model train_loss:1.0207252502441406,train_acc:0.6388922333717346
node7 epoch3:node_model train_loss:0.8432494014501571,train_acc:0.7137745022773743
node7 epoch4:node_model train_loss:0.7929281264543533,train_acc:0.7317940592765808
node7_model on test-dataset: loss:1.1669884276390077,acc:0.5956999659538269
node7 weight score:1671.8246332118006
node10: train data size:1975
node10 epoch0:node_model train_loss:1.5279755234718322,train_acc:0.47366663813591003
node10 epoch1:node_model train_loss:1.1505516916513443,train_acc:0.6060000658035278
node10 epoch2:node_model train_loss:1.0540370106697083,train_acc:0.6330000162124634
node10 epoch3:node_model train_loss:0.9465371549129487,train_acc:0.6660000681877136
node10 epoch4:node_model train_loss:0.8788579404354095,train_acc:0.6839999556541443
node10_model on test-dataset: loss:1.2846576535701753,acc:0.557499885559082
node10 weight score:1537.374563963639
node15: train data size:629
node15 epoch0:node_model train_loss:1.7423515149525233,train_acc:0.410196989774704
node15 epoch1:node_model train_loss:1.2734730754579817,train_acc:0.5253202319145203
node15 epoch2:node_model train_loss:1.141452533858163,train_acc:0.5873891711235046
node15 epoch3:node_model train_loss:0.9231278470584324,train_acc:0.6724631190299988
node15 epoch4:node_model train_loss:0.8292139087404523,train_acc:0.7107388973236084
node15_model on test-dataset: loss:1.3618919670581817,acc:0.535099983215332
node15 weight score:461.8574859199007
node18: train data size:472
node18 epoch0:node_model train_loss:1.6727644205093384,train_acc:0.4490000307559967
node18 epoch1:node_model train_loss:1.267388916015625,train_acc:0.585111141204834
node18 epoch2:node_model train_loss:1.0905110955238342,train_acc:0.5987777709960938
node18 epoch3:node_model train_loss:0.8525023221969604,train_acc:0.704444408416748
node18 epoch4:node_model train_loss:0.7215981960296631,train_acc:0.7603333592414856
node18_model on test-dataset: loss:1.3991217917203904,acc:0.5114999413490295
node18 weight score:337.35447678190945
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.0386977362632752,acc:0.6293999861180782
total cost energy:6.427538198573139 | all_enery_cp：4.380999999999999 | all_enery_tp: 2.0465381985731397
ef: 23.692294818447245
reward: 17.264756619874106
step 182:loss:48.82682418823242|running q:2.978257656097412
episode3,iteration2 selected nodes:[2, 15, 13, 17, 0],center node:2
################################################## episode3,iteration2 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:1.1796876788139343,train_acc:0.5972405672073364
node0 epoch1:node_model train_loss:1.0119299716674364,train_acc:0.6471270322799683
node0 epoch2:node_model train_loss:0.9257269742397162,train_acc:0.676742434501648
node0 epoch3:node_model train_loss:0.8856409868368735,train_acc:0.6927432417869568
node0 epoch4:node_model train_loss:0.7996511207177088,train_acc:0.7138230800628662
node0_model on test-dataset: loss:1.0973480702936649,acc:0.6373001337051392
node0 weight score:4723.205098098874
node2: train data size:4788
node2 epoch0:node_model train_loss:1.1392664772768815,train_acc:0.6021875143051147
node2 epoch1:node_model train_loss:0.9233100252846876,train_acc:0.6790056228637695
node2 epoch2:node_model train_loss:0.8348442936937014,train_acc:0.7027367353439331
node2 epoch3:node_model train_loss:0.786615160604318,train_acc:0.7266382575035095
node2 epoch4:node_model train_loss:0.7414107862859964,train_acc:0.7405397891998291
node2_model on test-dataset: loss:1.2092761020362377,acc:0.6152999997138977
node2 weight score:3959.39355118135
node13: train data size:1155
node13 epoch0:node_model train_loss:1.3525946140289307,train_acc:0.5435606241226196
node13 epoch1:node_model train_loss:0.9966465284427007,train_acc:0.6531060934066772
node13 epoch2:node_model train_loss:0.8855952173471451,train_acc:0.6968939304351807
node13 epoch3:node_model train_loss:0.7251212447881699,train_acc:0.7566666603088379
node13 epoch4:node_model train_loss:0.6607988352576891,train_acc:0.7734848856925964
node13_model on test-dataset: loss:1.2903683483600616,acc:0.5727999210357666
node13 weight score:895.0932510611391
node15: train data size:629
node15 epoch0:node_model train_loss:1.6210959468569075,train_acc:0.4799014925956726
node15 epoch1:node_model train_loss:1.1281561851501465,train_acc:0.6023153066635132
node15 epoch2:node_model train_loss:0.8672167573656354,train_acc:0.6826601624488831
node15 epoch3:node_model train_loss:0.7671816945075989,train_acc:0.7247290015220642
node15 epoch4:node_model train_loss:0.639433639390128,train_acc:0.7939408421516418
node15_model on test-dataset: loss:1.2404655423760413,acc:0.5852999091148376
node15 weight score:507.06769234007595
node17: train data size:442
node17 epoch0:node_model train_loss:1.5082619309425354,train_acc:0.5224761366844177
node17 epoch1:node_model train_loss:1.031352460384369,train_acc:0.6585714221000671
node17 epoch2:node_model train_loss:0.8764074802398681,train_acc:0.6880952715873718
node17 epoch3:node_model train_loss:0.750520122051239,train_acc:0.7493333220481873
node17 epoch4:node_model train_loss:0.6467962265014648,train_acc:0.7920952439308167
node17_model on test-dataset: loss:1.3225138568878174,acc:0.5565000176429749
node17 weight score:334.2119991393729
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8898705005645752,acc:0.7008999839425087
total cost energy:8.631913917603468 | all_enery_cp：6.0985 | all_enery_tp: 2.533413917603468
ef: 23.36477875439143
reward: 14.732864836787963
step 183:loss:116.2909164428711|running q:4.749135494232178
episode3,iteration3 selected nodes:[18, 15, 13, 7, 11],center node:11
################################################## episode3,iteration3 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:1.1438439160585403,train_acc:0.6319705843925476
node7 epoch1:node_model train_loss:0.8320158153772355,train_acc:0.7117548584938049
node7 epoch2:node_model train_loss:0.721953684091568,train_acc:0.7493920922279358
node7 epoch3:node_model train_loss:0.5978814557194709,train_acc:0.7987744212150574
node7 epoch4:node_model train_loss:0.5470615610480308,train_acc:0.8136962056159973
node7_model on test-dataset: loss:1.0291703462600708,acc:0.6529999375343323
node7 weight score:1895.7017242964591
node11: train data size:1682
node11 epoch0:node_model train_loss:1.0909215036560507,train_acc:0.6288666129112244
node11 epoch1:node_model train_loss:0.77528635894551,train_acc:0.7405450940132141
node11 epoch2:node_model train_loss:0.6894134283065796,train_acc:0.7508177757263184
node11 epoch3:node_model train_loss:0.6185737059396856,train_acc:0.790875256061554
node11 epoch4:node_model train_loss:0.5698477643377641,train_acc:0.8044046759605408
node11_model on test-dataset: loss:1.0914611215889454,acc:0.6459999680519104
node11 weight score:1541.0535169144184
node13: train data size:1155
node13 epoch0:node_model train_loss:1.1555683463811874,train_acc:0.6170454025268555
node13 epoch1:node_model train_loss:0.8876400887966156,train_acc:0.6954545974731445
node13 epoch2:node_model train_loss:0.7013033727804819,train_acc:0.7437121272087097
node13 epoch3:node_model train_loss:0.6046639755368233,train_acc:0.8010606169700623
node13 epoch4:node_model train_loss:0.531490276257197,train_acc:0.8231818675994873
node13_model on test-dataset: loss:1.035913723409176,acc:0.659800112247467
node13 weight score:1114.9577169408597
node15: train data size:629
node15 epoch0:node_model train_loss:1.3699861424309867,train_acc:0.5638916492462158
node15 epoch1:node_model train_loss:0.9939263548169818,train_acc:0.63453209400177
node15 epoch2:node_model train_loss:0.8294829726219177,train_acc:0.7043842673301697
node15 epoch3:node_model train_loss:0.6344723275729588,train_acc:0.7890148758888245
node15 epoch4:node_model train_loss:0.5693983095032829,train_acc:0.807733952999115
node15_model on test-dataset: loss:1.3138560104370116,acc:0.5837000012397766
node15 weight score:478.7434810232999
node18: train data size:472
node18 epoch0:node_model train_loss:1.0910914897918702,train_acc:0.6416666507720947
node18 epoch1:node_model train_loss:0.8436566114425659,train_acc:0.7124444246292114
node18 epoch2:node_model train_loss:0.7339807033538819,train_acc:0.7301111221313477
node18 epoch3:node_model train_loss:0.5490243136882782,train_acc:0.8015555739402771
node18 epoch4:node_model train_loss:0.39822002649307253,train_acc:0.8645555377006531
node18_model on test-dataset: loss:1.4293412210047245,acc:0.5800999402999878
node18 weight score:330.22205829075426
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8274241089820862,acc:0.7180999809503555
total cost energy:4.720087376308517 | all_enery_cp：2.9445000000000006 | all_enery_tp: 1.7755873763085166
ef: 23.648282396446355
reward: 18.92819502013784
step 184:loss:35.39674758911133|running q:6.259886741638184
episode3,iteration4 selected nodes:[9, 7, 13, 11, 17],center node:11
################################################## episode3,iteration4 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.9338142782449722,train_acc:0.6842744946479797
node7 epoch1:node_model train_loss:0.6799788117408753,train_acc:0.7647745013237
node7 epoch2:node_model train_loss:0.5764371171593666,train_acc:0.7977157831192017
node7 epoch3:node_model train_loss:0.48368521630764005,train_acc:0.8391568064689636
node7 epoch4:node_model train_loss:0.4438118800520897,train_acc:0.8520979881286621
node7_model on test-dataset: loss:1.1432911174744367,acc:0.6437999606132507
node7 weight score:1706.4770032586412
node9: train data size:1857
node9 epoch0:node_model train_loss:1.0422087499969883,train_acc:0.6655955910682678
node9 epoch1:node_model train_loss:0.7859764318717154,train_acc:0.7336472272872925
node9 epoch2:node_model train_loss:0.6786698087265617,train_acc:0.7599813938140869
node9 epoch3:node_model train_loss:0.5865221400009958,train_acc:0.799963116645813
node9 epoch4:node_model train_loss:0.56213998480847,train_acc:0.8122068643569946
node9_model on test-dataset: loss:1.0708129543066025,acc:0.6481999158859253
node9 weight score:1734.1964276127826
node11: train data size:1682
node11 epoch0:node_model train_loss:0.9309363084680894,train_acc:0.6792539358139038
node11 epoch1:node_model train_loss:0.681021553628585,train_acc:0.7579339742660522
node11 epoch2:node_model train_loss:0.588565377628102,train_acc:0.7942754030227661
node11 epoch3:node_model train_loss:0.5354657839326298,train_acc:0.8128980994224548
node11 epoch4:node_model train_loss:0.43964722577263327,train_acc:0.8515352606773376
node11_model on test-dataset: loss:1.060892714560032,acc:0.6553999781608582
node11 weight score:1585.4572068557852
node13: train data size:1155
node13 epoch0:node_model train_loss:1.0000737756490707,train_acc:0.6769697070121765
node13 epoch1:node_model train_loss:0.8077805936336517,train_acc:0.7169696688652039
node13 epoch2:node_model train_loss:0.6487912138303121,train_acc:0.7767423987388611
node13 epoch3:node_model train_loss:0.5549952288468679,train_acc:0.8050758242607117
node13 epoch4:node_model train_loss:0.40504318475723267,train_acc:0.8760606050491333
node13_model on test-dataset: loss:0.9599484980106354,acc:0.6792000532150269
node13 weight score:1203.189548599308
node17: train data size:442
node17 epoch0:node_model train_loss:1.126990807056427,train_acc:0.6333333849906921
node17 epoch1:node_model train_loss:0.9797916889190674,train_acc:0.659333348274231
node17 epoch2:node_model train_loss:0.6982748150825501,train_acc:0.7368571162223816
node17 epoch3:node_model train_loss:0.5289570927619934,train_acc:0.8099046945571899
node17 epoch4:node_model train_loss:0.4432081937789917,train_acc:0.8544761538505554
node17_model on test-dataset: loss:1.1875549548864364,acc:0.6173999905586243
node17 weight score:372.1933020289302
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8119004955887794,acc:0.723499983549118
total cost energy:4.837563876655656 | all_enery_cp：3.5435 | all_enery_tp: 1.2940638766556565
ef: 24.092367386416132
reward: 19.254803509760478
step 185:loss:77.13824462890625|running q:7.796176910400391
episode3,iteration5 selected nodes:[14, 3, 15, 16, 1],center node:14
################################################## episode3,iteration5 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.9501832539544386,train_acc:0.6823528409004211
node1 epoch1:node_model train_loss:0.8448431693455752,train_acc:0.7138970494270325
node1 epoch2:node_model train_loss:0.8150323532959994,train_acc:0.7208089828491211
node1 epoch3:node_model train_loss:0.7275629867525661,train_acc:0.7525001764297485
node1 epoch4:node_model train_loss:0.6483142748475075,train_acc:0.779485285282135
node1_model on test-dataset: loss:1.1443979495763779,acc:0.6358000636100769
node1 weight score:5861.59735997701
node3: train data size:4247
node3 epoch0:node_model train_loss:1.034640639327293,train_acc:0.6531617045402527
node3 epoch1:node_model train_loss:0.7875306841938995,train_acc:0.724438488483429
node3 epoch2:node_model train_loss:0.7515503916629526,train_acc:0.7364473342895508
node3 epoch3:node_model train_loss:0.6751880299213321,train_acc:0.765690267086029
node3 epoch4:node_model train_loss:0.6177231704079827,train_acc:0.7854874134063721
node3_model on test-dataset: loss:0.9760825245082378,acc:0.6820999979972839
node3 weight score:4351.066527023102
node14: train data size:1172
node14 epoch0:node_model train_loss:1.073668787876765,train_acc:0.6461111307144165
node14 epoch1:node_model train_loss:0.7318895061810812,train_acc:0.7391666769981384
node14 epoch2:node_model train_loss:0.5862847864627838,train_acc:0.7918519377708435
node14 epoch3:node_model train_loss:0.5303550586104393,train_acc:0.8188425898551941
node14 epoch4:node_model train_loss:0.43066392590602237,train_acc:0.8591203689575195
node14_model on test-dataset: loss:0.9978079715371132,acc:0.6711999177932739
node14 weight score:1174.5747011767662
node15: train data size:629
node15 epoch0:node_model train_loss:1.1779503652027674,train_acc:0.6173891425132751
node15 epoch1:node_model train_loss:0.8088102340698242,train_acc:0.7170936465263367
node15 epoch2:node_model train_loss:0.5680140001433236,train_acc:0.8074384927749634
node15 epoch3:node_model train_loss:0.49399860416139874,train_acc:0.8461576700210571
node15 epoch4:node_model train_loss:0.4067612758704594,train_acc:0.8702956438064575
node15_model on test-dataset: loss:1.1288498452305793,acc:0.6354000568389893
node15 weight score:557.2043107925662
node16: train data size:877
node16 epoch0:node_model train_loss:1.1579114132457309,train_acc:0.6402596831321716
node16 epoch1:node_model train_loss:0.8049278987778558,train_acc:0.7346897125244141
node16 epoch2:node_model train_loss:0.6028389467133416,train_acc:0.8013564348220825
node16 epoch3:node_model train_loss:0.5355219178729587,train_acc:0.8116883039474487
node16 epoch4:node_model train_loss:0.42316003640492755,train_acc:0.8712409734725952
node16_model on test-dataset: loss:0.9904462397098541,acc:0.6662999391555786
node16 weight score:885.4594674991268
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8128092893958092,acc:0.727499982714653
total cost energy:8.579568792762462 | all_enery_cp：6.8165 | all_enery_tp: 1.7630687927624629
ef: 24.37009154320294
reward: 15.790522750440477
step 186:loss:56.56510543823242|running q:9.287242889404297
episode3,iteration6 selected nodes:[12, 14, 8, 1, 2],center node:8
################################################## episode3,iteration6 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.7737287537140005,train_acc:0.7322794198989868
node1 epoch1:node_model train_loss:0.7046531132038902,train_acc:0.7555882334709167
node1 epoch2:node_model train_loss:0.6512325690949664,train_acc:0.7745587825775146
node1 epoch3:node_model train_loss:0.6062986114445854,train_acc:0.7841913104057312
node1 epoch4:node_model train_loss:0.5143334545633372,train_acc:0.822646975517273
node1_model on test-dataset: loss:0.8984719675779342,acc:0.7020000219345093
node1 weight score:7466.009226846738
node2: train data size:4788
node2 epoch0:node_model train_loss:0.897932897011439,train_acc:0.7009847164154053
node2 epoch1:node_model train_loss:0.6718076920757691,train_acc:0.7688069343566895
node2 epoch2:node_model train_loss:0.6124749376128117,train_acc:0.7877272367477417
node2 epoch3:node_model train_loss:0.569681982199351,train_acc:0.805795431137085
node2 epoch4:node_model train_loss:0.5378270211319128,train_acc:0.8100568652153015
node2_model on test-dataset: loss:0.9345103037357331,acc:0.7013000845909119
node2 weight score:5123.539013812716
node8: train data size:1798
node8 epoch0:node_model train_loss:0.9827455745802985,train_acc:0.6836281418800354
node8 epoch1:node_model train_loss:0.7699852652019925,train_acc:0.7402833700180054
node8 epoch2:node_model train_loss:0.6347682376702627,train_acc:0.7853401303291321
node8 epoch3:node_model train_loss:0.5240831954611672,train_acc:0.8208730220794678
node8 epoch4:node_model train_loss:0.4651418808433745,train_acc:0.8520521521568298
node8_model on test-dataset: loss:0.9546898095309735,acc:0.6835998892784119
node8 weight score:1883.3342328052433
node12: train data size:1336
node12 epoch0:node_model train_loss:0.9882018779005323,train_acc:0.6975396871566772
node12 epoch1:node_model train_loss:0.7058996558189392,train_acc:0.7523016333580017
node12 epoch2:node_model train_loss:0.6059768199920654,train_acc:0.802777886390686
node12 epoch3:node_model train_loss:0.48438490501471926,train_acc:0.8472222089767456
node12 epoch4:node_model train_loss:0.38493394425937105,train_acc:0.8847620487213135
node12_model on test-dataset: loss:1.0125114142894744,acc:0.6683998703956604
node12 weight score:1319.4912977228335
node14: train data size:1172
node14 epoch0:node_model train_loss:0.9051288863023123,train_acc:0.6969444155693054
node14 epoch1:node_model train_loss:0.6095804274082184,train_acc:0.7856018543243408
node14 epoch2:node_model train_loss:0.4968910614649455,train_acc:0.8279167413711548
node14 epoch3:node_model train_loss:0.4219147711992264,train_acc:0.8681018352508545
node14 epoch4:node_model train_loss:0.33914586901664734,train_acc:0.8963888883590698
node14_model on test-dataset: loss:1.0064420920610428,acc:0.6704000234603882
node14 weight score:1164.4981954201849
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7710093756020069,acc:0.7422999840974808
total cost energy:10.224528986457123 | all_enery_cp：7.901 | all_enery_tp: 2.3235289864571236
ef: 24.569452134970888
reward: 14.344923148513764
step 187:loss:31.21905517578125|running q:10.842290878295898
episode3,iteration7 selected nodes:[19, 17, 0, 5, 15],center node:15
################################################## episode3,iteration7 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.8939380840613291,train_acc:0.7039017081260681
node0 epoch1:node_model train_loss:0.6958102515110602,train_acc:0.7641286849975586
node0 epoch2:node_model train_loss:0.6078701409009787,train_acc:0.7904843688011169
node0 epoch3:node_model train_loss:0.5635998019805322,train_acc:0.8047104477882385
node0 epoch4:node_model train_loss:0.5153868055114379,train_acc:0.8233733773231506
node0_model on test-dataset: loss:0.9024128422141076,acc:0.7168000340461731
node0 weight score:5743.490958399144
node5: train data size:3735
node5 epoch0:node_model train_loss:0.8571118163435083,train_acc:0.72406005859375
node5 epoch1:node_model train_loss:0.6635726274628388,train_acc:0.7724060416221619
node5 epoch2:node_model train_loss:0.6309256302682977,train_acc:0.7809397578239441
node5 epoch3:node_model train_loss:0.5274140372088081,train_acc:0.8215413689613342
node5 epoch4:node_model train_loss:0.4931010870557082,train_acc:0.8286466598510742
node5_model on test-dataset: loss:0.8961683148145676,acc:0.7074000239372253
node5 weight score:4167.743869378862
node15: train data size:629
node15 epoch0:node_model train_loss:1.0477569103240967,train_acc:0.6473891735076904
node15 epoch1:node_model train_loss:0.7745389001710075,train_acc:0.7369458675384521
node15 epoch2:node_model train_loss:0.5274109542369843,train_acc:0.828226625919342
node15 epoch3:node_model train_loss:0.37427289145333426,train_acc:0.8752217292785645
node15 epoch4:node_model train_loss:0.35211933084896635,train_acc:0.8923646211624146
node15_model on test-dataset: loss:1.0079441076517106,acc:0.6831000447273254
node15 weight score:624.0425388917968
node17: train data size:442
node17 epoch0:node_model train_loss:0.9668087840080262,train_acc:0.6946666836738586
node17 epoch1:node_model train_loss:0.7588490605354309,train_acc:0.7539047598838806
node17 epoch2:node_model train_loss:0.5818834424018859,train_acc:0.7923809289932251
node17 epoch3:node_model train_loss:0.4630362272262573,train_acc:0.830380916595459
node17 epoch4:node_model train_loss:0.3281251072883606,train_acc:0.8989524245262146
node17_model on test-dataset: loss:1.0493480601906777,acc:0.6670998930931091
node17 weight score:421.2139105871925
node19: train data size:4281
node19 epoch0:node_model train_loss:0.9493579725886501,train_acc:0.6881997585296631
node19 epoch1:node_model train_loss:0.6933503857878751,train_acc:0.7529714703559875
node19 epoch2:node_model train_loss:0.6430391260357791,train_acc:0.777622640132904
node19 epoch3:node_model train_loss:0.5723815199940704,train_acc:0.798073410987854
node19 epoch4:node_model train_loss:0.5173807255057401,train_acc:0.8215360641479492
node19_model on test-dataset: loss:0.9981861370801925,acc:0.6972000002861023
node19 weight score:4288.779257666721
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7174146771430969,acc:0.759299986064434
total cost energy:9.382213595499957 | all_enery_cp：7.134999999999999 | all_enery_tp: 2.247213595499958
ef: 24.45810722506833
reward: 15.075893629568371
step 188:loss:41.8708610534668|running q:12.316840171813965
episode3,iteration8 selected nodes:[6, 1, 3, 9, 17],center node:6
################################################## episode3,iteration8 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.6768097636454246,train_acc:0.7630146145820618
node1 epoch1:node_model train_loss:0.5693133710061803,train_acc:0.7993382811546326
node1 epoch2:node_model train_loss:0.5193248356089872,train_acc:0.8175734877586365
node1 epoch3:node_model train_loss:0.4744005856268546,train_acc:0.8349999189376831
node1 epoch4:node_model train_loss:0.45502247354563546,train_acc:0.8427205681800842
node1_model on test-dataset: loss:0.9849865886569024,acc:0.6922001242637634
node1 weight score:6810.245009677567
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7951016828071239,train_acc:0.7334486842155457
node3 epoch1:node_model train_loss:0.6007273030835528,train_acc:0.7934191226959229
node3 epoch2:node_model train_loss:0.5478061160375906,train_acc:0.8015338778495789
node3 epoch3:node_model train_loss:0.4680662279905275,train_acc:0.8355467915534973
node3 epoch4:node_model train_loss:0.4334067809720372,train_acc:0.8494704365730286
node3_model on test-dataset: loss:0.8685575181245804,acc:0.7211000919342041
node3 weight score:4889.71646825448
node6: train data size:3007
node6 epoch0:node_model train_loss:0.882155922151381,train_acc:0.713686466217041
node6 epoch1:node_model train_loss:0.6869629121595814,train_acc:0.7643318772315979
node6 epoch2:node_model train_loss:0.6431609728643971,train_acc:0.7771427631378174
node6 epoch3:node_model train_loss:0.5682407271477484,train_acc:0.798617422580719
node6 epoch4:node_model train_loss:0.5204354024702503,train_acc:0.8236865997314453
node6_model on test-dataset: loss:1.0188498629629612,acc:0.674799919128418
node6 weight score:2951.367133971254
node9: train data size:1857
node9 epoch0:node_model train_loss:0.8544907036580538,train_acc:0.7190489768981934
node9 epoch1:node_model train_loss:0.6233655841727006,train_acc:0.7930101156234741
node9 epoch2:node_model train_loss:0.48996286172615855,train_acc:0.8305078148841858
node9 epoch3:node_model train_loss:0.4212716708057805,train_acc:0.86090487241745
node9 epoch4:node_model train_loss:0.35615189765629013,train_acc:0.8842012286186218
node9_model on test-dataset: loss:0.9744561997056007,acc:0.6856000423431396
node9 weight score:1905.6782650272332
node17: train data size:442
node17 epoch0:node_model train_loss:0.999934709072113,train_acc:0.6938095092773438
node17 epoch1:node_model train_loss:0.7328200697898865,train_acc:0.763619065284729
node17 epoch2:node_model train_loss:0.5130690932273865,train_acc:0.813428521156311
node17 epoch3:node_model train_loss:0.40654648542404176,train_acc:0.854952335357666
node17 epoch4:node_model train_loss:0.3268725574016571,train_acc:0.905714213848114
node17_model on test-dataset: loss:1.0007321202754975,acc:0.6831998825073242
node17 weight score:441.6766395769521
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6848971001803875,acc:0.765699982047081
total cost energy:9.506729784055029 | all_enery_cp：8.1305 | all_enery_tp: 1.37622978405503
ef: 24.613405440693754
reward: 15.106675656638725
step 189:loss:44.03359603881836|running q:13.739477157592773
episode3,iteration9 selected nodes:[5, 1, 7, 13, 6],center node:6
################################################## episode3,iteration9 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5848463887677473,train_acc:0.7977206110954285
node1 epoch1:node_model train_loss:0.49267440924749656,train_acc:0.831250011920929
node1 epoch2:node_model train_loss:0.41131671211298776,train_acc:0.8578676581382751
node1 epoch3:node_model train_loss:0.3839503601193428,train_acc:0.8679409027099609
node1 epoch4:node_model train_loss:0.3462217020637849,train_acc:0.8791176676750183
node1_model on test-dataset: loss:0.874089551717043,acc:0.7356001734733582
node1 weight score:7674.270887717336
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7701226410112882,train_acc:0.7400751709938049
node5 epoch1:node_model train_loss:0.5708285695628116,train_acc:0.803270697593689
node5 epoch2:node_model train_loss:0.45071751741986527,train_acc:0.8516165018081665
node5 epoch3:node_model train_loss:0.4165054042088358,train_acc:0.8617669343948364
node5 epoch4:node_model train_loss:0.3707575335314399,train_acc:0.8755638003349304
node5_model on test-dataset: loss:0.7915633463859558,acc:0.7408998608589172
node5 weight score:4718.510548843507
node6: train data size:3007
node6 epoch0:node_model train_loss:0.8289013331936251,train_acc:0.7286635041236877
node6 epoch1:node_model train_loss:0.6323923747385701,train_acc:0.7875574827194214
node6 epoch2:node_model train_loss:0.5164518154436543,train_acc:0.8152994513511658
node6 epoch3:node_model train_loss:0.5207211625191474,train_acc:0.8136866092681885
node6 epoch4:node_model train_loss:0.4745932134889787,train_acc:0.8431336283683777
node6_model on test-dataset: loss:0.9618854235857726,acc:0.7047999501228333
node6 weight score:3126.151957673223
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7834171086549759,train_acc:0.7497745752334595
node7 epoch1:node_model train_loss:0.5277992367744446,train_acc:0.8156372308731079
node7 epoch2:node_model train_loss:0.4121448457241058,train_acc:0.8621764183044434
node7 epoch3:node_model train_loss:0.35025721192359927,train_acc:0.88165682554245
node7 epoch4:node_model train_loss:0.29106862768530845,train_acc:0.9090785384178162
node7_model on test-dataset: loss:0.8234811207652092,acc:0.7327001094818115
node7 weight score:2369.2103568653265
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8933810045321783,train_acc:0.7262879610061646
node13 epoch1:node_model train_loss:0.5871438607573509,train_acc:0.8026515245437622
node13 epoch2:node_model train_loss:0.4112514331936836,train_acc:0.8590152263641357
node13 epoch3:node_model train_loss:0.32735300064086914,train_acc:0.9022728204727173
node13 epoch4:node_model train_loss:0.2455847809712092,train_acc:0.9378030300140381
node13_model on test-dataset: loss:0.8782344433665276,acc:0.7185999751091003
node13 weight score:1315.1385814163127
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6748810555040836,acc:0.7795999801158905
total cost energy:9.66318410833637 | all_enery_cp：8.278 | all_enery_tp: 1.3851841083363698
ef: 24.99145044476401
reward: 15.328266336427639
step 190:loss:66.64070129394531|running q:15.259088516235352
episode3,iteration10 selected nodes:[3, 16, 8, 0, 5],center node:3
################################################## episode3,iteration10 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.748122944854773,train_acc:0.7496407628059387
node0 epoch1:node_model train_loss:0.579631755558344,train_acc:0.8027479648590088
node0 epoch2:node_model train_loss:0.46917072053139025,train_acc:0.845528244972229
node0 epoch3:node_model train_loss:0.4140719301425494,train_acc:0.8617214560508728
node0 epoch4:node_model train_loss:0.351801644724149,train_acc:0.8838764429092407
node0_model on test-dataset: loss:0.769437904804945,acc:0.7526000142097473
node0 weight score:6736.086132010753
node3: train data size:4247
node3 epoch0:node_model train_loss:0.699253075344618,train_acc:0.7601383924484253
node3 epoch1:node_model train_loss:0.5200017863927886,train_acc:0.8171448707580566
node3 epoch2:node_model train_loss:0.4428130253109821,train_acc:0.8466798663139343
node3 epoch3:node_model train_loss:0.3920239334882692,train_acc:0.8642404079437256
node3 epoch4:node_model train_loss:0.3821480416281279,train_acc:0.8696140646934509
node3_model on test-dataset: loss:0.9757555491477251,acc:0.7138999104499817
node3 weight score:4352.524567971504
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6259563275073704,train_acc:0.7845113277435303
node5 epoch1:node_model train_loss:0.49767808145598363,train_acc:0.8333836197853088
node5 epoch2:node_model train_loss:0.4002114103028649,train_acc:0.8630076050758362
node5 epoch3:node_model train_loss:0.3453255491821389,train_acc:0.8875189423561096
node5 epoch4:node_model train_loss:0.3010750224715785,train_acc:0.8961655497550964
node5_model on test-dataset: loss:0.8946983484178781,acc:0.7255001664161682
node5 weight score:4174.591365463803
node8: train data size:1798
node8 epoch0:node_model train_loss:0.8893668585353427,train_acc:0.7275623679161072
node8 epoch1:node_model train_loss:0.6131098601553175,train_acc:0.7953289151191711
node8 epoch2:node_model train_loss:0.43447964555687374,train_acc:0.8492857217788696
node8 epoch3:node_model train_loss:0.35451968345377183,train_acc:0.8876643180847168
node8 epoch4:node_model train_loss:0.2999688817395104,train_acc:0.9060317277908325
node8_model on test-dataset: loss:0.8219196346402168,acc:0.7392998337745667
node8 weight score:2187.561805585832
node16: train data size:877
node16 epoch0:node_model train_loss:0.8595050109757317,train_acc:0.7318037748336792
node16 epoch1:node_model train_loss:0.5134439965089163,train_acc:0.816580057144165
node16 epoch2:node_model train_loss:0.3847469240427017,train_acc:0.8746896982192993
node16 epoch3:node_model train_loss:0.29085157480504775,train_acc:0.9098989963531494
node16 epoch4:node_model train_loss:0.22517427636517418,train_acc:0.9335641860961914
node16_model on test-dataset: loss:0.8511204475164413,acc:0.7270001173019409
node16 weight score:1030.4064513537123
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6509429882466793,acc:0.7863999843597412
total cost energy:9.968435556839445 | all_enery_cp：7.92 | all_enery_tp: 2.0484355568394452
ef: 24.79248192892184
reward: 14.824046372082396
step 191:loss:44.223880767822266|running q:16.647979736328125
episode3,iteration11 selected nodes:[2, 8, 3, 16, 19],center node:16
################################################## episode3,iteration11 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.7034704424440861,train_acc:0.7699715495109558
node2 epoch1:node_model train_loss:0.5384239864846071,train_acc:0.8144885897636414
node2 epoch2:node_model train_loss:0.44396537728607655,train_acc:0.8499528169631958
node2 epoch3:node_model train_loss:0.39174738091727096,train_acc:0.8687689304351807
node2 epoch4:node_model train_loss:0.34092752790699404,train_acc:0.8870739340782166
node2_model on test-dataset: loss:0.8531415639072657,acc:0.7394000291824341
node2 weight score:5612.1987282762875
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5938764990762223,train_acc:0.7874071002006531
node3 epoch1:node_model train_loss:0.43824072494063265,train_acc:0.8472636938095093
node3 epoch2:node_model train_loss:0.3748462297195612,train_acc:0.8664770126342773
node3 epoch3:node_model train_loss:0.33145563165808833,train_acc:0.8863630890846252
node3 epoch4:node_model train_loss:0.27769929759724193,train_acc:0.9056952595710754
node3_model on test-dataset: loss:0.8946642678976059,acc:0.7265998721122742
node3 weight score:4747.032101751568
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7785424788792928,train_acc:0.7408049702644348
node8 epoch1:node_model train_loss:0.5743432541688284,train_acc:0.807471752166748
node8 epoch2:node_model train_loss:0.36578453580538434,train_acc:0.8793310523033142
node8 epoch3:node_model train_loss:0.3059370385275947,train_acc:0.9038094282150269
node8 epoch4:node_model train_loss:0.23948156171374851,train_acc:0.9354648590087891
node8_model on test-dataset: loss:0.8298038670420647,acc:0.7311001420021057
node8 weight score:2166.7770799974537
node16: train data size:877
node16 epoch0:node_model train_loss:0.7383762730492486,train_acc:0.762251079082489
node16 epoch1:node_model train_loss:0.4501828716860877,train_acc:0.8552380204200745
node16 epoch2:node_model train_loss:0.3147551947169834,train_acc:0.8871283531188965
node16 epoch3:node_model train_loss:0.2527903268734614,train_acc:0.9332321882247925
node16 epoch4:node_model train_loss:0.15484442313512167,train_acc:0.9587877988815308
node16_model on test-dataset: loss:0.8645001535117626,acc:0.7257000803947449
node16 weight score:1014.4590448451173
node19: train data size:4281
node19 epoch0:node_model train_loss:0.7656698310097982,train_acc:0.7420414090156555
node19 epoch1:node_model train_loss:0.5586559592291366,train_acc:0.8113579750061035
node19 epoch2:node_model train_loss:0.4701033186080844,train_acc:0.8362274169921875
node19 epoch3:node_model train_loss:0.39544916915339096,train_acc:0.8634222745895386
node19 epoch4:node_model train_loss:0.36097148060798645,train_acc:0.8835315108299255
node19_model on test-dataset: loss:0.8714805579185486,acc:0.7213998436927795
node19 weight score:4912.329897783119
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6490167127549649,acc:0.7862999844551086
total cost energy:10.191503207854996 | all_enery_cp：7.9955 | all_enery_tp: 2.1960032078549965
ef: 24.952914534140056
reward: 14.76141132628506
step 192:loss:85.55864715576172|running q:17.900617599487305
episode3,iteration12 selected nodes:[12, 0, 14, 5, 19],center node:14
################################################## episode3,iteration12 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.6341003832908777,train_acc:0.7875648736953735
node0 epoch1:node_model train_loss:0.45111604837270886,train_acc:0.8430978059768677
node0 epoch2:node_model train_loss:0.3768935246536365,train_acc:0.8666473031044006
node0 epoch3:node_model train_loss:0.3314652147774513,train_acc:0.8915292620658875
node0 epoch4:node_model train_loss:0.296877127713882,train_acc:0.9028753638267517
node0_model on test-dataset: loss:0.8650570553541184,acc:0.7366001605987549
node0 weight score:5991.512314616399
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6128345422054592,train_acc:0.788383424282074
node5 epoch1:node_model train_loss:0.3895461739678132,train_acc:0.8668796420097351
node5 epoch2:node_model train_loss:0.34000132232904434,train_acc:0.8863158226013184
node5 epoch3:node_model train_loss:0.32597562396212626,train_acc:0.8898120522499084
node5 epoch4:node_model train_loss:0.27093489115175445,train_acc:0.9163908958435059
node5_model on test-dataset: loss:0.9150982193648816,acc:0.7234999537467957
node5 weight score:4081.5290872189153
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8122672310897282,train_acc:0.7441269755363464
node12 epoch1:node_model train_loss:0.5313014558383397,train_acc:0.8196826577186584
node12 epoch2:node_model train_loss:0.38196333604199545,train_acc:0.8747619390487671
node12 epoch3:node_model train_loss:0.28582226165703367,train_acc:0.9049206972122192
node12 epoch4:node_model train_loss:0.22949228116444179,train_acc:0.9219048023223877
node12_model on test-dataset: loss:0.8482100322842598,acc:0.7427000403404236
node12 weight score:1575.0815825675916
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7489080379406611,train_acc:0.7615277171134949
node14 epoch1:node_model train_loss:0.45857905969023705,train_acc:0.835601806640625
node14 epoch2:node_model train_loss:0.31611790508031845,train_acc:0.9018981456756592
node14 epoch3:node_model train_loss:0.222113107641538,train_acc:0.9328703284263611
node14 epoch4:node_model train_loss:0.19161018605033556,train_acc:0.9359259009361267
node14_model on test-dataset: loss:0.8263619896769524,acc:0.7457000017166138
node14 weight score:1418.2646523446306
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6431026465671007,train_acc:0.7757076621055603
node19 epoch1:node_model train_loss:0.4462801197240519,train_acc:0.8510422110557556
node19 epoch2:node_model train_loss:0.3971973941769711,train_acc:0.8635458946228027
node19 epoch3:node_model train_loss:0.33784334049668424,train_acc:0.8844473361968994
node19 epoch4:node_model train_loss:0.28417212637357936,train_acc:0.9097415804862976
node19_model on test-dataset: loss:0.8605342268943786,acc:0.7414000630378723
node19 weight score:4974.816650175434
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6507086277008056,acc:0.7868999820947647
total cost energy:9.978812160755487 | all_enery_cp：7.8535 | all_enery_tp: 2.1253121607554863
ef: 24.923878159990654
reward: 14.945065999235167
step 193:loss:33.78202819824219|running q:19.343196868896484
episode3,iteration13 selected nodes:[15, 12, 0, 7, 5],center node:5
################################################## episode3,iteration13 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.483667797767199,train_acc:0.8304449319839478
node0 epoch1:node_model train_loss:0.35263892349142295,train_acc:0.8768353462219238
node0 epoch2:node_model train_loss:0.3097580184157078,train_acc:0.8961838483810425
node0 epoch3:node_model train_loss:0.2700944554347258,train_acc:0.9109521508216858
node0 epoch4:node_model train_loss:0.2726314446100822,train_acc:0.9049906730651855
node0_model on test-dataset: loss:0.8260546088218689,acc:0.7433999180793762
node0 weight score:6274.40358621335
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5255132913589478,train_acc:0.817894697189331
node5 epoch1:node_model train_loss:0.3673343458457997,train_acc:0.8720676898956299
node5 epoch2:node_model train_loss:0.31166235200668635,train_acc:0.893909752368927
node5 epoch3:node_model train_loss:0.28059122084002747,train_acc:0.9091728925704956
node5 epoch4:node_model train_loss:0.2356102525403625,train_acc:0.9266541004180908
node5_model on test-dataset: loss:0.8329686461389065,acc:0.7409998774528503
node5 weight score:4483.962292353977
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7612026840448379,train_acc:0.7562353014945984
node7 epoch1:node_model train_loss:0.4585335299372673,train_acc:0.8430981040000916
node7 epoch2:node_model train_loss:0.3331355080008507,train_acc:0.8836176991462708
node7 epoch3:node_model train_loss:0.2502968370914459,train_acc:0.92461758852005
node7 epoch4:node_model train_loss:0.20467736572027206,train_acc:0.9450196623802185
node7_model on test-dataset: loss:0.809312057197094,acc:0.7468999028205872
node7 weight score:2410.689402993619
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7766967798982348,train_acc:0.7535713911056519
node12 epoch1:node_model train_loss:0.4631010230098452,train_acc:0.8388095498085022
node12 epoch2:node_model train_loss:0.33526222833565306,train_acc:0.8799206614494324
node12 epoch3:node_model train_loss:0.24366637851510728,train_acc:0.9246032238006592
node12 epoch4:node_model train_loss:0.19778589212468692,train_acc:0.9461905360221863
node12_model on test-dataset: loss:0.8138911867141724,acc:0.7469000220298767
node12 weight score:1641.49707210085
node15: train data size:629
node15 epoch0:node_model train_loss:0.8454783814293998,train_acc:0.7355172634124756
node15 epoch1:node_model train_loss:0.5468069272381919,train_acc:0.8139408826828003
node15 epoch2:node_model train_loss:0.3894179676260267,train_acc:0.8596552014350891
node15 epoch3:node_model train_loss:0.27168314158916473,train_acc:0.9230049848556519
node15 epoch4:node_model train_loss:0.20759101531335286,train_acc:0.9450738430023193
node15_model on test-dataset: loss:0.8917136052250862,acc:0.7279000878334045
node15 weight score:705.3834283948465
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6466781111061572,acc:0.7886999827623368
total cost energy:8.438954445729289 | all_enery_cp：6.417 | all_enery_tp: 2.021954445729289
ef: 24.655091950250828
reward: 16.21613750452154
step 194:loss:36.506187438964844|running q:20.75245475769043
episode3,iteration14 selected nodes:[17, 2, 3, 4, 9],center node:3
################################################## episode3,iteration14 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6486284329245487,train_acc:0.7854071855545044
node2 epoch1:node_model train_loss:0.4422678460056583,train_acc:0.8512406349182129
node2 epoch2:node_model train_loss:0.3670811839401722,train_acc:0.8770453929901123
node2 epoch3:node_model train_loss:0.3025815486907959,train_acc:0.8956912755966187
node2 epoch4:node_model train_loss:0.27481517133613426,train_acc:0.9057292342185974
node2_model on test-dataset: loss:0.810182531028986,acc:0.7513999938964844
node2 weight score:5909.779360360832
node3: train data size:4247
node3 epoch0:node_model train_loss:0.6557248331779657,train_acc:0.7749926447868347
node3 epoch1:node_model train_loss:0.40228639161863994,train_acc:0.8647946119308472
node3 epoch2:node_model train_loss:0.3249756052743557,train_acc:0.8817715048789978
node3 epoch3:node_model train_loss:0.25206812419170554,train_acc:0.9149677157402039
node3 epoch4:node_model train_loss:0.21266714968653613,train_acc:0.939242959022522
node3_model on test-dataset: loss:0.8522489823400974,acc:0.747599720954895
node3 weight score:4983.285504593537
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7827406613422292,train_acc:0.7617856860160828
node4 epoch1:node_model train_loss:0.5208649954625538,train_acc:0.820357084274292
node4 epoch2:node_model train_loss:0.5048947163990566,train_acc:0.837142825126648
node4 epoch3:node_model train_loss:0.39315684352602276,train_acc:0.8610714077949524
node4 epoch4:node_model train_loss:0.41621488864932743,train_acc:0.8553570508956909
node4_model on test-dataset: loss:0.9710331057012082,acc:0.7056999206542969
node4 weight score:2785.692870941459
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7695807281293368,train_acc:0.7623452544212341
node9 epoch1:node_model train_loss:0.49272304145913376,train_acc:0.8343213200569153
node9 epoch2:node_model train_loss:0.3692018624983336,train_acc:0.8838043212890625
node9 epoch3:node_model train_loss:0.27959398062605606,train_acc:0.9130101799964905
node9 epoch4:node_model train_loss:0.22114851129682442,train_acc:0.9270914196968079
node9_model on test-dataset: loss:0.7825792010128498,acc:0.7569000720977783
node9 weight score:2372.9227630846635
node17: train data size:442
node17 epoch0:node_model train_loss:0.8881216168403625,train_acc:0.7285714149475098
node17 epoch1:node_model train_loss:0.5383904635906219,train_acc:0.8003808856010437
node17 epoch2:node_model train_loss:0.2804723113775253,train_acc:0.914476215839386
node17 epoch3:node_model train_loss:0.27326317727565763,train_acc:0.9129523634910583
node17 epoch4:node_model train_loss:0.18597279191017152,train_acc:0.9444762468338013
node17_model on test-dataset: loss:1.035539120286703,acc:0.7097000479698181
node17 weight score:426.83080855277234
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6338426647335291,acc:0.7955999818444252
total cost energy:8.766713595499958 | all_enery_cp：7.0195 | all_enery_tp: 1.7472135954999581
ef: 24.709052952199137
reward: 15.94233935669918
step 195:loss:46.481788635253906|running q:22.12637710571289
episode3,iteration15 selected nodes:[5, 16, 15, 6, 10],center node:6
################################################## episode3,iteration15 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5046082256655944,train_acc:0.8252631425857544
node5 epoch1:node_model train_loss:0.3365904465317726,train_acc:0.8886091113090515
node5 epoch2:node_model train_loss:0.2558456306394778,train_acc:0.9199999570846558
node5 epoch3:node_model train_loss:0.22576911669028432,train_acc:0.9282707571983337
node5 epoch4:node_model train_loss:0.204445457184001,train_acc:0.9384210705757141
node5_model on test-dataset: loss:0.8216682271659375,acc:0.757000207901001
node5 weight score:4545.630312227844
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7368972561051769,train_acc:0.7683870792388916
node6 epoch1:node_model train_loss:0.5355871281316203,train_acc:0.8172350525856018
node6 epoch2:node_model train_loss:0.46238773388247334,train_acc:0.838847815990448
node6 epoch3:node_model train_loss:0.466486711175211,train_acc:0.8454838991165161
node6 epoch4:node_model train_loss:0.3180475782963537,train_acc:0.8989399671554565
node6_model on test-dataset: loss:0.7728817942738533,acc:0.7581000924110413
node6 weight score:3890.6337583293325
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8306580156087875,train_acc:0.7428333163261414
node10 epoch1:node_model train_loss:0.5914654031395912,train_acc:0.8013333678245544
node10 epoch2:node_model train_loss:0.4131783813238144,train_acc:0.8666664958000183
node10 epoch3:node_model train_loss:0.30387519523501394,train_acc:0.9045000076293945
node10 epoch4:node_model train_loss:0.2409565582871437,train_acc:0.9348332285881042
node10_model on test-dataset: loss:0.7561709587275982,acc:0.7567998170852661
node10 weight score:2611.8432309583986
node15: train data size:629
node15 epoch0:node_model train_loss:0.7629291159766061,train_acc:0.7590148448944092
node15 epoch1:node_model train_loss:0.5275750287941524,train_acc:0.8204434514045715
node15 epoch2:node_model train_loss:0.31633014338357107,train_acc:0.8772907257080078
node15 epoch3:node_model train_loss:0.24737987773759024,train_acc:0.9101478457450867
node15 epoch4:node_model train_loss:0.15502936393022537,train_acc:0.9600000381469727
node15_model on test-dataset: loss:0.8728473903238774,acc:0.7361997961997986
node15 weight score:720.6299829419255
node16: train data size:877
node16 epoch0:node_model train_loss:0.6963157455126444,train_acc:0.7792496681213379
node16 epoch1:node_model train_loss:0.393942697180642,train_acc:0.8692352175712585
node16 epoch2:node_model train_loss:0.32587625583012897,train_acc:0.8893505930900574
node16 epoch3:node_model train_loss:0.2338567293352551,train_acc:0.928340494632721
node16 epoch4:node_model train_loss:0.16777803583277595,train_acc:0.96155846118927
node16_model on test-dataset: loss:0.9103434641659259,acc:0.7211999297142029
node16 weight score:963.3726549610857
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6115999835729599,acc:0.7980999821424484
total cost energy:6.943013621557259 | all_enery_cp：5.1114999999999995 | all_enery_tp: 1.8315136215572592
ef: 24.660616377498936
reward: 17.717602755941677
step 196:loss:40.3961296081543|running q:23.492578506469727
episode3,iteration16 selected nodes:[14, 15, 16, 10, 8],center node:14
################################################## episode3,iteration16 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7666340437200334,train_acc:0.7519726157188416
node8 epoch1:node_model train_loss:0.47739144331879085,train_acc:0.8314625024795532
node8 epoch2:node_model train_loss:0.36222221040063435,train_acc:0.879830002784729
node8 epoch3:node_model train_loss:0.2555806760986646,train_acc:0.9210203886032104
node8 epoch4:node_model train_loss:0.20030294896827805,train_acc:0.9410316944122314
node8_model on test-dataset: loss:0.7544634592533112,acc:0.7593998312950134
node8 weight score:2383.150539562873
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6553007289767265,train_acc:0.7864999175071716
node10 epoch1:node_model train_loss:0.4493499293923378,train_acc:0.8525000810623169
node10 epoch2:node_model train_loss:0.3362404339015484,train_acc:0.8903331756591797
node10 epoch3:node_model train_loss:0.24637553095817566,train_acc:0.9253333210945129
node10 epoch4:node_model train_loss:0.2000877283513546,train_acc:0.9490000009536743
node10_model on test-dataset: loss:0.9050069931894541,acc:0.725399911403656
node10 weight score:2182.3035787156105
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7066347499688467,train_acc:0.774814784526825
node14 epoch1:node_model train_loss:0.46491318196058273,train_acc:0.8425924777984619
node14 epoch2:node_model train_loss:0.2653972680370013,train_acc:0.912222146987915
node14 epoch3:node_model train_loss:0.22541279469927153,train_acc:0.9340276718139648
node14 epoch4:node_model train_loss:0.16197959706187248,train_acc:0.9631943106651306
node14_model on test-dataset: loss:0.7856498371064663,acc:0.7563000917434692
node14 weight score:1491.758725893019
node15: train data size:629
node15 epoch0:node_model train_loss:0.7410168307168143,train_acc:0.7431527972221375
node15 epoch1:node_model train_loss:0.5337063797882625,train_acc:0.8334482908248901
node15 epoch2:node_model train_loss:0.316991035427366,train_acc:0.8810837268829346
node15 epoch3:node_model train_loss:0.20104632633072989,train_acc:0.940147876739502
node15 epoch4:node_model train_loss:0.12698032866631234,train_acc:0.9650738835334778
node15_model on test-dataset: loss:0.9303639309108257,acc:0.7339998483657837
node15 weight score:676.0795201768079
node16: train data size:877
node16 epoch0:node_model train_loss:0.6640618907080756,train_acc:0.7776911854743958
node16 epoch1:node_model train_loss:0.39214494327704114,train_acc:0.8741270303726196
node16 epoch2:node_model train_loss:0.24779190288649666,train_acc:0.9338960647583008
node16 epoch3:node_model train_loss:0.24062760008705986,train_acc:0.9386723637580872
node16 epoch4:node_model train_loss:0.13894945134719214,train_acc:0.9615583419799805
node16_model on test-dataset: loss:0.793113232254982,acc:0.7486000657081604
node16 weight score:1105.7689675741644
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6101111628860235,acc:0.8007999813556671
total cost energy:4.7691814124602425 | all_enery_cp：3.2255000000000003 | all_enery_tp: 1.543681412460242
ef: 24.730022057066208
reward: 19.960840644605966
step 197:loss:21.09776496887207|running q:24.80152702331543
episode3,iteration17 selected nodes:[4, 19, 6, 15, 1],center node:6
################################################## episode3,iteration17 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5547975696185056,train_acc:0.8123529553413391
node1 epoch1:node_model train_loss:0.41782673667458925,train_acc:0.8518384099006653
node1 epoch2:node_model train_loss:0.3821416817167226,train_acc:0.8713970184326172
node1 epoch3:node_model train_loss:0.3239336631753865,train_acc:0.8870588541030884
node1 epoch4:node_model train_loss:0.24874465781099656,train_acc:0.9157350063323975
node1_model on test-dataset: loss:0.8183207029104232,acc:0.7526000142097473
node1 weight score:8197.275195583417
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7008658360157695,train_acc:0.7796429395675659
node4 epoch1:node_model train_loss:0.47712453295077595,train_acc:0.8475000262260437
node4 epoch2:node_model train_loss:0.3742004324282919,train_acc:0.8678571581840515
node4 epoch3:node_model train_loss:0.374995443969965,train_acc:0.8839285373687744
node4 epoch4:node_model train_loss:0.33382718211838175,train_acc:0.8857142329216003
node4_model on test-dataset: loss:0.8469841513037681,acc:0.7487001419067383
node4 weight score:3193.684316095143
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6280786000913189,train_acc:0.7953916788101196
node6 epoch1:node_model train_loss:0.40720345103932964,train_acc:0.8670968413352966
node6 epoch2:node_model train_loss:0.30660509246010936,train_acc:0.8917511105537415
node6 epoch3:node_model train_loss:0.2700244327706675,train_acc:0.9127187728881836
node6 epoch4:node_model train_loss:0.2917109525972797,train_acc:0.9019355177879333
node6_model on test-dataset: loss:0.8501487064361573,acc:0.736500084400177
node6 weight score:3537.028260156288
node15: train data size:629
node15 epoch0:node_model train_loss:0.6601084343024662,train_acc:0.7769458293914795
node15 epoch1:node_model train_loss:0.3826749452522823,train_acc:0.8672906756401062
node15 epoch2:node_model train_loss:0.27536832434790476,train_acc:0.9025123715400696
node15 epoch3:node_model train_loss:0.2390265486070088,train_acc:0.9350739121437073
node15 epoch4:node_model train_loss:0.1453183133687292,train_acc:0.9671428203582764
node15_model on test-dataset: loss:0.8762009564042091,acc:0.7401998043060303
node15 weight score:717.8718482359537
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6354713717172312,train_acc:0.7866523265838623
node19 epoch1:node_model train_loss:0.4238444892472999,train_acc:0.8548176884651184
node19 epoch2:node_model train_loss:0.35190987240436467,train_acc:0.882012665271759
node19 epoch3:node_model train_loss:0.30343308975530225,train_acc:0.8944070339202881
node19 epoch4:node_model train_loss:0.23249762176081193,train_acc:0.9282371401786804
node19_model on test-dataset: loss:0.7899570013582706,acc:0.7531998753547668
node19 weight score:5419.282306048492
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6110235156863928,acc:0.8004999822378158
total cost energy:10.665036218750759 | all_enery_cp：8.665 | all_enery_tp: 2.00003621875076
ef: 24.88018663449295
reward: 14.215150415742192
step 198:loss:22.61489486694336|running q:26.047277450561523
episode3,iteration18 selected nodes:[5, 19, 10, 16, 4],center node:16
################################################## episode3,iteration18 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6486479075891631,train_acc:0.793571412563324
node4 epoch1:node_model train_loss:0.43273025857550756,train_acc:0.8453571200370789
node4 epoch2:node_model train_loss:0.35912497181977543,train_acc:0.8810714483261108
node4 epoch3:node_model train_loss:0.29008521884679794,train_acc:0.9021428823471069
node4 epoch4:node_model train_loss:0.23815139862043516,train_acc:0.9249999523162842
node4_model on test-dataset: loss:0.940975146740675,acc:0.7278999090194702
node4 weight score:2874.6774124369895
node5: train data size:3735
node5 epoch0:node_model train_loss:0.47854868126542943,train_acc:0.8378571271896362
node5 epoch1:node_model train_loss:0.29830223674836914,train_acc:0.9012030363082886
node5 epoch2:node_model train_loss:0.21605646276944562,train_acc:0.9295488595962524
node5 epoch3:node_model train_loss:0.16587994326102107,train_acc:0.9514286518096924
node5 epoch4:node_model train_loss:0.1470266280597762,train_acc:0.9606015086174011
node5_model on test-dataset: loss:0.8431323413550854,acc:0.758499801158905
node5 weight score:4429.909537092473
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6128411307930947,train_acc:0.7918334603309631
node10 epoch1:node_model train_loss:0.4146538972854614,train_acc:0.8594999313354492
node10 epoch2:node_model train_loss:0.2701160326600075,train_acc:0.9108333587646484
node10 epoch3:node_model train_loss:0.19831677861511707,train_acc:0.939666748046875
node10 epoch4:node_model train_loss:0.18091888427734376,train_acc:0.9563332796096802
node10_model on test-dataset: loss:0.7940318329632282,acc:0.7575998306274414
node10 weight score:2487.3058207623053
node16: train data size:877
node16 epoch0:node_model train_loss:0.632731580071979,train_acc:0.7826839685440063
node16 epoch1:node_model train_loss:0.311387300491333,train_acc:0.8946752548217773
node16 epoch2:node_model train_loss:0.26936318311426377,train_acc:0.9080086946487427
node16 epoch3:node_model train_loss:0.14503290504217148,train_acc:0.9652236104011536
node16 epoch4:node_model train_loss:0.11513386335637835,train_acc:0.9745597839355469
node16_model on test-dataset: loss:0.8592681649327278,acc:0.7473998665809631
node16 weight score:1020.6359734840872
node19: train data size:4281
node19 epoch0:node_model train_loss:0.5018599698709887,train_acc:0.8355153799057007
node19 epoch1:node_model train_loss:0.35585575880006304,train_acc:0.8791531324386597
node19 epoch2:node_model train_loss:0.2787149818830712,train_acc:0.9089345932006836
node19 epoch3:node_model train_loss:0.22398235492928084,train_acc:0.9281681180000305
node19 epoch4:node_model train_loss:0.1951127866672915,train_acc:0.942928671836853
node19_model on test-dataset: loss:0.8347576943784952,acc:0.7560000419616699
node19 weight score:5128.43430953619
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6434562566876412,acc:0.7984999805688858
total cost energy:8.79688270647016 | all_enery_cp：6.7865 | all_enery_tp: 2.0103827064701605
ef: 24.963750564664185
reward: 16.166867858194024
step 199:loss:49.492156982421875|running q:27.30466079711914
episode3,iteration19 selected nodes:[2, 8, 4, 9, 16],center node:9
################################################## episode3,iteration19 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5702516672511896,train_acc:0.8048200607299805
node2 epoch1:node_model train_loss:0.3857413089523713,train_acc:0.8670928478240967
node2 epoch2:node_model train_loss:0.28191527429347235,train_acc:0.9072727560997009
node2 epoch3:node_model train_loss:0.240197346235315,train_acc:0.9210512042045593
node2 epoch4:node_model train_loss:0.21053092398991188,train_acc:0.9381629824638367
node2_model on test-dataset: loss:0.7724041482806205,acc:0.7662000060081482
node2 weight score:6198.827402284331
node4: train data size:2705
node4 epoch0:node_model train_loss:0.584537534309285,train_acc:0.8067857623100281
node4 epoch1:node_model train_loss:0.4262210098760469,train_acc:0.848214328289032
node4 epoch2:node_model train_loss:0.29681664066655294,train_acc:0.8975000381469727
node4 epoch3:node_model train_loss:0.23969665222934314,train_acc:0.9264285564422607
node4 epoch4:node_model train_loss:0.19627461396157742,train_acc:0.9375
node4_model on test-dataset: loss:0.8828228349983692,acc:0.7544000744819641
node4 weight score:3064.034926107226
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6361481729480956,train_acc:0.7936394810676575
node8 epoch1:node_model train_loss:0.3911587256524298,train_acc:0.8653855323791504
node8 epoch2:node_model train_loss:0.28744332326783073,train_acc:0.909875214099884
node8 epoch3:node_model train_loss:0.19675254159503514,train_acc:0.9404988884925842
node8 epoch4:node_model train_loss:0.14635458671384388,train_acc:0.9599772691726685
node8_model on test-dataset: loss:0.7483989875018596,acc:0.7680999040603638
node8 weight score:2402.4618285517554
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7466946557948464,train_acc:0.782326877117157
node9 epoch1:node_model train_loss:0.5031419961076034,train_acc:0.8340535759925842
node9 epoch2:node_model train_loss:0.3072397708892822,train_acc:0.8998522758483887
node9 epoch3:node_model train_loss:0.218234121799469,train_acc:0.9353923797607422
node9 epoch4:node_model train_loss:0.17550387860913025,train_acc:0.9530193209648132
node9_model on test-dataset: loss:0.7633695592731238,acc:0.7661998867988586
node9 weight score:2432.635644743583
node16: train data size:877
node16 epoch0:node_model train_loss:0.5882737040519714,train_acc:0.7986868619918823
node16 epoch1:node_model train_loss:0.3257497598727544,train_acc:0.8816738128662109
node16 epoch2:node_model train_loss:0.1850718723403083,train_acc:0.9426695704460144
node16 epoch3:node_model train_loss:0.1384561053580708,train_acc:0.9707792401313782
node16 epoch4:node_model train_loss:0.0932727497484949,train_acc:0.9848917126655579
node16_model on test-dataset: loss:0.7948691117763519,acc:0.7584000825881958
node16 weight score:1103.3263049310146
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6188263902068138,acc:0.8028999817371368
total cost energy:7.9421354462178915 | all_enery_cp：6.0125 | all_enery_tp: 1.9296354462178915
ef: 24.9440732078726
reward: 17.00193776165471
step 200:loss:36.562591552734375|running q:28.66849708557129
episode3,iteration20 selected nodes:[10, 16, 2, 0, 12],center node:10
################################################## episode3,iteration20 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5533175222002543,train_acc:0.8207600712776184
node0 epoch1:node_model train_loss:0.34828422505121964,train_acc:0.8851437568664551
node0 epoch2:node_model train_loss:0.27349282944431674,train_acc:0.9103798866271973
node0 epoch3:node_model train_loss:0.23789730438819298,train_acc:0.9229540228843689
node0 epoch4:node_model train_loss:0.19681717928212422,train_acc:0.937455952167511
node0_model on test-dataset: loss:0.7566068163514137,acc:0.7765000462532043
node0 weight score:6850.321577849363
node2: train data size:4788
node2 epoch0:node_model train_loss:0.37165749942262966,train_acc:0.8698107004165649
node2 epoch1:node_model train_loss:0.27579838037490845,train_acc:0.9088255763053894
node2 epoch2:node_model train_loss:0.21741473643730083,train_acc:0.9281628131866455
node2 epoch3:node_model train_loss:0.19353517657145858,train_acc:0.9413161277770996
node2 epoch4:node_model train_loss:0.1736112921498716,train_acc:0.9467899203300476
node2_model on test-dataset: loss:0.7959394580125809,acc:0.7656002044677734
node2 weight score:6015.532904921419
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6121108919382096,train_acc:0.8003333210945129
node10 epoch1:node_model train_loss:0.38808233141899107,train_acc:0.8684999346733093
node10 epoch2:node_model train_loss:0.24687293246388436,train_acc:0.9265000224113464
node10 epoch3:node_model train_loss:0.17216462939977645,train_acc:0.9528332948684692
node10 epoch4:node_model train_loss:0.14338743984699248,train_acc:0.9604997634887695
node10_model on test-dataset: loss:0.7770116281509399,acc:0.7649999260902405
node10 weight score:2541.7895028159637
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7157491935150964,train_acc:0.7715079188346863
node12 epoch1:node_model train_loss:0.3968328045947211,train_acc:0.872936487197876
node12 epoch2:node_model train_loss:0.2766637695687158,train_acc:0.9072221517562866
node12 epoch3:node_model train_loss:0.1820423693529197,train_acc:0.9515873789787292
node12 epoch4:node_model train_loss:0.15302186299647605,train_acc:0.95714271068573
node12_model on test-dataset: loss:0.8850742107629777,acc:0.7494000196456909
node12 weight score:1509.4779440565803
node16: train data size:877
node16 epoch0:node_model train_loss:0.5631580584579043,train_acc:0.8081385493278503
node16 epoch1:node_model train_loss:0.3236925999323527,train_acc:0.8966810703277588
node16 epoch2:node_model train_loss:0.23430198265446556,train_acc:0.9233477115631104
node16 epoch3:node_model train_loss:0.1689541588226954,train_acc:0.9523375630378723
node16 epoch4:node_model train_loss:0.10617487132549286,train_acc:0.9733333587646484
node16_model on test-dataset: loss:0.8037807913124562,acc:0.7565001249313354
node16 weight score:1091.0935039489902
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6085463660955429,acc:0.8119999843835831
total cost energy:9.23990869276438 | all_enery_cp：7.0795 | all_enery_tp: 2.1604086927643795
ef: 24.982089485599754
reward: 15.742180792835374
step 201:loss:15.691555976867676|running q:30.067846298217773
episode3,iteration21 selected nodes:[15, 19, 17, 10, 1],center node:17
################################################## episode3,iteration21 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.484731018104974,train_acc:0.8362499475479126
node1 epoch1:node_model train_loss:0.3401087340624893,train_acc:0.8775733113288879
node1 epoch2:node_model train_loss:0.29916803166270256,train_acc:0.8968383073806763
node1 epoch3:node_model train_loss:0.2850225782569717,train_acc:0.9022794365882874
node1 epoch4:node_model train_loss:0.21763629159506628,train_acc:0.9275000095367432
node1_model on test-dataset: loss:0.7658923456817865,acc:0.7723999619483948
node1 weight score:8758.411071504615
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5706282243132591,train_acc:0.8104999661445618
node10 epoch1:node_model train_loss:0.36093599945306776,train_acc:0.8674999475479126
node10 epoch2:node_model train_loss:0.24553183168172837,train_acc:0.9153332114219666
node10 epoch3:node_model train_loss:0.15441697351634504,train_acc:0.95333331823349
node10 epoch4:node_model train_loss:0.12196244485676289,train_acc:0.9709998965263367
node10_model on test-dataset: loss:0.7336470675468445,acc:0.7757001519203186
node10 weight score:2692.030115521307
node15: train data size:629
node15 epoch0:node_model train_loss:0.7367900099073138,train_acc:0.7683743834495544
node15 epoch1:node_model train_loss:0.4624557707990919,train_acc:0.833793044090271
node15 epoch2:node_model train_loss:0.3102442409311022,train_acc:0.900295615196228
node15 epoch3:node_model train_loss:0.2298343288046973,train_acc:0.9245812892913818
node15 epoch4:node_model train_loss:0.12506031298211642,train_acc:0.9687192440032959
node15_model on test-dataset: loss:0.8292412583529949,acc:0.7519997358322144
node15 weight score:758.5247280740639
node17: train data size:442
node17 epoch0:node_model train_loss:0.722067654132843,train_acc:0.7859047651290894
node17 epoch1:node_model train_loss:0.36540977358818055,train_acc:0.8761904835700989
node17 epoch2:node_model train_loss:0.25573016703128815,train_acc:0.9229523539543152
node17 epoch3:node_model train_loss:0.19020787477493287,train_acc:0.9504761695861816
node17 epoch4:node_model train_loss:0.11520849466323853,train_acc:0.958476185798645
node17_model on test-dataset: loss:0.9150997523963451,acc:0.7414000630378723
node17 weight score:483.00745229418703
node19: train data size:4281
node19 epoch0:node_model train_loss:0.4957435643950174,train_acc:0.8291271924972534
node19 epoch1:node_model train_loss:0.3155851776516715,train_acc:0.8911915421485901
node19 epoch2:node_model train_loss:0.24828557184962338,train_acc:0.914269208908081
node19 epoch3:node_model train_loss:0.20495025137829226,train_acc:0.9375105500221252
node19 epoch4:node_model train_loss:0.16154852007017578,train_acc:0.9505886435508728
node19_model on test-dataset: loss:0.7995134343951941,acc:0.7658998966217041
node19 weight score:5354.506648457304
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6413989980518818,acc:0.7992999815940857
total cost energy:9.172885801740648 | all_enery_cp：7.0175 | all_enery_tp: 2.155385801740648
ef: 24.917316094563866
reward: 15.744430292823218
step 202:loss:24.216503143310547|running q:31.459762573242188
episode3,iteration22 selected nodes:[2, 11, 4, 16, 10],center node:11
################################################## episode3,iteration22 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.40972809369365376,train_acc:0.8577840328216553
node2 epoch1:node_model train_loss:0.23697616703187427,train_acc:0.9227743148803711
node2 epoch2:node_model train_loss:0.17696615836272636,train_acc:0.944327712059021
node2 epoch3:node_model train_loss:0.15640556160360575,train_acc:0.9534847736358643
node2 epoch4:node_model train_loss:0.12638419509554902,train_acc:0.9675947427749634
node2_model on test-dataset: loss:0.8131752249598503,acc:0.7634000778198242
node2 weight score:5888.029852651258
node4: train data size:2705
node4 epoch0:node_model train_loss:0.594600362437112,train_acc:0.7960714101791382
node4 epoch1:node_model train_loss:0.42065754319940296,train_acc:0.8585713505744934
node4 epoch2:node_model train_loss:0.33291360629456385,train_acc:0.8885714411735535
node4 epoch3:node_model train_loss:0.23794209797467505,train_acc:0.9267857670783997
node4 epoch4:node_model train_loss:0.2256706455456359,train_acc:0.9325000047683716
node4_model on test-dataset: loss:0.8146566575020552,acc:0.7618000507354736
node4 weight score:3320.417227417277
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5316526129841804,train_acc:0.8274999856948853
node10 epoch1:node_model train_loss:0.29555593207478525,train_acc:0.9008334279060364
node10 epoch2:node_model train_loss:0.2123654767870903,train_acc:0.9339999556541443
node10 epoch3:node_model train_loss:0.1527797646820545,train_acc:0.9569999575614929
node10 epoch4:node_model train_loss:0.11485800389200448,train_acc:0.9729997515678406
node10_model on test-dataset: loss:0.7645057790726423,acc:0.7717000246047974
node10 weight score:2583.368306771607
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7368797169012182,train_acc:0.7804877758026123
node11 epoch1:node_model train_loss:0.4275938044576084,train_acc:0.8477331399917603
node11 epoch2:node_model train_loss:0.29801276413833394,train_acc:0.9098134636878967
node11 epoch3:node_model train_loss:0.21038021760828354,train_acc:0.9386370182037354
node11 epoch4:node_model train_loss:0.16073676812298157,train_acc:0.9538593292236328
node11_model on test-dataset: loss:0.7606340759992599,acc:0.7745001912117004
node11 weight score:2211.3129730486025
node16: train data size:877
node16 epoch0:node_model train_loss:0.6506954994466569,train_acc:0.7886868715286255
node16 epoch1:node_model train_loss:0.3414301988151338,train_acc:0.8875613212585449
node16 epoch2:node_model train_loss:0.2191590873731507,train_acc:0.9345598220825195
node16 epoch3:node_model train_loss:0.13483709428045484,train_acc:0.9630013704299927
node16 epoch4:node_model train_loss:0.07855161734753185,train_acc:0.988556981086731
node16_model on test-dataset: loss:0.7934349632263183,acc:0.7661998867988586
node16 weight score:1105.3205878826966
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6193070247769356,acc:0.8087999796867371
total cost energy:7.570219600745605 | all_enery_cp：6.0135000000000005 | all_enery_tp: 1.5567196007456046
ef: 25.113534855752793
reward: 17.543315255007187
step 203:loss:30.390588760375977|running q:32.764617919921875
episode3,iteration23 selected nodes:[8, 10, 19, 4, 2],center node:10
################################################## episode3,iteration23 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.27893294890721637,train_acc:0.900814414024353
node2 epoch1:node_model train_loss:0.1633490171904365,train_acc:0.9490529298782349
node2 epoch2:node_model train_loss:0.13035858096554875,train_acc:0.9623293280601501
node2 epoch3:node_model train_loss:0.10611009178683162,train_acc:0.9716382026672363
node2 epoch4:node_model train_loss:0.09975317089507978,train_acc:0.9744695425033569
node2_model on test-dataset: loss:0.7702208130061626,acc:0.7775999903678894
node2 weight score:6216.399140543208
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4792752571270934,train_acc:0.8389285206794739
node4 epoch1:node_model train_loss:0.31422716059855055,train_acc:0.8889285922050476
node4 epoch2:node_model train_loss:0.2761556248047522,train_acc:0.9117857217788696
node4 epoch3:node_model train_loss:0.3181077880518777,train_acc:0.8871428966522217
node4 epoch4:node_model train_loss:0.18935708754828998,train_acc:0.9396427869796753
node4_model on test-dataset: loss:0.8187235409021377,acc:0.7650999426841736
node4 weight score:3303.9235649916773
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6411680231491724,train_acc:0.7918819785118103
node8 epoch1:node_model train_loss:0.3672957221666972,train_acc:0.8776417970657349
node8 epoch2:node_model train_loss:0.23744825522104898,train_acc:0.9288433790206909
node8 epoch3:node_model train_loss:0.19500134968095356,train_acc:0.9432539343833923
node8 epoch4:node_model train_loss:0.13631955782572427,train_acc:0.9627323150634766
node8_model on test-dataset: loss:0.7745438787341118,acc:0.7657997608184814
node8 weight score:2321.3662251628534
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4506463438272476,train_acc:0.843999981880188
node10 epoch1:node_model train_loss:0.2880403004586697,train_acc:0.9004999995231628
node10 epoch2:node_model train_loss:0.17139786891639233,train_acc:0.953499972820282
node10 epoch3:node_model train_loss:0.14208236075937747,train_acc:0.9594999551773071
node10 epoch4:node_model train_loss:0.09404942318797112,train_acc:0.9781665802001953
node10_model on test-dataset: loss:0.7508582608401775,acc:0.7745999097824097
node10 weight score:2630.3233286533487
node19: train data size:4281
node19 epoch0:node_model train_loss:0.43163064884585006,train_acc:0.8510968089103699
node19 epoch1:node_model train_loss:0.26122166666873664,train_acc:0.914091169834137
node19 epoch2:node_model train_loss:0.19936885060959084,train_acc:0.9353086948394775
node19 epoch3:node_model train_loss:0.13843187984339025,train_acc:0.9612317681312561
node19 epoch4:node_model train_loss:0.13045959815729496,train_acc:0.9657736420631409
node19_model on test-dataset: loss:0.835398514829576,acc:0.76149982213974
node19 weight score:5124.500371985145
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6226458579301835,acc:0.8065999794006348
total cost energy:9.513752738415299 | all_enery_cp：7.7735 | all_enery_tp: 1.7402527384152984
ef: 25.321560483935553
reward: 15.807807745520254
step 204:loss:28.87759017944336|running q:34.00802230834961
episode3,iteration24 selected nodes:[10, 13, 11, 3, 4],center node:10
################################################## episode3,iteration24 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.596690893866295,train_acc:0.8044137358665466
node3 epoch1:node_model train_loss:0.3803887162790742,train_acc:0.8753734230995178
node3 epoch2:node_model train_loss:0.30157703057278035,train_acc:0.8971745371818542
node3 epoch3:node_model train_loss:0.23466791559097377,train_acc:0.919183611869812
node3 epoch4:node_model train_loss:0.1970626200700915,train_acc:0.9359868764877319
node3_model on test-dataset: loss:0.8138222731649876,acc:0.7598999738693237
node3 weight score:5218.5841307626615
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4989582183105605,train_acc:0.8328572511672974
node4 epoch1:node_model train_loss:0.33840708062052727,train_acc:0.8796427845954895
node4 epoch2:node_model train_loss:0.23189178268824304,train_acc:0.9282142519950867
node4 epoch3:node_model train_loss:0.17766334878147713,train_acc:0.9421429634094238
node4 epoch4:node_model train_loss:0.11678414099982806,train_acc:0.9735713005065918
node4_model on test-dataset: loss:0.8059729227423668,acc:0.7639999389648438
node4 weight score:3356.192154441231
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4676603727042675,train_acc:0.8398333787918091
node10 epoch1:node_model train_loss:0.2578521423041821,train_acc:0.9136666655540466
node10 epoch2:node_model train_loss:0.17610369250178337,train_acc:0.9511666297912598
node10 epoch3:node_model train_loss:0.09707221444696187,train_acc:0.9763332605361938
node10 epoch4:node_model train_loss:0.07403361927717925,train_acc:0.9874998927116394
node10_model on test-dataset: loss:0.8203658440709114,acc:0.7599998116493225
node10 weight score:2407.4624928305566
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6498873356510612,train_acc:0.8007461428642273
node11 epoch1:node_model train_loss:0.3762246151180828,train_acc:0.8802725672721863
node11 epoch2:node_model train_loss:0.26574394720442157,train_acc:0.9179769158363342
node11 epoch3:node_model train_loss:0.18697969133363052,train_acc:0.9440602660179138
node11 epoch4:node_model train_loss:0.13130820674054763,train_acc:0.9654949307441711
node11_model on test-dataset: loss:0.8380629494786263,acc:0.7646998167037964
node11 weight score:2007.0091405978533
node13: train data size:1155
node13 epoch0:node_model train_loss:0.778024286031723,train_acc:0.7716666460037231
node13 epoch1:node_model train_loss:0.43109465142091113,train_acc:0.852045476436615
node13 epoch2:node_model train_loss:0.26636432607968646,train_acc:0.9314393401145935
node13 epoch3:node_model train_loss:0.18576541356742382,train_acc:0.9447726607322693
node13 epoch4:node_model train_loss:0.11458646133542061,train_acc:0.9731060266494751
node13_model on test-dataset: loss:0.775856363773346,acc:0.7750999927520752
node13 weight score:1488.6776134472937
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6153263661265373,acc:0.8115999758243561
total cost energy:6.9046772762414355 | all_enery_cp：5.882 | all_enery_tp: 1.022677276241436
ef: 25.190237520925862
reward: 18.285560244684426
step 205:loss:29.949064254760742|running q:35.169551849365234
episode3,iteration25 selected nodes:[7, 10, 16, 4, 5],center node:7
################################################## episode3,iteration25 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.40721255221537184,train_acc:0.8596428632736206
node4 epoch1:node_model train_loss:0.29577274567314554,train_acc:0.9028571248054504
node4 epoch2:node_model train_loss:0.24525391895856177,train_acc:0.9242855906486511
node4 epoch3:node_model train_loss:0.1543843030397381,train_acc:0.9535713791847229
node4 epoch4:node_model train_loss:0.1382028146513871,train_acc:0.9564283490180969
node4_model on test-dataset: loss:0.819956224411726,acc:0.7704999446868896
node4 weight score:3298.956602153597
node5: train data size:3735
node5 epoch0:node_model train_loss:0.49258561040225785,train_acc:0.840488612651825
node5 epoch1:node_model train_loss:0.3059789781507693,train_acc:0.8951879143714905
node5 epoch2:node_model train_loss:0.2193380180550249,train_acc:0.9271804690361023
node5 epoch3:node_model train_loss:0.14326248749306328,train_acc:0.9581577777862549
node5 epoch4:node_model train_loss:0.11913291061002958,train_acc:0.9668796062469482
node5_model on test-dataset: loss:0.8251237985491753,acc:0.7620999217033386
node5 weight score:4526.593471873303
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6961107090115547,train_acc:0.7902941107749939
node7 epoch1:node_model train_loss:0.38912531808018685,train_acc:0.8696373105049133
node7 epoch2:node_model train_loss:0.26778826043009757,train_acc:0.9195588231086731
node7 epoch3:node_model train_loss:0.19415689781308174,train_acc:0.9410588145256042
node7 epoch4:node_model train_loss:0.1386963203549385,train_acc:0.9649999737739563
node7_model on test-dataset: loss:0.8347797662764788,acc:0.7610000967979431
node7 weight score:2337.143374596156
node10: train data size:1975
node10 epoch0:node_model train_loss:0.40374128222465516,train_acc:0.864666759967804
node10 epoch1:node_model train_loss:0.2185963548719883,train_acc:0.9331666827201843
node10 epoch2:node_model train_loss:0.1338041365146637,train_acc:0.963666558265686
node10 epoch3:node_model train_loss:0.10307207833975554,train_acc:0.9713332056999207
node10 epoch4:node_model train_loss:0.08221274185925723,train_acc:0.982166588306427
node10_model on test-dataset: loss:0.8571070237457752,acc:0.7637999057769775
node10 weight score:2304.262997832813
node16: train data size:877
node16 epoch0:node_model train_loss:0.64470284514957,train_acc:0.8040260076522827
node16 epoch1:node_model train_loss:0.2902835359176,train_acc:0.8973448276519775
node16 epoch2:node_model train_loss:0.1982253616054853,train_acc:0.945454478263855
node16 epoch3:node_model train_loss:0.12373708271318012,train_acc:0.9648917317390442
node16 epoch4:node_model train_loss:0.10275722129477395,train_acc:0.9763346910476685
node16_model on test-dataset: loss:0.859948088824749,acc:0.7590000629425049
node16 weight score:1019.8290006069495
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6303386509418487,acc:0.8037999832630157
total cost energy:7.163857483392954 | all_enery_cp：5.6215 | all_enery_tp: 1.5423574833929545
ef: 24.913488038304706
reward: 17.749630554911754
step 206:loss:20.54420280456543|running q:36.42364501953125
episode3,iteration26 selected nodes:[14, 11, 5, 1, 7],center node:11
################################################## episode3,iteration26 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4297531130997574,train_acc:0.8583088517189026
node1 epoch1:node_model train_loss:0.3147495669477126,train_acc:0.8902204632759094
node1 epoch2:node_model train_loss:0.22480066284975586,train_acc:0.9250736236572266
node1 epoch3:node_model train_loss:0.21403997501029687,train_acc:0.9333822131156921
node1 epoch4:node_model train_loss:0.1639003983205732,train_acc:0.9488969445228577
node1_model on test-dataset: loss:0.7363528414070606,acc:0.7828997373580933
node1 weight score:9109.76317709593
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3359615253774743,train_acc:0.8851127624511719
node5 epoch1:node_model train_loss:0.19403816171382604,train_acc:0.935563862323761
node5 epoch2:node_model train_loss:0.13537218755013064,train_acc:0.958797037601471
node5 epoch3:node_model train_loss:0.12592738111944576,train_acc:0.9631578326225281
node5 epoch4:node_model train_loss:0.09744558130439959,train_acc:0.9771052002906799
node5_model on test-dataset: loss:0.7872738093137741,acc:0.7728999257087708
node5 weight score:4744.219807408056
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5366949632763862,train_acc:0.8246960639953613
node7 epoch1:node_model train_loss:0.30221129581332207,train_acc:0.8995783925056458
node7 epoch2:node_model train_loss:0.20204992219805717,train_acc:0.9345588088035583
node7 epoch3:node_model train_loss:0.14993175864219666,train_acc:0.9605782628059387
node7 epoch4:node_model train_loss:0.11120726894587278,train_acc:0.9670194983482361
node7_model on test-dataset: loss:0.7920787265896797,acc:0.7693002223968506
node7 weight score:2463.138996801609
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5937754985164193,train_acc:0.8187806010246277
node11 epoch1:node_model train_loss:0.34398913734099446,train_acc:0.8892969489097595
node11 epoch2:node_model train_loss:0.1950521131648737,train_acc:0.945566713809967
node11 epoch3:node_model train_loss:0.15222483259790084,train_acc:0.9573313593864441
node11 epoch4:node_model train_loss:0.10876911440316368,train_acc:0.9690242409706116
node11_model on test-dataset: loss:0.8126397620886564,acc:0.7686999440193176
node11 weight score:2069.797810135334
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6283562878767649,train_acc:0.8196296691894531
node14 epoch1:node_model train_loss:0.30170394231875736,train_acc:0.9085184931755066
node14 epoch2:node_model train_loss:0.23613260934750238,train_acc:0.9247221946716309
node14 epoch3:node_model train_loss:0.12154233331481616,train_acc:0.9708332419395447
node14 epoch4:node_model train_loss:0.08683497117211421,train_acc:0.9871758222579956
node14_model on test-dataset: loss:0.7460167580842971,acc:0.7764001488685608
node14 weight score:1571.0102853581852
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6212051322311163,acc:0.8101999831199646
total cost energy:8.97121359549996 | all_enery_cp：7.6240000000000006 | all_enery_tp: 1.347213595499958
ef: 25.35216009426599
reward: 16.38094649876603
step 207:loss:22.136810302734375|running q:37.59824752807617
episode3,iteration27 selected nodes:[7, 8, 16, 0, 15],center node:7
################################################## episode3,iteration27 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5194481430718532,train_acc:0.8166428208351135
node0 epoch1:node_model train_loss:0.31401616449539477,train_acc:0.8950648903846741
node0 epoch2:node_model train_loss:0.2522634519980504,train_acc:0.915187656879425
node0 epoch3:node_model train_loss:0.1909376630702844,train_acc:0.9416868090629578
node0 epoch4:node_model train_loss:0.15877683575336748,train_acc:0.9521106481552124
node0_model on test-dataset: loss:0.7187857313454151,acc:0.7846999764442444
node0 weight score:7210.77196440519
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4766690008342266,train_acc:0.8401177525520325
node7 epoch1:node_model train_loss:0.27261221706867217,train_acc:0.9060980081558228
node7 epoch2:node_model train_loss:0.1763960227370262,train_acc:0.9455391764640808
node7 epoch3:node_model train_loss:0.13583178780972957,train_acc:0.957558810710907
node7 epoch4:node_model train_loss:0.09306705836206675,train_acc:0.9795195460319519
node7_model on test-dataset: loss:0.7934022599458694,acc:0.7775999903678894
node7 weight score:2459.030051329963
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6457142300075955,train_acc:0.7969841361045837
node8 epoch1:node_model train_loss:0.36781347460216945,train_acc:0.8837642669677734
node8 epoch2:node_model train_loss:0.2280118308133549,train_acc:0.9281971454620361
node8 epoch3:node_model train_loss:0.15327500013841522,train_acc:0.9593877792358398
node8 epoch4:node_model train_loss:0.09468740038573742,train_acc:0.9816325306892395
node8_model on test-dataset: loss:0.7584942965209485,acc:0.771399974822998
node8 weight score:2370.4858536801694
node15: train data size:629
node15 epoch0:node_model train_loss:0.834183428968702,train_acc:0.7529556155204773
node15 epoch1:node_model train_loss:0.3794130895818983,train_acc:0.8775862455368042
node15 epoch2:node_model train_loss:0.3210098147392273,train_acc:0.8967980146408081
node15 epoch3:node_model train_loss:0.16882731871945517,train_acc:0.9393595457077026
node15 epoch4:node_model train_loss:0.14639009428875788,train_acc:0.960295557975769
node15_model on test-dataset: loss:0.8839976358413696,acc:0.7539000511169434
node15 weight score:711.5403644731828
node16: train data size:877
node16 epoch0:node_model train_loss:0.6615034838517507,train_acc:0.8085859417915344
node16 epoch1:node_model train_loss:0.3748495148287879,train_acc:0.8810100555419922
node16 epoch2:node_model train_loss:0.1951308680905236,train_acc:0.9356709718704224
node16 epoch3:node_model train_loss:0.14426996144983503,train_acc:0.9586724042892456
node16 epoch4:node_model train_loss:0.09643594258361393,train_acc:0.9755555391311646
node16_model on test-dataset: loss:0.7938254813104868,acc:0.7681999206542969
node16 weight score:1104.7768314922375
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6060179579257965,acc:0.8140999817848206
total cost energy:7.216427970602522 | all_enery_cp：5.219 | all_enery_tp: 1.9974279706025215
ef: 24.751104615312496
reward: 17.534676644709975
step 208:loss:18.086170196533203|running q:38.80723571777344
episode3,iteration28 selected nodes:[4, 12, 15, 14, 9],center node:14
################################################## episode3,iteration28 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5143561065196991,train_acc:0.8289286494255066
node4 epoch1:node_model train_loss:0.3137840804244791,train_acc:0.8917858004570007
node4 epoch2:node_model train_loss:0.19121363461765992,train_acc:0.9350000023841858
node4 epoch3:node_model train_loss:0.13094850908964872,train_acc:0.9660712480545044
node4 epoch4:node_model train_loss:0.11018026889567929,train_acc:0.971428394317627
node4_model on test-dataset: loss:0.7723959602415562,acc:0.7813999056816101
node4 weight score:3502.089781974065
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7103145279382405,train_acc:0.7891782522201538
node9 epoch1:node_model train_loss:0.37299461427487823,train_acc:0.8755033612251282
node9 epoch2:node_model train_loss:0.28699241108015966,train_acc:0.902751624584198
node9 epoch3:node_model train_loss:0.18657968467787692,train_acc:0.9444690346717834
node9 epoch4:node_model train_loss:0.12513726969298564,train_acc:0.9724929332733154
node9_model on test-dataset: loss:0.7668738385289907,acc:0.7750000357627869
node9 weight score:2421.519559934497
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6788744138819831,train_acc:0.7972221970558167
node12 epoch1:node_model train_loss:0.41466920503548216,train_acc:0.8679364919662476
node12 epoch2:node_model train_loss:0.24254010724169867,train_acc:0.9196031093597412
node12 epoch3:node_model train_loss:0.16779861279896327,train_acc:0.9467460513114929
node12 epoch4:node_model train_loss:0.11751938265349184,train_acc:0.9680158495903015
node12_model on test-dataset: loss:0.8077939765155315,acc:0.7631999254226685
node12 weight score:1653.8870539279303
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5760556856791178,train_acc:0.8196296095848083
node14 epoch1:node_model train_loss:0.3401414143542449,train_acc:0.8884258270263672
node14 epoch2:node_model train_loss:0.1977456814299027,train_acc:0.9363889098167419
node14 epoch3:node_model train_loss:0.1408094068368276,train_acc:0.9590276479721069
node14 epoch4:node_model train_loss:0.08546513815720876,train_acc:0.9816666841506958
node14_model on test-dataset: loss:0.7814054858684539,acc:0.7751001715660095
node14 weight score:1499.8614946981586
node15: train data size:629
node15 epoch0:node_model train_loss:0.7138755534376416,train_acc:0.7740886807441711
node15 epoch1:node_model train_loss:0.37970674463680815,train_acc:0.8602955937385559
node15 epoch2:node_model train_loss:0.2948418025459562,train_acc:0.9009358882904053
node15 epoch3:node_model train_loss:0.25385357226644245,train_acc:0.9288670420646667
node15 epoch4:node_model train_loss:0.13978469318577222,train_acc:0.9585714936256409
node15_model on test-dataset: loss:1.0026153554022312,acc:0.7368001341819763
node15 weight score:627.3592326417707
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6089318405836821,acc:0.8167999821901322
total cost energy:5.743977360306145 | all_enery_cp：3.8495 | all_enery_tp: 1.8944773603061453
ef: 24.708487802794394
reward: 18.964510442488248
step 209:loss:21.251089096069336|running q:39.89165496826172
episode3,iteration29 selected nodes:[5, 0, 14, 13, 15],center node:14
################################################## episode3,iteration29 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.33389396879535455,train_acc:0.8744879364967346
node0 epoch1:node_model train_loss:0.22009849834900636,train_acc:0.925532877445221
node0 epoch2:node_model train_loss:0.1743213044336209,train_acc:0.9417261481285095
node0 epoch3:node_model train_loss:0.12882637691039306,train_acc:0.9584962725639343
node0 epoch4:node_model train_loss:0.09999569075611922,train_acc:0.973422110080719
node0_model on test-dataset: loss:0.8652588300406933,acc:0.7689000368118286
node0 weight score:5990.115119375601
node5: train data size:3735
node5 epoch0:node_model train_loss:0.37673288270046834,train_acc:0.8694360852241516
node5 epoch1:node_model train_loss:0.20220971891754552,train_acc:0.9295865297317505
node5 epoch2:node_model train_loss:0.1388826972167743,train_acc:0.9600374102592468
node5 epoch3:node_model train_loss:0.10492418029982793,train_acc:0.9726690053939819
node5 epoch4:node_model train_loss:0.07400106795524296,train_acc:0.9868419170379639
node5_model on test-dataset: loss:0.7789908175170421,acc:0.7783001065254211
node5 weight score:4794.664989639995
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7218640844027201,train_acc:0.7856818437576294
node13 epoch1:node_model train_loss:0.5021305854121844,train_acc:0.8300000429153442
node13 epoch2:node_model train_loss:0.2569988742470741,train_acc:0.9184091091156006
node13 epoch3:node_model train_loss:0.17861887191732725,train_acc:0.9536363482475281
node13 epoch4:node_model train_loss:0.13594878216584524,train_acc:0.9587878584861755
node13_model on test-dataset: loss:0.7816355849802494,acc:0.7688000202178955
node13 weight score:1477.670697437841
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4691362219552199,train_acc:0.8489815592765808
node14 epoch1:node_model train_loss:0.2698027491569519,train_acc:0.9067592620849609
node14 epoch2:node_model train_loss:0.17962546770771345,train_acc:0.9424999952316284
node14 epoch3:node_model train_loss:0.09890734559545915,train_acc:0.9737036228179932
node14 epoch4:node_model train_loss:0.06735197144250075,train_acc:0.9883332252502441
node14_model on test-dataset: loss:0.8306402507424354,acc:0.7690999507904053
node14 weight score:1410.9597975205916
node15: train data size:629
node15 epoch0:node_model train_loss:0.642119973897934,train_acc:0.7970936298370361
node15 epoch1:node_model train_loss:0.3828183284827641,train_acc:0.8675863146781921
node15 epoch2:node_model train_loss:0.20420807280710765,train_acc:0.9372907280921936
node15 epoch3:node_model train_loss:0.14816454265798842,train_acc:0.9593596458435059
node15 epoch4:node_model train_loss:0.11448529628770691,train_acc:0.9687192440032959
node15_model on test-dataset: loss:0.8493007817864417,acc:0.7637997269630432
node15 weight score:740.6092323110133
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6319706714898348,acc:0.8106999778747559
total cost energy:8.14600819396308 | all_enery_cp：5.936999999999999 | all_enery_tp: 2.209008193963081
ef: 24.73261659506554
reward: 16.58660840110246
step 210:loss:21.334314346313477|running q:41.060794830322266
episode3,iteration30 selected nodes:[17, 16, 6, 4, 18],center node:16
################################################## episode3,iteration30 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.37205065094998907,train_acc:0.8671428561210632
node4 epoch1:node_model train_loss:0.20995978598615953,train_acc:0.9299999475479126
node4 epoch2:node_model train_loss:0.13400242810270616,train_acc:0.9596426486968994
node4 epoch3:node_model train_loss:0.09859311780227083,train_acc:0.9757142066955566
node4 epoch4:node_model train_loss:0.10280116953487907,train_acc:0.9703569412231445
node4_model on test-dataset: loss:0.8241737905144692,acc:0.7734999656677246
node4 weight score:3282.074765216052
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6608781151233181,train_acc:0.8037787079811096
node6 epoch1:node_model train_loss:0.4223946227181342,train_acc:0.85691237449646
node6 epoch2:node_model train_loss:0.33959030864700196,train_acc:0.8711981773376465
node6 epoch3:node_model train_loss:0.2625203459493576,train_acc:0.9140090942382812
node6 epoch4:node_model train_loss:0.2827830208886054,train_acc:0.904009222984314
node6_model on test-dataset: loss:0.8349301791191102,acc:0.7633000016212463
node6 weight score:3601.4987542701156
node16: train data size:877
node16 epoch0:node_model train_loss:0.5728235675228966,train_acc:0.8234631419181824
node16 epoch1:node_model train_loss:0.3475029501650069,train_acc:0.885454535484314
node16 epoch2:node_model train_loss:0.17415263089868757,train_acc:0.9421212077140808
node16 epoch3:node_model train_loss:0.1290177669790056,train_acc:0.9675613045692444
node16 epoch4:node_model train_loss:0.08071148809459475,train_acc:0.9866665601730347
node16_model on test-dataset: loss:0.8559268237650395,acc:0.7627000212669373
node16 weight score:1024.620301233538
node17: train data size:442
node17 epoch0:node_model train_loss:0.803789210319519,train_acc:0.7691428661346436
node17 epoch1:node_model train_loss:0.42904308438301086,train_acc:0.8637142181396484
node17 epoch2:node_model train_loss:0.25680305659770963,train_acc:0.9197142720222473
node17 epoch3:node_model train_loss:0.13658030033111573,train_acc:0.9632380604743958
node17 epoch4:node_model train_loss:0.08778908923268318,train_acc:0.977238118648529
node17_model on test-dataset: loss:0.8999031630158424,acc:0.7565001249313354
node17 weight score:491.163958707209
node18: train data size:472
node18 epoch0:node_model train_loss:0.7000301241874695,train_acc:0.7995554804801941
node18 epoch1:node_model train_loss:0.32930226922035216,train_acc:0.9097777605056763
node18 epoch2:node_model train_loss:0.19667682349681853,train_acc:0.9376665949821472
node18 epoch3:node_model train_loss:0.19023275673389434,train_acc:0.9468889236450195
node18 epoch4:node_model train_loss:0.08312993049621582,train_acc:0.9732222557067871
node18_model on test-dataset: loss:0.8932723355293274,acc:0.7563000917434692
node18 weight score:528.3942883110853
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6251478877663612,acc:0.810699982047081
total cost energy:5.576952672877486 | all_enery_cp：3.7515000000000005 | all_enery_tp: 1.8254526728774847
ef: 24.55059761019651
reward: 18.973644937319026
step 211:loss:37.23847198486328|running q:42.14167785644531
episode3,iteration31 selected nodes:[15, 18, 8, 17, 7],center node:15
################################################## episode3,iteration31 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5252329602837562,train_acc:0.8266177177429199
node7 epoch1:node_model train_loss:0.2719921715557575,train_acc:0.9091176390647888
node7 epoch2:node_model train_loss:0.16455560363829136,train_acc:0.9565391540527344
node7 epoch3:node_model train_loss:0.11774948555976153,train_acc:0.9660588502883911
node7 epoch4:node_model train_loss:0.0960547724738717,train_acc:0.9790195822715759
node7_model on test-dataset: loss:0.7773918251693249,acc:0.7807999849319458
node7 weight score:2509.6739338300213
node8: train data size:1798
node8 epoch0:node_model train_loss:0.606831356883049,train_acc:0.8081632852554321
node8 epoch1:node_model train_loss:0.3364700848857562,train_acc:0.8826530575752258
node8 epoch2:node_model train_loss:0.22792263080676398,train_acc:0.9215646386146545
node8 epoch3:node_model train_loss:0.15144080337550905,train_acc:0.9555214643478394
node8 epoch4:node_model train_loss:0.10134852222270435,train_acc:0.973877489566803
node8_model on test-dataset: loss:0.7616506962478161,acc:0.7766000032424927
node8 weight score:2360.6621891867735
node15: train data size:629
node15 epoch0:node_model train_loss:0.6452121990067619,train_acc:0.7996551394462585
node15 epoch1:node_model train_loss:0.41290956309863497,train_acc:0.8588670492172241
node15 epoch2:node_model train_loss:0.2704472967556545,train_acc:0.9045813679695129
node15 epoch3:node_model train_loss:0.1430799045733043,train_acc:0.9579310417175293
node15 epoch4:node_model train_loss:0.11839303800037929,train_acc:0.9693595767021179
node15_model on test-dataset: loss:0.8595946779847146,acc:0.758400022983551
node15 weight score:731.7402214200132
node17: train data size:442
node17 epoch0:node_model train_loss:0.7193692982196808,train_acc:0.7688571214675903
node17 epoch1:node_model train_loss:0.3729210078716278,train_acc:0.8814285397529602
node17 epoch2:node_model train_loss:0.2550360858440399,train_acc:0.923714280128479
node17 epoch3:node_model train_loss:0.1622951880097389,train_acc:0.9537141919136047
node17 epoch4:node_model train_loss:0.10086284428834916,train_acc:0.9692381024360657
node17_model on test-dataset: loss:0.857836335003376,acc:0.756600022315979
node17 weight score:515.2497999496146
node18: train data size:472
node18 epoch0:node_model train_loss:0.6297280788421631,train_acc:0.8138888478279114
node18 epoch1:node_model train_loss:0.3692790389060974,train_acc:0.8790000081062317
node18 epoch2:node_model train_loss:0.23613736033439636,train_acc:0.9273333549499512
node18 epoch3:node_model train_loss:0.10327714681625366,train_acc:0.9764444231987
node18 epoch4:node_model train_loss:0.09242312386631965,train_acc:0.9716667532920837
node18_model on test-dataset: loss:0.9396296831965446,acc:0.7422999143600464
node18 weight score:502.3255527585016
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6151492279022932,acc:0.8145999825000763
total cost energy:4.929710536948946 | all_enery_cp：2.646 | all_enery_tp: 2.2837105369489454
ef: 24.33524187913769
reward: 19.405531342188745
step 212:loss:11.758767127990723|running q:43.49937057495117
episode3,iteration32 selected nodes:[14, 11, 7, 4, 0],center node:7
################################################## episode3,iteration32 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.30979520383362585,train_acc:0.8916078805923462
node0 epoch1:node_model train_loss:0.16213557396370631,train_acc:0.9454193711280823
node0 epoch2:node_model train_loss:0.14078713681262273,train_acc:0.9552664160728455
node0 epoch3:node_model train_loss:0.10632198998847833,train_acc:0.9699212312698364
node0 epoch4:node_model train_loss:0.09422817811943017,train_acc:0.9750346541404724
node0_model on test-dataset: loss:0.8219772471487522,acc:0.7755998969078064
node0 weight score:6305.527334215905
node4: train data size:2705
node4 epoch0:node_model train_loss:0.32133056316524744,train_acc:0.8882143497467041
node4 epoch1:node_model train_loss:0.1578520161232778,train_acc:0.9514284729957581
node4 epoch2:node_model train_loss:0.10892410869044918,train_acc:0.9682140946388245
node4 epoch3:node_model train_loss:0.09964287813220706,train_acc:0.9724998474121094
node4 epoch4:node_model train_loss:0.06519454531371593,train_acc:0.9871427416801453
node4_model on test-dataset: loss:0.7901865920424461,acc:0.7793999910354614
node4 weight score:3423.242088945362
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3039492089301348,train_acc:0.8910391926765442
node7 epoch1:node_model train_loss:0.17658451721072196,train_acc:0.9410587549209595
node7 epoch2:node_model train_loss:0.10933954827487469,train_acc:0.9684998393058777
node7 epoch3:node_model train_loss:0.09753223583102226,train_acc:0.9770194888114929
node7 epoch4:node_model train_loss:0.05654691252857447,train_acc:0.9899998903274536
node7_model on test-dataset: loss:0.7707194121927023,acc:0.7801998257637024
node7 weight score:2531.4011417584393
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6149702264982111,train_acc:0.8061692714691162
node11 epoch1:node_model train_loss:0.3528208215447033,train_acc:0.8800144195556641
node11 epoch2:node_model train_loss:0.23470409828073838,train_acc:0.9312481880187988
node11 epoch3:node_model train_loss:0.1221697830102023,train_acc:0.9666714668273926
node11 epoch4:node_model train_loss:0.0907817938748528,train_acc:0.9750357270240784
node11_model on test-dataset: loss:0.7790862306207419,acc:0.7772999405860901
node11 weight score:2158.9394522604457
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5098184744517008,train_acc:0.8366204500198364
node14 epoch1:node_model train_loss:0.24003689487775168,train_acc:0.9178704023361206
node14 epoch2:node_model train_loss:0.16733430325984955,train_acc:0.9513425827026367
node14 epoch3:node_model train_loss:0.11888969006637733,train_acc:0.9710184931755066
node14 epoch4:node_model train_loss:0.08691107885291179,train_acc:0.9806944131851196
node14_model on test-dataset: loss:0.7647933553159237,acc:0.7824000120162964
node14 weight score:1532.4400922859297
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.618294622823596,acc:0.818399983048439
total cost energy:7.709941361516796 | all_enery_cp：6.3465 | all_enery_tp: 1.363441361516796
ef: 25.205318253677543
reward: 17.49537689216075
step 213:loss:13.124407768249512|running q:44.495872497558594
episode3,iteration33 selected nodes:[19, 18, 1, 4, 15],center node:15
################################################## episode3,iteration33 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.39250076934695244,train_acc:0.8666911721229553
node1 epoch1:node_model train_loss:0.2602757044574794,train_acc:0.9141175150871277
node1 epoch2:node_model train_loss:0.18066664500271573,train_acc:0.9401469826698303
node1 epoch3:node_model train_loss:0.15780964438967846,train_acc:0.9512499570846558
node1 epoch4:node_model train_loss:0.1509157272384447,train_acc:0.9522058963775635
node1_model on test-dataset: loss:0.7851247870922089,acc:0.7836999893188477
node1 weight score:8543.864759185319
node4: train data size:2705
node4 epoch0:node_model train_loss:0.2387917728296348,train_acc:0.9214285016059875
node4 epoch1:node_model train_loss:0.1870608561273132,train_acc:0.9339286088943481
node4 epoch2:node_model train_loss:0.117205127541508,train_acc:0.9635714292526245
node4 epoch3:node_model train_loss:0.08575489917503935,train_acc:0.9782142639160156
node4 epoch4:node_model train_loss:0.10480642797691482,train_acc:0.9699998497962952
node4_model on test-dataset: loss:0.9643395703285933,acc:0.7476997971534729
node4 weight score:2805.028522347461
node15: train data size:629
node15 epoch0:node_model train_loss:0.5756933603967939,train_acc:0.8182266354560852
node15 epoch1:node_model train_loss:0.2431229225226811,train_acc:0.9250738620758057
node15 epoch2:node_model train_loss:0.20899644174746104,train_acc:0.9301478266716003
node15 epoch3:node_model train_loss:0.1517705778990473,train_acc:0.9480788111686707
node15 epoch4:node_model train_loss:0.07712555836353983,train_acc:0.9871428608894348
node15_model on test-dataset: loss:0.9145233196020126,acc:0.7510998249053955
node15 weight score:687.7900065727484
node18: train data size:472
node18 epoch0:node_model train_loss:0.5875317931175232,train_acc:0.8338889479637146
node18 epoch1:node_model train_loss:0.2871167451143265,train_acc:0.8941110968589783
node18 epoch2:node_model train_loss:0.13279412239789962,train_acc:0.9672222137451172
node18 epoch3:node_model train_loss:0.10713879317045212,train_acc:0.964888870716095
node18 epoch4:node_model train_loss:0.07312326990067959,train_acc:0.9788889288902283
node18_model on test-dataset: loss:0.9045660665631294,acc:0.751700222492218
node18 weight score:521.7971549533681
node19: train data size:4281
node19 epoch0:node_model train_loss:0.49285565212715504,train_acc:0.841314971446991
node19 epoch1:node_model train_loss:0.301297378054885,train_acc:0.8956100940704346
node19 epoch2:node_model train_loss:0.21053773944461068,train_acc:0.9262934923171997
node19 epoch3:node_model train_loss:0.15832195517628692,train_acc:0.9482772350311279
node19 epoch4:node_model train_loss:0.1044407450529032,train_acc:0.9736806154251099
node19_model on test-dataset: loss:0.7467617237567902,acc:0.7913998961448669
node19 weight score:5732.752314169575
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.635209536254406,acc:0.81579998254776
total cost energy:9.573774822415794 | all_enery_cp：7.3975 | all_enery_tp: 2.1762748224157944
ef: 24.93518946653224
reward: 15.361414644116445
step 214:loss:22.980907440185547|running q:45.676212310791016
episode3,iteration34 selected nodes:[17, 0, 11, 13, 16],center node:16
################################################## episode3,iteration34 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.25644099784012026,train_acc:0.9074164032936096
node0 epoch1:node_model train_loss:0.13913335378926533,train_acc:0.9555376768112183
node0 epoch2:node_model train_loss:0.10188492287236911,train_acc:0.970882773399353
node0 epoch3:node_model train_loss:0.07750161601087222,train_acc:0.9828058481216431
node0 epoch4:node_model train_loss:0.07037394071141115,train_acc:0.9824954867362976
node0_model on test-dataset: loss:0.8626270250231027,acc:0.773699939250946
node0 weight score:6008.390474273849
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6385648671318503,train_acc:0.8187804818153381
node11 epoch1:node_model train_loss:0.34167344955837026,train_acc:0.8853083848953247
node11 epoch2:node_model train_loss:0.20340627300388672,train_acc:0.9356959462165833
node11 epoch3:node_model train_loss:0.12295291266020607,train_acc:0.9656239748001099
node11 epoch4:node_model train_loss:0.0963979444521315,train_acc:0.9756239652633667
node11_model on test-dataset: loss:0.7908525705337525,acc:0.7800999879837036
node11 weight score:2126.81865453735
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7701033999522527,train_acc:0.7762879133224487
node13 epoch1:node_model train_loss:0.4255168065428734,train_acc:0.876893937587738
node13 epoch2:node_model train_loss:0.24514151364564896,train_acc:0.9268181324005127
node13 epoch3:node_model train_loss:0.1562336434920629,train_acc:0.9611363410949707
node13 epoch4:node_model train_loss:0.11387149554987748,train_acc:0.96916663646698
node13_model on test-dataset: loss:0.8011790827661752,acc:0.7729000449180603
node13 weight score:1441.6252556322513
node16: train data size:877
node16 epoch0:node_model train_loss:0.5525084071689181,train_acc:0.8403607606887817
node16 epoch1:node_model train_loss:0.26121247477001613,train_acc:0.9235641956329346
node16 epoch2:node_model train_loss:0.1891177776787016,train_acc:0.9453390836715698
node16 epoch3:node_model train_loss:0.13026183388299412,train_acc:0.9620057344436646
node16 epoch4:node_model train_loss:0.07689600479271677,train_acc:0.980779230594635
node16_model on test-dataset: loss:0.8318320833146572,acc:0.7735997438430786
node16 weight score:1054.299320249057
node17: train data size:442
node17 epoch0:node_model train_loss:0.7809245407581329,train_acc:0.779619038105011
node17 epoch1:node_model train_loss:0.3732350319623947,train_acc:0.8969523310661316
node17 epoch2:node_model train_loss:0.2226048916578293,train_acc:0.9061905145645142
node17 epoch3:node_model train_loss:0.1195858396589756,train_acc:0.9620000123977661
node17 epoch4:node_model train_loss:0.08930993601679801,train_acc:0.9799999594688416
node17_model on test-dataset: loss:0.9233511817455292,acc:0.7472999691963196
node17 weight score:478.6911077153014
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6600864384323358,acc:0.8113999789953232
total cost energy:6.3931738882210585 | all_enery_cp：4.6695 | all_enery_tp: 1.7236738882210587
ef: 24.822533299680806
reward: 18.42935941145975
step 215:loss:23.647912979125977|running q:46.67888641357422
episode3,iteration35 selected nodes:[2, 0, 16, 15, 8],center node:2
################################################## episode3,iteration35 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.12651123536320832,train_acc:0.959497332572937
node0 epoch1:node_model train_loss:0.10418014801465549,train_acc:0.9676134586334229
node0 epoch2:node_model train_loss:0.08868587984202,train_acc:0.9737279415130615
node0 epoch3:node_model train_loss:0.07650020080976762,train_acc:0.9783828258514404
node0 epoch4:node_model train_loss:0.06153099457375132,train_acc:0.9872289896011353
node0_model on test-dataset: loss:0.8385727328062057,acc:0.7797999382019043
node0 weight score:6180.739961166603
node2: train data size:4788
node2 epoch0:node_model train_loss:0.43015055855115253,train_acc:0.8527840971946716
node2 epoch1:node_model train_loss:0.24376568291336298,train_acc:0.9148579835891724
node2 epoch2:node_model train_loss:0.16011343446249762,train_acc:0.9478029608726501
node2 epoch3:node_model train_loss:0.12826732825487852,train_acc:0.9625662565231323
node2 epoch4:node_model train_loss:0.08461272514735659,train_acc:0.9791097044944763
node2_model on test-dataset: loss:0.7712794618308544,acc:0.7871999144554138
node2 weight score:6207.866586560337
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5595368097225825,train_acc:0.818208634853363
node8 epoch1:node_model train_loss:0.2923041151629554,train_acc:0.89984130859375
node8 epoch2:node_model train_loss:0.1946405896710025,train_acc:0.9416212439537048
node8 epoch3:node_model train_loss:0.12227154440350002,train_acc:0.9660656452178955
node8 epoch4:node_model train_loss:0.080098959720797,train_acc:0.9810883402824402
node8_model on test-dataset: loss:0.771617460846901,acc:0.7820999026298523
node8 weight score:2330.1701830678853
node15: train data size:629
node15 epoch0:node_model train_loss:0.6899445865835462,train_acc:0.7864531874656677
node15 epoch1:node_model train_loss:0.37494111699717386,train_acc:0.876798152923584
node15 epoch2:node_model train_loss:0.2017595853124346,train_acc:0.9266502261161804
node15 epoch3:node_model train_loss:0.17086129103388106,train_acc:0.9345813393592834
node15 epoch4:node_model train_loss:0.15348391447748458,train_acc:0.9523645043373108
node15_model on test-dataset: loss:0.8331118096411229,acc:0.7653000950813293
node15 weight score:755.0007006513957
node16: train data size:877
node16 epoch0:node_model train_loss:0.5164586140049828,train_acc:0.8335786461830139
node16 epoch1:node_model train_loss:0.2795831660429637,train_acc:0.8982395529747009
node16 epoch2:node_model train_loss:0.1799277522497707,train_acc:0.9372293949127197
node16 epoch3:node_model train_loss:0.10520135404335128,train_acc:0.9748917818069458
node16 epoch4:node_model train_loss:0.06308405639396773,train_acc:0.9866665601730347
node16_model on test-dataset: loss:0.8724711021780968,acc:0.7663000226020813
node16 weight score:1005.1908857618286
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6424508508294821,acc:0.8179999846220016
total cost energy:8.924750742420521 | all_enery_cp：6.6375 | all_enery_tp: 2.2872507424205204
ef: 24.535484542275707
reward: 15.610733799855186
step 216:loss:28.001005172729492|running q:47.673946380615234
episode3,iteration36 selected nodes:[19, 1, 6, 12, 10],center node:10
################################################## episode3,iteration36 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3171132121454267,train_acc:0.8891913294792175
node1 epoch1:node_model train_loss:0.20361458849819267,train_acc:0.9309557676315308
node1 epoch2:node_model train_loss:0.1752047865268062,train_acc:0.9444851875305176
node1 epoch3:node_model train_loss:0.12075238926884006,train_acc:0.9642645716667175
node1 epoch4:node_model train_loss:0.09991243782946292,train_acc:0.971911609172821
node1_model on test-dataset: loss:0.8267348405718803,acc:0.7811999320983887
node1 weight score:8113.84699277927
node6: train data size:3007
node6 epoch0:node_model train_loss:0.613033639807855,train_acc:0.8215206861495972
node6 epoch1:node_model train_loss:0.38047657666667817,train_acc:0.879585325717926
node6 epoch2:node_model train_loss:0.27260444866072747,train_acc:0.9066819548606873
node6 epoch3:node_model train_loss:0.22957151815775903,train_acc:0.9263595342636108
node6 epoch4:node_model train_loss:0.18380712447387557,train_acc:0.9422579407691956
node6_model on test-dataset: loss:0.783669495433569,acc:0.7763999700546265
node6 weight score:3837.076749218575
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5801151275634766,train_acc:0.8263333439826965
node10 epoch1:node_model train_loss:0.33264420107007026,train_acc:0.8940000534057617
node10 epoch2:node_model train_loss:0.20074146948754787,train_acc:0.9346665740013123
node10 epoch3:node_model train_loss:0.11698727123439312,train_acc:0.9673332571983337
node10 epoch4:node_model train_loss:0.08182956892997026,train_acc:0.9809999465942383
node10_model on test-dataset: loss:0.8015660567581654,acc:0.7773998379707336
node10 weight score:2463.9266886969276
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7147760433810098,train_acc:0.7880951762199402
node12 epoch1:node_model train_loss:0.39960042600120815,train_acc:0.8666666746139526
node12 epoch2:node_model train_loss:0.27227028352873667,train_acc:0.9132540225982666
node12 epoch3:node_model train_loss:0.1620162491287504,train_acc:0.9590476155281067
node12 epoch4:node_model train_loss:0.1060153812702213,train_acc:0.9730159044265747
node12_model on test-dataset: loss:0.7952981913089752,acc:0.7775999903678894
node12 weight score:1679.8730521454952
node19: train data size:4281
node19 epoch0:node_model train_loss:0.4423670470714569,train_acc:0.8495779037475586
node19 epoch1:node_model train_loss:0.24552294507969258,train_acc:0.9203702211380005
node19 epoch2:node_model train_loss:0.15862466256285823,train_acc:0.947332501411438
node19 epoch3:node_model train_loss:0.12625773095114287,train_acc:0.9637897610664368
node19 epoch4:node_model train_loss:0.0996555469410364,train_acc:0.9751161932945251
node19_model on test-dataset: loss:0.8298913156241179,acc:0.7783001065254211
node19 weight score:5158.506806135793
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6323189184814691,acc:0.8178999817371368
total cost energy:10.728649535609133 | all_enery_cp：8.653500000000001 | all_enery_tp: 2.075149535609132
ef: 25.152778406846632
reward: 14.4241288712375
step 217:loss:21.54214859008789|running q:48.746578216552734
episode3,iteration37 selected nodes:[3, 19, 4, 15, 7],center node:7
################################################## episode3,iteration37 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5766024388546167,train_acc:0.8252002000808716
node3 epoch1:node_model train_loss:0.34420392229113467,train_acc:0.8849679827690125
node3 epoch2:node_model train_loss:0.23824228866155758,train_acc:0.9192726016044617
node3 epoch3:node_model train_loss:0.18054487920084664,train_acc:0.9434584379196167
node3 epoch4:node_model train_loss:0.14367035715732462,train_acc:0.9513953328132629
node3_model on test-dataset: loss:0.7769380618631839,acc:0.7812998294830322
node3 weight score:5466.330211465276
node4: train data size:2705
node4 epoch0:node_model train_loss:0.40007366532725946,train_acc:0.8700000047683716
node4 epoch1:node_model train_loss:0.194370019917447,train_acc:0.9357143044471741
node4 epoch2:node_model train_loss:0.11574373713561467,train_acc:0.9635711908340454
node4 epoch3:node_model train_loss:0.16583355529499905,train_acc:0.9553569555282593
node4 epoch4:node_model train_loss:0.18856346953128064,train_acc:0.931071400642395
node4_model on test-dataset: loss:0.9617000246047973,acc:0.7513000965118408
node4 weight score:2812.727389823659
node7: train data size:1951
node7 epoch0:node_model train_loss:0.49159117639064787,train_acc:0.8366959691047668
node7 epoch1:node_model train_loss:0.2293753456324339,train_acc:0.920598030090332
node7 epoch2:node_model train_loss:0.1305186167359352,train_acc:0.9595391154289246
node7 epoch3:node_model train_loss:0.10741268768906594,train_acc:0.9725391268730164
node7 epoch4:node_model train_loss:0.0725015178322792,train_acc:0.9839998483657837
node7_model on test-dataset: loss:0.8111494515836238,acc:0.7824998497962952
node7 weight score:2405.228772812485
node15: train data size:629
node15 epoch0:node_model train_loss:0.6201164765017373,train_acc:0.8098029494285583
node15 epoch1:node_model train_loss:0.3054806664586067,train_acc:0.8965024948120117
node15 epoch2:node_model train_loss:0.17511485729898726,train_acc:0.9472906589508057
node15 epoch3:node_model train_loss:0.10475393384695053,train_acc:0.9722167253494263
node15 epoch4:node_model train_loss:0.06165795347520283,train_acc:0.9900000095367432
node15_model on test-dataset: loss:0.8703870502114296,acc:0.7715998888015747
node15 weight score:722.6670018208645
node19: train data size:4281
node19 epoch0:node_model train_loss:0.33579944004846174,train_acc:0.8859260082244873
node19 epoch1:node_model train_loss:0.20532072387462438,train_acc:0.9321073293685913
node19 epoch2:node_model train_loss:0.12436769755427228,train_acc:0.9646108746528625
node19 epoch3:node_model train_loss:0.09332296572798907,train_acc:0.9745421409606934
node19 epoch4:node_model train_loss:0.07324689219510833,train_acc:0.9853488802909851
node19_model on test-dataset: loss:0.8386334355175495,acc:0.7817999720573425
node19 weight score:5104.733270452122
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6388421232998371,acc:0.8178999811410904
total cost energy:8.786646987510101 | all_enery_cp：6.906499999999999 | all_enery_tp: 1.880146987510102
ef: 24.86501442737841
reward: 16.07836743986831
step 218:loss:50.03401184082031|running q:49.89950942993164
episode3,iteration38 selected nodes:[19, 15, 18, 1, 0],center node:15
################################################## episode3,iteration38 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.1835723605293494,train_acc:0.9344925880432129
node0 epoch1:node_model train_loss:0.11463380676622574,train_acc:0.965729832649231
node0 epoch2:node_model train_loss:0.07034032172165237,train_acc:0.9826529026031494
node0 epoch3:node_model train_loss:0.06180788408248471,train_acc:0.9838069677352905
node0 epoch4:node_model train_loss:0.05002503587792699,train_acc:0.9885753393173218
node0_model on test-dataset: loss:0.8286737079918385,acc:0.784500241279602
node0 weight score:6254.572758872962
node1: train data size:6708
node1 epoch0:node_model train_loss:0.24385055965360472,train_acc:0.910882294178009
node1 epoch1:node_model train_loss:0.14708619488074498,train_acc:0.950735330581665
node1 epoch2:node_model train_loss:0.1159828490961124,train_acc:0.9649264216423035
node1 epoch3:node_model train_loss:0.17461965268696933,train_acc:0.9398529529571533
node1 epoch4:node_model train_loss:0.09317800178028204,train_acc:0.9704414010047913
node1_model on test-dataset: loss:0.8177980546653271,acc:0.7810999155044556
node1 weight score:8202.514009091352
node15: train data size:629
node15 epoch0:node_model train_loss:0.6631703845092228,train_acc:0.8120197057723999
node15 epoch1:node_model train_loss:0.25747107395104,train_acc:0.9088670611381531
node15 epoch2:node_model train_loss:0.17375430038997106,train_acc:0.9587192535400391
node15 epoch3:node_model train_loss:0.1120822908622878,train_acc:0.9757142663002014
node15 epoch4:node_model train_loss:0.12120187069688525,train_acc:0.9566502571105957
node15_model on test-dataset: loss:0.9308561439812183,acc:0.7658997774124146
node15 weight score:675.7220265097066
node18: train data size:472
node18 epoch0:node_model train_loss:0.6395529210567474,train_acc:0.8334444165229797
node18 epoch1:node_model train_loss:0.3576482176780701,train_acc:0.8796667456626892
node18 epoch2:node_model train_loss:0.1664326772093773,train_acc:0.9461111426353455
node18 epoch3:node_model train_loss:0.11879434287548066,train_acc:0.9676666259765625
node18 epoch4:node_model train_loss:0.07900136187672616,train_acc:0.9844444394111633
node18_model on test-dataset: loss:0.9066612243652343,acc:0.7630999088287354
node18 weight score:520.5913601637188
node19: train data size:4281
node19 epoch0:node_model train_loss:0.23816612085630728,train_acc:0.9131066799163818
node19 epoch1:node_model train_loss:0.1344565484066342,train_acc:0.9570457339286804
node19 epoch2:node_model train_loss:0.0932619261533715,train_acc:0.9759919047355652
node19 epoch3:node_model train_loss:0.09134455536340558,train_acc:0.9766349792480469
node19 epoch4:node_model train_loss:0.0718846263060736,train_acc:0.9805338382720947
node19_model on test-dataset: loss:0.8123189632594585,acc:0.7842998504638672
node19 weight score:5270.0973307607355
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.650737923681736,acc:0.8169999819993973
total cost energy:10.590820376686505 | all_enery_cp：8.6365 | all_enery_tp: 1.9543203766865056
ef: 25.030703163637654
reward: 14.439882786951149
step 219:loss:43.234012603759766|running q:50.823570251464844
episode3,iteration39 selected nodes:[7, 5, 8, 1, 13],center node:7
################################################## episode3,iteration39 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.168734747752109,train_acc:0.9408823847770691
node1 epoch1:node_model train_loss:0.11978052375728593,train_acc:0.9611027836799622
node1 epoch2:node_model train_loss:0.13673748273183317,train_acc:0.9529410004615784
node1 epoch3:node_model train_loss:0.09236340772579699,train_acc:0.9730882048606873
node1 epoch4:node_model train_loss:0.09020692005972653,train_acc:0.9751474857330322
node1_model on test-dataset: loss:0.9333405411243438,acc:0.7688000798225403
node1 weight score:7187.087353903264
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4357958224258925,train_acc:0.8537593483924866
node5 epoch1:node_model train_loss:0.2370061452843641,train_acc:0.92169189453125
node5 epoch2:node_model train_loss:0.1615003806195761,train_acc:0.9469171762466431
node5 epoch3:node_model train_loss:0.13935282453894615,train_acc:0.9597743153572083
node5 epoch4:node_model train_loss:0.08097843717979758,train_acc:0.9792479276657104
node5_model on test-dataset: loss:0.7913569068163633,acc:0.7834998965263367
node5 weight score:4719.741456514157
node7: train data size:1951
node7 epoch0:node_model train_loss:0.455972358211875,train_acc:0.8495197296142578
node7 epoch1:node_model train_loss:0.23517941683530807,train_acc:0.9205392003059387
node7 epoch2:node_model train_loss:0.15542427785694599,train_acc:0.9505196809768677
node7 epoch3:node_model train_loss:0.0993628405034542,train_acc:0.9740195274353027
node7 epoch4:node_model train_loss:0.06269592363387347,train_acc:0.9860195517539978
node7_model on test-dataset: loss:0.8005941635370255,acc:0.7862001657485962
node7 weight score:2436.9400738327654
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5606963468922509,train_acc:0.8270635008811951
node8 epoch1:node_model train_loss:0.31336545944213867,train_acc:0.8949320316314697
node8 epoch2:node_model train_loss:0.19557276161180603,train_acc:0.9360317587852478
node8 epoch3:node_model train_loss:0.11587048653099272,train_acc:0.9643990397453308
node8 epoch4:node_model train_loss:0.08452403897212611,train_acc:0.9833219647407532
node8_model on test-dataset: loss:0.7977259567379952,acc:0.7771000266075134
node8 weight score:2253.906852112792
node13: train data size:1155
node13 epoch0:node_model train_loss:0.784365713596344,train_acc:0.7862879037857056
node13 epoch1:node_model train_loss:0.3171830189724763,train_acc:0.9007575511932373
node13 epoch2:node_model train_loss:0.24109849582115808,train_acc:0.925757646560669
node13 epoch3:node_model train_loss:0.12100847872594993,train_acc:0.9703029990196228
node13 epoch4:node_model train_loss:0.12181426708896954,train_acc:0.9676515460014343
node13_model on test-dataset: loss:0.8020851528644561,acc:0.7790997624397278
node13 weight score:1439.996733358288
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6573364460468292,acc:0.8164999800920486
total cost energy:9.417150317030929 | all_enery_cp：7.6735 | all_enery_tp: 1.7436503170309292
ef: 24.987801610085445
reward: 15.570651293054516
step 220:loss:44.47393798828125|running q:51.967403411865234
episode3,iteration40 selected nodes:[5, 2, 0, 8, 19],center node:2
################################################## episode3,iteration40 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.21089690508177647,train_acc:0.9249558448791504
node0 epoch1:node_model train_loss:0.09246198969105116,train_acc:0.9722288846969604
node0 epoch2:node_model train_loss:0.06184855827058737,train_acc:0.9855769872665405
node0 epoch3:node_model train_loss:0.04607170193384473,train_acc:0.9913462400436401
node0 epoch4:node_model train_loss:0.034004037375920095,train_acc:0.9938068985939026
node0_model on test-dataset: loss:0.8110644136369228,acc:0.7923001050949097
node0 weight score:6390.367907721072
node2: train data size:4788
node2 epoch0:node_model train_loss:0.39367784031977254,train_acc:0.8679072260856628
node2 epoch1:node_model train_loss:0.20836327550932765,train_acc:0.9302462935447693
node2 epoch2:node_model train_loss:0.13201583448487023,train_acc:0.9587311744689941
node2 epoch3:node_model train_loss:0.11634041019715369,train_acc:0.9634658694267273
node2 epoch4:node_model train_loss:0.09127035686591019,train_acc:0.9728882312774658
node2_model on test-dataset: loss:0.8085400228202343,acc:0.7849997282028198
node2 weight score:5921.784778568141
node5: train data size:3735
node5 epoch0:node_model train_loss:0.33202124210564715,train_acc:0.8861654996871948
node5 epoch1:node_model train_loss:0.17422100960424072,train_acc:0.9371803998947144
node5 epoch2:node_model train_loss:0.11088640576130465,train_acc:0.9669171571731567
node5 epoch3:node_model train_loss:0.08520652172400763,train_acc:0.9765787720680237
node5 epoch4:node_model train_loss:0.06299116540896266,train_acc:0.9836840629577637
node5_model on test-dataset: loss:0.7964970844984055,acc:0.787899911403656
node5 weight score:4689.282701332319
node8: train data size:1798
node8 epoch0:node_model train_loss:0.49818352195951676,train_acc:0.8358956575393677
node8 epoch1:node_model train_loss:0.2346674245264795,train_acc:0.9238322377204895
node8 epoch2:node_model train_loss:0.1646127767033047,train_acc:0.9460430145263672
node8 epoch3:node_model train_loss:0.11089571503301461,train_acc:0.9672108888626099
node8 epoch4:node_model train_loss:0.07473430886036819,train_acc:0.9799886345863342
node8_model on test-dataset: loss:0.8147564586997033,acc:0.7760999202728271
node8 weight score:2206.794411755247
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3161413513643797,train_acc:0.8884553909301758
node19 epoch1:node_model train_loss:0.15728363290775654,train_acc:0.9462934136390686
node19 epoch2:node_model train_loss:0.10758909912303437,train_acc:0.9671545624732971
node19 epoch3:node_model train_loss:0.07837153130839038,train_acc:0.9805338382720947
node19 epoch4:node_model train_loss:0.06029557683613411,train_acc:0.9861697554588318
node19_model on test-dataset: loss:0.78840905867517,acc:0.7917000651359558
node19 weight score:5429.922389772796
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6405073228478432,acc:0.8181999832391739
total cost energy:11.793152965829771 | all_enery_cp：9.8925 | all_enery_tp: 1.9006529658297708
ef: 25.196772591407118
reward: 13.403619625577347
step 221:loss:31.6176700592041|running q:52.93497848510742
episode3,iteration41 selected nodes:[1, 6, 14, 5, 8],center node:6
################################################## episode3,iteration41 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.17082939258612254,train_acc:0.9411762952804565
node1 epoch1:node_model train_loss:0.11307183406589662,train_acc:0.9617646336555481
node1 epoch2:node_model train_loss:0.08887169729260837,train_acc:0.9738236665725708
node1 epoch3:node_model train_loss:0.06516852155875634,train_acc:0.9805882573127747
node1 epoch4:node_model train_loss:0.07295056043521446,train_acc:0.9801470637321472
node1_model on test-dataset: loss:0.8943427350372076,acc:0.7804996967315674
node1 weight score:7500.4802266559755
node5: train data size:3735
node5 epoch0:node_model train_loss:0.24048758925575958,train_acc:0.9148496389389038
node5 epoch1:node_model train_loss:0.13772953517342867,train_acc:0.9516162872314453
node5 epoch2:node_model train_loss:0.09493461015977357,train_acc:0.9697366952896118
node5 epoch3:node_model train_loss:0.07965593167433613,train_acc:0.979248046875
node5 epoch4:node_model train_loss:0.07674715932654708,train_acc:0.9773682951927185
node5_model on test-dataset: loss:0.9280239197611809,acc:0.7767000198364258
node5 weight score:4024.680744178631
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6125038535364212,train_acc:0.822949230670929
node6 epoch1:node_model train_loss:0.3693005904074638,train_acc:0.8795852661132812
node6 epoch2:node_model train_loss:0.22732683992193592,train_acc:0.9248387217521667
node6 epoch3:node_model train_loss:0.14564220572731668,train_acc:0.9561289548873901
node6 epoch4:node_model train_loss:0.09496018146314929,train_acc:0.978064239025116
node6_model on test-dataset: loss:0.7968583039939403,acc:0.7885997891426086
node6 weight score:3773.569259338316
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4703374223576652,train_acc:0.8509182929992676
node8 epoch1:node_model train_loss:0.2416131297747294,train_acc:0.919387698173523
node8 epoch2:node_model train_loss:0.13953295639819568,train_acc:0.9527323842048645
node8 epoch3:node_model train_loss:0.09784374551640616,train_acc:0.9716324210166931
node8 epoch4:node_model train_loss:0.0740832272503111,train_acc:0.9822108149528503
node8_model on test-dataset: loss:0.8276891988515854,acc:0.7809000015258789
node8 weight score:2172.312991995928
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5523514126737913,train_acc:0.8252314925193787
node14 epoch1:node_model train_loss:0.2706401360531648,train_acc:0.9205557107925415
node14 epoch2:node_model train_loss:0.20327337955435118,train_acc:0.9376851916313171
node14 epoch3:node_model train_loss:0.10093818015108506,train_acc:0.9671759009361267
node14 epoch4:node_model train_loss:0.060260430754472814,train_acc:0.9850000143051147
node14_model on test-dataset: loss:0.7911523726582527,acc:0.7857999801635742
node14 weight score:1481.383410457468
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.676161392852664,acc:0.81379998087883
total cost energy:9.680820393249938 | all_enery_cp：8.21 | all_enery_tp: 1.4708203932499369
ef: 24.973449367140024
reward: 15.292628973890086
step 222:loss:50.344825744628906|running q:53.92768096923828
episode3,iteration42 selected nodes:[0, 15, 17, 11, 7],center node:11
################################################## episode3,iteration42 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.18214355753018305,train_acc:0.9332643747329712
node0 epoch1:node_model train_loss:0.09541579199811587,train_acc:0.9706118106842041
node0 epoch2:node_model train_loss:0.07180758433129925,train_acc:0.9791519641876221
node0 epoch3:node_model train_loss:0.054153868033049196,train_acc:0.9868838787078857
node0 epoch4:node_model train_loss:0.04453460196964443,train_acc:0.9901925325393677
node0_model on test-dataset: loss:0.913929650336504,acc:0.7757999300956726
node0 weight score:5671.1148370026585
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4420422002673149,train_acc:0.8626176714897156
node7 epoch1:node_model train_loss:0.20576393604278564,train_acc:0.9266177415847778
node7 epoch2:node_model train_loss:0.12887726575136185,train_acc:0.9545783996582031
node7 epoch3:node_model train_loss:0.09043708518147468,train_acc:0.9794999361038208
node7 epoch4:node_model train_loss:0.06681938553228975,train_acc:0.9844999313354492
node7_model on test-dataset: loss:0.8240357282757759,acc:0.7859999537467957
node7 weight score:2367.615787827914
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5863101324614357,train_acc:0.8323099613189697
node11 epoch1:node_model train_loss:0.3221610030707191,train_acc:0.8960257768630981
node11 epoch2:node_model train_loss:0.17506225582431345,train_acc:0.9473887085914612
node11 epoch3:node_model train_loss:0.1270736142554704,train_acc:0.9656239748001099
node11 epoch4:node_model train_loss:0.07835049160263118,train_acc:0.9788951873779297
node11_model on test-dataset: loss:0.7739672593772411,acc:0.7925999760627747
node11 weight score:2173.218543318475
node15: train data size:629
node15 epoch0:node_model train_loss:0.7373752338545663,train_acc:0.7875862717628479
node15 epoch1:node_model train_loss:0.40023033959524973,train_acc:0.890788197517395
node15 epoch2:node_model train_loss:0.20807390553610666,train_acc:0.9393595457077026
node15 epoch3:node_model train_loss:0.11776464805006981,train_acc:0.9750738739967346
node15 epoch4:node_model train_loss:0.05544060628329005,train_acc:0.9842856526374817
node15_model on test-dataset: loss:0.9198925134539604,acc:0.7666000127792358
node15 weight score:683.7755398598325
node17: train data size:442
node17 epoch0:node_model train_loss:0.7814195156097412,train_acc:0.7868571281433105
node17 epoch1:node_model train_loss:0.3213972419500351,train_acc:0.898476243019104
node17 epoch2:node_model train_loss:0.21701230704784394,train_acc:0.9301905035972595
node17 epoch3:node_model train_loss:0.13701460808515548,train_acc:0.9632380604743958
node17 epoch4:node_model train_loss:0.11210309937596322,train_acc:0.9719999432563782
node17_model on test-dataset: loss:0.9815299117565155,acc:0.7510998845100403
node17 weight score:450.31740215538673
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6922997939586639,acc:0.8163999807834625
total cost energy:6.569740552495637 | all_enery_cp：4.9435 | all_enery_tp: 1.6262405524956371
ef: 24.5671130513745
reward: 17.99737249887886
step 223:loss:36.50257110595703|running q:55.013607025146484
episode3,iteration43 selected nodes:[6, 2, 12, 10, 0],center node:6
################################################## episode3,iteration43 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.09149400825397326,train_acc:0.9713807702064514
node0 epoch1:node_model train_loss:0.07585256381963308,train_acc:0.9756904244422913
node0 epoch2:node_model train_loss:0.050314833589184746,train_acc:0.9884617328643799
node0 epoch3:node_model train_loss:0.03361654544893939,train_acc:0.9934619665145874
node0 epoch4:node_model train_loss:0.03644577508720641,train_acc:0.9920762777328491
node0_model on test-dataset: loss:0.9046189427375794,acc:0.7843998670578003
node0 weight score:5729.484266950107
node2: train data size:4788
node2 epoch0:node_model train_loss:0.37489333593597013,train_acc:0.8744791746139526
node2 epoch1:node_model train_loss:0.19127900013700128,train_acc:0.932121217250824
node2 epoch2:node_model train_loss:0.1261401428685834,train_acc:0.9603598117828369
node2 epoch3:node_model train_loss:0.0834122485248372,train_acc:0.9760417342185974
node2 epoch4:node_model train_loss:0.06374317617155612,train_acc:0.9839016795158386
node2_model on test-dataset: loss:0.7724448820948601,acc:0.792499840259552
node2 weight score:6198.50051568082
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5553114558419874,train_acc:0.8285253047943115
node6 epoch1:node_model train_loss:0.29932215395233325,train_acc:0.9006451964378357
node6 epoch2:node_model train_loss:0.19267838472320187,train_acc:0.9323962926864624
node6 epoch3:node_model train_loss:0.19738888175737473,train_acc:0.9316129088401794
node6 epoch4:node_model train_loss:0.12025692410046054,train_acc:0.9629031419754028
node6_model on test-dataset: loss:0.7805077354609966,acc:0.7868998646736145
node6 weight score:3852.6203692573977
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6210252046585083,train_acc:0.8210000395774841
node10 epoch1:node_model train_loss:0.2914313681423664,train_acc:0.9041666388511658
node10 epoch2:node_model train_loss:0.20543738268315792,train_acc:0.9403333067893982
node10 epoch3:node_model train_loss:0.13672708664089442,train_acc:0.9571666121482849
node10 epoch4:node_model train_loss:0.08810230940580369,train_acc:0.9808332324028015
node10_model on test-dataset: loss:0.7953041729331016,acc:0.7824999094009399
node10 weight score:2483.326590273192
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7480576080935342,train_acc:0.8059524297714233
node12 epoch1:node_model train_loss:0.36563941836357117,train_acc:0.8899207711219788
node12 epoch2:node_model train_loss:0.23017462821943419,train_acc:0.9172222018241882
node12 epoch3:node_model train_loss:0.1601989125566823,train_acc:0.9430158734321594
node12 epoch4:node_model train_loss:0.12456543583955083,train_acc:0.9615872502326965
node12_model on test-dataset: loss:0.8775594425201416,acc:0.7744999527931213
node12 weight score:1522.4039936979384
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6540271808207035,acc:0.8166999793052674
total cost energy:9.88886245051193 | all_enery_cp：8.1445 | all_enery_tp: 1.7443624505119284
ef: 25.035418712193305
reward: 15.146556261681376
step 224:loss:36.5282096862793|running q:55.967674255371094
episode3,iteration44 selected nodes:[1, 8, 16, 0, 9],center node:9
################################################## episode3,iteration44 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.08623326637853797,train_acc:0.9734964966773987
node0 epoch1:node_model train_loss:0.05982225696341349,train_acc:0.9824607372283936
node0 epoch2:node_model train_loss:0.05836689740849229,train_acc:0.9824607968330383
node0 epoch3:node_model train_loss:0.045421714500452466,train_acc:0.9884616732597351
node0 epoch4:node_model train_loss:0.032997760670976,train_acc:0.9926532506942749
node0_model on test-dataset: loss:0.9345790276676417,acc:0.7783999443054199
node0 weight score:5545.812442351527
node1: train data size:6708
node1 epoch0:node_model train_loss:0.16936681693529382,train_acc:0.9405883550643921
node1 epoch1:node_model train_loss:0.09197434654240222,train_acc:0.9697058200836182
node1 epoch2:node_model train_loss:0.07339574777356841,train_acc:0.9788236021995544
node1 epoch3:node_model train_loss:0.11076944047475562,train_acc:0.9648528695106506
node1 epoch4:node_model train_loss:0.08580171234686584,train_acc:0.9723531007766724
node1_model on test-dataset: loss:1.0608943705260754,acc:0.75819993019104
node1 weight score:6322.966910149257
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5175743202368418,train_acc:0.8464739918708801
node8 epoch1:node_model train_loss:0.27511438147889244,train_acc:0.9048412442207336
node8 epoch2:node_model train_loss:0.18039455388983092,train_acc:0.9405102133750916
node8 epoch3:node_model train_loss:0.10385746446748574,train_acc:0.9705100655555725
node8 epoch4:node_model train_loss:0.06124391737911436,train_acc:0.9883219599723816
node8_model on test-dataset: loss:0.8183192624151707,acc:0.7794997096061707
node8 weight score:2197.1864559235955
node9: train data size:1857
node9 epoch0:node_model train_loss:0.732970477719056,train_acc:0.8066851496696472
node9 epoch1:node_model train_loss:0.4057912214806205,train_acc:0.8640535473823547
node9 epoch2:node_model train_loss:0.22399280769260307,train_acc:0.9330194592475891
node9 epoch3:node_model train_loss:0.1666150336202822,train_acc:0.9451246857643127
node9 epoch4:node_model train_loss:0.10732802042835637,train_acc:0.9744690656661987
node9_model on test-dataset: loss:0.802035975754261,acc:0.7834998369216919
node9 weight score:2315.3574853716705
node16: train data size:877
node16 epoch0:node_model train_loss:0.6262812680668302,train_acc:0.8304618000984192
node16 epoch1:node_model train_loss:0.29287680155701107,train_acc:0.898787796497345
node16 epoch2:node_model train_loss:0.22153754615121418,train_acc:0.9281240105628967
node16 epoch3:node_model train_loss:0.13102022434274355,train_acc:0.9553390145301819
node16 epoch4:node_model train_loss:0.06772290832466549,train_acc:0.984112560749054
node16_model on test-dataset: loss:0.8637509460747242,acc:0.7774999737739563
node16 weight score:1015.3389747189111
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7023958969116211,acc:0.8129999810457229
total cost energy:9.99655914659306 | all_enery_cp：8.211500000000001 | all_enery_tp: 1.785059146593059
ef: 24.752807407432368
reward: 14.756248260839307
step 225:loss:34.04951095581055|running q:57.07024383544922
episode3,iteration45 selected nodes:[7, 11, 6, 8, 10],center node:7
################################################## episode3,iteration45 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4785311465421992,train_acc:0.8503226041793823
node6 epoch1:node_model train_loss:0.24180680411236902,train_acc:0.9158065319061279
node6 epoch2:node_model train_loss:0.2098013537545358,train_acc:0.9470045566558838
node6 epoch3:node_model train_loss:0.18088840753320726,train_acc:0.9363595843315125
node6 epoch4:node_model train_loss:0.1679594882072941,train_acc:0.9411980509757996
node6_model on test-dataset: loss:0.8925698678195476,acc:0.7717999219894409
node6 weight score:3368.923944683208
node7: train data size:1951
node7 epoch0:node_model train_loss:0.462828017026186,train_acc:0.8495783805847168
node7 epoch1:node_model train_loss:0.21599669568240643,train_acc:0.9240588545799255
node7 epoch2:node_model train_loss:0.1337834012694657,train_acc:0.9565000534057617
node7 epoch3:node_model train_loss:0.08027099259197712,train_acc:0.9785194396972656
node7 epoch4:node_model train_loss:0.05818334836512804,train_acc:0.9865195155143738
node7_model on test-dataset: loss:0.8068814659118653,acc:0.7887001633644104
node7 weight score:2417.951189144188
node8: train data size:1798
node8 epoch0:node_model train_loss:0.45535171528657276,train_acc:0.8504195213317871
node8 epoch1:node_model train_loss:0.22899707489543492,train_acc:0.9288321733474731
node8 epoch2:node_model train_loss:0.1405395944085386,train_acc:0.9566099643707275
node8 epoch3:node_model train_loss:0.08012086815304226,train_acc:0.9777663350105286
node8 epoch4:node_model train_loss:0.05361180152330133,train_acc:0.9888660907745361
node8_model on test-dataset: loss:0.800849886238575,acc:0.7842000126838684
node8 weight score:2245.114884694348
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5585126832127572,train_acc:0.8405000567436218
node10 epoch1:node_model train_loss:0.2915825121104717,train_acc:0.9023333787918091
node10 epoch2:node_model train_loss:0.16865240186452865,train_acc:0.951333224773407
node10 epoch3:node_model train_loss:0.11248353254050017,train_acc:0.9673334360122681
node10 epoch4:node_model train_loss:0.07603589221835136,train_acc:0.9823331832885742
node10_model on test-dataset: loss:0.8077796265482903,acc:0.7851001024246216
node10 weight score:2444.9737714224602
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6435046616722556,train_acc:0.8145337700843811
node11 epoch1:node_model train_loss:0.3193453515277189,train_acc:0.8904016613960266
node11 epoch2:node_model train_loss:0.17351702425409765,train_acc:0.9432711005210876
node11 epoch3:node_model train_loss:0.1077256428406519,train_acc:0.9688234925270081
node11 epoch4:node_model train_loss:0.07330864580238566,train_acc:0.983988344669342
node11_model on test-dataset: loss:0.8279702173173428,acc:0.7848001718521118
node11 weight score:2031.4740371335438
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6503020034730435,acc:0.8194999843835831
total cost energy:6.322727766016838 | all_enery_cp：5.2065 | all_enery_tp: 1.116227766016838
ef: 25.121752373590677
reward: 18.79902460757384
step 226:loss:35.103233337402344|running q:58.07046890258789
episode3,iteration46 selected nodes:[19, 11, 13, 9, 6],center node:11
################################################## episode3,iteration46 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3029096448613751,train_acc:0.8908755779266357
node6 epoch1:node_model train_loss:0.21081292941685645,train_acc:0.9279721975326538
node6 epoch2:node_model train_loss:0.18468965349658842,train_acc:0.9375574588775635
node6 epoch3:node_model train_loss:0.20499100716364,train_acc:0.9250690937042236
node6 epoch4:node_model train_loss:0.13513206197850167,train_acc:0.9551611542701721
node6_model on test-dataset: loss:0.8834650902450085,acc:0.7745998501777649
node6 weight score:3403.6432601610536
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6038096421643308,train_acc:0.8160296082496643
node9 epoch1:node_model train_loss:0.3103694970670499,train_acc:0.8956416249275208
node9 epoch2:node_model train_loss:0.17851334378907555,train_acc:0.9395936727523804
node9 epoch3:node_model train_loss:0.13791471837382568,train_acc:0.9553831815719604
node9 epoch4:node_model train_loss:0.09628946608618687,train_acc:0.9721051454544067
node9_model on test-dataset: loss:0.8786120998859406,acc:0.7750999927520752
node9 weight score:2113.5606944646806
node11: train data size:1682
node11 epoch0:node_model train_loss:0.430411754285588,train_acc:0.8647919297218323
node11 epoch1:node_model train_loss:0.23799794386414921,train_acc:0.9271304607391357
node11 epoch2:node_model train_loss:0.1342687808415469,train_acc:0.956671416759491
node11 epoch3:node_model train_loss:0.09167473730357255,train_acc:0.9716354608535767
node11 epoch4:node_model train_loss:0.06913041674038943,train_acc:0.983529269695282
node11_model on test-dataset: loss:0.8312654274702073,acc:0.7748997807502747
node11 weight score:2023.421093210668
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7345920006434122,train_acc:0.8097728490829468
node13 epoch1:node_model train_loss:0.3738429894049962,train_acc:0.8728787302970886
node13 epoch2:node_model train_loss:0.23717107189198336,train_acc:0.9149999618530273
node13 epoch3:node_model train_loss:0.16495901190986237,train_acc:0.9439393877983093
node13 epoch4:node_model train_loss:0.07234220424046119,train_acc:0.9850000143051147
node13_model on test-dataset: loss:0.8448280987143516,acc:0.7774999141693115
node13 weight score:1367.1420277777977
node19: train data size:4281
node19 epoch0:node_model train_loss:0.378854738418446,train_acc:0.8733247518539429
node19 epoch1:node_model train_loss:0.17803348912749178,train_acc:0.9375107288360596
node19 epoch2:node_model train_loss:0.11734414377877879,train_acc:0.9611770510673523
node19 epoch3:node_model train_loss:0.09725030329684879,train_acc:0.9703559875488281
node19 epoch4:node_model train_loss:0.0739951665664828,train_acc:0.9813548922538757
node19_model on test-dataset: loss:0.8348489708453417,acc:0.7865001559257507
node19 weight score:5127.873602892742
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6613898354768754,acc:0.8180999803543091
total cost energy:7.507351461583877 | all_enery_cp：5.991 | all_enery_tp: 1.5163514615838767
ef: 24.949307634792433
reward: 17.441956173208556
step 227:loss:40.13187789916992|running q:59.063323974609375
episode3,iteration47 selected nodes:[0, 4, 15, 9, 19],center node:9
################################################## episode3,iteration47 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.12324571315772258,train_acc:0.9564204216003418
node0 epoch1:node_model train_loss:0.06509703057459913,train_acc:0.9822685122489929
node0 epoch2:node_model train_loss:0.0409096847467411,train_acc:0.9922289252281189
node0 epoch3:node_model train_loss:0.03547088812606839,train_acc:0.9928058981895447
node0 epoch4:node_model train_loss:0.03228546555440586,train_acc:0.9932301044464111
node0_model on test-dataset: loss:0.8548900865763426,acc:0.7914999127388
node0 weight score:6062.767695385075
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5144524616854531,train_acc:0.8424999713897705
node4 epoch1:node_model train_loss:0.32972983942766276,train_acc:0.8871429562568665
node4 epoch2:node_model train_loss:0.15576349198818207,train_acc:0.9482143521308899
node4 epoch3:node_model train_loss:0.09000813881201404,train_acc:0.975356936454773
node4 epoch4:node_model train_loss:0.06637003705171603,train_acc:0.9832141399383545
node4_model on test-dataset: loss:0.8048145018517971,acc:0.7892000675201416
node4 weight score:3361.02293606299
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4910599730516735,train_acc:0.8566944003105164
node9 epoch1:node_model train_loss:0.2909423626567188,train_acc:0.9052447080612183
node9 epoch2:node_model train_loss:0.18812041925756554,train_acc:0.9393350481987
node9 epoch3:node_model train_loss:0.09758366409100984,train_acc:0.9699999094009399
node9 epoch4:node_model train_loss:0.0636159018858483,train_acc:0.9832870960235596
node9_model on test-dataset: loss:0.7996263699233532,acc:0.7889999747276306
node9 weight score:2322.334617576456
node15: train data size:629
node15 epoch0:node_model train_loss:0.7371488383838108,train_acc:0.8039408922195435
node15 epoch1:node_model train_loss:0.30904250059809,train_acc:0.8852216601371765
node15 epoch2:node_model train_loss:0.17724883662802832,train_acc:0.9357143640518188
node15 epoch3:node_model train_loss:0.14572583032505854,train_acc:0.9572906494140625
node15 epoch4:node_model train_loss:0.06654140193547521,train_acc:0.9857142567634583
node15_model on test-dataset: loss:0.9221873509883881,acc:0.7725998759269714
node15 weight score:682.0739834761844
node19: train data size:4281
node19 epoch0:node_model train_loss:0.23178632843286492,train_acc:0.9180447459220886
node19 epoch1:node_model train_loss:0.12556358747357546,train_acc:0.9603013396263123
node19 epoch2:node_model train_loss:0.08851677426245323,train_acc:0.9715186953544617
node19 epoch3:node_model train_loss:0.06184925905667072,train_acc:0.9838442802429199
node19 epoch4:node_model train_loss:0.04687702629801839,train_acc:0.9911627769470215
node19_model on test-dataset: loss:0.8476109205186367,acc:0.7886000871658325
node19 weight score:5050.666404086132
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.671397675126791,acc:0.8210999804735184
total cost energy:9.306240735793482 | all_enery_cp：7.327499999999999 | all_enery_tp: 1.9787407357934832
ef: 24.869133691836232
reward: 15.56289295604275
step 228:loss:25.070804595947266|running q:60.04771423339844
episode3,iteration48 selected nodes:[17, 15, 12, 16, 18],center node:16
################################################## episode3,iteration48 ##################################################
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7995477552924838,train_acc:0.8107143044471741
node12 epoch1:node_model train_loss:0.3678055554628372,train_acc:0.8911905884742737
node12 epoch2:node_model train_loss:0.285903840724911,train_acc:0.9129364490509033
node12 epoch3:node_model train_loss:0.12674233051283018,train_acc:0.9560317993164062
node12 epoch4:node_model train_loss:0.07997901578034673,train_acc:0.9780158996582031
node12_model on test-dataset: loss:0.8269892679154873,acc:0.7891001105308533
node12 weight score:1615.4985945192823
node15: train data size:629
node15 epoch0:node_model train_loss:0.6802722385951451,train_acc:0.7778818011283875
node15 epoch1:node_model train_loss:0.3816052164350237,train_acc:0.8763054609298706
node15 epoch2:node_model train_loss:0.17990641721657344,train_acc:0.9225123524665833
node15 epoch3:node_model train_loss:0.12381151025848729,train_acc:0.9707881212234497
node15 epoch4:node_model train_loss:0.07930674031376839,train_acc:0.977290689945221
node15_model on test-dataset: loss:0.9010265770554543,acc:0.775999903678894
node15 weight score:698.0926157090345
node16: train data size:877
node16 epoch0:node_model train_loss:0.5795114106602139,train_acc:0.8382394909858704
node16 epoch1:node_model train_loss:0.3146273311641481,train_acc:0.9031168818473816
node16 epoch2:node_model train_loss:0.17187279048893186,train_acc:0.9607791900634766
node16 epoch3:node_model train_loss:0.11064548583494292,train_acc:0.965122640132904
node16 epoch4:node_model train_loss:0.07296762967275248,train_acc:0.9799999594688416
node16_model on test-dataset: loss:0.8771120622754097,acc:0.7753998637199402
node16 weight score:999.8722372200434
node17: train data size:442
node17 epoch0:node_model train_loss:0.7910748660564423,train_acc:0.7854285836219788
node17 epoch1:node_model train_loss:0.3004977345466614,train_acc:0.8977142572402954
node17 epoch2:node_model train_loss:0.16361488699913024,train_acc:0.9449524283409119
node17 epoch3:node_model train_loss:0.11957645192742347,train_acc:0.9639999270439148
node17 epoch4:node_model train_loss:0.11183776184916497,train_acc:0.9792380332946777
node17_model on test-dataset: loss:0.9398008045554161,acc:0.7709000110626221
node17 weight score:470.3124298867709
node18: train data size:472
node18 epoch0:node_model train_loss:0.6450128674507141,train_acc:0.8278889060020447
node18 epoch1:node_model train_loss:0.3481436550617218,train_acc:0.890999972820282
node18 epoch2:node_model train_loss:0.14474835991859436,train_acc:0.9536666870117188
node18 epoch3:node_model train_loss:0.08410799577832222,train_acc:0.9744443893432617
node18 epoch4:node_model train_loss:0.05379037857055664,train_acc:0.9844444394111633
node18_model on test-dataset: loss:0.9722937351465225,acc:0.7605999112129211
node18 weight score:485.45000645187804
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6619473163038492,acc:0.8232999795675278
total cost energy:3.4879019513592784 | all_enery_cp：1.878 | all_enery_tp: 1.6099019513592787
ef: 24.109778426027013
reward: 20.621876474667737
step 229:loss:37.195289611816406|running q:61.09581756591797
episode3,iteration49 selected nodes:[10, 8, 4, 3, 15],center node:4
################################################## episode3,iteration49 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5424057397731515,train_acc:0.8316824436187744
node3 epoch1:node_model train_loss:0.33990739875061565,train_acc:0.880346417427063
node3 epoch2:node_model train_loss:0.21620551739321198,train_acc:0.9249676465988159
node3 epoch3:node_model train_loss:0.17220773816455243,train_acc:0.9412764310836792
node3 epoch4:node_model train_loss:0.12988771506866745,train_acc:0.9588074088096619
node3_model on test-dataset: loss:0.8312815760076046,acc:0.7842998504638672
node3 weight score:5108.9788617679515
node4: train data size:2705
node4 epoch0:node_model train_loss:0.40520425353731426,train_acc:0.8639285564422607
node4 epoch1:node_model train_loss:0.21837149081485613,train_acc:0.9207143783569336
node4 epoch2:node_model train_loss:0.17385459318757057,train_acc:0.9489285945892334
node4 epoch3:node_model train_loss:0.18751548337084906,train_acc:0.936071515083313
node4 epoch4:node_model train_loss:0.09391280649495977,train_acc:0.9696427583694458
node4_model on test-dataset: loss:0.8413599271699786,acc:0.7865999341011047
node4 weight score:3215.033082332091
node8: train data size:1798
node8 epoch0:node_model train_loss:0.48572955694463515,train_acc:0.8420294523239136
node8 epoch1:node_model train_loss:0.2186940395169788,train_acc:0.9232993721961975
node8 epoch2:node_model train_loss:0.12003281878100501,train_acc:0.9643990397453308
node8 epoch3:node_model train_loss:0.07955894908971256,train_acc:0.9766438603401184
node8 epoch4:node_model train_loss:0.05487477996697029,train_acc:0.9871994853019714
node8_model on test-dataset: loss:0.7919460305571556,acc:0.7934001088142395
node8 weight score:2270.3567296562596
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5031497359275818,train_acc:0.8416666984558105
node10 epoch1:node_model train_loss:0.2864029735326767,train_acc:0.9106666445732117
node10 epoch2:node_model train_loss:0.15635851472616197,train_acc:0.9503332376480103
node10 epoch3:node_model train_loss:0.08223283682018519,train_acc:0.9753332138061523
node10 epoch4:node_model train_loss:0.06078840754926205,train_acc:0.9869998097419739
node10_model on test-dataset: loss:0.7694017366319895,acc:0.7916999459266663
node10 weight score:2566.9294803589155
node15: train data size:629
node15 epoch0:node_model train_loss:0.4687575570174626,train_acc:0.8604432940483093
node15 epoch1:node_model train_loss:0.2581612595490047,train_acc:0.91379314661026
node15 epoch2:node_model train_loss:0.1375186549765723,train_acc:0.9444335103034973
node15 epoch3:node_model train_loss:0.10511479792850358,train_acc:0.9685714244842529
node15 epoch4:node_model train_loss:0.06741888182503837,train_acc:0.9857142567634583
node15_model on test-dataset: loss:0.8675769555568695,acc:0.7730000615119934
node15 weight score:725.0077309813576
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6376629272103309,acc:0.825599980354309
total cost energy:7.040375801966598 | all_enery_cp：5.677 | all_enery_tp: 1.3633758019665982
ef: 25.01782520920505
reward: 17.977449407238453
step 230:loss:35.115478515625|running q:62.20153045654297
episode3,iteration50 selected nodes:[17, 15, 16, 14, 7],center node:14
################################################## episode3,iteration50 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4207981899380684,train_acc:0.8626959919929504
node7 epoch1:node_model train_loss:0.19741926416754724,train_acc:0.9405391812324524
node7 epoch2:node_model train_loss:0.10762878153473139,train_acc:0.9705392122268677
node7 epoch3:node_model train_loss:0.09212556406855583,train_acc:0.9724998474121094
node7 epoch4:node_model train_loss:0.04891317114233971,train_acc:0.9895195364952087
node7_model on test-dataset: loss:0.7977516884356737,acc:0.7904001474380493
node7 weight score:2445.623153522561
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5383083547155062,train_acc:0.8345834016799927
node14 epoch1:node_model train_loss:0.34785618136326474,train_acc:0.8880555629730225
node14 epoch2:node_model train_loss:0.17469377319018045,train_acc:0.9420832395553589
node14 epoch3:node_model train_loss:0.09850108157843351,train_acc:0.9710184931755066
node14 epoch4:node_model train_loss:0.05176763407265147,train_acc:0.9916666746139526
node14_model on test-dataset: loss:0.8170469285547733,acc:0.7841999530792236
node14 weight score:1434.434129840109
node15: train data size:629
node15 epoch0:node_model train_loss:0.4315307268074581,train_acc:0.8387192487716675
node15 epoch1:node_model train_loss:0.265810740845544,train_acc:0.8974385261535645
node15 epoch2:node_model train_loss:0.16848760843276978,train_acc:0.9436452984809875
node15 epoch3:node_model train_loss:0.12158402694123131,train_acc:0.9557143449783325
node15 epoch4:node_model train_loss:0.08758432258452688,train_acc:0.9744335412979126
node15_model on test-dataset: loss:0.9366327615082264,acc:0.7723999619483948
node15 weight score:671.5545578259976
node16: train data size:877
node16 epoch0:node_model train_loss:0.4497046007050408,train_acc:0.8563492298126221
node16 epoch1:node_model train_loss:0.2219068772262997,train_acc:0.9208946228027344
node16 epoch2:node_model train_loss:0.12429755594995287,train_acc:0.959004282951355
node16 epoch3:node_model train_loss:0.07819518902235562,train_acc:0.9793362021446228
node16 epoch4:node_model train_loss:0.056227241539292865,train_acc:0.9877777099609375
node16_model on test-dataset: loss:0.8136926006525755,acc:0.7869001030921936
node16 weight score:1077.802599282152
node17: train data size:442
node17 epoch0:node_model train_loss:0.6443521797657012,train_acc:0.8414285778999329
node17 epoch1:node_model train_loss:0.3008422046899796,train_acc:0.9109522700309753
node17 epoch2:node_model train_loss:0.19330518543720246,train_acc:0.9497143030166626
node17 epoch3:node_model train_loss:0.10921311378479004,train_acc:0.9704761505126953
node17 epoch4:node_model train_loss:0.06226200982928276,train_acc:0.983238160610199
node17_model on test-dataset: loss:0.9320124309509993,acc:0.7629998326301575
node17 weight score:474.24260162388134
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6336871229857206,acc:0.8265999829769135
total cost energy:3.7128387165490544 | all_enery_cp：2.5355 | all_enery_tp: 1.1773387165490548
ef: 24.587490636751493
reward: 20.87465192020244
step 231:loss:33.42618942260742|running q:63.20997619628906
episode3,iteration51 selected nodes:[5, 13, 19, 18, 9],center node:9
################################################## episode3,iteration51 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3950272935785745,train_acc:0.8743609189987183
node5 epoch1:node_model train_loss:0.1905904763231152,train_acc:0.9327066540718079
node5 epoch2:node_model train_loss:0.13394040535939367,train_acc:0.9571052193641663
node5 epoch3:node_model train_loss:0.09795560648566798,train_acc:0.9688719511032104
node5 epoch4:node_model train_loss:0.08254630152920359,train_acc:0.9763532876968384
node5_model on test-dataset: loss:0.8627626427263021,acc:0.7794002294540405
node5 weight score:4329.116509029089
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5209891137323881,train_acc:0.8423546552658081
node9 epoch1:node_model train_loss:0.3036938121444301,train_acc:0.8972207307815552
node9 epoch2:node_model train_loss:0.16914163490659312,train_acc:0.9480147361755371
node9 epoch3:node_model train_loss:0.13170175705301135,train_acc:0.9622344970703125
node9 epoch4:node_model train_loss:0.08077811900722354,train_acc:0.979732096195221
node9_model on test-dataset: loss:0.8041730257123709,acc:0.7907000780105591
node9 weight score:2309.204537611778
node13: train data size:1155
node13 epoch0:node_model train_loss:0.653434673945109,train_acc:0.8235605955123901
node13 epoch1:node_model train_loss:0.38200271005431813,train_acc:0.8849242925643921
node13 epoch2:node_model train_loss:0.20836160952846208,train_acc:0.9369696378707886
node13 epoch3:node_model train_loss:0.10641610839714606,train_acc:0.9671211242675781
node13 epoch4:node_model train_loss:0.060787195805460215,train_acc:0.9874998927116394
node13_model on test-dataset: loss:0.8412126874923707,acc:0.7820999622344971
node13 weight score:1373.01780771165
node18: train data size:472
node18 epoch0:node_model train_loss:0.5542117118835449,train_acc:0.8351110816001892
node18 epoch1:node_model train_loss:0.24537881910800935,train_acc:0.9296666979789734
node18 epoch2:node_model train_loss:0.16701414287090302,train_acc:0.9481111764907837
node18 epoch3:node_model train_loss:0.09745231494307519,train_acc:0.969333291053772
node18 epoch4:node_model train_loss:0.04509462229907513,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.9854247215390205,acc:0.7580998539924622
node18 weight score:478.98128561544297
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2529825956322426,train_acc:0.9117111563682556
node19 epoch1:node_model train_loss:0.12727434387387232,train_acc:0.9584410786628723
node19 epoch2:node_model train_loss:0.088613786063222,train_acc:0.9745418429374695
node19 epoch3:node_model train_loss:0.06017336582894935,train_acc:0.9827359914779663
node19 epoch4:node_model train_loss:0.045566447663965615,train_acc:0.9903015494346619
node19_model on test-dataset: loss:0.7709996313601732,acc:0.7981001138687134
node19 weight score:5552.531837722924
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6511234858632088,acc:0.8190999799966812
total cost energy:7.872589799425157 | all_enery_cp：5.749999999999999 | all_enery_tp: 2.1225897994251572
ef: 24.634226732619748
reward: 16.76163693319459
step 232:loss:37.24735641479492|running q:64.08502960205078
episode3,iteration52 selected nodes:[4, 18, 3, 9, 17],center node:3
################################################## episode3,iteration52 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.39190629401872323,train_acc:0.8716526627540588
node3 epoch1:node_model train_loss:0.2363339387053667,train_acc:0.9192132353782654
node3 epoch2:node_model train_loss:0.17509149829315585,train_acc:0.935665488243103
node3 epoch3:node_model train_loss:0.12290472424654074,train_acc:0.9592723250389099
node3 epoch4:node_model train_loss:0.08884561659638272,train_acc:0.9775556325912476
node3_model on test-dataset: loss:0.7921316714584827,acc:0.7947000861167908
node3 weight score:5361.482381054618
node4: train data size:2705
node4 epoch0:node_model train_loss:0.36117241052644594,train_acc:0.8864285349845886
node4 epoch1:node_model train_loss:0.201570328058941,train_acc:0.9332143664360046
node4 epoch2:node_model train_loss:0.12910124180572374,train_acc:0.954642653465271
node4 epoch3:node_model train_loss:0.11429034279925483,train_acc:0.9649998545646667
node4 epoch4:node_model train_loss:0.1585228999278375,train_acc:0.9399998784065247
node4_model on test-dataset: loss:0.9021118836477399,acc:0.7773000001907349
node4 weight score:2998.519417638288
node9: train data size:1857
node9 epoch0:node_model train_loss:0.3945215524811494,train_acc:0.8764358162879944
node9 epoch1:node_model train_loss:0.21417142607663808,train_acc:0.9284025430679321
node9 epoch2:node_model train_loss:0.13214341884380892,train_acc:0.9559094309806824
node9 epoch3:node_model train_loss:0.09041428497355235,train_acc:0.9780240654945374
node9 epoch4:node_model train_loss:0.0655387619412259,train_acc:0.9822344779968262
node9_model on test-dataset: loss:0.8581578567624092,acc:0.785300076007843
node9 weight score:2163.937538258922
node17: train data size:442
node17 epoch0:node_model train_loss:0.6002053916454315,train_acc:0.8179046511650085
node17 epoch1:node_model train_loss:0.23321924954652787,train_acc:0.9272381067276001
node17 epoch2:node_model train_loss:0.13767714202404022,train_acc:0.9699999690055847
node17 epoch3:node_model train_loss:0.07195054665207863,train_acc:0.9819999933242798
node17 epoch4:node_model train_loss:0.05215517543256283,train_acc:0.9892380833625793
node17_model on test-dataset: loss:0.8951385660655796,acc:0.7714000344276428
node17 weight score:493.77830065207826
node18: train data size:472
node18 epoch0:node_model train_loss:0.5656841039657593,train_acc:0.8465555310249329
node18 epoch1:node_model train_loss:0.262439626455307,train_acc:0.9057778716087341
node18 epoch2:node_model train_loss:0.11163465082645416,train_acc:0.9644444584846497
node18 epoch3:node_model train_loss:0.08457651399075986,train_acc:0.9756666421890259
node18 epoch4:node_model train_loss:0.04831452704966068,train_acc:0.9920000433921814
node18_model on test-dataset: loss:0.9193916657567024,acc:0.7735999822616577
node18 weight score:513.3829439399169
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6670503290742635,acc:0.8167999798059463
total cost energy:6.9306680412292465 | all_enery_cp：4.8614999999999995 | all_enery_tp: 2.069168041229247
ef: 24.309455333316485
reward: 17.37878729208724
step 233:loss:27.433948516845703|running q:64.86617279052734
episode3,iteration53 selected nodes:[12, 0, 6, 13, 9],center node:6
################################################## episode3,iteration53 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.16813226144474286,train_acc:0.9426923394203186
node0 epoch1:node_model train_loss:0.06702223112090276,train_acc:0.9829195141792297
node0 epoch2:node_model train_loss:0.048061297848247565,train_acc:0.9873079657554626
node0 epoch3:node_model train_loss:0.032809779209156446,train_acc:0.9936540126800537
node0 epoch4:node_model train_loss:0.027178313278664764,train_acc:0.9953848123550415
node0_model on test-dataset: loss:0.8341146400570869,acc:0.7923000454902649
node0 weight score:6213.774163759163
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4245379725771566,train_acc:0.8703224658966064
node6 epoch1:node_model train_loss:0.22715243216483824,train_acc:0.9303225874900818
node6 epoch2:node_model train_loss:0.1621849390287553,train_acc:0.9501380920410156
node6 epoch3:node_model train_loss:0.19346482955640362,train_acc:0.9295851588249207
node6 epoch4:node_model train_loss:0.1311951948990745,train_acc:0.9573270082473755
node6_model on test-dataset: loss:0.799949124455452,acc:0.7911999225616455
node6 weight score:3758.9890507686355
node9: train data size:1857
node9 epoch0:node_model train_loss:0.3591711364294353,train_acc:0.8826131224632263
node9 epoch1:node_model train_loss:0.15711799852157893,train_acc:0.9417082071304321
node9 epoch2:node_model train_loss:0.09437240326875135,train_acc:0.9789472818374634
node9 epoch3:node_model train_loss:0.06754717230796814,train_acc:0.9834163188934326
node9 epoch4:node_model train_loss:0.04015727645080341,train_acc:0.9932871460914612
node9_model on test-dataset: loss:0.8018529723584652,acc:0.7909999489784241
node9 weight score:2315.8859092809294
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5605584308505058,train_acc:0.8357142806053162
node12 epoch1:node_model train_loss:0.32262867795569555,train_acc:0.9019047617912292
node12 epoch2:node_model train_loss:0.1823839159416301,train_acc:0.945476233959198
node12 epoch3:node_model train_loss:0.11317914564694677,train_acc:0.9631745219230652
node12 epoch4:node_model train_loss:0.0914016594844205,train_acc:0.9780158996582031
node12_model on test-dataset: loss:0.8468347127735615,acc:0.789099931716919
node12 weight score:1577.6396265385952
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6752100388209025,train_acc:0.8149242401123047
node13 epoch1:node_model train_loss:0.326065460840861,train_acc:0.8967424631118774
node13 epoch2:node_model train_loss:0.22020127127567926,train_acc:0.9394696950912476
node13 epoch3:node_model train_loss:0.11042805792142947,train_acc:0.9696211814880371
node13 epoch4:node_model train_loss:0.06845651380717754,train_acc:0.9818181395530701
node13_model on test-dataset: loss:0.8309001694619655,acc:0.7835001349449158
node13 weight score:1390.0586886965007
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6457406713813544,acc:0.820699982047081
total cost energy:8.141431011989061 | all_enery_cp：6.268999999999999 | all_enery_tp: 1.872431011989062
ef: 24.878114316073635
reward: 16.736683304084572
step 234:loss:34.667137145996094|running q:65.70858764648438
episode3,iteration54 selected nodes:[14, 19, 7, 9, 10],center node:14
################################################## episode3,iteration54 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3454576127231121,train_acc:0.8871176838874817
node7 epoch1:node_model train_loss:0.1752349678426981,train_acc:0.9410196542739868
node7 epoch2:node_model train_loss:0.09356577210128307,train_acc:0.970019519329071
node7 epoch3:node_model train_loss:0.0604141335003078,train_acc:0.9839999079704285
node7 epoch4:node_model train_loss:0.039599024970084426,train_acc:0.9920392036437988
node7_model on test-dataset: loss:0.7933382200449706,acc:0.7987998723983765
node7 weight score:2459.2285493183567
node9: train data size:1857
node9 epoch0:node_model train_loss:0.28949841935383647,train_acc:0.90261310338974
node9 epoch1:node_model train_loss:0.15747680711118797,train_acc:0.944192111492157
node9 epoch2:node_model train_loss:0.09975005451001619,train_acc:0.9707847237586975
node9 epoch3:node_model train_loss:0.07479953530587648,train_acc:0.9764449596405029
node9 epoch4:node_model train_loss:0.047935195953438155,train_acc:0.9899999499320984
node9_model on test-dataset: loss:0.88542589828372,acc:0.7887002229690552
node9 weight score:2097.2957800303184
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4550459496676922,train_acc:0.8489999771118164
node10 epoch1:node_model train_loss:0.25363722667098043,train_acc:0.92249995470047
node10 epoch2:node_model train_loss:0.1443929649889469,train_acc:0.9549999237060547
node10 epoch3:node_model train_loss:0.09160745069384575,train_acc:0.9741665124893188
node10 epoch4:node_model train_loss:0.06226558517664671,train_acc:0.9833332300186157
node10_model on test-dataset: loss:0.8613961534202099,acc:0.7739999890327454
node10 weight score:2292.7894351027444
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5622525910536448,train_acc:0.8404630422592163
node14 epoch1:node_model train_loss:0.27307096247871715,train_acc:0.9103703498840332
node14 epoch2:node_model train_loss:0.15168848695854345,train_acc:0.9521758556365967
node14 epoch3:node_model train_loss:0.08074044746657212,train_acc:0.9788424968719482
node14 epoch4:node_model train_loss:0.04878284207855662,train_acc:0.9871758818626404
node14_model on test-dataset: loss:0.8029310682415962,acc:0.7960999608039856
node14 weight score:1459.652075198258
node19: train data size:4281
node19 epoch0:node_model train_loss:0.21293993606123812,train_acc:0.9216564297676086
node19 epoch1:node_model train_loss:0.10545357446684393,train_acc:0.9673326015472412
node19 epoch2:node_model train_loss:0.06392829813236414,train_acc:0.9831466674804688
node19 epoch3:node_model train_loss:0.03866346442508836,train_acc:0.9925580620765686
node19 epoch4:node_model train_loss:0.03195804509139338,train_acc:0.9934884905815125
node19_model on test-dataset: loss:0.7830238349735736,acc:0.7966000437736511
node19 weight score:5467.266523431537
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.648707644790411,acc:0.8292999839782715
total cost energy:7.102819196258327 | all_enery_cp：5.617999999999999 | all_enery_tp: 1.4848191962583275
ef: 25.06049432718665
reward: 17.957675130928322
step 235:loss:39.04121017456055|running q:66.66780853271484
episode3,iteration55 selected nodes:[15, 6, 0, 14, 11],center node:11
################################################## episode3,iteration55 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.12286156688172084,train_acc:0.9554193615913391
node0 epoch1:node_model train_loss:0.061236075340555265,train_acc:0.9845367074012756
node0 epoch2:node_model train_loss:0.03961766478963769,train_acc:0.9901924729347229
node0 epoch3:node_model train_loss:0.02933447829519327,train_acc:0.9948079586029053
node0 epoch4:node_model train_loss:0.022407843319412608,train_acc:0.995922327041626
node0_model on test-dataset: loss:0.8585137712955475,acc:0.7951000928878784
node0 weight score:6037.177472620561
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3363701989573817,train_acc:0.8867741227149963
node6 epoch1:node_model train_loss:0.20393788405964453,train_acc:0.937004566192627
node6 epoch2:node_model train_loss:0.1614560076305943,train_acc:0.9496773481369019
node6 epoch3:node_model train_loss:0.12474867719556054,train_acc:0.9626265168190002
node6 epoch4:node_model train_loss:0.1604796652832339,train_acc:0.9451611638069153
node6_model on test-dataset: loss:0.9058735616505146,acc:0.7746999263763428
node6 weight score:3319.4477985660637
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5297472003628226,train_acc:0.8443328738212585
node11 epoch1:node_model train_loss:0.2702091778902447,train_acc:0.9132137298583984
node11 epoch2:node_model train_loss:0.129092426641899,train_acc:0.9615781307220459
node11 epoch3:node_model train_loss:0.09562249908990719,train_acc:0.9753656983375549
node11 epoch4:node_model train_loss:0.07216581318746595,train_acc:0.9835293889045715
node11_model on test-dataset: loss:0.8058607259392738,acc:0.7985000610351562
node11 weight score:2087.209297908815
node14: train data size:1172
node14 epoch0:node_model train_loss:0.3940785800417264,train_acc:0.8697222471237183
node14 epoch1:node_model train_loss:0.17084460084637007,train_acc:0.9425462484359741
node14 epoch2:node_model train_loss:0.12904800785084566,train_acc:0.9625462889671326
node14 epoch3:node_model train_loss:0.06756147835403681,train_acc:0.9818517565727234
node14 epoch4:node_model train_loss:0.03788113873451948,train_acc:0.9930092096328735
node14_model on test-dataset: loss:0.7716857039928436,acc:0.7959000468254089
node14 weight score:1518.7530285138841
node15: train data size:629
node15 epoch0:node_model train_loss:0.5666114134447915,train_acc:0.8255172967910767
node15 epoch1:node_model train_loss:0.30416771130902426,train_acc:0.9031527638435364
node15 epoch2:node_model train_loss:0.10950190999678203,train_acc:0.9579310417175293
node15 epoch3:node_model train_loss:0.07391208889228958,train_acc:0.977931022644043
node15 epoch4:node_model train_loss:0.0565822393234287,train_acc:0.9885714054107666
node15_model on test-dataset: loss:0.8847296415269374,acc:0.780299961566925
node15 weight score:710.9516517547906
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6810863834619522,acc:0.8216999793052673
total cost energy:7.385028137423857 | all_enery_cp：5.8365 | all_enery_tp: 1.548528137423857
ef: 24.871402132679794
reward: 17.48637399525594
step 236:loss:46.56897735595703|running q:67.57217407226562
episode3,iteration56 selected nodes:[19, 6, 10, 14, 7],center node:14
################################################## episode3,iteration56 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.2319426519736167,train_acc:0.9183870553970337
node6 epoch1:node_model train_loss:0.14157087019374293,train_acc:0.949999988079071
node6 epoch2:node_model train_loss:0.08921692027680335,train_acc:0.9751609563827515
node6 epoch3:node_model train_loss:0.06292180626863433,train_acc:0.9858062267303467
node6 epoch4:node_model train_loss:0.04921282767768829,train_acc:0.9909676313400269
node6_model on test-dataset: loss:0.8211299259960652,acc:0.7914999723434448
node6 weight score:3662.027049315469
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3327770046889782,train_acc:0.8732353448867798
node7 epoch1:node_model train_loss:0.16306700389832257,train_acc:0.9395588040351868
node7 epoch2:node_model train_loss:0.09364779377356172,train_acc:0.9769999384880066
node7 epoch3:node_model train_loss:0.062495188973844054,train_acc:0.9834999442100525
node7 epoch4:node_model train_loss:0.031967059476301074,train_acc:0.9939998984336853
node7_model on test-dataset: loss:0.8134367821365595,acc:0.7925001382827759
node7 weight score:2398.465428223612
node10: train data size:1975
node10 epoch0:node_model train_loss:0.42375680431723595,train_acc:0.85833340883255
node10 epoch1:node_model train_loss:0.2242510125041008,train_acc:0.9286665916442871
node10 epoch2:node_model train_loss:0.12034168820828199,train_acc:0.9654998779296875
node10 epoch3:node_model train_loss:0.06859699198976159,train_acc:0.981833279132843
node10 epoch4:node_model train_loss:0.05836447179317474,train_acc:0.9846665263175964
node10_model on test-dataset: loss:0.7762913887947798,acc:0.7988998889923096
node10 weight score:2544.147762692896
node14: train data size:1172
node14 epoch0:node_model train_loss:0.3460960077742736,train_acc:0.8937500715255737
node14 epoch1:node_model train_loss:0.16948386592169604,train_acc:0.9438425302505493
node14 epoch2:node_model train_loss:0.09400152290860812,train_acc:0.9691666960716248
node14 epoch3:node_model train_loss:0.05785683790842692,train_acc:0.9805092215538025
node14 epoch4:node_model train_loss:0.03209152305498719,train_acc:0.9933333396911621
node14_model on test-dataset: loss:0.8237375062704086,acc:0.7894999384880066
node14 weight score:1422.7833394480244
node19: train data size:4281
node19 epoch0:node_model train_loss:0.16121031751119813,train_acc:0.9400690793991089
node19 epoch1:node_model train_loss:0.08127936053761216,train_acc:0.9748291969299316
node19 epoch2:node_model train_loss:0.05095803174515103,train_acc:0.9873873591423035
node19 epoch3:node_model train_loss:0.03684214034745859,train_acc:0.9915733337402344
node19 epoch4:node_model train_loss:0.02957494834134745,train_acc:0.9948292970657349
node19_model on test-dataset: loss:0.8026087929308414,acc:0.7994999289512634
node19 weight score:5333.856341602381
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6562074286490679,acc:0.825699982047081
total cost energy:7.764477664211887 | all_enery_cp：6.1930000000000005 | all_enery_tp: 1.5714776642118866
ef: 25.128043278244725
reward: 17.363565614032836
step 237:loss:23.399639129638672|running q:68.49993896484375
episode3,iteration57 selected nodes:[11, 16, 10, 17, 12],center node:17
################################################## episode3,iteration57 ##################################################
node10: train data size:1975
node10 epoch0:node_model train_loss:0.35212866216897964,train_acc:0.878166675567627
node10 epoch1:node_model train_loss:0.1805551316589117,train_acc:0.9363332986831665
node10 epoch2:node_model train_loss:0.1029235653579235,train_acc:0.9688333868980408
node10 epoch3:node_model train_loss:0.08116391589865088,train_acc:0.9781665802001953
node10 epoch4:node_model train_loss:0.05689800037071109,train_acc:0.9839998483657837
node10_model on test-dataset: loss:0.8397105012834072,acc:0.7914997935295105
node10 weight score:2352.0010729667247
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4681728780269623,train_acc:0.8541320562362671
node11 epoch1:node_model train_loss:0.21672150185879538,train_acc:0.9239310622215271
node11 epoch2:node_model train_loss:0.15327116233461044,train_acc:0.9524245858192444
node11 epoch3:node_model train_loss:0.09221586519304444,train_acc:0.9722237586975098
node11 epoch4:node_model train_loss:0.07283956163069781,train_acc:0.9822237491607666
node11_model on test-dataset: loss:0.8255115796625614,acc:0.7926998138427734
node11 weight score:2037.524416904654
node12: train data size:1336
node12 epoch0:node_model train_loss:0.595423508967672,train_acc:0.8335715532302856
node12 epoch1:node_model train_loss:0.3002321517893246,train_acc:0.9063493013381958
node12 epoch2:node_model train_loss:0.1983277355985982,train_acc:0.9410317540168762
node12 epoch3:node_model train_loss:0.08377253264188766,train_acc:0.9757143259048462
node12 epoch4:node_model train_loss:0.056875478476285934,train_acc:0.9878571033477783
node12_model on test-dataset: loss:0.7934594529867173,acc:0.7986000776290894
node12 weight score:1683.7659378447977
node16: train data size:877
node16 epoch0:node_model train_loss:0.5131823354297214,train_acc:0.8541269302368164
node16 epoch1:node_model train_loss:0.24609233438968658,train_acc:0.9208946228027344
node16 epoch2:node_model train_loss:0.15799487796094683,train_acc:0.9497835636138916
node16 epoch3:node_model train_loss:0.08280991928444968,train_acc:0.9748916625976562
node16 epoch4:node_model train_loss:0.06324370846980149,train_acc:0.9896681308746338
node16_model on test-dataset: loss:0.8859633372724056,acc:0.7800999283790588
node16 weight score:989.8829478655394
node17: train data size:442
node17 epoch0:node_model train_loss:0.7442026436328888,train_acc:0.815142810344696
node17 epoch1:node_model train_loss:0.3376263022422791,train_acc:0.8944762349128723
node17 epoch2:node_model train_loss:0.17787912338972092,train_acc:0.9321904182434082
node17 epoch3:node_model train_loss:0.20169741213321685,train_acc:0.9417142868041992
node17 epoch4:node_model train_loss:0.09265723899006843,train_acc:0.9792380332946777
node17_model on test-dataset: loss:0.9063401164114475,acc:0.7728000283241272
node17 weight score:487.67564405076723
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6538256305456162,acc:0.827199981212616
total cost energy:4.453129758820094 | all_enery_cp：3.1560000000000006 | all_enery_tp: 1.2971297588200936
ef: 24.845197637354513
reward: 20.39206787853442
step 238:loss:30.134984970092773|running q:69.36717987060547
episode3,iteration58 selected nodes:[2, 3, 10, 18, 19],center node:10
################################################## episode3,iteration58 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.39330288395285606,train_acc:0.8712027072906494
node2 epoch1:node_model train_loss:0.20979662301639715,train_acc:0.9279260635375977
node2 epoch2:node_model train_loss:0.1336603257805109,train_acc:0.9585795402526855
node2 epoch3:node_model train_loss:0.09770074432405333,train_acc:0.9680112600326538
node2 epoch4:node_model train_loss:0.06915644265245646,train_acc:0.9791383147239685
node2_model on test-dataset: loss:0.7906016302108765,acc:0.7952000498771667
node2 weight score:6056.147390845755
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3710816097120906,train_acc:0.875462532043457
node3 epoch1:node_model train_loss:0.22878401691830436,train_acc:0.9207818508148193
node3 epoch2:node_model train_loss:0.1398686009437539,train_acc:0.9548539519309998
node3 epoch3:node_model train_loss:0.08776467475433682,train_acc:0.9741265773773193
node3 epoch4:node_model train_loss:0.06859652152241663,train_acc:0.9810734987258911
node3_model on test-dataset: loss:0.776046562641859,acc:0.8030999302864075
node3 weight score:5472.609769112482
node10: train data size:1975
node10 epoch0:node_model train_loss:0.18684734515845774,train_acc:0.936500072479248
node10 epoch1:node_model train_loss:0.12516030203551054,train_acc:0.9551666378974915
node10 epoch2:node_model train_loss:0.07569584902375937,train_acc:0.9779998660087585
node10 epoch3:node_model train_loss:0.051904698740690945,train_acc:0.9899998903274536
node10 epoch4:node_model train_loss:0.03268630867823959,train_acc:0.9959999322891235
node10_model on test-dataset: loss:0.8519854682683945,acc:0.7913998961448669
node10 weight score:2318.1146551877937
node18: train data size:472
node18 epoch0:node_model train_loss:0.5351505994796752,train_acc:0.8526666760444641
node18 epoch1:node_model train_loss:0.28049049973487855,train_acc:0.9221111536026001
node18 epoch2:node_model train_loss:0.16861107647418977,train_acc:0.9576666951179504
node18 epoch3:node_model train_loss:0.09812698476016521,train_acc:0.9792222380638123
node18 epoch4:node_model train_loss:0.03605453800410032,train_acc:0.9924444556236267
node18_model on test-dataset: loss:0.9408680285513401,acc:0.7721999883651733
node18 weight score:501.6644052904435
node19: train data size:4281
node19 epoch0:node_model train_loss:0.1304921146568864,train_acc:0.9556502103805542
node19 epoch1:node_model train_loss:0.07236931165463703,train_acc:0.9765114784240723
node19 epoch2:node_model train_loss:0.05315876925407454,train_acc:0.986046552658081
node19 epoch3:node_model train_loss:0.03870289460864178,train_acc:0.9916279911994934
node19 epoch4:node_model train_loss:0.02634117566049099,train_acc:0.9953489303588867
node19_model on test-dataset: loss:0.8373571237921715,acc:0.7959999442100525
node19 weight score:5112.513978041376
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6461271648108959,acc:0.8290999811887741
total cost energy:10.36417070663223 | all_enery_cp：7.881499999999999 | all_enery_tp: 2.4826707066322307
ef: 24.826474239601485
reward: 14.462303532969255
step 239:loss:69.25535583496094|running q:70.44020080566406
episode3,iteration59 selected nodes:[4, 9, 0, 6, 10],center node:6
################################################## episode3,iteration59 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.12061894133400458,train_acc:0.9606510400772095
node0 epoch1:node_model train_loss:0.06225098425952288,train_acc:0.9787672758102417
node0 epoch2:node_model train_loss:0.04223755238434443,train_acc:0.9903059601783752
node0 epoch3:node_model train_loss:0.026976269284764733,train_acc:0.9944231510162354
node0 epoch4:node_model train_loss:0.022855118543912586,train_acc:0.9959616661071777
node0_model on test-dataset: loss:0.8149114513397216,acc:0.8040000200271606
node0 weight score:6360.2002297048375
node4: train data size:2705
node4 epoch0:node_model train_loss:0.41505871659943033,train_acc:0.8653571605682373
node4 epoch1:node_model train_loss:0.2648671828210354,train_acc:0.9235713481903076
node4 epoch2:node_model train_loss:0.18157938814588956,train_acc:0.9396429657936096
node4 epoch3:node_model train_loss:0.09545253470007863,train_acc:0.9749999046325684
node4 epoch4:node_model train_loss:0.05808789908353772,train_acc:0.9874998331069946
node4_model on test-dataset: loss:0.8095568303763866,acc:0.7934999465942383
node4 weight score:3341.334293655909
node6: train data size:3007
node6 epoch0:node_model train_loss:0.2802767665876496,train_acc:0.9058064818382263
node6 epoch1:node_model train_loss:0.12239097794818302,train_acc:0.9606450796127319
node6 epoch2:node_model train_loss:0.07483643413551393,train_acc:0.9796770811080933
node6 epoch3:node_model train_loss:0.06411138026704712,train_acc:0.9802302122116089
node6 epoch4:node_model train_loss:0.09097834880794248,train_acc:0.9767740368843079
node6_model on test-dataset: loss:0.8782754984498023,acc:0.7827998995780945
node6 weight score:3423.7548528992293
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4258009289440356,train_acc:0.863259494304657
node9 epoch1:node_model train_loss:0.18664120294545827,train_acc:0.9402586221694946
node9 epoch2:node_model train_loss:0.122544340985386,train_acc:0.9599999189376831
node9 epoch3:node_model train_loss:0.06895284898775189,train_acc:0.9817080497741699
node9 epoch4:node_model train_loss:0.051972212367936185,train_acc:0.985650897026062
node9_model on test-dataset: loss:0.8277109139412642,acc:0.7958003282546997
node9 weight score:2243.536926627714
node10: train data size:1975
node10 epoch0:node_model train_loss:0.23274137042462825,train_acc:0.9194999933242798
node10 epoch1:node_model train_loss:0.11737387310713529,train_acc:0.9614999890327454
node10 epoch2:node_model train_loss:0.07333611808717251,train_acc:0.9806665778160095
node10 epoch3:node_model train_loss:0.037113807955756785,train_acc:0.9920000433921814
node10 epoch4:node_model train_loss:0.03211155384778976,train_acc:0.9924999475479126
node10_model on test-dataset: loss:0.8098887008428574,acc:0.7927998900413513
node10 weight score:2438.606685023019
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6716538263857365,acc:0.828199982047081
total cost energy:8.766146615193176 | all_enery_cp：7.3635 | all_enery_tp: 1.402646615193176
ef: 25.142368071845745
reward: 16.37622145665257
step 240:loss:22.660770416259766|running q:71.29057312011719
episode3_cost time: 25426.80711531639
episode4,iteration0 selected nodes:[11, 14, 1, 6, 17],center node:11
################################################## episode4,iteration0 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:1.8777664040817934,train_acc:0.3506617844104767
node1 epoch1:node_model train_loss:1.445014452233034,train_acc:0.48838233947753906
node1 epoch2:node_model train_loss:1.3058073415475733,train_acc:0.5425735712051392
node1 epoch3:node_model train_loss:1.2411294982713812,train_acc:0.5698529481887817
node1 epoch4:node_model train_loss:1.1282520785051233,train_acc:0.6063234806060791
node1_model on test-dataset: loss:1.48945801705122,acc:0.5156000256538391
node1 weight score:4503.65161233633
node6: train data size:3007
node6 epoch0:node_model train_loss:2.094454776856207,train_acc:0.28115206956863403
node6 epoch1:node_model train_loss:1.710162628081537,train_acc:0.39382481575012207
node6 epoch2:node_model train_loss:1.561213416437949,train_acc:0.43576034903526306
node6 epoch3:node_model train_loss:1.4247906592584425,train_acc:0.4854377806186676
node6 epoch4:node_model train_loss:1.394552053943757,train_acc:0.4989861845970154
node6_model on test-dataset: loss:1.5563524040579795,acc:0.45760008692741394
node6 weight score:1932.081700879346
node11: train data size:1682
node11 epoch0:node_model train_loss:2.335858800831963,train_acc:0.23571017384529114
node11 epoch1:node_model train_loss:1.7567639350891113,train_acc:0.35100436210632324
node11 epoch2:node_model train_loss:1.566383775542764,train_acc:0.44137734174728394
node11 epoch3:node_model train_loss:1.4107279216541964,train_acc:0.4950214922428131
node11 epoch4:node_model train_loss:1.2771500769783468,train_acc:0.5326112508773804
node11_model on test-dataset: loss:1.5746245634555818,acc:0.44809994101524353
node11 weight score:1068.1911352308503
node14: train data size:1172
node14 epoch0:node_model train_loss:2.421807567278544,train_acc:0.2038888931274414
node14 epoch1:node_model train_loss:1.7741113404432933,train_acc:0.35879623889923096
node14 epoch2:node_model train_loss:1.5732103288173676,train_acc:0.4362037181854248
node14 epoch3:node_model train_loss:1.4442101021607716,train_acc:0.4857407212257385
node14 epoch4:node_model train_loss:1.293866087992986,train_acc:0.5264352560043335
node14_model on test-dataset: loss:1.6011248922348023,acc:0.4177999794483185
node14 weight score:731.9853720868442
node17: train data size:442
node17 epoch0:node_model train_loss:3.0129071712493896,train_acc:0.16609524190425873
node17 epoch1:node_model train_loss:2.305480146408081,train_acc:0.24438095092773438
node17 epoch2:node_model train_loss:1.977125096321106,train_acc:0.27580952644348145
node17 epoch3:node_model train_loss:1.7580482244491578,train_acc:0.3616190552711487
node17 epoch4:node_model train_loss:1.6463966131210328,train_acc:0.4041905105113983
node17_model on test-dataset: loss:2.222408183813095,acc:0.2564000189304352
node17 weight score:198.88335690054868
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.4163933461904525,acc:0.4889999887347221
total cost energy:7.848897840021019 | all_enery_cp：6.5055000000000005 | all_enery_tp: 1.343397840021018
ef: 23.338116824026752
reward: 15.489218984005733
step 241:loss:49.234867095947266|running q:1.419761300086975
episode4,iteration1 selected nodes:[0, 9, 19, 11, 5],center node:9
################################################## episode4,iteration1 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:1.3436011190597827,train_acc:0.531116783618927
node0 epoch1:node_model train_loss:1.1302272104299986,train_acc:0.5991681814193726
node0 epoch2:node_model train_loss:1.0366565470512097,train_acc:0.6332507133483887
node0 epoch3:node_model train_loss:0.9279919988833941,train_acc:0.6761700510978699
node0 epoch4:node_model train_loss:0.8726213264923829,train_acc:0.6962835788726807
node0_model on test-dataset: loss:1.1663688138127326,acc:0.6127999424934387
node0 weight score:4443.705917562505
node5: train data size:3735
node5 epoch0:node_model train_loss:1.3736567591366016,train_acc:0.5227066874504089
node5 epoch1:node_model train_loss:1.1212174390491687,train_acc:0.597067654132843
node5 epoch2:node_model train_loss:0.9915401731666765,train_acc:0.6544359922409058
node5 epoch3:node_model train_loss:0.9274995279939551,train_acc:0.6744361519813538
node5 epoch4:node_model train_loss:0.8940540614881014,train_acc:0.6866916418075562
node5_model on test-dataset: loss:1.1854472583532334,acc:0.6139999628067017
node5 weight score:3150.709551758957
node9: train data size:1857
node9 epoch0:node_model train_loss:1.4902243237746389,train_acc:0.48388731479644775
node9 epoch1:node_model train_loss:1.1584720642943132,train_acc:0.6069344282150269
node9 epoch2:node_model train_loss:0.9925914563630757,train_acc:0.6549676060676575
node9 epoch3:node_model train_loss:0.9028785134616651,train_acc:0.6861402988433838
node9 epoch4:node_model train_loss:0.8123998108663057,train_acc:0.7096860408782959
node9_model on test-dataset: loss:1.2507900154590608,acc:0.5641000270843506
node9 weight score:1484.6616754599293
node11: train data size:1682
node11 epoch0:node_model train_loss:1.5261130122577442,train_acc:0.486929714679718
node11 epoch1:node_model train_loss:1.136033065178815,train_acc:0.601492166519165
node11 epoch2:node_model train_loss:0.9812207712846643,train_acc:0.6582208871841431
node11 epoch3:node_model train_loss:0.8761133306166705,train_acc:0.6790673732757568
node11 epoch4:node_model train_loss:0.7850370897966272,train_acc:0.7332424521446228
node11_model on test-dataset: loss:1.1839627094566822,acc:0.5957998633384705
node11 weight score:1420.6528521256093
node19: train data size:4281
node19 epoch0:node_model train_loss:1.3214074913845506,train_acc:0.5372007489204407
node19 epoch1:node_model train_loss:1.1204486170480417,train_acc:0.6010192036628723
node19 epoch2:node_model train_loss:0.9879374088242997,train_acc:0.6484036445617676
node19 epoch3:node_model train_loss:0.9159370134043139,train_acc:0.6724116802215576
node19 epoch4:node_model train_loss:0.8246656171111173,train_acc:0.699744462966919
node19_model on test-dataset: loss:1.060144872367382,acc:0.6450999975204468
node19 weight score:4038.12734616186
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9214838200807571,acc:0.6817999842762947
total cost energy:9.809224255080032 | all_enery_cp：8.369 | all_enery_tp: 1.4402242550800328
ef: 24.37435367608317
reward: 14.565129421003139
step 242:loss:26.686479568481445|running q:2.9363856315612793
episode4,iteration2 selected nodes:[4, 0, 6, 5, 10],center node:6
################################################## episode4,iteration2 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:1.0687775566027715,train_acc:0.6272404789924622
node0 epoch1:node_model train_loss:0.9054717742479764,train_acc:0.6865500211715698
node0 epoch2:node_model train_loss:0.8218016177415848,train_acc:0.7139017581939697
node0 epoch3:node_model train_loss:0.7666168980873548,train_acc:0.7356279492378235
node0 epoch4:node_model train_loss:0.702929899096489,train_acc:0.7547889947891235
node0_model on test-dataset: loss:1.0637399977445603,acc:0.6470998525619507
node0 weight score:4872.431243527059
node4: train data size:2705
node4 epoch0:node_model train_loss:1.1695792313132967,train_acc:0.6142857670783997
node4 epoch1:node_model train_loss:1.036076850124768,train_acc:0.626428484916687
node4 epoch2:node_model train_loss:0.9072629745517459,train_acc:0.6850000023841858
node4 epoch3:node_model train_loss:0.8047992948974881,train_acc:0.7199999690055847
node4 epoch4:node_model train_loss:0.7051442467740604,train_acc:0.7553570866584778
node4_model on test-dataset: loss:1.1086492639780046,acc:0.6238999366760254
node4 weight score:2439.9060080498703
node5: train data size:3735
node5 epoch0:node_model train_loss:1.087920899453916,train_acc:0.6226316094398499
node5 epoch1:node_model train_loss:0.9238132128590032,train_acc:0.6811652779579163
node5 epoch2:node_model train_loss:0.8291829165659452,train_acc:0.7111653089523315
node5 epoch3:node_model train_loss:0.7395609381951784,train_acc:0.746616542339325
node5 epoch4:node_model train_loss:0.6795391699201182,train_acc:0.7656015157699585
node5_model on test-dataset: loss:1.101770211532712,acc:0.6398999094963074
node5 weight score:3389.999076852974
node6: train data size:3007
node6 epoch0:node_model train_loss:1.233528329480079,train_acc:0.5905067920684814
node6 epoch1:node_model train_loss:1.0411834486069218,train_acc:0.6430414915084839
node6 epoch2:node_model train_loss:0.9564207773054799,train_acc:0.6632717847824097
node6 epoch3:node_model train_loss:0.8599609636491344,train_acc:0.6989400386810303
node6 epoch4:node_model train_loss:0.7611947386495529,train_acc:0.7380183935165405
node6_model on test-dataset: loss:1.1709324171394109,acc:0.6010999083518982
node6 weight score:2568.0389030018523
node10: train data size:1975
node10 epoch0:node_model train_loss:1.255237066745758,train_acc:0.575666606426239
node10 epoch1:node_model train_loss:0.9588687062263489,train_acc:0.6639999747276306
node10 epoch2:node_model train_loss:0.8173847317695617,train_acc:0.7138332724571228
node10 epoch3:node_model train_loss:0.6961280226707458,train_acc:0.7639999389648438
node10 epoch4:node_model train_loss:0.7001527741551399,train_acc:0.762333333492279
node10_model on test-dataset: loss:1.059878175854683,acc:0.6345000863075256
node10 weight score:1863.4217073177917
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8101293796300888,acc:0.7190999835729599
total cost energy:9.805146615193177 | all_enery_cp：8.3025 | all_enery_tp: 1.5026466151931759
ef: 24.43803751217693
reward: 14.632890896983753
step 243:loss:38.42536926269531|running q:4.423826694488525
episode4,iteration3 selected nodes:[19, 10, 0, 6, 13],center node:10
################################################## episode4,iteration3 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.9217207110845126,train_acc:0.6902432441711426
node0 epoch1:node_model train_loss:0.729791053212606,train_acc:0.7381371855735779
node0 epoch2:node_model train_loss:0.6906098803648582,train_acc:0.7590986490249634
node0 epoch3:node_model train_loss:0.6144162766062297,train_acc:0.789330244064331
node0 epoch4:node_model train_loss:0.6216556080258809,train_acc:0.7802131772041321
node0_model on test-dataset: loss:1.0697180488705635,acc:0.6565999388694763
node0 weight score:4845.201972119988
node6: train data size:3007
node6 epoch0:node_model train_loss:1.0499766180592198,train_acc:0.6362672448158264
node6 epoch1:node_model train_loss:0.7924269082084778,train_acc:0.7266820669174194
node6 epoch2:node_model train_loss:0.7148296583083368,train_acc:0.7488478422164917
node6 epoch3:node_model train_loss:0.7441175781911419,train_acc:0.7408755421638489
node6 epoch4:node_model train_loss:0.6321680228556356,train_acc:0.7844240665435791
node6_model on test-dataset: loss:1.027325801551342,acc:0.6615999937057495
node6 weight score:2927.016916599579
node10: train data size:1975
node10 epoch0:node_model train_loss:1.0701520830392837,train_acc:0.6318333148956299
node10 epoch1:node_model train_loss:0.8026668637990951,train_acc:0.7206667065620422
node10 epoch2:node_model train_loss:0.674273669719696,train_acc:0.7691667079925537
node10 epoch3:node_model train_loss:0.6503216400742531,train_acc:0.7766667604446411
node10 epoch4:node_model train_loss:0.5918438106775283,train_acc:0.8018333315849304
node10_model on test-dataset: loss:1.0974191492795944,acc:0.6317000985145569
node10 weight score:1799.6769978877237
node13: train data size:1155
node13 epoch0:node_model train_loss:1.1199022680521011,train_acc:0.6402272582054138
node13 epoch1:node_model train_loss:0.881613110502561,train_acc:0.7072727084159851
node13 epoch2:node_model train_loss:0.7143085524439812,train_acc:0.7464393973350525
node13 epoch3:node_model train_loss:0.5732070232431093,train_acc:0.8056060671806335
node13 epoch4:node_model train_loss:0.5157605434457461,train_acc:0.8375000357627869
node13_model on test-dataset: loss:1.0827693903446198,acc:0.6475999355316162
node13 weight score:1066.7091352041184
node19: train data size:4281
node19 epoch0:node_model train_loss:0.9652906878049984,train_acc:0.6633015871047974
node19 epoch1:node_model train_loss:0.8276188623073489,train_acc:0.7086907029151917
node19 epoch2:node_model train_loss:0.7844817679981853,train_acc:0.7252569198608398
node19 epoch3:node_model train_loss:0.6486653761808262,train_acc:0.774039626121521
node19 epoch4:node_model train_loss:0.5978018863256588,train_acc:0.7907838821411133
node19_model on test-dataset: loss:1.079447887390852,acc:0.6511999368667603
node19 weight score:3965.9163263060923
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7899232912063598,acc:0.7257999819517136
total cost energy:10.025642367670226 | all_enery_cp：7.8004999999999995 | all_enery_tp: 2.2251423676702258
ef: 24.36448725105797
reward: 14.338844883387743
step 244:loss:23.748640060424805|running q:5.98537540435791
episode4,iteration4 selected nodes:[18, 4, 5, 14, 19],center node:14
################################################## episode4,iteration4 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.940233085836683,train_acc:0.682857096195221
node4 epoch1:node_model train_loss:0.8234433829784393,train_acc:0.7132143378257751
node4 epoch2:node_model train_loss:0.7668481256280627,train_acc:0.7285714745521545
node4 epoch3:node_model train_loss:0.748222787465368,train_acc:0.7317857146263123
node4 epoch4:node_model train_loss:0.6379429389323507,train_acc:0.770357072353363
node4_model on test-dataset: loss:1.0198004300892354,acc:0.6607998609542847
node4 weight score:2652.4797599499984
node5: train data size:3735
node5 epoch0:node_model train_loss:0.9556741855646435,train_acc:0.6800751686096191
node5 epoch1:node_model train_loss:0.7098252836026644,train_acc:0.7553383111953735
node5 epoch2:node_model train_loss:0.645706276360311,train_acc:0.7804886698722839
node5 epoch3:node_model train_loss:0.5704862012674934,train_acc:0.8049248456954956
node5 epoch4:node_model train_loss:0.5102680827441969,train_acc:0.8270301222801208
node5_model on test-dataset: loss:0.9651469305157662,acc:0.6789999008178711
node5 weight score:3869.877095297861
node14: train data size:1172
node14 epoch0:node_model train_loss:1.0116340965032578,train_acc:0.6587036848068237
node14 epoch1:node_model train_loss:0.7244563897450765,train_acc:0.7391666769981384
node14 epoch2:node_model train_loss:0.5553255851070086,train_acc:0.8222686052322388
node14 epoch3:node_model train_loss:0.41561142851909,train_acc:0.8693981170654297
node14 epoch4:node_model train_loss:0.3495124379793803,train_acc:0.8907407522201538
node14_model on test-dataset: loss:0.9722244434058667,acc:0.6765000224113464
node14 weight score:1205.4829601838499
node18: train data size:472
node18 epoch0:node_model train_loss:0.9882488250732422,train_acc:0.6721110939979553
node18 epoch1:node_model train_loss:0.7348739862442016,train_acc:0.7595555186271667
node18 epoch2:node_model train_loss:0.5703303933143615,train_acc:0.8083333373069763
node18 epoch3:node_model train_loss:0.4192152500152588,train_acc:0.8625555038452148
node18 epoch4:node_model train_loss:0.3149280399084091,train_acc:0.9130000472068787
node18_model on test-dataset: loss:1.1119421845674515,acc:0.6534999012947083
node18 weight score:424.48250147430934
node19: train data size:4281
node19 epoch0:node_model train_loss:0.829949535602747,train_acc:0.7114814519882202
node19 epoch1:node_model train_loss:0.6970014884028324,train_acc:0.7576771974563599
node19 epoch2:node_model train_loss:0.6002159700837246,train_acc:0.7959545850753784
node19 epoch3:node_model train_loss:0.5614242539849392,train_acc:0.8054609298706055
node19 epoch4:node_model train_loss:0.5176909510479417,train_acc:0.8172953128814697
node19_model on test-dataset: loss:0.9617623871564865,acc:0.6816999912261963
node19 weight score:4451.2033919906735
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7478600792586804,acc:0.7421999809145927
total cost energy:8.078494209933725 | all_enery_cp：6.182499999999999 | all_enery_tp: 1.8959942099337261
ef: 24.39451893120666
reward: 16.31602472127294
step 245:loss:16.768369674682617|running q:7.52346658706665
episode4,iteration5 selected nodes:[9, 17, 3, 12, 15],center node:9
################################################## episode4,iteration5 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.9344858915306801,train_acc:0.6811577677726746
node3 epoch1:node_model train_loss:0.7406346409819847,train_acc:0.7417961359024048
node3 epoch2:node_model train_loss:0.6419651972693067,train_acc:0.775660514831543
node3 epoch3:node_model train_loss:0.6258840588636176,train_acc:0.7790350914001465
node3 epoch4:node_model train_loss:0.522000334290571,train_acc:0.8192082643508911
node3_model on test-dataset: loss:1.0306881742179395,acc:0.6728000640869141
node3 weight score:4120.547907928135
node9: train data size:1857
node9 epoch0:node_model train_loss:1.015644776193719,train_acc:0.6889103651046753
node9 epoch1:node_model train_loss:0.7352254186805925,train_acc:0.756417453289032
node9 epoch2:node_model train_loss:0.5671465192970476,train_acc:0.801034152507782
node9 epoch3:node_model train_loss:0.49460425031812566,train_acc:0.8260295391082764
node9 epoch4:node_model train_loss:0.41749503110584457,train_acc:0.8601108193397522
node9_model on test-dataset: loss:0.9725177831947803,acc:0.6828998923301697
node9 weight score:1909.4766513159702
node12: train data size:1336
node12 epoch0:node_model train_loss:1.061245743717466,train_acc:0.6465079188346863
node12 epoch1:node_model train_loss:0.7500923893281392,train_acc:0.7372222542762756
node12 epoch2:node_model train_loss:0.5765365830489567,train_acc:0.8009523153305054
node12 epoch3:node_model train_loss:0.46833834264959606,train_acc:0.8379365801811218
node12 epoch4:node_model train_loss:0.4097321948834828,train_acc:0.8676191568374634
node12_model on test-dataset: loss:0.9930023837089539,acc:0.6814000606536865
node12 weight score:1345.4146957934975
node15: train data size:629
node15 epoch0:node_model train_loss:1.2129939879689897,train_acc:0.6086699962615967
node15 epoch1:node_model train_loss:0.8756681340081351,train_acc:0.6950246691703796
node15 epoch2:node_model train_loss:0.5936656722000667,train_acc:0.8066502213478088
node15 epoch3:node_model train_loss:0.4532424977847508,train_acc:0.8710837960243225
node15 epoch4:node_model train_loss:0.4017391928604671,train_acc:0.8661577105522156
node15_model on test-dataset: loss:1.0024842128157616,acc:0.678199827671051
node15 weight score:627.4413022757484
node17: train data size:442
node17 epoch0:node_model train_loss:1.0490798354148865,train_acc:0.6653333306312561
node17 epoch1:node_model train_loss:0.7126992821693421,train_acc:0.7468571662902832
node17 epoch2:node_model train_loss:0.5495719134807586,train_acc:0.8063809275627136
node17 epoch3:node_model train_loss:0.4126773178577423,train_acc:0.8881905674934387
node17 epoch4:node_model train_loss:0.2923723071813583,train_acc:0.9184762239456177
node17_model on test-dataset: loss:1.066408623456955,acc:0.6547999382019043
node17 weight score:414.4752679954684
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8070053113996982,acc:0.730099981725216
total cost energy:6.422719397135701 | all_enery_cp：4.2555 | all_enery_tp: 2.167219397135702
ef: 23.86332872578761
reward: 17.44060932865191
step 246:loss:40.80691146850586|running q:8.907143592834473
episode4,iteration6 selected nodes:[14, 0, 12, 5, 1],center node:1
################################################## episode4,iteration6 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.7850060474414092,train_acc:0.7271756529808044
node0 epoch1:node_model train_loss:0.6308772758795664,train_acc:0.7799074053764343
node0 epoch2:node_model train_loss:0.565663738319507,train_acc:0.8056373000144958
node0 epoch3:node_model train_loss:0.5205170380381438,train_acc:0.821795642375946
node0 epoch4:node_model train_loss:0.4601323323754164,train_acc:0.8415247201919556
node0_model on test-dataset: loss:0.8636340317130089,acc:0.7191998958587646
node0 weight score:6001.384625521964
node1: train data size:6708
node1 epoch0:node_model train_loss:0.8816616771852269,train_acc:0.7076469659805298
node1 epoch1:node_model train_loss:0.7574073251555947,train_acc:0.7511029243469238
node1 epoch2:node_model train_loss:0.6399660325225662,train_acc:0.778161883354187
node1 epoch3:node_model train_loss:0.574237144606955,train_acc:0.8021323680877686
node1 epoch4:node_model train_loss:0.6136975091169862,train_acc:0.7877942323684692
node1_model on test-dataset: loss:1.2815402440726757,acc:0.6222999691963196
node1 weight score:5234.326452895686
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7943045559682345,train_acc:0.7280450463294983
node5 epoch1:node_model train_loss:0.6228967112930197,train_acc:0.7889097332954407
node5 epoch2:node_model train_loss:0.5420963638707211,train_acc:0.8075563311576843
node5 epoch3:node_model train_loss:0.4805799965795718,train_acc:0.8349624872207642
node5 epoch4:node_model train_loss:0.43460709327145625,train_acc:0.8491728901863098
node5_model on test-dataset: loss:0.9441798759996891,acc:0.6907000541687012
node5 weight score:3955.81402965766
node12: train data size:1336
node12 epoch0:node_model train_loss:0.9248015539986747,train_acc:0.7026984095573425
node12 epoch1:node_model train_loss:0.6250962295702526,train_acc:0.7937301397323608
node12 epoch2:node_model train_loss:0.44180428981781006,train_acc:0.8593651056289673
node12 epoch3:node_model train_loss:0.3953944912978581,train_acc:0.8723809719085693
node12 epoch4:node_model train_loss:0.31156421984945026,train_acc:0.9076191782951355
node12_model on test-dataset: loss:1.010067661702633,acc:0.6887997984886169
node12 weight score:1322.683668288078
node14: train data size:1172
node14 epoch0:node_model train_loss:0.8888716995716095,train_acc:0.7085647583007812
node14 epoch1:node_model train_loss:0.5909753342469534,train_acc:0.8025925159454346
node14 epoch2:node_model train_loss:0.5169040784239769,train_acc:0.8166204690933228
node14 epoch3:node_model train_loss:0.37409449368715286,train_acc:0.8675925731658936
node14 epoch4:node_model train_loss:0.292300986746947,train_acc:0.9118518233299255
node14_model on test-dataset: loss:0.8693622027337551,acc:0.7127000093460083
node14 weight score:1348.1147401101457
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7187994185090065,acc:0.758099981546402
total cost energy:10.955854381999831 | all_enery_cp：9.067 | all_enery_tp: 1.888854381999832
ef: 24.439303015394085
reward: 13.483448633394254
step 247:loss:15.197309494018555|running q:10.3246488571167
episode4,iteration7 selected nodes:[9, 15, 16, 1, 18],center node:15
################################################## episode4,iteration7 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.7364632154212278,train_acc:0.7533088326454163
node1 epoch1:node_model train_loss:0.6236444461871596,train_acc:0.7893381714820862
node1 epoch2:node_model train_loss:0.5757853292367038,train_acc:0.7970588803291321
node1 epoch3:node_model train_loss:0.5981603025951806,train_acc:0.7883822917938232
node1 epoch4:node_model train_loss:0.47989321379538846,train_acc:0.8377941250801086
node1_model on test-dataset: loss:0.8538030390441418,acc:0.7248998880386353
node1 weight score:7856.612934418467
node9: train data size:1857
node9 epoch0:node_model train_loss:0.8441008172537151,train_acc:0.7191689014434814
node9 epoch1:node_model train_loss:0.5717165454437858,train_acc:0.8049677610397339
node9 epoch2:node_model train_loss:0.4416043742706901,train_acc:0.8626132011413574
node9 epoch3:node_model train_loss:0.3783079079891506,train_acc:0.8784118294715881
node9 epoch4:node_model train_loss:0.3230884122221093,train_acc:0.8898613452911377
node9_model on test-dataset: loss:0.8971178840100765,acc:0.7092000246047974
node9 weight score:2069.9620786727533
node15: train data size:629
node15 epoch0:node_model train_loss:0.9753861682755607,train_acc:0.6758127808570862
node15 epoch1:node_model train_loss:0.6150830004896436,train_acc:0.7905910611152649
node15 epoch2:node_model train_loss:0.4918555063860757,train_acc:0.8452216982841492
node15 epoch3:node_model train_loss:0.38026741998536245,train_acc:0.8753695487976074
node15 epoch4:node_model train_loss:0.3260350057056972,train_acc:0.8895074129104614
node15_model on test-dataset: loss:0.9893863663077355,acc:0.6925000548362732
node15 weight score:635.7475920628948
node16: train data size:877
node16 epoch0:node_model train_loss:0.9775106774436103,train_acc:0.6961472034454346
node16 epoch1:node_model train_loss:0.5722037984265221,train_acc:0.8085713982582092
node16 epoch2:node_model train_loss:0.5114509330855476,train_acc:0.8263492584228516
node16 epoch3:node_model train_loss:0.41285644306076896,train_acc:0.8744588494300842
node16 epoch4:node_model train_loss:0.3670734895600213,train_acc:0.8830159306526184
node16_model on test-dataset: loss:1.0338588613271713,acc:0.6799001693725586
node16 weight score:848.2782638958952
node18: train data size:472
node18 epoch0:node_model train_loss:0.7601735353469848,train_acc:0.7496666312217712
node18 epoch1:node_model train_loss:0.43648816347122193,train_acc:0.8417777419090271
node18 epoch2:node_model train_loss:0.3552685916423798,train_acc:0.8821111917495728
node18 epoch3:node_model train_loss:0.2886551797389984,train_acc:0.8985555768013
node18 epoch4:node_model train_loss:0.21161895394325256,train_acc:0.9436666369438171
node18_model on test-dataset: loss:0.9713641104102134,acc:0.6921999454498291
node18 weight score:485.9145967423805
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7163056474924088,acc:0.7619999819993972
total cost energy:7.0258203766865055 | all_enery_cp：5.2715 | all_enery_tp: 1.7543203766865056
ef: 24.5058189508695
reward: 17.479998574182993
step 248:loss:17.30093765258789|running q:11.808348655700684
episode4,iteration8 selected nodes:[15, 7, 0, 4, 19],center node:7
################################################## episode4,iteration8 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.7201357793349487,train_acc:0.7562094926834106
node0 epoch1:node_model train_loss:0.5380624349300678,train_acc:0.8128752708435059
node0 epoch2:node_model train_loss:0.48430854827165604,train_acc:0.8298771381378174
node0 epoch3:node_model train_loss:0.44043236627028537,train_acc:0.8446805477142334
node0 epoch4:node_model train_loss:0.3980776839531385,train_acc:0.8638021945953369
node0_model on test-dataset: loss:0.8328924061357975,acc:0.7350998520851135
node0 weight score:6222.892611119504
node4: train data size:2705
node4 epoch0:node_model train_loss:0.86444733611175,train_acc:0.7189284563064575
node4 epoch1:node_model train_loss:0.7106795055525643,train_acc:0.7453571557998657
node4 epoch2:node_model train_loss:0.688755815582616,train_acc:0.7564285397529602
node4 epoch3:node_model train_loss:0.4783291617142303,train_acc:0.8314284682273865
node4 epoch4:node_model train_loss:0.4288955969469888,train_acc:0.8628571629524231
node4_model on test-dataset: loss:0.9389263460785151,acc:0.6951998472213745
node4 weight score:2880.950152583962
node7: train data size:1951
node7 epoch0:node_model train_loss:0.8680778324604035,train_acc:0.718294084072113
node7 epoch1:node_model train_loss:0.6275803357362747,train_acc:0.7962745428085327
node7 epoch2:node_model train_loss:0.4821805953979492,train_acc:0.8321176767349243
node7 epoch3:node_model train_loss:0.4182415157556534,train_acc:0.8561568260192871
node7 epoch4:node_model train_loss:0.33132199123501777,train_acc:0.8911373019218445
node7_model on test-dataset: loss:0.8880894249677658,acc:0.7150999307632446
node7 weight score:2196.8508408607768
node15: train data size:629
node15 epoch0:node_model train_loss:0.9456759095191956,train_acc:0.6991626024246216
node15 epoch1:node_model train_loss:0.6174900872366769,train_acc:0.8017241954803467
node15 epoch2:node_model train_loss:0.4833858140877315,train_acc:0.833152711391449
node15 epoch3:node_model train_loss:0.4069729064192091,train_acc:0.8626601696014404
node15 epoch4:node_model train_loss:0.27240869615759167,train_acc:0.9107882380485535
node15_model on test-dataset: loss:0.9999230839312077,acc:0.6843000054359436
node15 weight score:629.0483839287721
node19: train data size:4281
node19 epoch0:node_model train_loss:0.7942267476126205,train_acc:0.7358455061912537
node19 epoch1:node_model train_loss:0.5939576008985209,train_acc:0.7923974394798279
node19 epoch2:node_model train_loss:0.4906721420066301,train_acc:0.8312603831291199
node19 epoch3:node_model train_loss:0.43242799334747845,train_acc:0.8590580821037292
node19 epoch4:node_model train_loss:0.37947221689446026,train_acc:0.8715331554412842
node19_model on test-dataset: loss:0.9383503459393978,acc:0.6994999647140503
node19 weight score:4562.26186575785
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6744199411571026,acc:0.7771999818086625
total cost energy:9.47825378526008 | all_enery_cp：7.374499999999999 | all_enery_tp: 2.103753785260081
ef: 24.65257280423576
reward: 15.17431901897568
step 249:loss:20.512880325317383|running q:13.174530029296875
episode4,iteration9 selected nodes:[4, 0, 13, 11, 3],center node:3
################################################## episode4,iteration9 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5479859067843511,train_acc:0.800987184047699
node0 epoch1:node_model train_loss:0.4397852776142267,train_acc:0.8436446785926819
node0 epoch2:node_model train_loss:0.3819645336614205,train_acc:0.8721849918365479
node0 epoch3:node_model train_loss:0.33284036070108414,train_acc:0.8871107697486877
node0 epoch4:node_model train_loss:0.3321153083099769,train_acc:0.8864896297454834
node0_model on test-dataset: loss:0.8669303604215384,acc:0.7281999588012695
node0 weight score:5978.565564920123
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7365392501964125,train_acc:0.7567095160484314
node3 epoch1:node_model train_loss:0.5350585143233455,train_acc:0.8152251839637756
node3 epoch2:node_model train_loss:0.45849684365960053,train_acc:0.841068685054779
node3 epoch3:node_model train_loss:0.41075989603996277,train_acc:0.8587183356285095
node3 epoch4:node_model train_loss:0.3490865774625956,train_acc:0.8811876177787781
node3_model on test-dataset: loss:0.8251578898727894,acc:0.739599883556366
node3 weight score:5146.893766785335
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7496149103556361,train_acc:0.7399999499320984
node4 epoch1:node_model train_loss:0.6053214105112212,train_acc:0.7807142734527588
node4 epoch2:node_model train_loss:0.5626444816589355,train_acc:0.8028571009635925
node4 epoch3:node_model train_loss:0.6229957118630409,train_acc:0.7875000834465027
node4 epoch4:node_model train_loss:0.4545771900032248,train_acc:0.8410714268684387
node4_model on test-dataset: loss:0.9613866755366325,acc:0.7027999758720398
node4 weight score:2813.6441546686788
node11: train data size:1682
node11 epoch0:node_model train_loss:0.8139651137239793,train_acc:0.7338880896568298
node11 epoch1:node_model train_loss:0.5900796949863434,train_acc:0.8000860810279846
node11 epoch2:node_model train_loss:0.4479215969057644,train_acc:0.8583213686943054
node11 epoch3:node_model train_loss:0.34011789455133323,train_acc:0.8889095783233643
node11 epoch4:node_model train_loss:0.26699306158458486,train_acc:0.9288952350616455
node11_model on test-dataset: loss:0.8187494659423828,acc:0.7291999459266663
node11 weight score:2054.3524850596555
node13: train data size:1155
node13 epoch0:node_model train_loss:0.9073998431364695,train_acc:0.7157575488090515
node13 epoch1:node_model train_loss:0.5685083046555519,train_acc:0.8105303049087524
node13 epoch2:node_model train_loss:0.4527205377817154,train_acc:0.8421969413757324
node13 epoch3:node_model train_loss:0.3049357322355111,train_acc:0.8994697332382202
node13 epoch4:node_model train_loss:0.2491565483311812,train_acc:0.9396212100982666
node13_model on test-dataset: loss:0.816775581240654,acc:0.7313998937606812
node13 weight score:1414.0971235276095
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6372324913740158,acc:0.7907999795675278
total cost energy:9.054831380576221 | all_enery_cp：7.486 | all_enery_tp: 1.568831380576221
ef: 24.924185061050746
reward: 15.869353680474525
step 250:loss:34.793785095214844|running q:14.507320404052734
episode4,iteration10 selected nodes:[10, 19, 6, 9, 2],center node:9
################################################## episode4,iteration10 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.7980676889419556,train_acc:0.7420170307159424
node2 epoch1:node_model train_loss:0.5926037083069483,train_acc:0.8018938302993774
node2 epoch2:node_model train_loss:0.507526176671187,train_acc:0.8284471035003662
node2 epoch3:node_model train_loss:0.43923309445381165,train_acc:0.8447443842887878
node2 epoch4:node_model train_loss:0.41392704161504906,train_acc:0.8628787994384766
node2_model on test-dataset: loss:0.8137568014860154,acc:0.7407000660896301
node2 weight score:5883.8217895771195
node6: train data size:3007
node6 epoch0:node_model train_loss:0.8213982024500447,train_acc:0.7448847889900208
node6 epoch1:node_model train_loss:0.5935115847856768,train_acc:0.7870046496391296
node6 epoch2:node_model train_loss:0.5057338245453373,train_acc:0.8332718014717102
node6 epoch3:node_model train_loss:0.4553812102925393,train_acc:0.8425804972648621
node6 epoch4:node_model train_loss:0.3764953747872383,train_acc:0.8747465014457703
node6_model on test-dataset: loss:0.8335036343336105,acc:0.734799861907959
node6 weight score:3607.662733713343
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7586758074007536,train_acc:0.7527239918708801
node9 epoch1:node_model train_loss:0.5388673669413516,train_acc:0.8169436454772949
node9 epoch2:node_model train_loss:0.3793586492538452,train_acc:0.869981586933136
node9 epoch3:node_model train_loss:0.2770213672989293,train_acc:0.9181440472602844
node9 epoch4:node_model train_loss:0.22178488185531214,train_acc:0.935115396976471
node9_model on test-dataset: loss:0.7841464821994305,acc:0.751800000667572
node9 weight score:2368.179979321405
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8508922964334488,train_acc:0.7248333096504211
node10 epoch1:node_model train_loss:0.555818285048008,train_acc:0.8034999966621399
node10 epoch2:node_model train_loss:0.42460009306669233,train_acc:0.8641666769981384
node10 epoch3:node_model train_loss:0.33616670668125154,train_acc:0.8943333625793457
node10 epoch4:node_model train_loss:0.30041242241859434,train_acc:0.9078332185745239
node10_model on test-dataset: loss:0.8163438820838929,acc:0.7297999262809753
node10 weight score:2419.323575940067
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6739276862421701,train_acc:0.7715215682983398
node19 epoch1:node_model train_loss:0.47800635321195734,train_acc:0.8355700373649597
node19 epoch2:node_model train_loss:0.3975146904934284,train_acc:0.8608498573303223
node19 epoch3:node_model train_loss:0.3741432317467623,train_acc:0.8719580769538879
node19 epoch4:node_model train_loss:0.3099904247494631,train_acc:0.904461681842804
node19_model on test-dataset: loss:0.9751766945421696,acc:0.7084001898765564
node19 weight score:4389.973657040547
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6488465227186679,acc:0.7867999798059464
total cost energy:9.469298244508295 | all_enery_cp：7.954 | all_enery_tp: 1.515298244508295
ef: 25.170636695990545
reward: 15.70133845148225
step 251:loss:17.53192138671875|running q:15.96888542175293
episode4,iteration11 selected nodes:[14, 2, 0, 10, 16],center node:14
################################################## episode4,iteration11 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5106267739947026,train_acc:0.8244048357009888
node0 epoch1:node_model train_loss:0.3878523105612168,train_acc:0.8664551973342896
node0 epoch2:node_model train_loss:0.3024378444712896,train_acc:0.9019532203674316
node0 epoch3:node_model train_loss:0.2770802298417458,train_acc:0.9116430878639221
node0 epoch4:node_model train_loss:0.22557021534213653,train_acc:0.933378279209137
node0_model on test-dataset: loss:0.9062465599179268,acc:0.7293999791145325
node0 weight score:5719.194123583093
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6572359763085842,train_acc:0.7830302715301514
node2 epoch1:node_model train_loss:0.47485618976255256,train_acc:0.8420171141624451
node2 epoch2:node_model train_loss:0.4107970924427112,train_acc:0.857158899307251
node2 epoch3:node_model train_loss:0.34233617689460516,train_acc:0.8866002559661865
node2 epoch4:node_model train_loss:0.3193233155955871,train_acc:0.8957103490829468
node2_model on test-dataset: loss:0.8414411203563213,acc:0.7338998913764954
node2 weight score:5690.237717372841
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7255611181259155,train_acc:0.768166720867157
node10 epoch1:node_model train_loss:0.5162055939435959,train_acc:0.8266666531562805
node10 epoch2:node_model train_loss:0.38090610802173613,train_acc:0.8811665773391724
node10 epoch3:node_model train_loss:0.32659642547369006,train_acc:0.9001666307449341
node10 epoch4:node_model train_loss:0.2749349310994148,train_acc:0.9116665720939636
node10_model on test-dataset: loss:0.9205326282978058,acc:0.723800003528595
node10 weight score:2145.4970082397326
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6561000073949496,train_acc:0.7834258079528809
node14 epoch1:node_model train_loss:0.3893682224055131,train_acc:0.8700463175773621
node14 epoch2:node_model train_loss:0.3207780122756958,train_acc:0.9144443869590759
node14 epoch3:node_model train_loss:0.25438261156280834,train_acc:0.9252315759658813
node14 epoch4:node_model train_loss:0.19328801706433296,train_acc:0.939536988735199
node14_model on test-dataset: loss:0.8014695432782173,acc:0.7499001026153564
node14 weight score:1462.3138331697764
node16: train data size:877
node16 epoch0:node_model train_loss:0.962710526254442,train_acc:0.7199278473854065
node16 epoch1:node_model train_loss:0.5887177685896555,train_acc:0.8071284294128418
node16 epoch2:node_model train_loss:0.4284095929728614,train_acc:0.8456853628158569
node16 epoch3:node_model train_loss:0.28665441771348316,train_acc:0.9267820715904236
node16 epoch4:node_model train_loss:0.22578147384855482,train_acc:0.9364501237869263
node16_model on test-dataset: loss:0.9123649637401104,acc:0.7105000615119934
node16 weight score:961.2381391815652
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6258152706921101,acc:0.7939999783039093
total cost energy:8.92460810378532 | all_enery_cp：6.9975000000000005 | all_enery_tp: 1.9271081037853208
ef: 24.930379243016827
reward: 16.005771139231506
step 252:loss:14.4885892868042|running q:17.434858322143555
episode4,iteration12 selected nodes:[13, 7, 8, 15, 17],center node:17
################################################## episode4,iteration12 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7811901986598968,train_acc:0.7591764330863953
node7 epoch1:node_model train_loss:0.4753517836332321,train_acc:0.8460980653762817
node7 epoch2:node_model train_loss:0.3468375504016876,train_acc:0.8796372413635254
node7 epoch3:node_model train_loss:0.272146125882864,train_acc:0.9091176986694336
node7 epoch4:node_model train_loss:0.2340848423540592,train_acc:0.9385784268379211
node7_model on test-dataset: loss:0.8505279111862183,acc:0.7426000237464905
node7 weight score:2293.869459591244
node8: train data size:1798
node8 epoch0:node_model train_loss:0.9035540355576409,train_acc:0.7297279238700867
node8 epoch1:node_model train_loss:0.5216153230932024,train_acc:0.8203288316726685
node8 epoch2:node_model train_loss:0.4052854809496138,train_acc:0.8570747375488281
node8 epoch3:node_model train_loss:0.32188936157359016,train_acc:0.8931859135627747
node8 epoch4:node_model train_loss:0.2411569050616688,train_acc:0.9321655631065369
node8_model on test-dataset: loss:0.7861591371893882,acc:0.7442998290061951
node8 weight score:2287.0687561147765
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7908774465322495,train_acc:0.7486364245414734
node13 epoch1:node_model train_loss:0.5312971274058024,train_acc:0.8270455598831177
node13 epoch2:node_model train_loss:0.39178238809108734,train_acc:0.8695454597473145
node13 epoch3:node_model train_loss:0.3164989948272705,train_acc:0.8934091329574585
node13 epoch4:node_model train_loss:0.22175397537648678,train_acc:0.9329544901847839
node13_model on test-dataset: loss:0.8593415512889624,acc:0.7362000346183777
node13 weight score:1344.0523133875781
node15: train data size:629
node15 epoch0:node_model train_loss:0.8496570842606681,train_acc:0.7299507260322571
node15 epoch1:node_model train_loss:0.5257359232221331,train_acc:0.833004891872406
node15 epoch2:node_model train_loss:0.42102684719221933,train_acc:0.8660098314285278
node15 epoch3:node_model train_loss:0.27764091960021425,train_acc:0.9109359383583069
node15 epoch4:node_model train_loss:0.2124162188598088,train_acc:0.9444335103034973
node15_model on test-dataset: loss:1.0453715256601572,acc:0.7009000182151794
node15 weight score:601.699955049745
node17: train data size:442
node17 epoch0:node_model train_loss:0.8816300511360169,train_acc:0.7519047856330872
node17 epoch1:node_model train_loss:0.47548826336860656,train_acc:0.836190402507782
node17 epoch2:node_model train_loss:0.33218892812728884,train_acc:0.889714241027832
node17 epoch3:node_model train_loss:0.26488819122314455,train_acc:0.9184762239456177
node17 epoch4:node_model train_loss:0.1633031338453293,train_acc:0.9504761695861816
node17_model on test-dataset: loss:0.9134160953760148,acc:0.7214999794960022
node17 weight score:483.8977572625839
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6299390913546086,acc:0.7934999823570251
total cost energy:5.0250880889733 | all_enery_cp：2.9875 | all_enery_tp: 2.0375880889733
ef: 24.369319402458903
reward: 19.3442313134856
step 253:loss:21.784385681152344|running q:18.81455421447754
episode4,iteration13 selected nodes:[8, 12, 3, 14, 15],center node:8
################################################## episode4,iteration13 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.668462053287861,train_acc:0.780633270740509
node3 epoch1:node_model train_loss:0.4646312012228855,train_acc:0.8360118269920349
node3 epoch2:node_model train_loss:0.3731199007394702,train_acc:0.8710984587669373
node3 epoch3:node_model train_loss:0.33781052987242854,train_acc:0.8821178674697876
node3 epoch4:node_model train_loss:0.27961135257122127,train_acc:0.9091241359710693
node3_model on test-dataset: loss:0.7977316126227378,acc:0.7531997561454773
node3 weight score:5323.845680424959
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6118091444174448,train_acc:0.7930951714515686
node8 epoch1:node_model train_loss:0.4249069177442127,train_acc:0.8437301516532898
node8 epoch2:node_model train_loss:0.2905399286084705,train_acc:0.9076757431030273
node8 epoch3:node_model train_loss:0.22195476873053444,train_acc:0.9360316395759583
node8 epoch4:node_model train_loss:0.17179930748211014,train_acc:0.9577323198318481
node8_model on test-dataset: loss:0.8147937385737896,acc:0.7465998530387878
node8 weight score:2206.693442621699
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8548462284462792,train_acc:0.7303968071937561
node12 epoch1:node_model train_loss:0.5420333274773189,train_acc:0.830396831035614
node12 epoch2:node_model train_loss:0.42247963164533886,train_acc:0.8530159592628479
node12 epoch3:node_model train_loss:0.28905943248953136,train_acc:0.9003968238830566
node12 epoch4:node_model train_loss:0.2216515125972884,train_acc:0.940000057220459
node12_model on test-dataset: loss:0.815283058360219,acc:0.7418000102043152
node12 weight score:1638.6946672080987
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6607232987880707,train_acc:0.7869443893432617
node14 epoch1:node_model train_loss:0.3628099759419759,train_acc:0.8820833563804626
node14 epoch2:node_model train_loss:0.25136445090174675,train_acc:0.9170370101928711
node14 epoch3:node_model train_loss:0.183487668633461,train_acc:0.9465277194976807
node14 epoch4:node_model train_loss:0.14471896489461264,train_acc:0.9605555534362793
node14_model on test-dataset: loss:0.8202828033268452,acc:0.7485000491142273
node14 weight score:1428.775533568039
node15: train data size:629
node15 epoch0:node_model train_loss:0.8286547149930682,train_acc:0.7361577153205872
node15 epoch1:node_model train_loss:0.4463031802858625,train_acc:0.842512309551239
node15 epoch2:node_model train_loss:0.3093406047139849,train_acc:0.9023645520210266
node15 epoch3:node_model train_loss:0.2279880472591945,train_acc:0.9323645830154419
node15 epoch4:node_model train_loss:0.1797020850437028,train_acc:0.9495073556900024
node15_model on test-dataset: loss:1.0175157752633095,acc:0.7040000557899475
node15 weight score:618.1722340739428
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6484622837603092,acc:0.7937999805808067
total cost energy:6.547297133654856 | all_enery_cp：4.591 | all_enery_tp: 1.9562971336548565
ef: 24.65302174178716
reward: 18.105724608132306
step 254:loss:38.999977111816406|running q:20.17690658569336
episode4,iteration14 selected nodes:[11, 18, 14, 4, 7],center node:11
################################################## episode4,iteration14 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7190774604678154,train_acc:0.7621428370475769
node4 epoch1:node_model train_loss:0.5240524421845164,train_acc:0.8146427869796753
node4 epoch2:node_model train_loss:0.3825148934764521,train_acc:0.8771427869796753
node4 epoch3:node_model train_loss:0.3191408222275121,train_acc:0.8821428418159485
node4 epoch4:node_model train_loss:0.32703083913241116,train_acc:0.8910714387893677
node4_model on test-dataset: loss:0.8230349479615688,acc:0.7436000108718872
node4 weight score:3286.616208339076
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7071247667074203,train_acc:0.7761961221694946
node7 epoch1:node_model train_loss:0.3594086468219757,train_acc:0.8815783858299255
node7 epoch2:node_model train_loss:0.26862181127071383,train_acc:0.9146568179130554
node7 epoch3:node_model train_loss:0.22965492084622383,train_acc:0.9346176385879517
node7 epoch4:node_model train_loss:0.17285744287073612,train_acc:0.9540196657180786
node7_model on test-dataset: loss:0.7753354270756244,acc:0.7581000328063965
node7 weight score:2516.3302641267082
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7461311711984522,train_acc:0.7636442184448242
node11 epoch1:node_model train_loss:0.43858661195811105,train_acc:0.8472740650177002
node11 epoch2:node_model train_loss:0.3487535925472484,train_acc:0.883027195930481
node11 epoch3:node_model train_loss:0.24722346049897811,train_acc:0.9275896549224854
node11 epoch4:node_model train_loss:0.1949086053406491,train_acc:0.9451649785041809
node11_model on test-dataset: loss:0.8209647750854492,acc:0.7481998801231384
node11 weight score:2048.8089757869707
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5671295002102852,train_acc:0.8050925135612488
node14 epoch1:node_model train_loss:0.3066684591273467,train_acc:0.9008795619010925
node14 epoch2:node_model train_loss:0.22187557816505432,train_acc:0.9363889098167419
node14 epoch3:node_model train_loss:0.19231343641877174,train_acc:0.9312499761581421
node14 epoch4:node_model train_loss:0.13597847831745943,train_acc:0.9672222137451172
node14_model on test-dataset: loss:0.806470835506916,acc:0.7519000172615051
node14 weight score:1453.2453604020616
node18: train data size:472
node18 epoch0:node_model train_loss:0.6692358672618866,train_acc:0.7835555076599121
node18 epoch1:node_model train_loss:0.3417462229728699,train_acc:0.8901111483573914
node18 epoch2:node_model train_loss:0.23757079541683196,train_acc:0.9301111102104187
node18 epoch3:node_model train_loss:0.13695330321788787,train_acc:0.9664444327354431
node18 epoch4:node_model train_loss:0.11710512340068817,train_acc:0.9748889207839966
node18_model on test-dataset: loss:0.8836193306744099,acc:0.7345998883247375
node18 weight score:534.1666751900422
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6227537019550801,acc:0.7961999815702439
total cost energy:5.380949493661166 | all_enery_cp：3.9909999999999997 | all_enery_tp: 1.3899494936611667
ef: 24.839479317348392
reward: 19.458529823687226
step 255:loss:19.119436264038086|running q:21.640348434448242
episode4,iteration15 selected nodes:[4, 13, 6, 7, 8],center node:4
################################################## episode4,iteration15 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5760891682335308,train_acc:0.7975000143051147
node4 epoch1:node_model train_loss:0.39585644166384426,train_acc:0.8628571629524231
node4 epoch2:node_model train_loss:0.2818185069066073,train_acc:0.910714328289032
node4 epoch3:node_model train_loss:0.2365316871021475,train_acc:0.9214286804199219
node4 epoch4:node_model train_loss:0.26321255043148994,train_acc:0.9135714173316956
node4_model on test-dataset: loss:0.9267603462934494,acc:0.7272999882698059
node4 weight score:2918.7696806607746
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7296490486591093,train_acc:0.7789400219917297
node6 epoch1:node_model train_loss:0.4718487330021397,train_acc:0.8415207862854004
node6 epoch2:node_model train_loss:0.4152755843054864,train_acc:0.8549769520759583
node6 epoch3:node_model train_loss:0.3694784223071991,train_acc:0.8764516711235046
node6 epoch4:node_model train_loss:0.2875318779580055,train_acc:0.9061290621757507
node6_model on test-dataset: loss:0.7561689233779907,acc:0.7603997588157654
node6 weight score:3976.624676093536
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4927320942282677,train_acc:0.8256765604019165
node7 epoch1:node_model train_loss:0.29745459407567976,train_acc:0.906539261341095
node7 epoch2:node_model train_loss:0.19498533830046655,train_acc:0.9475587010383606
node7 epoch3:node_model train_loss:0.14980258233845234,train_acc:0.9655782580375671
node7 epoch4:node_model train_loss:0.1107790069654584,train_acc:0.9750195741653442
node7_model on test-dataset: loss:0.8599929273128509,acc:0.7494001388549805
node7 weight score:2268.6233084452556
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6559939583142599,train_acc:0.7851814031600952
node8 epoch1:node_model train_loss:0.39211880829599166,train_acc:0.871009111404419
node8 epoch2:node_model train_loss:0.26252663880586624,train_acc:0.9259862899780273
node8 epoch3:node_model train_loss:0.21730797779228953,train_acc:0.9366100430488586
node8 epoch4:node_model train_loss:0.173800195256869,train_acc:0.9549545645713806
node8_model on test-dataset: loss:0.7691680401563644,acc:0.7610000371932983
node8 weight score:2337.5906253651465
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7669795428713163,train_acc:0.7512878775596619
node13 epoch1:node_model train_loss:0.46465345720450085,train_acc:0.845075786113739
node13 epoch2:node_model train_loss:0.28214025249083835,train_acc:0.9106060862541199
node13 epoch3:node_model train_loss:0.2005596086382866,train_acc:0.9487878680229187
node13 epoch4:node_model train_loss:0.17077171864608923,train_acc:0.9624999761581421
node13_model on test-dataset: loss:0.9008852702379226,acc:0.7252001762390137
node13 weight score:1282.0722439994672
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6254995137453079,acc:0.7967999845743179
total cost energy:6.722764669113385 | all_enery_cp：5.308000000000001 | all_enery_tp: 1.4147646691133842
ef: 24.960552754477224
reward: 18.23778808536384
step 256:loss:33.850196838378906|running q:23.037593841552734
episode4,iteration16 selected nodes:[9, 8, 16, 10, 14],center node:14
################################################## episode4,iteration16 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5636986593405405,train_acc:0.8063945770263672
node8 epoch1:node_model train_loss:0.3183857566780514,train_acc:0.8937300443649292
node8 epoch2:node_model train_loss:0.20618974417448044,train_acc:0.9410430192947388
node8 epoch3:node_model train_loss:0.16686718579795626,train_acc:0.9544103741645813
node8 epoch4:node_model train_loss:0.14724562731054094,train_acc:0.9655441641807556
node8_model on test-dataset: loss:0.8034279373288155,acc:0.7534000873565674
node8 weight score:2237.9107278468214
node9: train data size:1857
node9 epoch0:node_model train_loss:0.747853735559865,train_acc:0.7698429226875305
node9 epoch1:node_model train_loss:0.4575963679112886,train_acc:0.8420867323875427
node9 epoch2:node_model train_loss:0.33526615014201716,train_acc:0.8961679339408875
node9 epoch3:node_model train_loss:0.2198157761442034,train_acc:0.9349953532218933
node9 epoch4:node_model train_loss:0.17623136780763926,train_acc:0.9460479617118835
node9_model on test-dataset: loss:0.8782665465772151,acc:0.7320998907089233
node9 weight score:2114.3922733219315
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7040054589509964,train_acc:0.7701666951179504
node10 epoch1:node_model train_loss:0.4517876885831356,train_acc:0.8498333096504211
node10 epoch2:node_model train_loss:0.3502809889614582,train_acc:0.8873332142829895
node10 epoch3:node_model train_loss:0.25476205013692377,train_acc:0.9268333315849304
node10 epoch4:node_model train_loss:0.19112588576972483,train_acc:0.9469999670982361
node10_model on test-dataset: loss:0.7372016781568527,acc:0.7653000354766846
node10 weight score:2679.04978857059
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5393267671267191,train_acc:0.8103241324424744
node14 epoch1:node_model train_loss:0.30921972418824833,train_acc:0.8827314376831055
node14 epoch2:node_model train_loss:0.22343988716602325,train_acc:0.9317128658294678
node14 epoch3:node_model train_loss:0.15697898777822653,train_acc:0.9582406878471375
node14 epoch4:node_model train_loss:0.09653688780963421,train_acc:0.9805092215538025
node14_model on test-dataset: loss:0.7718158507347107,acc:0.7638999819755554
node14 weight score:1518.4969301736212
node16: train data size:877
node16 epoch0:node_model train_loss:0.714638974931505,train_acc:0.7706926465034485
node16 epoch1:node_model train_loss:0.40656934844122994,train_acc:0.8780086636543274
node16 epoch2:node_model train_loss:0.27959349585904014,train_acc:0.9150071740150452
node16 epoch3:node_model train_loss:0.23356038166417015,train_acc:0.9364502429962158
node16 epoch4:node_model train_loss:0.17178863286972046,train_acc:0.9564501643180847
node16_model on test-dataset: loss:0.8651149154454469,acc:0.7400998473167419
node16 weight score:1013.7381570267269
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5975436681509018,acc:0.8062999796867371
total cost energy:5.331425977444875 | all_enery_cp：3.8394999999999997 | all_enery_tp: 1.491925977444875
ef: 24.965298606629847
reward: 19.63387262918497
step 257:loss:27.798051834106445|running q:24.560754776000977
episode4,iteration17 selected nodes:[6, 9, 14, 16, 19],center node:14
################################################## episode4,iteration17 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5458168829641035,train_acc:0.819677472114563
node6 epoch1:node_model train_loss:0.39547300434881644,train_acc:0.8603687286376953
node6 epoch2:node_model train_loss:0.3846943825483322,train_acc:0.8661289811134338
node6 epoch3:node_model train_loss:0.25873124695593314,train_acc:0.918940007686615
node6 epoch4:node_model train_loss:0.2248481206836239,train_acc:0.9345160126686096
node6_model on test-dataset: loss:0.8010803437232972,acc:0.7523000836372375
node6 weight score:3753.680917976255
node9: train data size:1857
node9 epoch0:node_model train_loss:0.565662106401042,train_acc:0.7997044920921326
node9 epoch1:node_model train_loss:0.3546819843743977,train_acc:0.8834071755409241
node9 epoch2:node_model train_loss:0.23767109450541044,train_acc:0.9277470111846924
node9 epoch3:node_model train_loss:0.18809566372319272,train_acc:0.9364358186721802
node9 epoch4:node_model train_loss:0.12486138979071065,train_acc:0.9715788960456848
node9_model on test-dataset: loss:0.7545841154456139,acc:0.7659997344017029
node9 weight score:2460.9582444011066
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4382852291067441,train_acc:0.8375000357627869
node14 epoch1:node_model train_loss:0.2550173836449782,train_acc:0.9147221446037292
node14 epoch2:node_model train_loss:0.18594132053355375,train_acc:0.9385185241699219
node14 epoch3:node_model train_loss:0.1253853409240643,train_acc:0.960185170173645
node14 epoch4:node_model train_loss:0.1112415250390768,train_acc:0.9755091667175293
node14_model on test-dataset: loss:0.8066317331790924,acc:0.7569999694824219
node14 weight score:1452.9554836392567
node16: train data size:877
node16 epoch0:node_model train_loss:0.6091493566830953,train_acc:0.8053535223007202
node16 epoch1:node_model train_loss:0.3674156930711534,train_acc:0.8713419437408447
node16 epoch2:node_model train_loss:0.2261739886469311,train_acc:0.9264501929283142
node16 epoch3:node_model train_loss:0.14768044650554657,train_acc:0.9666666388511658
node16 epoch4:node_model train_loss:0.09784203602208032,train_acc:0.9837806820869446
node16_model on test-dataset: loss:0.8088819159567356,acc:0.7434000372886658
node16 weight score:1084.2126430317028
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6208070495793986,train_acc:0.7929027676582336
node19 epoch1:node_model train_loss:0.44339285756266394,train_acc:0.8492364287376404
node19 epoch2:node_model train_loss:0.34262735032758046,train_acc:0.8902068138122559
node19 epoch3:node_model train_loss:0.25974310449389526,train_acc:0.9202612042427063
node19 epoch4:node_model train_loss:0.2274431832307993,train_acc:0.9289202094078064
node19_model on test-dataset: loss:0.724553226083517,acc:0.7694000601768494
node19 weight score:5908.468620228796
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.616490990370512,acc:0.8031999814510346
total cost energy:6.846190079283666 | all_enery_cp：5.5969999999999995 | all_enery_tp: 1.2491900792836665
ef: 25.266470936374354
reward: 18.420280857090688
step 258:loss:37.64944839477539|running q:25.872028350830078
episode4,iteration18 selected nodes:[15, 18, 0, 8, 7],center node:7
################################################## episode4,iteration18 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5191678155500156,train_acc:0.823324978351593
node0 epoch1:node_model train_loss:0.3412938699699365,train_acc:0.8798332810401917
node0 epoch2:node_model train_loss:0.2767788191827444,train_acc:0.9062233567237854
node0 epoch3:node_model train_loss:0.2217486443427893,train_acc:0.929265558719635
node0 epoch4:node_model train_loss:0.2074899494361419,train_acc:0.937455952167511
node0_model on test-dataset: loss:0.7980496093630791,acc:0.7640000581741333
node0 weight score:6494.583719095528
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5645210236310959,train_acc:0.8136764764785767
node7 epoch1:node_model train_loss:0.31412456184625626,train_acc:0.8891569972038269
node7 epoch2:node_model train_loss:0.19611993990838528,train_acc:0.9455195665359497
node7 epoch3:node_model train_loss:0.14432434029877186,train_acc:0.964039146900177
node7 epoch4:node_model train_loss:0.12849796041846276,train_acc:0.971019446849823
node7_model on test-dataset: loss:0.7799520649015903,acc:0.7656999826431274
node7 weight score:2501.4357776540605
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5386165959967507,train_acc:0.8165192604064941
node8 epoch1:node_model train_loss:0.3117283648914761,train_acc:0.890396773815155
node8 epoch2:node_model train_loss:0.20746504680977929,train_acc:0.9404647946357727
node8 epoch3:node_model train_loss:0.15407917524377504,train_acc:0.9554534554481506
node8 epoch4:node_model train_loss:0.12162575415439075,train_acc:0.9749658703804016
node8_model on test-dataset: loss:0.7621448779851199,acc:0.7659999132156372
node8 weight score:2359.1315141464534
node15: train data size:629
node15 epoch0:node_model train_loss:0.7897529644625527,train_acc:0.7404434084892273
node15 epoch1:node_model train_loss:0.48816818850381033,train_acc:0.846650242805481
node15 epoch2:node_model train_loss:0.3205035626888275,train_acc:0.8902956247329712
node15 epoch3:node_model train_loss:0.19567638316324779,train_acc:0.9385714530944824
node15 epoch4:node_model train_loss:0.12781757648502076,train_acc:0.9636452794075012
node15_model on test-dataset: loss:0.8520628530532122,acc:0.7419998645782471
node15 weight score:738.2084522827078
node18: train data size:472
node18 epoch0:node_model train_loss:0.6891200363636016,train_acc:0.7991110682487488
node18 epoch1:node_model train_loss:0.4536171853542328,train_acc:0.845111072063446
node18 epoch2:node_model train_loss:0.2856698393821716,train_acc:0.8937777876853943
node18 epoch3:node_model train_loss:0.13827962130308152,train_acc:0.9784443974494934
node18 epoch4:node_model train_loss:0.09764653220772743,train_acc:0.984000027179718
node18_model on test-dataset: loss:0.92699029520154,acc:0.7321001291275024
node18 weight score:509.1746941076454
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.613938317745924,acc:0.8051999819278717
total cost energy:7.225136274336041 | all_enery_cp：5.016500000000001 | all_enery_tp: 2.208636274336041
ef: 24.50959714049184
reward: 17.2844608661558
step 259:loss:33.85135269165039|running q:27.265790939331055
episode4,iteration19 selected nodes:[6, 8, 2, 1, 7],center node:6
################################################## episode4,iteration19 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.6185778368483571,train_acc:0.7999264597892761
node1 epoch1:node_model train_loss:0.44313519097426357,train_acc:0.848602831363678
node1 epoch2:node_model train_loss:0.43895744072163806,train_acc:0.8492648005485535
node1 epoch3:node_model train_loss:0.3367041537866873,train_acc:0.8874999284744263
node1 epoch4:node_model train_loss:0.26529362469035034,train_acc:0.9081615805625916
node1_model on test-dataset: loss:0.7788473757728934,acc:0.7609000205993652
node1 weight score:8612.727228288186
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6220060220609108,train_acc:0.7932764887809753
node2 epoch1:node_model train_loss:0.4337563772375385,train_acc:0.8542803525924683
node2 epoch2:node_model train_loss:0.34777423025419313,train_acc:0.8806154727935791
node2 epoch3:node_model train_loss:0.29109777867173153,train_acc:0.9032292366027832
node2 epoch4:node_model train_loss:0.26959969848394394,train_acc:0.9076040983200073
node2_model on test-dataset: loss:0.7336180041730404,acc:0.7668999433517456
node2 weight score:6526.557381040831
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5534359072485278,train_acc:0.8121657371520996
node6 epoch1:node_model train_loss:0.3698297974563414,train_acc:0.873225748538971
node6 epoch2:node_model train_loss:0.2777985372850972,train_acc:0.9111981391906738
node6 epoch3:node_model train_loss:0.2915050430643943,train_acc:0.9078800082206726
node6 epoch4:node_model train_loss:0.33021418317671747,train_acc:0.900783360004425
node6_model on test-dataset: loss:0.8021956795454025,acc:0.7541001439094543
node6 weight score:3748.4619733978643
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4571098178625107,train_acc:0.8347156643867493
node7 epoch1:node_model train_loss:0.26341066770255567,train_acc:0.9065195918083191
node7 epoch2:node_model train_loss:0.1793556109070778,train_acc:0.9530979990959167
node7 epoch3:node_model train_loss:0.11303651258349419,train_acc:0.9765195250511169
node7 epoch4:node_model train_loss:0.0928894991055131,train_acc:0.9794999361038208
node7_model on test-dataset: loss:0.7669406175613404,acc:0.770899772644043
node7 weight score:2543.873613323078
node8: train data size:1798
node8 epoch0:node_model train_loss:0.46585862504111397,train_acc:0.8364625573158264
node8 epoch1:node_model train_loss:0.2579106878903177,train_acc:0.9204988479614258
node8 epoch2:node_model train_loss:0.17831681544582048,train_acc:0.9471656084060669
node8 epoch3:node_model train_loss:0.13519087433815002,train_acc:0.9704874753952026
node8 epoch4:node_model train_loss:0.10063209757208824,train_acc:0.9805100560188293
node8_model on test-dataset: loss:0.7841291050612926,acc:0.7650999426841736
node8 weight score:2292.9897492574983
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5910718319565058,acc:0.8121999847888947
total cost energy:10.373213595499958 | all_enery_cp：9.126000000000001 | all_enery_tp: 1.2472135954999581
ef: 25.37955053812707
reward: 15.006336942627112
step 260:loss:19.48862075805664|running q:28.56147003173828
episode4,iteration20 selected nodes:[5, 9, 8, 7, 0],center node:9
################################################## episode4,iteration20 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.38696656834620696,train_acc:0.8676390051841736
node0 epoch1:node_model train_loss:0.24878514271516067,train_acc:0.913185715675354
node0 epoch2:node_model train_loss:0.18886810225936082,train_acc:0.9436096549034119
node0 epoch3:node_model train_loss:0.1576283678698998,train_acc:0.9512232542037964
node0 epoch4:node_model train_loss:0.13680529666061586,train_acc:0.9614155292510986
node0_model on test-dataset: loss:0.8701014410704374,acc:0.7532000541687012
node0 weight score:5956.776710567958
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6504365276349219,train_acc:0.7921053171157837
node5 epoch1:node_model train_loss:0.4606272523340426,train_acc:0.8415414094924927
node5 epoch2:node_model train_loss:0.3611349392878382,train_acc:0.8751879334449768
node5 epoch3:node_model train_loss:0.28313609604772766,train_acc:0.905037522315979
node5 epoch4:node_model train_loss:0.22123643128495468,train_acc:0.9321805238723755
node5_model on test-dataset: loss:0.7817851123213768,acc:0.7684999108314514
node5 weight score:4777.5276621852745
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4509088292717934,train_acc:0.8481569290161133
node7 epoch1:node_model train_loss:0.25076973289251325,train_acc:0.9155980348587036
node7 epoch2:node_model train_loss:0.16267058923840522,train_acc:0.9545586705207825
node7 epoch3:node_model train_loss:0.12617727406322957,train_acc:0.970019519329071
node7 epoch4:node_model train_loss:0.09257982429116965,train_acc:0.9830195307731628
node7_model on test-dataset: loss:0.7758874532580375,acc:0.7745000123977661
node7 weight score:2514.539952679392
node8: train data size:1798
node8 epoch0:node_model train_loss:0.48207644787099624,train_acc:0.8324490189552307
node8 epoch1:node_model train_loss:0.2948039414154159,train_acc:0.8949092030525208
node8 epoch2:node_model train_loss:0.1925812591281202,train_acc:0.9476528167724609
node8 epoch3:node_model train_loss:0.15906927982966104,train_acc:0.9560883641242981
node8 epoch4:node_model train_loss:0.09870870349307855,train_acc:0.9782992601394653
node8_model on test-dataset: loss:0.8084521078318357,acc:0.7602999806404114
node8 weight score:2224.0031073974237
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5896112965910059,train_acc:0.8099722862243652
node9 epoch1:node_model train_loss:0.3999235528080087,train_acc:0.8649770021438599
node9 epoch2:node_model train_loss:0.22222328342889486,train_acc:0.9296029806137085
node9 epoch3:node_model train_loss:0.17054056611500287,train_acc:0.9516990184783936
node9 epoch4:node_model train_loss:0.10381687354100377,train_acc:0.981052577495575
node9_model on test-dataset: loss:0.7536662152409553,acc:0.7726999521255493
node9 weight score:2463.9554784956053
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6074814729392528,acc:0.8136999821662902
total cost energy:8.678044976076178 | all_enery_cp：7.262 | all_enery_tp: 1.4160449760761789
ef: 25.248621060275962
reward: 16.570576084199786
step 261:loss:26.33135986328125|running q:29.848800659179688
episode4,iteration21 selected nodes:[10, 4, 8, 7, 3],center node:4
################################################## episode4,iteration21 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5769637150819912,train_acc:0.8040079474449158
node3 epoch1:node_model train_loss:0.3899808605743009,train_acc:0.8676990270614624
node3 epoch2:node_model train_loss:0.30057715919128686,train_acc:0.9011032581329346
node3 epoch3:node_model train_loss:0.2209513911674189,train_acc:0.9303166270256042
node3 epoch4:node_model train_loss:0.1965615042420321,train_acc:0.9394755363464355
node3_model on test-dataset: loss:0.7757310596108437,acc:0.769800066947937
node3 weight score:5474.835572692638
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6024650995220456,train_acc:0.7992857694625854
node4 epoch1:node_model train_loss:0.41928225117070334,train_acc:0.8574999570846558
node4 epoch2:node_model train_loss:0.3432596331196172,train_acc:0.8882143497467041
node4 epoch3:node_model train_loss:0.31670938432216644,train_acc:0.8839285969734192
node4 epoch4:node_model train_loss:0.22969463999782289,train_acc:0.9292857050895691
node4_model on test-dataset: loss:0.7624696634709835,acc:0.7698997855186462
node4 weight score:3547.682130310672
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4232874274253845,train_acc:0.8501372337341309
node7 epoch1:node_model train_loss:0.24931953735649587,train_acc:0.9150587916374207
node7 epoch2:node_model train_loss:0.1521448276937008,train_acc:0.9590392112731934
node7 epoch3:node_model train_loss:0.12195640243589878,train_acc:0.9650588035583496
node7 epoch4:node_model train_loss:0.08264175597578287,train_acc:0.9795195460319519
node7_model on test-dataset: loss:0.7670789819955826,acc:0.7716001868247986
node7 weight score:2543.4147536208147
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4776848488383823,train_acc:0.8370862603187561
node8 epoch1:node_model train_loss:0.23303490546014574,train_acc:0.9271087646484375
node8 epoch2:node_model train_loss:0.16748538240790367,train_acc:0.9521881937980652
node8 epoch3:node_model train_loss:0.12311115881635083,train_acc:0.970544159412384
node8 epoch4:node_model train_loss:0.10372200856606166,train_acc:0.976643979549408
node8_model on test-dataset: loss:0.8109837345778942,acc:0.7609000205993652
node8 weight score:2217.0604949750737
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7062409088015557,train_acc:0.784333348274231
node10 epoch1:node_model train_loss:0.41808622181415556,train_acc:0.8646666407585144
node10 epoch2:node_model train_loss:0.29662456512451174,train_acc:0.9016666412353516
node10 epoch3:node_model train_loss:0.20506016351282597,train_acc:0.9336665272712708
node10 epoch4:node_model train_loss:0.15534620732069016,train_acc:0.9573333859443665
node10_model on test-dataset: loss:0.7618903616070747,acc:0.76829993724823
node10 weight score:2592.236494282566
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5934810161590576,acc:0.8119999819993973
total cost energy:7.095649122254148 | all_enery_cp：6.338 | all_enery_tp: 0.7576491222541475
ef: 25.443045279522632
reward: 18.347396157268484
step 262:loss:26.15223503112793|running q:31.15550994873047
episode4,iteration22 selected nodes:[10, 13, 2, 15, 5],center node:5
################################################## episode4,iteration22 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5457126659651598,train_acc:0.814185619354248
node2 epoch1:node_model train_loss:0.3487024186179042,train_acc:0.8753976821899414
node2 epoch2:node_model train_loss:0.2693850154367586,train_acc:0.9126039743423462
node2 epoch3:node_model train_loss:0.21338476163024703,train_acc:0.9303692579269409
node2 epoch4:node_model train_loss:0.18887961640333137,train_acc:0.9391193389892578
node2_model on test-dataset: loss:0.7637782701104879,acc:0.7675001621246338
node2 weight score:6268.8350629658125
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5655180357004467,train_acc:0.8219924569129944
node5 epoch1:node_model train_loss:0.3844335298789175,train_acc:0.8740602135658264
node5 epoch2:node_model train_loss:0.3163969301079449,train_acc:0.8925186991691589
node5 epoch3:node_model train_loss:0.20929757956611483,train_acc:0.9344736337661743
node5 epoch4:node_model train_loss:0.175032099218745,train_acc:0.9498496055603027
node5_model on test-dataset: loss:0.7890923516452313,acc:0.7688001990318298
node5 weight score:4733.286277851571
node10: train data size:1975
node10 epoch0:node_model train_loss:0.529398413002491,train_acc:0.8165000081062317
node10 epoch1:node_model train_loss:0.32772805243730546,train_acc:0.8880001306533813
node10 epoch2:node_model train_loss:0.20985939987003804,train_acc:0.9368333220481873
node10 epoch3:node_model train_loss:0.16920832321047782,train_acc:0.957833468914032
node10 epoch4:node_model train_loss:0.11480838283896447,train_acc:0.9771665930747986
node10_model on test-dataset: loss:0.7486601604521275,acc:0.7740999460220337
node10 weight score:2638.0460779524674
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7154961129029592,train_acc:0.7759848833084106
node13 epoch1:node_model train_loss:0.416439451277256,train_acc:0.8671212196350098
node13 epoch2:node_model train_loss:0.27558793127536774,train_acc:0.9132575392723083
node13 epoch3:node_model train_loss:0.1757549668351809,train_acc:0.9518181681632996
node13 epoch4:node_model train_loss:0.13830525490144888,train_acc:0.9674999713897705
node13_model on test-dataset: loss:0.8179674714803695,acc:0.7581999897956848
node13 weight score:1412.0365910268583
node15: train data size:629
node15 epoch0:node_model train_loss:0.6642397046089172,train_acc:0.7875862717628479
node15 epoch1:node_model train_loss:0.41296849719115664,train_acc:0.8623645305633545
node15 epoch2:node_model train_loss:0.19907078359808242,train_acc:0.9407881498336792
node15 epoch3:node_model train_loss:0.17170310871941702,train_acc:0.9565024375915527
node15 epoch4:node_model train_loss:0.11661139982087272,train_acc:0.9693595767021179
node15_model on test-dataset: loss:0.8250066624581813,acc:0.7559998631477356
node15 weight score:762.4180853592601
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5976262165606022,acc:0.8104999816417694
total cost energy:8.39679007923706 | all_enery_cp：6.141 | all_enery_tp: 2.2557900792370615
ef: 24.704065879092376
reward: 16.307275799855315
step 263:loss:27.205066680908203|running q:32.47570037841797
episode4,iteration23 selected nodes:[11, 7, 9, 15, 2],center node:9
################################################## episode4,iteration23 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3592024032647411,train_acc:0.8739109039306641
node2 epoch1:node_model train_loss:0.2847103141248226,train_acc:0.9016759395599365
node2 epoch2:node_model train_loss:0.2204914460889995,train_acc:0.9254827499389648
node2 epoch3:node_model train_loss:0.16203991478929916,train_acc:0.948248028755188
node2 epoch4:node_model train_loss:0.12891571200452745,train_acc:0.9668182134628296
node2_model on test-dataset: loss:0.7523064108192921,acc:0.7767998576164246
node2 weight score:6364.428019144054
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4425040394067764,train_acc:0.843176543712616
node7 epoch1:node_model train_loss:0.22709163427352905,train_acc:0.926558792591095
node7 epoch2:node_model train_loss:0.13651788905262946,train_acc:0.9605979919433594
node7 epoch3:node_model train_loss:0.10196465738117695,train_acc:0.9725586771965027
node7 epoch4:node_model train_loss:0.086109752766788,train_acc:0.9804998636245728
node7_model on test-dataset: loss:0.7689691452682018,acc:0.7755998969078064
node7 weight score:2537.1629174009686
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5898237526416779,train_acc:0.813130259513855
node9 epoch1:node_model train_loss:0.3835468511832388,train_acc:0.8751245737075806
node9 epoch2:node_model train_loss:0.21394116156979612,train_acc:0.938679575920105
node9 epoch3:node_model train_loss:0.14622459717487035,train_acc:0.9552540183067322
node9 epoch4:node_model train_loss:0.11844130252536975,train_acc:0.9667035341262817
node9_model on test-dataset: loss:0.8191908341646195,acc:0.76500004529953
node9 weight score:2266.8710666101383
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6617904726196738,train_acc:0.7881348729133606
node11 epoch1:node_model train_loss:0.37244706732385296,train_acc:0.8772739768028259
node11 epoch2:node_model train_loss:0.25210139593657327,train_acc:0.9208608269691467
node11 epoch3:node_model train_loss:0.15634588678093517,train_acc:0.9594834446907043
node11 epoch4:node_model train_loss:0.13773217884933248,train_acc:0.9669296145439148
node11_model on test-dataset: loss:0.7969363574683667,acc:0.7648000121116638
node11 weight score:2110.582588229782
node15: train data size:629
node15 epoch0:node_model train_loss:0.6840040768895831,train_acc:0.7726600766181946
node15 epoch1:node_model train_loss:0.395359354359763,train_acc:0.8874384760856628
node15 epoch2:node_model train_loss:0.31327239104679655,train_acc:0.8925123810768127
node15 epoch3:node_model train_loss:0.21972185160432542,train_acc:0.9252216815948486
node15 epoch4:node_model train_loss:0.12960399687290192,train_acc:0.9657142758369446
node15_model on test-dataset: loss:0.8712253433465957,acc:0.7446998953819275
node15 weight score:721.9716515406368
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6068963158130646,acc:0.8128999829292297
total cost energy:6.630769903474535 | all_enery_cp：5.4535 | all_enery_tp: 1.177269903474535
ef: 25.084802959262387
reward: 18.454033055787853
step 264:loss:51.81588363647461|running q:33.95918273925781
episode4,iteration24 selected nodes:[3, 17, 13, 1, 2],center node:3
################################################## episode4,iteration24 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5358577190953142,train_acc:0.8223530054092407
node1 epoch1:node_model train_loss:0.39797093171407194,train_acc:0.8639706373214722
node1 epoch2:node_model train_loss:0.3107641126741381,train_acc:0.8957353234291077
node1 epoch3:node_model train_loss:0.25481818430125713,train_acc:0.9138236045837402
node1 epoch4:node_model train_loss:0.3077472680631806,train_acc:0.8927940130233765
node1_model on test-dataset: loss:0.8033195331692695,acc:0.7653999328613281
node1 weight score:8350.350916447267
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2624638667330146,train_acc:0.9030207395553589
node2 epoch1:node_model train_loss:0.17216116997102895,train_acc:0.9457575678825378
node2 epoch2:node_model train_loss:0.14317945297807455,train_acc:0.9584848284721375
node2 epoch3:node_model train_loss:0.11906920637314518,train_acc:0.9678313136100769
node2 epoch4:node_model train_loss:0.10741647526932259,train_acc:0.9719126224517822
node2_model on test-dataset: loss:0.8175266346335411,acc:0.7628999948501587
node2 weight score:5856.689919523216
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5119897740524869,train_acc:0.8230429291725159
node3 epoch1:node_model train_loss:0.30694436697765837,train_acc:0.8932260274887085
node3 epoch2:node_model train_loss:0.2232243725380232,train_acc:0.9252005219459534
node3 epoch3:node_model train_loss:0.19275750185168067,train_acc:0.9435427188873291
node3 epoch4:node_model train_loss:0.1430616359724555,train_acc:0.9598810076713562
node3_model on test-dataset: loss:0.8058795243501663,acc:0.7680999636650085
node3 weight score:5270.018497398399
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7056396578749021,train_acc:0.7753031253814697
node13 epoch1:node_model train_loss:0.3826248273253441,train_acc:0.8647727370262146
node13 epoch2:node_model train_loss:0.24760989534358183,train_acc:0.920757532119751
node13 epoch3:node_model train_loss:0.17670674932499728,train_acc:0.9475757479667664
node13 epoch4:node_model train_loss:0.13354843420286974,train_acc:0.9612878561019897
node13_model on test-dataset: loss:0.8164114344120026,acc:0.7607000470161438
node13 weight score:1414.7278581807914
node17: train data size:442
node17 epoch0:node_model train_loss:0.7185395002365113,train_acc:0.7683809399604797
node17 epoch1:node_model train_loss:0.3969156086444855,train_acc:0.8732380867004395
node17 epoch2:node_model train_loss:0.18540184050798417,train_acc:0.9444762468338013
node17 epoch3:node_model train_loss:0.1660373717546463,train_acc:0.9552380442619324
node17 epoch4:node_model train_loss:0.0944852888584137,train_acc:0.9852380752563477
node17_model on test-dataset: loss:0.9049281302839518,acc:0.740600049495697
node17 weight score:488.43657878256874
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6245544117689132,acc:0.8094999802112579
total cost energy:10.87990195135928 | all_enery_cp：8.67 | all_enery_tp: 2.209901951359279
ef: 24.715946671906952
reward: 13.836044720547672
step 265:loss:25.3906307220459|running q:35.18369674682617
episode4,iteration25 selected nodes:[3, 1, 0, 13, 12],center node:3
################################################## episode4,iteration25 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.43048073007510257,train_acc:0.853141725063324
node0 epoch1:node_model train_loss:0.2589128091931343,train_acc:0.9096502661705017
node0 epoch2:node_model train_loss:0.18649201066448137,train_acc:0.9426483511924744
node0 epoch3:node_model train_loss:0.1376134746063214,train_acc:0.9606115221977234
node0 epoch4:node_model train_loss:0.116126494983641,train_acc:0.9677270650863647
node0_model on test-dataset: loss:0.7138289655745029,acc:0.7920999526977539
node0 weight score:7260.842932912682
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3793219227124663,train_acc:0.8655147552490234
node1 epoch1:node_model train_loss:0.2939548469422495,train_acc:0.8968382477760315
node1 epoch2:node_model train_loss:0.25581625859965296,train_acc:0.9111763834953308
node1 epoch3:node_model train_loss:0.23336880478788824,train_acc:0.922793984413147
node1 epoch4:node_model train_loss:0.17948467624099815,train_acc:0.9428675174713135
node1_model on test-dataset: loss:0.904739725291729,acc:0.7546000480651855
node1 weight score:7414.287018111245
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4099418943704561,train_acc:0.8582484126091003
node3 epoch1:node_model train_loss:0.24166987801707068,train_acc:0.9217118620872498
node3 epoch2:node_model train_loss:0.19649577573981397,train_acc:0.928223729133606
node3 epoch3:node_model train_loss:0.1575989153149516,train_acc:0.9511032700538635
node3 epoch4:node_model train_loss:0.12769322495820912,train_acc:0.9648837447166443
node3_model on test-dataset: loss:0.8105596831440925,acc:0.7699000239372253
node3 weight score:5239.58949392381
node12: train data size:1336
node12 epoch0:node_model train_loss:0.749158433505467,train_acc:0.7760317921638489
node12 epoch1:node_model train_loss:0.40949888208082746,train_acc:0.8567460775375366
node12 epoch2:node_model train_loss:0.2750989645719528,train_acc:0.9088889956474304
node12 epoch3:node_model train_loss:0.19844205571072443,train_acc:0.9345238208770752
node12 epoch4:node_model train_loss:0.1492555152092661,train_acc:0.9615872502326965
node12_model on test-dataset: loss:0.8342475046217441,acc:0.758499801158905
node12 weight score:1601.44320791916
node13: train data size:1155
node13 epoch0:node_model train_loss:0.643906757235527,train_acc:0.7971969842910767
node13 epoch1:node_model train_loss:0.3602376791338126,train_acc:0.8702272772789001
node13 epoch2:node_model train_loss:0.2370801568031311,train_acc:0.9240908622741699
node13 epoch3:node_model train_loss:0.16417382347087064,train_acc:0.9526514410972595
node13 epoch4:node_model train_loss:0.12090136172870795,train_acc:0.9693180918693542
node13_model on test-dataset: loss:0.8472983549535275,acc:0.7556999921798706
node13 weight score:1363.1561931491647
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6261910365521908,acc:0.8111999779939651
total cost energy:11.356942273101028 | all_enery_cp：9.314499999999999 | all_enery_tp: 2.042442273101029
ef: 24.960396889545176
reward: 13.603454616444148
step 266:loss:20.93231201171875|running q:36.52741241455078
episode4,iteration26 selected nodes:[13, 10, 7, 5, 9],center node:7
################################################## episode4,iteration26 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5428605848237088,train_acc:0.8189097046852112
node5 epoch1:node_model train_loss:0.35610851645469666,train_acc:0.8744736909866333
node5 epoch2:node_model train_loss:0.25868064949387,train_acc:0.9087968468666077
node5 epoch3:node_model train_loss:0.18440071728668714,train_acc:0.9424059987068176
node5 epoch4:node_model train_loss:0.1490763510136228,train_acc:0.9558271169662476
node5_model on test-dataset: loss:0.810733310431242,acc:0.774899959564209
node5 weight score:4606.9403488716825
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4567090794444084,train_acc:0.8511764407157898
node7 epoch1:node_model train_loss:0.24155907928943635,train_acc:0.9255391955375671
node7 epoch2:node_model train_loss:0.13423179052770137,train_acc:0.9605392813682556
node7 epoch3:node_model train_loss:0.10684708841145038,train_acc:0.970519483089447
node7 epoch4:node_model train_loss:0.08332038056105376,train_acc:0.9840195775032043
node7_model on test-dataset: loss:0.7865179550647735,acc:0.7740999460220337
node7 weight score:2480.5536700549014
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5788219382888392,train_acc:0.824976921081543
node9 epoch1:node_model train_loss:0.3381376384120238,train_acc:0.8772114515304565
node9 epoch2:node_model train_loss:0.23848281957601247,train_acc:0.9174885153770447
node9 epoch3:node_model train_loss:0.12855642661452293,train_acc:0.9623544812202454
node9 epoch4:node_model train_loss:0.09781276395446376,train_acc:0.976047933101654
node9_model on test-dataset: loss:0.8273908686637879,acc:0.7653000354766846
node9 weight score:2244.4047551539948
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6288677483797074,train_acc:0.8090000152587891
node10 epoch1:node_model train_loss:0.3790763556957245,train_acc:0.8761666417121887
node10 epoch2:node_model train_loss:0.2324552372097969,train_acc:0.921833336353302
node10 epoch3:node_model train_loss:0.15993078835308552,train_acc:0.9559999704360962
node10 epoch4:node_model train_loss:0.10869110450148582,train_acc:0.9733331799507141
node10_model on test-dataset: loss:0.7252969397604465,acc:0.7817000150680542
node10 weight score:2723.022656971791
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6491861616571745,train_acc:0.7984848022460938
node13 epoch1:node_model train_loss:0.30880868807435036,train_acc:0.8931061029434204
node13 epoch2:node_model train_loss:0.23560080863535404,train_acc:0.9284090399742126
node13 epoch3:node_model train_loss:0.13902590982615948,train_acc:0.955833375453949
node13 epoch4:node_model train_loss:0.09932944364845753,train_acc:0.9754545092582703
node13_model on test-dataset: loss:0.8050491490960121,acc:0.7685999870300293
node13 weight score:1434.6950137105875
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6076783528178931,acc:0.8121999800205231
total cost energy:6.859429753251346 | all_enery_cp：5.336499999999999 | all_enery_tp: 1.522929753251347
ef: 25.080288298576324
reward: 18.22085854532498
step 267:loss:17.507678985595703|running q:37.74535369873047
episode4,iteration27 selected nodes:[1, 13, 16, 0, 3],center node:3
################################################## episode4,iteration27 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.33268125613148397,train_acc:0.8802920579910278
node0 epoch1:node_model train_loss:0.19132219169002312,train_acc:0.9371501207351685
node0 epoch2:node_model train_loss:0.16163587448402092,train_acc:0.9498423933982849
node0 epoch3:node_model train_loss:0.12338020939093369,train_acc:0.9640732407569885
node0 epoch4:node_model train_loss:0.09452148555563046,train_acc:0.9761884808540344
node0_model on test-dataset: loss:0.8238901023566723,acc:0.7718999981880188
node0 weight score:6290.8875651915705
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3413448601084597,train_acc:0.8749264478683472
node1 epoch1:node_model train_loss:0.26270149100352735,train_acc:0.9082353115081787
node1 epoch2:node_model train_loss:0.24200630768695297,train_acc:0.9183087944984436
node1 epoch3:node_model train_loss:0.20450668113634868,train_acc:0.93522047996521
node1 epoch4:node_model train_loss:0.19267586026998126,train_acc:0.9388232231140137
node1_model on test-dataset: loss:0.7744851519167423,acc:0.7803000211715698
node1 weight score:8661.237705330617
node3: train data size:4247
node3 epoch0:node_model train_loss:0.36865289717219596,train_acc:0.8685700297355652
node3 epoch1:node_model train_loss:0.22330501173124756,train_acc:0.92333984375
node3 epoch2:node_model train_loss:0.16649965075559395,train_acc:0.9491835236549377
node3 epoch3:node_model train_loss:0.1391796441965325,train_acc:0.956595778465271
node3 epoch4:node_model train_loss:0.11227075972182807,train_acc:0.9673823714256287
node3_model on test-dataset: loss:0.8524458055198193,acc:0.7620999813079834
node3 weight score:4982.134902300552
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5895099366704623,train_acc:0.8175000548362732
node13 epoch1:node_model train_loss:0.2714923607806365,train_acc:0.9035606384277344
node13 epoch2:node_model train_loss:0.20883136490980783,train_acc:0.9354545474052429
node13 epoch3:node_model train_loss:0.14055074254671732,train_acc:0.9621211290359497
node13 epoch4:node_model train_loss:0.09324360080063343,train_acc:0.9801515340805054
node13_model on test-dataset: loss:0.7921845419704914,acc:0.7705999612808228
node13 weight score:1457.993609831159
node16: train data size:877
node16 epoch0:node_model train_loss:0.6630699833234152,train_acc:0.7964646220207214
node16 epoch1:node_model train_loss:0.3840176893605126,train_acc:0.8729004263877869
node16 epoch2:node_model train_loss:0.21529540088441637,train_acc:0.9312265515327454
node16 epoch3:node_model train_loss:0.15162802073690626,train_acc:0.9527849555015564
node16 epoch4:node_model train_loss:0.11541259495748414,train_acc:0.9715583920478821
node16_model on test-dataset: loss:0.79126110881567,acc:0.7652000188827515
node16 weight score:1108.3572669363982
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6114084699749947,acc:0.821699982881546
total cost energy:11.311454457418924 | all_enery_cp：9.085 | all_enery_tp: 2.2264544574189227
ef: 24.846128574993745
reward: 13.534674117574822
step 268:loss:30.472206115722656|running q:39.04192352294922
episode4,iteration28 selected nodes:[13, 6, 1, 0, 16],center node:6
################################################## episode4,iteration28 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.27019710609546077,train_acc:0.9058387875556946
node0 epoch1:node_model train_loss:0.16680455365433142,train_acc:0.9475741386413574
node0 epoch2:node_model train_loss:0.11820777176091304,train_acc:0.9639942049980164
node0 epoch3:node_model train_loss:0.08745620209866992,train_acc:0.9778058528900146
node0 epoch4:node_model train_loss:0.07602715169867644,train_acc:0.9837280511856079
node0_model on test-dataset: loss:0.7842849914729595,acc:0.7865999937057495
node0 weight score:6608.567110618614
node1: train data size:6708
node1 epoch0:node_model train_loss:0.2663227014024468,train_acc:0.9062498211860657
node1 epoch1:node_model train_loss:0.20247661327833638,train_acc:0.9335293769836426
node1 epoch2:node_model train_loss:0.16592593173332074,train_acc:0.9447058439254761
node1 epoch3:node_model train_loss:0.14675994140698628,train_acc:0.9563233852386475
node1 epoch4:node_model train_loss:0.10459680252653711,train_acc:0.9695587158203125
node1_model on test-dataset: loss:0.8434621123969555,acc:0.7702999114990234
node1 weight score:7952.935764876465
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6384255203508562,train_acc:0.8079723715782166
node6 epoch1:node_model train_loss:0.35975015692172513,train_acc:0.8791705369949341
node6 epoch2:node_model train_loss:0.302810481479091,train_acc:0.8904606699943542
node6 epoch3:node_model train_loss:0.2798332173016764,train_acc:0.9047464728355408
node6 epoch4:node_model train_loss:0.22245793405079073,train_acc:0.9280643463134766
node6_model on test-dataset: loss:0.8615625408291817,acc:0.7596997022628784
node6 weight score:3490.170309756056
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5660761222243309,train_acc:0.8171969652175903
node13 epoch1:node_model train_loss:0.3050910383462906,train_acc:0.9040908813476562
node13 epoch2:node_model train_loss:0.19920417418082556,train_acc:0.9271211624145508
node13 epoch3:node_model train_loss:0.12634992226958275,train_acc:0.9689394235610962
node13 epoch4:node_model train_loss:0.099943065084517,train_acc:0.9751514792442322
node13_model on test-dataset: loss:0.8086607800424099,acc:0.7698999047279358
node13 weight score:1428.2873962793478
node16: train data size:877
node16 epoch0:node_model train_loss:0.6088775396347046,train_acc:0.8239105343818665
node16 epoch1:node_model train_loss:0.32534264028072357,train_acc:0.8938961029052734
node16 epoch2:node_model train_loss:0.20137617737054825,train_acc:0.9404472708702087
node16 epoch3:node_model train_loss:0.14195959601137373,train_acc:0.9624531269073486
node16 epoch4:node_model train_loss:0.08906984039478832,train_acc:0.9775612950325012
node16_model on test-dataset: loss:0.8369574726372957,acc:0.760400116443634
node16 weight score:1047.842965349874
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.651369014903903,acc:0.8102999782562256
total cost energy:10.31612201029552 | all_enery_cp：8.465 | all_enery_tp: 1.8511220102955188
ef: 24.837815636119124
reward: 14.521693625823605
step 269:loss:21.620325088500977|running q:40.262447357177734
episode4,iteration29 selected nodes:[3, 17, 12, 15, 19],center node:17
################################################## episode4,iteration29 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.42565699822680897,train_acc:0.8500842452049255
node3 epoch1:node_model train_loss:0.22825196454691332,train_acc:0.9180502891540527
node3 epoch2:node_model train_loss:0.16920582724865094,train_acc:0.9439780712127686
node3 epoch3:node_model train_loss:0.11929627161386401,train_acc:0.9642699360847473
node3 epoch4:node_model train_loss:0.10814790826204211,train_acc:0.96685791015625
node3_model on test-dataset: loss:0.7985676491260528,acc:0.7812998294830322
node3 weight score:5318.272039504591
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6785609743424824,train_acc:0.8043650984764099
node12 epoch1:node_model train_loss:0.34987522129501614,train_acc:0.8857936859130859
node12 epoch2:node_model train_loss:0.22107084946972982,train_acc:0.928174614906311
node12 epoch3:node_model train_loss:0.15560766575591906,train_acc:0.9496031403541565
node12 epoch4:node_model train_loss:0.11814826780131885,train_acc:0.9764286279678345
node12_model on test-dataset: loss:0.8052557497471571,acc:0.7731000185012817
node12 weight score:1659.10023047894
node15: train data size:629
node15 epoch0:node_model train_loss:0.6839007820401873,train_acc:0.7904433012008667
node15 epoch1:node_model train_loss:0.329847799880164,train_acc:0.8944335579872131
node15 epoch2:node_model train_loss:0.20233871362039021,train_acc:0.9315763711929321
node15 epoch3:node_model train_loss:0.161754877439567,train_acc:0.9544335603713989
node15 epoch4:node_model train_loss:0.13044877776077815,train_acc:0.9636452794075012
node15_model on test-dataset: loss:0.8870445062220097,acc:0.7584002614021301
node15 weight score:709.0963255935816
node17: train data size:442
node17 epoch0:node_model train_loss:0.8997144103050232,train_acc:0.781333327293396
node17 epoch1:node_model train_loss:0.3720206439495087,train_acc:0.8741905093193054
node17 epoch2:node_model train_loss:0.27268803119659424,train_acc:0.9101905226707458
node17 epoch3:node_model train_loss:0.2064802810549736,train_acc:0.9477142691612244
node17 epoch4:node_model train_loss:0.1259566441178322,train_acc:0.965238094329834
node17_model on test-dataset: loss:0.9296495576202869,acc:0.7469001412391663
node17 weight score:475.4479753977722
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6094000263269558,train_acc:0.8132041096687317
node19 epoch1:node_model train_loss:0.3677982791911724,train_acc:0.8785787224769592
node19 epoch2:node_model train_loss:0.26640215758667435,train_acc:0.9057880640029907
node19 epoch3:node_model train_loss:0.20398858796025432,train_acc:0.9339132308959961
node19 epoch4:node_model train_loss:0.1614044603220252,train_acc:0.9518345594406128
node19_model on test-dataset: loss:0.7489039161801339,acc:0.7810999751091003
node19 weight score:5716.354137705284
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6150029378384352,acc:0.8167999774217606
total cost energy:7.374606781186547 | all_enery_cp：5.467499999999999 | all_enery_tp: 1.9071067811865479
ef: 24.78774568479568
reward: 17.413138903609134
step 270:loss:21.353057861328125|running q:41.54002380371094
episode4,iteration30 selected nodes:[3, 19, 14, 6, 9],center node:9
################################################## episode4,iteration30 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2731193460004274,train_acc:0.9012467265129089
node3 epoch1:node_model train_loss:0.16649129397647325,train_acc:0.9427015781402588
node3 epoch2:node_model train_loss:0.12494853987943294,train_acc:0.9552001953125
node3 epoch3:node_model train_loss:0.09852376831478851,train_acc:0.9746509790420532
node3 epoch4:node_model train_loss:0.07080605929327566,train_acc:0.9844184517860413
node3_model on test-dataset: loss:0.8285381709039211,acc:0.778799831867218
node3 weight score:5125.895401254229
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5580942460125492,train_acc:0.8296773433685303
node6 epoch1:node_model train_loss:0.33631203395705067,train_acc:0.8896772861480713
node6 epoch2:node_model train_loss:0.20307538610312245,train_acc:0.9315206408500671
node6 epoch3:node_model train_loss:0.21465034590613458,train_acc:0.9299079179763794
node6 epoch4:node_model train_loss:0.25002891498227275,train_acc:0.9121197462081909
node6_model on test-dataset: loss:1.0052330435812473,acc:0.7401998043060303
node6 weight score:2991.346155203225
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5688599646091461,train_acc:0.8280147910118103
node9 epoch1:node_model train_loss:0.2908723715104555,train_acc:0.9061772227287292
node9 epoch2:node_model train_loss:0.17240111451399953,train_acc:0.9493350982666016
node9 epoch3:node_model train_loss:0.13550398106637754,train_acc:0.9617080688476562
node9 epoch4:node_model train_loss:0.09874401221934118,train_acc:0.9732778668403625
node9_model on test-dataset: loss:0.7616812996566296,acc:0.7811999917030334
node9 weight score:2438.0275593442384
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5450452292958895,train_acc:0.8366204500198364
node14 epoch1:node_model train_loss:0.3051082342863083,train_acc:0.9092592000961304
node14 epoch2:node_model train_loss:0.20664726942777634,train_acc:0.9287499785423279
node14 epoch3:node_model train_loss:0.11364297568798065,train_acc:0.966342568397522
node14 epoch4:node_model train_loss:0.08999097812920809,train_acc:0.9826852083206177
node14_model on test-dataset: loss:0.7944951373338699,acc:0.774199903011322
node14 weight score:1475.1506270169803
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3842879218417545,train_acc:0.8650503158569336
node19 epoch1:node_model train_loss:0.22387655304614887,train_acc:0.9224086403846741
node19 epoch2:node_model train_loss:0.1770063673340997,train_acc:0.9435716867446899
node19 epoch3:node_model train_loss:0.14925658174378928,train_acc:0.9545419216156006
node19 epoch4:node_model train_loss:0.1298486706475879,train_acc:0.9622163772583008
node19_model on test-dataset: loss:0.8215715308487416,acc:0.7723999619483948
node19 weight score:5210.74530853987
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6357827626168728,acc:0.8127999800443649
total cost energy:8.822224255080032 | all_enery_cp：7.281999999999999 | all_enery_tp: 1.540224255080033
ef: 25.099407552023823
reward: 16.27718329694379
step 271:loss:23.20673942565918|running q:42.768985748291016
episode4,iteration31 selected nodes:[16, 4, 10, 8, 11],center node:10
################################################## episode4,iteration31 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6632234996982983,train_acc:0.7821429371833801
node4 epoch1:node_model train_loss:0.4418226767863546,train_acc:0.8485714793205261
node4 epoch2:node_model train_loss:0.30774684622883797,train_acc:0.8978571891784668
node4 epoch3:node_model train_loss:0.20111566329641,train_acc:0.936071515083313
node4 epoch4:node_model train_loss:0.16635360781635558,train_acc:0.9482141733169556
node4_model on test-dataset: loss:0.8059762094914913,acc:0.7723999619483948
node4 weight score:3356.178467980644
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6111763152811263,train_acc:0.8087075352668762
node8 epoch1:node_model train_loss:0.379753145078818,train_acc:0.8659636378288269
node8 epoch2:node_model train_loss:0.21891648901833427,train_acc:0.9349092841148376
node8 epoch3:node_model train_loss:0.17087732379635176,train_acc:0.9499545693397522
node8 epoch4:node_model train_loss:0.08160087259279357,train_acc:0.9833105206489563
node8_model on test-dataset: loss:0.7420052073895931,acc:0.7866001129150391
node8 weight score:2423.163587120154
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6202756613492966,train_acc:0.8050000071525574
node10 epoch1:node_model train_loss:0.3377606764435768,train_acc:0.8838332295417786
node10 epoch2:node_model train_loss:0.22073811255395412,train_acc:0.9315000772476196
node10 epoch3:node_model train_loss:0.14280192069709302,train_acc:0.9641667604446411
node10 epoch4:node_model train_loss:0.11250274665653706,train_acc:0.967166543006897
node10_model on test-dataset: loss:0.7885755625367165,acc:0.7752000093460083
node10 weight score:2504.515855965348
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6404148778494667,train_acc:0.8106169700622559
node11 epoch1:node_model train_loss:0.3903227378340328,train_acc:0.8734146356582642
node11 epoch2:node_model train_loss:0.26497552412397724,train_acc:0.9157675504684448
node11 epoch3:node_model train_loss:0.15042748740490744,train_acc:0.9572595953941345
node11 epoch4:node_model train_loss:0.11428314074873924,train_acc:0.9675895571708679
node11_model on test-dataset: loss:0.7820301933586598,acc:0.7798998951911926
node11 weight score:2150.812096878452
node16: train data size:877
node16 epoch0:node_model train_loss:0.6911203232076433,train_acc:0.7952381372451782
node16 epoch1:node_model train_loss:0.31238733066452873,train_acc:0.8997835516929626
node16 epoch2:node_model train_loss:0.2048676759004593,train_acc:0.9378931522369385
node16 epoch3:node_model train_loss:0.13292301446199417,train_acc:0.9634487628936768
node16 epoch4:node_model train_loss:0.08463414054777887,train_acc:0.9830015301704407
node16_model on test-dataset: loss:0.7885079311579466,acc:0.7760000228881836
node16 weight score:1112.2272400127924
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6128866812586784,acc:0.8180999833345414
total cost energy:5.623362717754106 | all_enery_cp：4.5185 | all_enery_tp: 1.1048627177541055
ef: 25.15414876252215
reward: 19.530786044768046
step 272:loss:27.340618133544922|running q:44.008949279785156
episode4,iteration32 selected nodes:[1, 19, 6, 7, 14],center node:6
################################################## episode4,iteration32 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3204395963865168,train_acc:0.885661780834198
node1 epoch1:node_model train_loss:0.23303551741820924,train_acc:0.9162497520446777
node1 epoch2:node_model train_loss:0.17819702669101603,train_acc:0.9397793412208557
node1 epoch3:node_model train_loss:0.1464509027736152,train_acc:0.9517646431922913
node1 epoch4:node_model train_loss:0.10366610552677337,train_acc:0.9716176390647888
node1_model on test-dataset: loss:0.7863529007136821,acc:0.7865999341011047
node1 weight score:8530.521085268358
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5173868661926638,train_acc:0.8305990695953369
node6 epoch1:node_model train_loss:0.3205979441202456,train_acc:0.8932256102561951
node6 epoch2:node_model train_loss:0.19356235693539342,train_acc:0.9347464442253113
node6 epoch3:node_model train_loss:0.1730985004575022,train_acc:0.9489399790763855
node6 epoch4:node_model train_loss:0.1945062810855527,train_acc:0.9403223991394043
node6_model on test-dataset: loss:1.042325195968151,acc:0.7236999869346619
node6 weight score:2884.8962028659253
node7: train data size:1951
node7 epoch0:node_model train_loss:0.453136420994997,train_acc:0.8466764688491821
node7 epoch1:node_model train_loss:0.22473863512277603,train_acc:0.9265783429145813
node7 epoch2:node_model train_loss:0.14993424993008375,train_acc:0.9550979733467102
node7 epoch3:node_model train_loss:0.09246775079518557,train_acc:0.9789997935295105
node7 epoch4:node_model train_loss:0.0771352343261242,train_acc:0.9810194969177246
node7_model on test-dataset: loss:0.8283551581203937,acc:0.7786002159118652
node7 weight score:2355.269935696399
node14: train data size:1172
node14 epoch0:node_model train_loss:0.48934923360745114,train_acc:0.8376389741897583
node14 epoch1:node_model train_loss:0.2924399158606927,train_acc:0.9073610305786133
node14 epoch2:node_model train_loss:0.1793883672604958,train_acc:0.9383795261383057
node14 epoch3:node_model train_loss:0.0989026886721452,train_acc:0.9698610305786133
node14 epoch4:node_model train_loss:0.06696041362981002,train_acc:0.986527681350708
node14_model on test-dataset: loss:0.7670197813212871,acc:0.7875998616218567
node14 weight score:1527.991883052982
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3717348918665287,train_acc:0.8705626130104065
node19 epoch1:node_model train_loss:0.19509498376485912,train_acc:0.9318200945854187
node19 epoch2:node_model train_loss:0.14506771839981855,train_acc:0.9508900046348572
node19 epoch3:node_model train_loss:0.11792981217420379,train_acc:0.9668129086494446
node19 epoch4:node_model train_loss:0.09049380830554075,train_acc:0.9775106906890869
node19_model on test-dataset: loss:0.7895179073512554,acc:0.7805997133255005
node19 weight score:5422.296264770331
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6230976360291243,acc:0.8217999792098999
total cost energy:10.15833138217799 | all_enery_cp：8.5595 | all_enery_tp: 1.598831382177989
ef: 25.036266688467386
reward: 14.877935306289396
step 273:loss:22.946685791015625|running q:45.12260055541992
episode4,iteration33 selected nodes:[11, 19, 7, 12, 2],center node:11
################################################## episode4,iteration33 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.4556248417745034,train_acc:0.8478977084159851
node2 epoch1:node_model train_loss:0.2549852626398206,train_acc:0.9153692722320557
node2 epoch2:node_model train_loss:0.1800301680341363,train_acc:0.9408426284790039
node2 epoch3:node_model train_loss:0.14864021120592952,train_acc:0.9503974914550781
node2 epoch4:node_model train_loss:0.10502468484143417,train_acc:0.9730113744735718
node2_model on test-dataset: loss:0.7164754566550254,acc:0.7953001260757446
node2 weight score:6682.713211633942
node7: train data size:1951
node7 epoch0:node_model train_loss:0.41634232476353644,train_acc:0.8671372532844543
node7 epoch1:node_model train_loss:0.2088204562664032,train_acc:0.9220979809761047
node7 epoch2:node_model train_loss:0.12877343073487282,train_acc:0.9590391516685486
node7 epoch3:node_model train_loss:0.09568039588630199,train_acc:0.976019561290741
node7 epoch4:node_model train_loss:0.06336983805522323,train_acc:0.9859998822212219
node7_model on test-dataset: loss:0.7862482884526253,acc:0.783799946308136
node7 weight score:2481.4044477472407
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5632092041127822,train_acc:0.8280631303787231
node11 epoch1:node_model train_loss:0.30165254719117107,train_acc:0.8990960717201233
node11 epoch2:node_model train_loss:0.18581557755961137,train_acc:0.9502007961273193
node11 epoch3:node_model train_loss:0.11649119415703942,train_acc:0.9698708653450012
node11 epoch4:node_model train_loss:0.093427032670554,train_acc:0.9745767116546631
node11_model on test-dataset: loss:0.8122153221070767,acc:0.7730998992919922
node11 weight score:2070.879425958745
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6734763192278999,train_acc:0.8002380728721619
node12 epoch1:node_model train_loss:0.37106639998299734,train_acc:0.8908730745315552
node12 epoch2:node_model train_loss:0.22370995634368487,train_acc:0.9329364895820618
node12 epoch3:node_model train_loss:0.14415948891213962,train_acc:0.9588888883590698
node12 epoch4:node_model train_loss:0.0988092196307012,train_acc:0.9799998998641968
node12_model on test-dataset: loss:0.8527970089763403,acc:0.7676999568939209
node12 weight score:1566.6096221463945
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3014573641987734,train_acc:0.8963623046875
node19 epoch1:node_model train_loss:0.1667687962221545,train_acc:0.9410535097122192
node19 epoch2:node_model train_loss:0.128636589750301,train_acc:0.9655957818031311
node19 epoch3:node_model train_loss:0.0967026045329349,train_acc:0.9737209677696228
node19 epoch4:node_model train_loss:0.07612550761117491,train_acc:0.9815733432769775
node19_model on test-dataset: loss:0.8744390031695366,acc:0.7693999409675598
node19 weight score:4895.7102605017235
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6106761260330678,acc:0.817299981713295
total cost energy:8.719 | all_enery_cp：7.019 | all_enery_tp: 1.7
ef: 25.138353145610726
reward: 16.419353145610728
step 274:loss:9.246585845947266|running q:46.36354446411133
episode4,iteration34 selected nodes:[14, 6, 16, 7, 3],center node:7
################################################## episode4,iteration34 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.34250900773114934,train_acc:0.8796486258506775
node3 epoch1:node_model train_loss:0.19113502859376197,train_acc:0.9371500015258789
node3 epoch2:node_model train_loss:0.12020292289035264,train_acc:0.9606084823608398
node3 epoch3:node_model train_loss:0.08829499607862429,train_acc:0.9753192663192749
node3 epoch4:node_model train_loss:0.06578457996595738,train_acc:0.9843592047691345
node3_model on test-dataset: loss:0.7635705722868442,acc:0.7932001352310181
node3 weight score:5562.026817351683
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4843048232216989,train_acc:0.8470047116279602
node6 epoch1:node_model train_loss:0.2936252264245864,train_acc:0.9033639430999756
node6 epoch2:node_model train_loss:0.24816154400187154,train_acc:0.9118432402610779
node6 epoch3:node_model train_loss:0.1758903375556392,train_acc:0.9414284825325012
node6 epoch4:node_model train_loss:0.1649531917946954,train_acc:0.9474193453788757
node6_model on test-dataset: loss:0.7993352907150983,acc:0.7738999724388123
node6 weight score:3761.875692126503
node7: train data size:1951
node7 epoch0:node_model train_loss:0.36366058960556985,train_acc:0.8756372332572937
node7 epoch1:node_model train_loss:0.18206004947423934,train_acc:0.9380588531494141
node7 epoch2:node_model train_loss:0.10207080524414777,train_acc:0.9710588455200195
node7 epoch3:node_model train_loss:0.07080040257424117,train_acc:0.9845194816589355
node7 epoch4:node_model train_loss:0.05034571858122945,train_acc:0.9909998774528503
node7_model on test-dataset: loss:0.7732276078313589,acc:0.7866001129150391
node7 weight score:2523.1897829823915
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5196436196565628,train_acc:0.8381018042564392
node14 epoch1:node_model train_loss:0.2655845234791438,train_acc:0.9142128825187683
node14 epoch2:node_model train_loss:0.16945751445988813,train_acc:0.9435648322105408
node14 epoch3:node_model train_loss:0.08785377225528161,train_acc:0.9768518209457397
node14 epoch4:node_model train_loss:0.07625629318257172,train_acc:0.9818518161773682
node14_model on test-dataset: loss:0.7973582102358341,acc:0.7817001342773438
node14 weight score:1469.8538059241384
node16: train data size:877
node16 epoch0:node_model train_loss:0.6666931807994843,train_acc:0.8180230259895325
node16 epoch1:node_model train_loss:0.3266250458028581,train_acc:0.8934632539749146
node16 epoch2:node_model train_loss:0.2072139953573545,train_acc:0.9360028505325317
node16 epoch3:node_model train_loss:0.15050550633006626,train_acc:0.950115442276001
node16 epoch4:node_model train_loss:0.09626994695928362,train_acc:0.9748917818069458
node16_model on test-dataset: loss:0.7922443190217018,acc:0.7760998606681824
node16 weight score:1106.9817465942301
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.610914982110262,acc:0.8248999816179275
total cost energy:6.960508749109257 | all_enery_cp：5.627 | all_enery_tp: 1.3335087491092574
ef: 25.073931102150535
reward: 18.11342235304128
step 275:loss:28.998018264770508|running q:47.59039306640625
episode4,iteration35 selected nodes:[16, 8, 0, 11, 18],center node:11
################################################## episode4,iteration35 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.365930628318053,train_acc:0.8777570128440857
node0 epoch1:node_model train_loss:0.223429681876531,train_acc:0.922189474105835
node0 epoch2:node_model train_loss:0.13405310247953123,train_acc:0.9609221816062927
node0 epoch3:node_model train_loss:0.09437611935516962,train_acc:0.9743443727493286
node0 epoch4:node_model train_loss:0.07525184055647025,train_acc:0.9824607372283936
node0_model on test-dataset: loss:0.7583477559685707,acc:0.7941999435424805
node0 weight score:6834.595288516693
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5243814024660323,train_acc:0.8465307354927063
node8 epoch1:node_model train_loss:0.25925053573317,train_acc:0.9110317230224609
node8 epoch2:node_model train_loss:0.18381010699603292,train_acc:0.9455214738845825
node8 epoch3:node_model train_loss:0.11352637203203307,train_acc:0.9666665196418762
node8 epoch4:node_model train_loss:0.07394129700130886,train_acc:0.9849771857261658
node8_model on test-dataset: loss:0.7618667680025101,acc:0.7851999402046204
node8 weight score:2359.992685747485
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5019261188366834,train_acc:0.83453369140625
node11 epoch1:node_model train_loss:0.23837792172151454,train_acc:0.9202724695205688
node11 epoch2:node_model train_loss:0.1588435041553834,train_acc:0.955365777015686
node11 epoch3:node_model train_loss:0.11913284177289289,train_acc:0.9647775292396545
node11 epoch4:node_model train_loss:0.05814368212047745,train_acc:0.9886943697929382
node11_model on test-dataset: loss:0.7719405952095986,acc:0.7843999266624451
node11 weight score:2178.9241431762
node16: train data size:877
node16 epoch0:node_model train_loss:0.5393291844262017,train_acc:0.8361327648162842
node16 epoch1:node_model train_loss:0.30845265752739376,train_acc:0.8940116167068481
node16 epoch2:node_model train_loss:0.1737827393743727,train_acc:0.9416738152503967
node16 epoch3:node_model train_loss:0.12582981917593214,train_acc:0.9626694917678833
node16 epoch4:node_model train_loss:0.06343280068702167,train_acc:0.9863348007202148
node16_model on test-dataset: loss:0.8089285555481911,acc:0.7757998704910278
node16 weight score:1084.1501316586339
node18: train data size:472
node18 epoch0:node_model train_loss:0.6328627347946167,train_acc:0.818222165107727
node18 epoch1:node_model train_loss:0.32801288962364195,train_acc:0.8956667184829712
node18 epoch2:node_model train_loss:0.21569458097219468,train_acc:0.9393333792686462
node18 epoch3:node_model train_loss:0.11682036370038987,train_acc:0.9684444665908813
node18 epoch4:node_model train_loss:0.07629453018307686,train_acc:0.9772221446037292
node18_model on test-dataset: loss:0.9238448282331229,acc:0.7617000937461853
node18 weight score:510.9083101138448
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6317914824187756,acc:0.8195999777317047
total cost energy:6.900812211415271 | all_enery_cp：5.005999999999999 | all_enery_tp: 1.8948122114152723
ef: 24.737811287249894
reward: 17.836999075834623
step 276:loss:21.759958267211914|running q:48.81564712524414
episode4,iteration36 selected nodes:[3, 15, 16, 7, 18],center node:15
################################################## episode4,iteration36 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2499780745007271,train_acc:0.912379801273346
node3 epoch1:node_model train_loss:0.13787168567610342,train_acc:0.9561898708343506
node3 epoch2:node_model train_loss:0.09572176453332569,train_acc:0.9734586477279663
node3 epoch3:node_model train_loss:0.07004488458813624,train_acc:0.982093095779419
node3 epoch4:node_model train_loss:0.05217318851933923,train_acc:0.9872093200683594
node3_model on test-dataset: loss:0.7989838286489248,acc:0.7907999753952026
node3 weight score:5315.501825840008
node7: train data size:1951
node7 epoch0:node_model train_loss:0.38558872789144516,train_acc:0.8691568374633789
node7 epoch1:node_model train_loss:0.18506474904716014,train_acc:0.9350782632827759
node7 epoch2:node_model train_loss:0.10699708946049213,train_acc:0.9695195555686951
node7 epoch3:node_model train_loss:0.07022717548534274,train_acc:0.9830194711685181
node7 epoch4:node_model train_loss:0.05662904838100076,train_acc:0.9875194430351257
node7_model on test-dataset: loss:0.7869293658435345,acc:0.7924001216888428
node7 weight score:2479.256823652351
node15: train data size:629
node15 epoch0:node_model train_loss:0.75221700327737,train_acc:0.7882265448570251
node15 epoch1:node_model train_loss:0.3176873028278351,train_acc:0.9017241597175598
node15 epoch2:node_model train_loss:0.22204123330967768,train_acc:0.9267980456352234
node15 epoch3:node_model train_loss:0.1378846519759723,train_acc:0.9579310417175293
node15 epoch4:node_model train_loss:0.07323018567902702,train_acc:0.9836452603340149
node15_model on test-dataset: loss:0.846392882168293,acc:0.7646998167037964
node15 weight score:743.1536975932796
node16: train data size:877
node16 epoch0:node_model train_loss:0.5174426999357011,train_acc:0.8445743322372437
node16 epoch1:node_model train_loss:0.28618644840187496,train_acc:0.8946753740310669
node16 epoch2:node_model train_loss:0.15117192351155812,train_acc:0.9543433785438538
node16 epoch3:node_model train_loss:0.10415565429462327,train_acc:0.9718902707099915
node16 epoch4:node_model train_loss:0.053649155216084585,train_acc:0.985223650932312
node16_model on test-dataset: loss:0.794815075993538,acc:0.7810999751091003
node16 weight score:1103.4013149583618
node18: train data size:472
node18 epoch0:node_model train_loss:0.665074634552002,train_acc:0.8386666178703308
node18 epoch1:node_model train_loss:0.24782350063323974,train_acc:0.9157778024673462
node18 epoch2:node_model train_loss:0.16078350245952605,train_acc:0.9556666612625122
node18 epoch3:node_model train_loss:0.0974866159260273,train_acc:0.9716666340827942
node18 epoch4:node_model train_loss:0.07574174478650093,train_acc:0.9784443974494934
node18_model on test-dataset: loss:0.8970615506917238,acc:0.7667999267578125
node18 weight score:526.1623348320313
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.656118957400322,acc:0.8181999808549881
total cost energy:6.1768405611671415 | all_enery_cp：4.087999999999999 | all_enery_tp: 2.088840561167142
ef: 24.692778696281472
reward: 18.515938135114332
step 277:loss:15.88196086883545|running q:50.035133361816406
episode4,iteration37 selected nodes:[13, 8, 4, 16, 19],center node:8
################################################## episode4,iteration37 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6261256784200668,train_acc:0.8214285373687744
node4 epoch1:node_model train_loss:0.42361425342304365,train_acc:0.856071412563324
node4 epoch2:node_model train_loss:0.33654262870550156,train_acc:0.8971428871154785
node4 epoch3:node_model train_loss:0.24024539893226965,train_acc:0.92249995470047
node4 epoch4:node_model train_loss:0.17009012361190148,train_acc:0.9453569650650024
node4_model on test-dataset: loss:0.8334537874907255,acc:0.7719998955726624
node4 weight score:3245.5308747758263
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4835884753200743,train_acc:0.8476191163063049
node8 epoch1:node_model train_loss:0.2750363209181362,train_acc:0.9099092483520508
node8 epoch2:node_model train_loss:0.18374703700343767,train_acc:0.9388208389282227
node8 epoch3:node_model train_loss:0.09238307488461335,train_acc:0.9794330596923828
node8 epoch4:node_model train_loss:0.05931224239369234,train_acc:0.9872108101844788
node8_model on test-dataset: loss:0.7554969894886017,acc:0.7886000871658325
node8 weight score:2379.8903569649856
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7114223738511404,train_acc:0.7892424464225769
node13 epoch1:node_model train_loss:0.3519606826206048,train_acc:0.8806060552597046
node13 epoch2:node_model train_loss:0.22305152378976345,train_acc:0.9271212816238403
node13 epoch3:node_model train_loss:0.1458405777812004,train_acc:0.9569697380065918
node13 epoch4:node_model train_loss:0.08587371278554201,train_acc:0.9816665649414062
node13_model on test-dataset: loss:0.8329221791774034,acc:0.7737998962402344
node13 weight score:1386.6841691509303
node16: train data size:877
node16 epoch0:node_model train_loss:0.5122012429767184,train_acc:0.8427994251251221
node16 epoch1:node_model train_loss:0.25965536137421924,train_acc:0.9086724519729614
node16 epoch2:node_model train_loss:0.16737756547000673,train_acc:0.9464501142501831
node16 epoch3:node_model train_loss:0.10231075353092617,train_acc:0.9696680903434753
node16 epoch4:node_model train_loss:0.0653014855666293,train_acc:0.9855554699897766
node16_model on test-dataset: loss:0.8352705188095569,acc:0.7741999626159668
node16 weight score:1049.9592410491355
node19: train data size:4281
node19 epoch0:node_model train_loss:0.37186364725578663,train_acc:0.8719580173492432
node19 epoch1:node_model train_loss:0.1815794498421425,train_acc:0.9399597644805908
node19 epoch2:node_model train_loss:0.13623207365704137,train_acc:0.9585098624229431
node19 epoch3:node_model train_loss:0.09166634377352027,train_acc:0.9738442897796631
node19 epoch4:node_model train_loss:0.06567704980803091,train_acc:0.9858827590942383
node19_model on test-dataset: loss:0.8058906313031912,acc:0.7841999530792236
node19 weight score:5312.135212537802
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6296845638751983,acc:0.8206999826431275
total cost energy:7.254970086568532 | all_enery_cp：5.4079999999999995 | all_enery_tp: 1.8469700865685326
ef: 24.910782052384494
reward: 17.655811965815964
step 278:loss:21.712562561035156|running q:51.22884750366211
episode4,iteration38 selected nodes:[5, 9, 10, 12, 2],center node:9
################################################## episode4,iteration38 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.378793445105354,train_acc:0.8747159838676453
node2 epoch1:node_model train_loss:0.20678844830642143,train_acc:0.9292044639587402
node2 epoch2:node_model train_loss:0.14440429249467948,train_acc:0.9536930918693542
node2 epoch3:node_model train_loss:0.10016995528712869,train_acc:0.9710795879364014
node2 epoch4:node_model train_loss:0.0830315831117332,train_acc:0.9814299941062927
node2_model on test-dataset: loss:0.8885246416926385,acc:0.7680997848510742
node2 weight score:5388.708174574501
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5618798152396554,train_acc:0.821654200553894
node5 epoch1:node_model train_loss:0.32007608699955437,train_acc:0.8855637907981873
node5 epoch2:node_model train_loss:0.2206498968758081,train_acc:0.9228196740150452
node5 epoch3:node_model train_loss:0.17548247975738426,train_acc:0.9435338377952576
node5 epoch4:node_model train_loss:0.13135325869447306,train_acc:0.9637969136238098
node5_model on test-dataset: loss:0.7372491852939129,acc:0.7909999489784241
node5 weight score:5066.129708249185
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5536149473566758,train_acc:0.83274245262146
node9 epoch1:node_model train_loss:0.30524095814479024,train_acc:0.9048568606376648
node9 epoch2:node_model train_loss:0.19310672894904488,train_acc:0.9348569512367249
node9 epoch3:node_model train_loss:0.13958370724790975,train_acc:0.9574884176254272
node9 epoch4:node_model train_loss:0.0820368418568059,train_acc:0.9769712686538696
node9_model on test-dataset: loss:0.7700170275568962,acc:0.7882001996040344
node9 weight score:2411.634981491091
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6144829392433167,train_acc:0.8245000839233398
node10 epoch1:node_model train_loss:0.32933270260691644,train_acc:0.900833249092102
node10 epoch2:node_model train_loss:0.19937785156071186,train_acc:0.9326665997505188
node10 epoch3:node_model train_loss:0.1461457207798958,train_acc:0.9538334012031555
node10 epoch4:node_model train_loss:0.07977865301072598,train_acc:0.9788331985473633
node10_model on test-dataset: loss:0.7361889131367206,acc:0.7925999164581299
node10 weight score:2682.735320727677
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6825359208243233,train_acc:0.8048412799835205
node12 epoch1:node_model train_loss:0.40812612644263674,train_acc:0.8807936906814575
node12 epoch2:node_model train_loss:0.22624752989837102,train_acc:0.9190476536750793
node12 epoch3:node_model train_loss:0.1765636588845934,train_acc:0.9384920597076416
node12 epoch4:node_model train_loss:0.0897694815482412,train_acc:0.9785714149475098
node12_model on test-dataset: loss:0.7756293150782585,acc:0.786799967288971
node12 weight score:1722.472286733002
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6101106483489275,acc:0.8217999815940857
total cost energy:8.559056291411146 | all_enery_cp：6.8455 | all_enery_tp: 1.7135562914111455
ef: 25.14119602702854
reward: 16.582139735617393
step 279:loss:11.220370292663574|running q:52.339515686035156
episode4,iteration39 selected nodes:[5, 19, 6, 0, 11],center node:6
################################################## episode4,iteration39 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.2628989744071777,train_acc:0.904107928276062
node0 epoch1:node_model train_loss:0.14361331712167996,train_acc:0.9528406262397766
node0 epoch2:node_model train_loss:0.09251298382878304,train_acc:0.973190426826477
node0 epoch3:node_model train_loss:0.07658936126300922,train_acc:0.9796155691146851
node0 epoch4:node_model train_loss:0.06590807516700946,train_acc:0.9833433628082275
node0_model on test-dataset: loss:0.8075089436769486,acc:0.7915998697280884
node0 weight score:6418.504761568941
node5: train data size:3735
node5 epoch0:node_model train_loss:0.402322839357351,train_acc:0.8656014800071716
node5 epoch1:node_model train_loss:0.22549295895978025,train_acc:0.9211653470993042
node5 epoch2:node_model train_loss:0.1636485941708088,train_acc:0.9487593770027161
node5 epoch3:node_model train_loss:0.11820011693788202,train_acc:0.9637593030929565
node5 epoch4:node_model train_loss:0.11008810565659874,train_acc:0.9674434661865234
node5_model on test-dataset: loss:0.779675474613905,acc:0.7891001105308533
node5 weight score:4790.454646338043
node6: train data size:3007
node6 epoch0:node_model train_loss:0.46807469715995176,train_acc:0.8489400744438171
node6 epoch1:node_model train_loss:0.27377214811501965,train_acc:0.9083871245384216
node6 epoch2:node_model train_loss:0.1707740643722636,train_acc:0.9425805807113647
node6 epoch3:node_model train_loss:0.1273504095452447,train_acc:0.9638708233833313
node6 epoch4:node_model train_loss:0.10058547195888334,train_acc:0.9744238257408142
node6_model on test-dataset: loss:0.7575008547306061,acc:0.7904998660087585
node6 weight score:3969.632484532832
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5286355141331168,train_acc:0.8282784223556519
node11 epoch1:node_model train_loss:0.2333615019040949,train_acc:0.9198134541511536
node11 epoch2:node_model train_loss:0.1421724474605392,train_acc:0.9569297432899475
node11 epoch3:node_model train_loss:0.09654770090299494,train_acc:0.975753128528595
node11 epoch4:node_model train_loss:0.06155220200033749,train_acc:0.9864705204963684
node11_model on test-dataset: loss:0.7760771292448044,acc:0.7877999544143677
node11 weight score:2167.3103569444743
node19: train data size:4281
node19 epoch0:node_model train_loss:0.26019415328668993,train_acc:0.9069911241531372
node19 epoch1:node_model train_loss:0.15189934677855912,train_acc:0.9516419768333435
node19 epoch2:node_model train_loss:0.0877700001001358,train_acc:0.9742546677589417
node19 epoch3:node_model train_loss:0.06865294468264248,train_acc:0.9833248257637024
node19 epoch4:node_model train_loss:0.060131552217658174,train_acc:0.9871546626091003
node19_model on test-dataset: loss:0.8051174902915954,acc:0.7814000248908997
node19 weight score:5317.23636813494
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6106614790856838,acc:0.8264999812841416
total cost energy:10.437696413877289 | all_enery_cp：8.943999999999999 | all_enery_tp: 1.4936964138772901
ef: 25.353061383045272
reward: 14.915364969167983
step 280:loss:18.045429229736328|running q:53.41850280761719
episode4,iteration40 selected nodes:[19, 0, 6, 10, 18],center node:6
################################################## episode4,iteration40 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.19623194448649883,train_acc:0.9301136136054993
node0 epoch1:node_model train_loss:0.11700046388432384,train_acc:0.961844265460968
node0 epoch2:node_model train_loss:0.07848594910823382,train_acc:0.9786145687103271
node0 epoch3:node_model train_loss:0.054196789824905306,train_acc:0.9883828163146973
node0 epoch4:node_model train_loss:0.045260344130488545,train_acc:0.9911539554595947
node0_model on test-dataset: loss:0.7745577618479729,acc:0.8012000322341919
node0 weight score:6691.560339714598
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3840556091839267,train_acc:0.8786175847053528
node6 epoch1:node_model train_loss:0.26842469097145144,train_acc:0.9095852971076965
node6 epoch2:node_model train_loss:0.1816461361463993,train_acc:0.9405528903007507
node6 epoch3:node_model train_loss:0.1277276217456787,train_acc:0.962257981300354
node6 epoch4:node_model train_loss:0.10693070761138393,train_acc:0.9709674715995789
node6_model on test-dataset: loss:0.8475046745687723,acc:0.777899980545044
node6 weight score:3548.0630257644575
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5197943918406963,train_acc:0.8454999327659607
node10 epoch1:node_model train_loss:0.3028726547956467,train_acc:0.901166558265686
node10 epoch2:node_model train_loss:0.21888218708336354,train_acc:0.9251666069030762
node10 epoch3:node_model train_loss:0.12165826950222254,train_acc:0.9639999270439148
node10 epoch4:node_model train_loss:0.08198660332709551,train_acc:0.9803333282470703
node10_model on test-dataset: loss:0.7269928248226643,acc:0.7956998348236084
node10 weight score:2716.670553773021
node18: train data size:472
node18 epoch0:node_model train_loss:0.596337902545929,train_acc:0.8366667032241821
node18 epoch1:node_model train_loss:0.2357148140668869,train_acc:0.9221111536026001
node18 epoch2:node_model train_loss:0.1626474067568779,train_acc:0.9584444165229797
node18 epoch3:node_model train_loss:0.067163398116827,train_acc:0.9872221946716309
node18 epoch4:node_model train_loss:0.041552460938692096,train_acc:0.9939999580383301
node18_model on test-dataset: loss:0.9011295227706433,acc:0.7610998153686523
node18 weight score:523.7870784088539
node19: train data size:4281
node19 epoch0:node_model train_loss:0.19692320220692214,train_acc:0.9265402555465698
node19 epoch1:node_model train_loss:0.11124375003368356,train_acc:0.9639678001403809
node19 epoch2:node_model train_loss:0.07945207610379817,train_acc:0.9770456552505493
node19 epoch3:node_model train_loss:0.054023674277718675,train_acc:0.9871003031730652
node19 epoch4:node_model train_loss:0.052804235888775,train_acc:0.9865115880966187
node19_model on test-dataset: loss:0.785805379897356,acc:0.7948998212814331
node19 weight score:5447.913834032538
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6361959330737591,acc:0.8226999795436859
total cost energy:9.612211184795624 | all_enery_cp：7.458999999999999 | all_enery_tp: 2.1532111847956252
ef: 24.856840967715105
reward: 15.24462978291948
step 281:loss:12.169812202453613|running q:54.48617935180664
episode4,iteration41 selected nodes:[10, 0, 4, 14, 3],center node:3
################################################## episode4,iteration41 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.13157781543066868,train_acc:0.9585750102996826
node0 epoch1:node_model train_loss:0.0826637322942798,train_acc:0.9742655158042908
node0 epoch2:node_model train_loss:0.07788400960942873,train_acc:0.9769578576087952
node0 epoch3:node_model train_loss:0.06576256414588827,train_acc:0.9811145663261414
node0 epoch4:node_model train_loss:0.05468900645008454,train_acc:0.986499011516571
node0_model on test-dataset: loss:0.8302629022300243,acc:0.7900999188423157
node0 weight score:6242.600971425855
node3: train data size:4247
node3 epoch0:node_model train_loss:0.26012990051923796,train_acc:0.9023502469062805
node3 epoch1:node_model train_loss:0.13532858454557353,train_acc:0.9559574723243713
node3 epoch2:node_model train_loss:0.08513442487564198,train_acc:0.9760463833808899
node3 epoch3:node_model train_loss:0.06987288013793701,train_acc:0.9810736775398254
node3 epoch4:node_model train_loss:0.05065421465524407,train_acc:0.9883720874786377
node3_model on test-dataset: loss:0.7746519359946251,acc:0.7986000776290894
node3 weight score:5482.462255189494
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5271959127193051,train_acc:0.8371428847312927
node4 epoch1:node_model train_loss:0.2744890825290765,train_acc:0.9117857217788696
node4 epoch2:node_model train_loss:0.18934894406369754,train_acc:0.9410713911056519
node4 epoch3:node_model train_loss:0.1601583305746317,train_acc:0.9503571391105652
node4 epoch4:node_model train_loss:0.12614312022924423,train_acc:0.962142825126648
node4_model on test-dataset: loss:0.8284573113918304,acc:0.7816999554634094
node4 weight score:3265.104867570699
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4640620470046997,train_acc:0.859000027179718
node10 epoch1:node_model train_loss:0.25861306935548783,train_acc:0.9146665930747986
node10 epoch2:node_model train_loss:0.15795316100120543,train_acc:0.9549999237060547
node10 epoch3:node_model train_loss:0.10728970300406218,train_acc:0.9726665616035461
node10 epoch4:node_model train_loss:0.057843296229839324,train_acc:0.9859998822212219
node10_model on test-dataset: loss:0.8024147841334343,acc:0.7772999405860901
node10 weight score:2461.3205527274727
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5005944296717644,train_acc:0.8529629111289978
node14 epoch1:node_model train_loss:0.2707921223094066,train_acc:0.9120370149612427
node14 epoch2:node_model train_loss:0.18144601210951805,train_acc:0.9487037062644958
node14 epoch3:node_model train_loss:0.10135862898702423,train_acc:0.9760184288024902
node14 epoch4:node_model train_loss:0.06323528413971265,train_acc:0.9830090999603271
node14_model on test-dataset: loss:0.8031666721403599,acc:0.7907000780105591
node14 weight score:1459.223895429744
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6294769228249788,acc:0.8257999801635743
total cost energy:9.111399531493252 | all_enery_cp：7.641 | all_enery_tp: 1.4703995314932514
ef: 25.14651109060691
reward: 16.03511155911366
step 282:loss:12.04607105255127|running q:55.58795166015625
episode4,iteration42 selected nodes:[6, 9, 13, 2, 17],center node:9
################################################## episode4,iteration42 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3657235326245427,train_acc:0.8806153535842896
node2 epoch1:node_model train_loss:0.19642275385558605,train_acc:0.9312878847122192
node2 epoch2:node_model train_loss:0.13326352974399924,train_acc:0.9566099643707275
node2 epoch3:node_model train_loss:0.09312270449784894,train_acc:0.9745263457298279
node2 epoch4:node_model train_loss:0.07339320618969698,train_acc:0.983096718788147
node2_model on test-dataset: loss:0.7437162074446678,acc:0.7984000444412231
node2 weight score:6437.939568980316
node6: train data size:3007
node6 epoch0:node_model train_loss:0.38608561912851946,train_acc:0.8687096834182739
node6 epoch1:node_model train_loss:0.22214596165764716,train_acc:0.9218432903289795
node6 epoch2:node_model train_loss:0.1656157053526371,train_acc:0.9490322470664978
node6 epoch3:node_model train_loss:0.1052117323082301,train_acc:0.9687094688415527
node6 epoch4:node_model train_loss:0.06273970200169471,train_acc:0.9845159649848938
node6_model on test-dataset: loss:0.7817616870999337,acc:0.7930001020431519
node6 weight score:3846.440737144504
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5824200165899176,train_acc:0.8268235921859741
node9 epoch1:node_model train_loss:0.2590332501812985,train_acc:0.9084025025367737
node9 epoch2:node_model train_loss:0.17675728978295074,train_acc:0.9445982575416565
node9 epoch3:node_model train_loss:0.11080576891177579,train_acc:0.9676269888877869
node9 epoch4:node_model train_loss:0.07556400212802385,train_acc:0.9821051359176636
node9_model on test-dataset: loss:0.8084224799275398,acc:0.7836998701095581
node9 weight score:2297.06625694828
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6428894971807798,train_acc:0.8156818747520447
node13 epoch1:node_model train_loss:0.3093865302701791,train_acc:0.8953787684440613
node13 epoch2:node_model train_loss:0.21665601991117,train_acc:0.926742434501648
node13 epoch3:node_model train_loss:0.1459123349438111,train_acc:0.9594696760177612
node13 epoch4:node_model train_loss:0.09311130177229643,train_acc:0.9734847545623779
node13_model on test-dataset: loss:0.90520986571908,acc:0.7602999210357666
node13 weight score:1275.9472070959941
node17: train data size:442
node17 epoch0:node_model train_loss:0.8525578141212463,train_acc:0.7971428632736206
node17 epoch1:node_model train_loss:0.3871747970581055,train_acc:0.8641905188560486
node17 epoch2:node_model train_loss:0.2428086668252945,train_acc:0.9217142462730408
node17 epoch3:node_model train_loss:0.16179690212011338,train_acc:0.939714252948761
node17 epoch4:node_model train_loss:0.14773711264133454,train_acc:0.9569523930549622
node17_model on test-dataset: loss:0.9533918289095163,acc:0.7583997845649719
node17 weight score:463.60791712003333
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6409808544814587,acc:0.8210999822616577
total cost energy:7.30103912635191 | all_enery_cp：5.6245 | all_enery_tp: 1.676539126351909
ef: 24.81230464112975
reward: 17.51126551477784
step 283:loss:7.608278274536133|running q:56.68047332763672
episode4,iteration43 selected nodes:[15, 11, 4, 16, 8],center node:11
################################################## episode4,iteration43 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.48744795311774525,train_acc:0.8471428751945496
node4 epoch1:node_model train_loss:0.3040996244443314,train_acc:0.9050000309944153
node4 epoch2:node_model train_loss:0.16794626574431146,train_acc:0.9500000476837158
node4 epoch3:node_model train_loss:0.12031395081430674,train_acc:0.9653570652008057
node4 epoch4:node_model train_loss:0.1038435170693057,train_acc:0.9724997282028198
node4_model on test-dataset: loss:0.8047570231556892,acc:0.7875999212265015
node4 weight score:3361.2629926395657
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5309382379055023,train_acc:0.836462676525116
node8 epoch1:node_model train_loss:0.24470844119787216,train_acc:0.919387698173523
node8 epoch2:node_model train_loss:0.15950520833333334,train_acc:0.9538434147834778
node8 epoch3:node_model train_loss:0.09553264971408579,train_acc:0.9710770845413208
node8 epoch4:node_model train_loss:0.06672197156068352,train_acc:0.9844329953193665
node8_model on test-dataset: loss:0.7901097181439399,acc:0.7855001091957092
node8 weight score:2275.6333186531515
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5312750339508057,train_acc:0.8390387296676636
node11 epoch1:node_model train_loss:0.28831535928389607,train_acc:0.9066139459609985
node11 epoch2:node_model train_loss:0.18451638344456167,train_acc:0.9409900307655334
node11 epoch3:node_model train_loss:0.1221964972860673,train_acc:0.9619655609130859
node11 epoch4:node_model train_loss:0.07447803502573687,train_acc:0.9804589748382568
node11_model on test-dataset: loss:0.8124835777282715,acc:0.7871999144554138
node11 weight score:2070.1956890044753
node15: train data size:629
node15 epoch0:node_model train_loss:0.619323900767735,train_acc:0.8088670969009399
node15 epoch1:node_model train_loss:0.2685042939015797,train_acc:0.900295615196228
node15 epoch2:node_model train_loss:0.20504670696599142,train_acc:0.927931010723114
node15 epoch3:node_model train_loss:0.12595982264195169,train_acc:0.9600000381469727
node15 epoch4:node_model train_loss:0.10467449788536344,train_acc:0.9658620357513428
node15_model on test-dataset: loss:0.8992214155197144,acc:0.7708001732826233
node15 weight score:699.4940168728776
node16: train data size:877
node16 epoch0:node_model train_loss:0.5603863530688815,train_acc:0.8454545736312866
node16 epoch1:node_model train_loss:0.25516490389903385,train_acc:0.9181240797042847
node16 epoch2:node_model train_loss:0.17570622265338898,train_acc:0.9527849555015564
node16 epoch3:node_model train_loss:0.10249818406171268,train_acc:0.9771139621734619
node16 epoch4:node_model train_loss:0.07317485411961873,train_acc:0.9771139621734619
node16_model on test-dataset: loss:0.811003956347704,acc:0.7828999161720276
node16 weight score:1081.3757357602844
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.651088149100542,acc:0.8206999808549881
total cost energy:5.533205430228724 | all_enery_cp：3.8455 | all_enery_tp: 1.6877054302287244
ef: 24.66564770785639
reward: 19.132442277627668
step 284:loss:8.485845565795898|running q:57.701454162597656
episode4,iteration44 selected nodes:[2, 1, 7, 0, 6],center node:1
################################################## episode4,iteration44 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.17227382657046503,train_acc:0.9370365738868713
node0 epoch1:node_model train_loss:0.0800493833823846,train_acc:0.9776135087013245
node0 epoch2:node_model train_loss:0.058710196592773385,train_acc:0.9851483702659607
node0 epoch3:node_model train_loss:0.04397390983425654,train_acc:0.9909221529960632
node0 epoch4:node_model train_loss:0.04120961934901201,train_acc:0.9928847551345825
node0_model on test-dataset: loss:0.8538483791053295,acc:0.7839000821113586
node0 weight score:6070.164360364304
node1: train data size:6708
node1 epoch0:node_model train_loss:0.358224270372268,train_acc:0.8766176700592041
node1 epoch1:node_model train_loss:0.21790705350063302,train_acc:0.9224997162818909
node1 epoch2:node_model train_loss:0.1579045973279897,train_acc:0.9491175413131714
node1 epoch3:node_model train_loss:0.12754823672859109,train_acc:0.9608088135719299
node1 epoch4:node_model train_loss:0.13167424724601648,train_acc:0.9543381929397583
node1_model on test-dataset: loss:0.8459046523272992,acc:0.7828998565673828
node1 weight score:7929.971754552459
node2: train data size:4788
node2 epoch0:node_model train_loss:0.20737913995981216,train_acc:0.9285228252410889
node2 epoch1:node_model train_loss:0.13962729934913418,train_acc:0.9531342387199402
node2 epoch2:node_model train_loss:0.10282063049574693,train_acc:0.9692896604537964
node2 epoch3:node_model train_loss:0.06779334725191195,train_acc:0.9818751215934753
node2 epoch4:node_model train_loss:0.053198124708918236,train_acc:0.9879168272018433
node2_model on test-dataset: loss:0.7913831622898578,acc:0.7879999876022339
node2 weight score:6050.1666299621265
node6: train data size:3007
node6 epoch0:node_model train_loss:0.38732616266896647,train_acc:0.8735021352767944
node6 epoch1:node_model train_loss:0.21664997142168782,train_acc:0.9193547368049622
node6 epoch2:node_model train_loss:0.1244986052474668,train_acc:0.9638707041740417
node6 epoch3:node_model train_loss:0.08699477902583537,train_acc:0.9758062958717346
node6 epoch4:node_model train_loss:0.0675696207270507,train_acc:0.9835482239723206
node6_model on test-dataset: loss:0.7409709712862969,acc:0.803399920463562
node6 weight score:4058.1886693617225
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4045303545892239,train_acc:0.8666568994522095
node7 epoch1:node_model train_loss:0.20347169041633606,train_acc:0.9345588684082031
node7 epoch2:node_model train_loss:0.11363750118762254,train_acc:0.9660391807556152
node7 epoch3:node_model train_loss:0.07483759466558695,train_acc:0.9815195202827454
node7 epoch4:node_model train_loss:0.04553246949799359,train_acc:0.9910588264465332
node7_model on test-dataset: loss:0.7590177893638611,acc:0.7956001162528992
node7 weight score:2570.4272381219794
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6337145288288594,acc:0.8238999807834625
total cost energy:11.644083281533689 | all_enery_cp：10.818500000000002 | all_enery_tp: 0.8255832815336874
ef: 25.423126937485108
reward: 13.779043655951419
step 285:loss:9.340773582458496|running q:58.734004974365234
episode4,iteration45 selected nodes:[5, 19, 13, 15, 0],center node:5
################################################## episode4,iteration45 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.13303510851871508,train_acc:0.9522242546081543
node0 epoch1:node_model train_loss:0.08429136897365634,train_acc:0.9768048524856567
node0 epoch2:node_model train_loss:0.06432531437335107,train_acc:0.9824214577674866
node0 epoch3:node_model train_loss:0.058950141681214936,train_acc:0.9839597344398499
node0 epoch4:node_model train_loss:0.04711243563976426,train_acc:0.9884222745895386
node0_model on test-dataset: loss:0.8183958938717842,acc:0.7940000891685486
node0 weight score:6333.12072899037
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4746358229925758,train_acc:0.8460150361061096
node5 epoch1:node_model train_loss:0.27289393034420517,train_acc:0.9056013822555542
node5 epoch2:node_model train_loss:0.1583530171529243,train_acc:0.9455262422561646
node5 epoch3:node_model train_loss:0.11674397262303453,train_acc:0.9627443552017212
node5 epoch4:node_model train_loss:0.09718052209600021,train_acc:0.9756390452384949
node5_model on test-dataset: loss:0.7829560853540898,acc:0.7934998869895935
node5 weight score:4770.382489984552
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6672905931870142,train_acc:0.8214394450187683
node13 epoch1:node_model train_loss:0.2561447856326898,train_acc:0.913787841796875
node13 epoch2:node_model train_loss:0.17989474534988403,train_acc:0.9290909171104431
node13 epoch3:node_model train_loss:0.11744230364759763,train_acc:0.9634848833084106
node13 epoch4:node_model train_loss:0.06768673410018285,train_acc:0.9816665649414062
node13_model on test-dataset: loss:0.7717515996098518,acc:0.7887999415397644
node13 weight score:1496.5955374551786
node15: train data size:629
node15 epoch0:node_model train_loss:0.5998979977199009,train_acc:0.818867027759552
node15 epoch1:node_model train_loss:0.2779122569731304,train_acc:0.9023644924163818
node15 epoch2:node_model train_loss:0.2039410833801542,train_acc:0.9309359192848206
node15 epoch3:node_model train_loss:0.10676381098372596,train_acc:0.9622167348861694
node15 epoch4:node_model train_loss:0.08680483379534312,train_acc:0.9701478481292725
node15_model on test-dataset: loss:0.8474036739766597,acc:0.7735001444816589
node15 weight score:742.2672562277854
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2806146746111471,train_acc:0.8971834182739258
node19 epoch1:node_model train_loss:0.14219244326962982,train_acc:0.9492074847221375
node19 epoch2:node_model train_loss:0.09250487976296004,train_acc:0.972107470035553
node19 epoch3:node_model train_loss:0.0729845879209596,train_acc:0.9778667688369751
node19 epoch4:node_model train_loss:0.054842509502588316,train_acc:0.9869221448898315
node19_model on test-dataset: loss:0.7948351559042931,acc:0.7925000786781311
node19 weight score:5386.022457863552
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6468464183807373,acc:0.8236999797821045
total cost energy:9.946409072880368 | all_enery_cp：7.491499999999999 | all_enery_tp: 2.454909072880369
ef: 24.726369872655045
reward: 14.779960799774678
step 286:loss:6.110034465789795|running q:59.761417388916016
episode4,iteration46 selected nodes:[4, 0, 8, 11, 1],center node:4
################################################## episode4,iteration46 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.09293417242140724,train_acc:0.9700348377227783
node0 epoch1:node_model train_loss:0.0814970604215677,train_acc:0.9741913080215454
node0 epoch2:node_model train_loss:0.0539410265090947,train_acc:0.9859223365783691
node0 epoch3:node_model train_loss:0.04217422300448211,train_acc:0.9907300472259521
node0 epoch4:node_model train_loss:0.03977603436662601,train_acc:0.9901531934738159
node0_model on test-dataset: loss:0.8189833714812994,acc:0.7919999361038208
node0 weight score:6328.577820359749
node1: train data size:6708
node1 epoch0:node_model train_loss:0.29931549618349357,train_acc:0.8911028504371643
node1 epoch1:node_model train_loss:0.18467776871779384,train_acc:0.9369115829467773
node1 epoch2:node_model train_loss:0.15012953014058225,train_acc:0.9468382596969604
node1 epoch3:node_model train_loss:0.1644267534070155,train_acc:0.9418380856513977
node1 epoch4:node_model train_loss:0.1138414883547846,train_acc:0.9634560942649841
node1_model on test-dataset: loss:0.8226909518241883,acc:0.7922002077102661
node1 weight score:8153.730128094956
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4437392625425543,train_acc:0.8664286136627197
node4 epoch1:node_model train_loss:0.27172981415476116,train_acc:0.9071428179740906
node4 epoch2:node_model train_loss:0.22088746034673282,train_acc:0.9285714030265808
node4 epoch3:node_model train_loss:0.18637523613870144,train_acc:0.9407142400741577
node4 epoch4:node_model train_loss:0.16429874447307416,train_acc:0.9603570103645325
node4_model on test-dataset: loss:0.7944538933038712,acc:0.7916000485420227
node4 weight score:3404.854608680686
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4409226369526651,train_acc:0.8542970418930054
node8 epoch1:node_model train_loss:0.2496094418068727,train_acc:0.9104534983634949
node8 epoch2:node_model train_loss:0.1579794180062082,train_acc:0.9566099643707275
node8 epoch3:node_model train_loss:0.09312920293046369,train_acc:0.9772108197212219
node8 epoch4:node_model train_loss:0.05928291752934456,train_acc:0.9844330549240112
node8_model on test-dataset: loss:0.7889329713582992,acc:0.7952999472618103
node8 weight score:2279.0275793701444
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5307178655091453,train_acc:0.8244045376777649
node11 epoch1:node_model train_loss:0.24979853717719808,train_acc:0.9152367115020752
node11 epoch2:node_model train_loss:0.13877508394858418,train_acc:0.9559539556503296
node11 epoch3:node_model train_loss:0.08384848627097466,train_acc:0.9792826175689697
node11 epoch4:node_model train_loss:0.07528049246791531,train_acc:0.9778478145599365
node11_model on test-dataset: loss:0.7547899336367846,acc:0.8039998412132263
node11 weight score:2228.4345949020058
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6479724667966366,acc:0.828399984240532
total cost energy:10.919068459165608 | all_enery_cp：9.038 | all_enery_tp: 1.8810684591656075
ef: 25.228605906242066
reward: 14.309537447076458
step 287:loss:9.057247161865234|running q:60.75518798828125
episode4,iteration47 selected nodes:[3, 8, 0, 7, 9],center node:7
################################################## episode4,iteration47 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.09488712770577806,train_acc:0.9689596891403198
node0 epoch1:node_model train_loss:0.06797182083559725,train_acc:0.9804587960243225
node0 epoch2:node_model train_loss:0.04593113394310841,train_acc:0.9892308712005615
node0 epoch3:node_model train_loss:0.05165602469172042,train_acc:0.9848079085350037
node0 epoch4:node_model train_loss:0.04861524554256063,train_acc:0.9876531362533569
node0_model on test-dataset: loss:0.9406174059212208,acc:0.7709000110626221
node0 weight score:5510.210599307249
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2884899460992148,train_acc:0.8967688679695129
node3 epoch1:node_model train_loss:0.15136561941268833,train_acc:0.95087069272995
node3 epoch2:node_model train_loss:0.09510630161263221,train_acc:0.9702324271202087
node3 epoch3:node_model train_loss:0.055734238089170565,train_acc:0.9864819645881653
node3 epoch4:node_model train_loss:0.046576261000577796,train_acc:0.9890697598457336
node3_model on test-dataset: loss:0.7878195203840732,acc:0.7997998595237732
node3 weight score:5390.828597303005
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4248358801007271,train_acc:0.8516373038291931
node7 epoch1:node_model train_loss:0.22347087115049363,train_acc:0.9245784878730774
node7 epoch2:node_model train_loss:0.11929215919226407,train_acc:0.9615392088890076
node7 epoch3:node_model train_loss:0.07567708734422922,train_acc:0.9779998064041138
node7 epoch4:node_model train_loss:0.05665418179705739,train_acc:0.9885196089744568
node7_model on test-dataset: loss:0.8145764096081257,acc:0.7912998199462891
node7 weight score:2395.1098718149497
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3782961856987741,train_acc:0.884886622428894
node8 epoch1:node_model train_loss:0.23488760739564896,train_acc:0.9249659776687622
node8 epoch2:node_model train_loss:0.15304655333360037,train_acc:0.9527323842048645
node8 epoch3:node_model train_loss:0.09131355604363812,train_acc:0.9777663350105286
node8 epoch4:node_model train_loss:0.04507915173760719,train_acc:0.9883106350898743
node8_model on test-dataset: loss:0.7607570038735867,acc:0.8004999160766602
node8 weight score:2363.4353556326505
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5556514106298748,train_acc:0.836823582649231
node9 epoch1:node_model train_loss:0.28109384288913325,train_acc:0.9105077981948853
node9 epoch2:node_model train_loss:0.15267891358388097,train_acc:0.9538041949272156
node9 epoch3:node_model train_loss:0.10377286688277595,train_acc:0.9706555604934692
node9 epoch4:node_model train_loss:0.07366602593346645,train_acc:0.9838133454322815
node9_model on test-dataset: loss:0.8059150543808937,acc:0.7875000834465027
node9 weight score:2304.213067997039
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6622704887390136,acc:0.8204999816417694
total cost energy:8.812427190999916 | all_enery_cp：7.518 | all_enery_tp: 1.2944271909999159
ef: 25.213893511557064
reward: 16.401466320557148
step 288:loss:7.807613372802734|running q:61.79814147949219
episode4,iteration48 selected nodes:[15, 3, 7, 8, 14],center node:7
################################################## episode4,iteration48 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.17737652101489002,train_acc:0.9385154843330383
node3 epoch1:node_model train_loss:0.10670092034825059,train_acc:0.9662491679191589
node3 epoch2:node_model train_loss:0.07077763723426087,train_acc:0.9804351925849915
node3 epoch3:node_model train_loss:0.0639758498169655,train_acc:0.9815980792045593
node3 epoch4:node_model train_loss:0.05625526000593984,train_acc:0.9836616516113281
node3_model on test-dataset: loss:0.8900529186427594,acc:0.783799946308136
node3 weight score:4771.626395513927
node7: train data size:1951
node7 epoch0:node_model train_loss:0.34730828255414964,train_acc:0.8811176419258118
node7 epoch1:node_model train_loss:0.1627149160951376,train_acc:0.9420587420463562
node7 epoch2:node_model train_loss:0.08818246386945247,train_acc:0.976019561290741
node7 epoch3:node_model train_loss:0.05440803552046418,train_acc:0.9865391850471497
node7 epoch4:node_model train_loss:0.03692797040566802,train_acc:0.9954999089241028
node7_model on test-dataset: loss:0.7863009943068028,acc:0.7949999570846558
node7 weight score:2481.238118896171
node8: train data size:1798
node8 epoch0:node_model train_loss:0.358270520137416,train_acc:0.8781859278678894
node8 epoch1:node_model train_loss:0.2108594895237022,train_acc:0.9382540583610535
node8 epoch2:node_model train_loss:0.10782467180656062,train_acc:0.968877375125885
node8 epoch3:node_model train_loss:0.07900732590092553,train_acc:0.977154016494751
node8 epoch4:node_model train_loss:0.041685101886590324,train_acc:0.9916552305221558
node8_model on test-dataset: loss:0.7942824840545655,acc:0.7934998869895935
node8 weight score:2263.6782707605084
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5750277017553648,train_acc:0.8261110782623291
node14 epoch1:node_model train_loss:0.27469370017449063,train_acc:0.9162037372589111
node14 epoch2:node_model train_loss:0.13912286857763925,train_acc:0.9526851177215576
node14 epoch3:node_model train_loss:0.0915371294443806,train_acc:0.9688425064086914
node14 epoch4:node_model train_loss:0.050104139683147274,train_acc:0.9896758794784546
node14_model on test-dataset: loss:0.8021832095086574,acc:0.796000063419342
node14 weight score:1461.0128784892643
node15: train data size:629
node15 epoch0:node_model train_loss:0.7297630522932325,train_acc:0.8133004307746887
node15 epoch1:node_model train_loss:0.3347226700612477,train_acc:0.876010000705719
node15 epoch2:node_model train_loss:0.1903584280184337,train_acc:0.9344335198402405
node15 epoch3:node_model train_loss:0.11716215418917793,train_acc:0.965221643447876
node15 epoch4:node_model train_loss:0.07010558460439954,train_acc:0.9857142567634583
node15_model on test-dataset: loss:0.9565584094077348,acc:0.7673999667167664
node15 weight score:657.5656999235972
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6725753561407327,acc:0.8228999799489976
total cost energy:6.562419221493264 | all_enery_cp：4.8985 | all_enery_tp: 1.6639192214932639
ef: 24.710912613411967
reward: 18.148493391918702
step 289:loss:6.674309730529785|running q:62.88084411621094
episode4,iteration49 selected nodes:[8, 0, 6, 15, 7],center node:6
################################################## episode4,iteration49 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.10656178986223844,train_acc:0.9643443822860718
node0 epoch1:node_model train_loss:0.05151304140543708,train_acc:0.986075222492218
node0 epoch2:node_model train_loss:0.03481476857828406,train_acc:0.9928454160690308
node0 epoch3:node_model train_loss:0.028482694232549805,train_acc:0.9945762157440186
node0 epoch4:node_model train_loss:0.01980569319298061,train_acc:0.9978846311569214
node0_model on test-dataset: loss:0.7882778466492891,acc:0.8069002628326416
node0 weight score:6575.092807734271
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4623226379194567,train_acc:0.8608755469322205
node6 epoch1:node_model train_loss:0.22203462883349387,train_acc:0.9238709211349487
node6 epoch2:node_model train_loss:0.12849980088010912,train_acc:0.9612901210784912
node6 epoch3:node_model train_loss:0.09698755684639176,train_acc:0.9709674119949341
node6 epoch4:node_model train_loss:0.06427657688336988,train_acc:0.9851611256599426
node6_model on test-dataset: loss:0.770626058280468,acc:0.7977997660636902
node6 weight score:3902.022216468584
node7: train data size:1951
node7 epoch0:node_model train_loss:0.24143475629389285,train_acc:0.9135196805000305
node7 epoch1:node_model train_loss:0.11582957413047552,train_acc:0.9629999995231628
node7 epoch2:node_model train_loss:0.07329765846952796,train_acc:0.9789999127388
node7 epoch3:node_model train_loss:0.062268400005996226,train_acc:0.9854998588562012
node7 epoch4:node_model train_loss:0.04299408537335694,train_acc:0.9884999394416809
node7_model on test-dataset: loss:0.8931599462777376,acc:0.7858999967575073
node7 weight score:2184.3791900105152
node8: train data size:1798
node8 epoch0:node_model train_loss:0.2900464278128412,train_acc:0.8971202969551086
node8 epoch1:node_model train_loss:0.14129647488395372,train_acc:0.9493763446807861
node8 epoch2:node_model train_loss:0.0865889032267862,train_acc:0.9721766710281372
node8 epoch3:node_model train_loss:0.047592161844174065,train_acc:0.9894331097602844
node8 epoch4:node_model train_loss:0.047231283452775746,train_acc:0.9883332848548889
node8_model on test-dataset: loss:0.7822639341652393,acc:0.797799825668335
node8 weight score:2298.456980403502
node15: train data size:629
node15 epoch0:node_model train_loss:0.6044891391481672,train_acc:0.8239409327507019
node15 epoch1:node_model train_loss:0.3682646836553301,train_acc:0.8930050134658813
node15 epoch2:node_model train_loss:0.1859958033476557,train_acc:0.940000057220459
node15 epoch3:node_model train_loss:0.09097090948905263,train_acc:0.9701477885246277
node15 epoch4:node_model train_loss:0.06466413289308548,train_acc:0.9842857122421265
node15_model on test-dataset: loss:0.9262311486899852,acc:0.7734000086784363
node15 weight score:679.0961423502393
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6465727737545968,acc:0.8258999824523926
total cost energy:7.905359193188069 | all_enery_cp：6.284 | all_enery_tp: 1.6213591931880695
ef: 24.875001328837836
reward: 16.969642135649767
step 290:loss:7.528428554534912|running q:63.969085693359375
episode4,iteration50 selected nodes:[16, 15, 19, 3, 4],center node:16
################################################## episode4,iteration50 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1620382181433744,train_acc:0.9431666135787964
node3 epoch1:node_model train_loss:0.08829669407460579,train_acc:0.9725282192230225
node3 epoch2:node_model train_loss:0.05288398815968702,train_acc:0.9846214652061462
node3 epoch3:node_model train_loss:0.04718175599741381,train_acc:0.9867145419120789
node3 epoch4:node_model train_loss:0.03659214282971482,train_acc:0.9925582408905029
node3_model on test-dataset: loss:0.9389904701709747,acc:0.7781998515129089
node3 weight score:4522.942601565159
node4: train data size:2705
node4 epoch0:node_model train_loss:0.49937621876597404,train_acc:0.8614285588264465
node4 epoch1:node_model train_loss:0.29331720407520023,train_acc:0.904285728931427
node4 epoch2:node_model train_loss:0.2499308381229639,train_acc:0.9057143330574036
node4 epoch3:node_model train_loss:0.2457032578864268,train_acc:0.9182142019271851
node4 epoch4:node_model train_loss:0.22017317478145873,train_acc:0.9296427965164185
node4_model on test-dataset: loss:0.8730675628781319,acc:0.7814999222755432
node4 weight score:3098.271101818016
node15: train data size:629
node15 epoch0:node_model train_loss:0.5115618429013661,train_acc:0.8466503024101257
node15 epoch1:node_model train_loss:0.2474367320537567,train_acc:0.9117242097854614
node15 epoch2:node_model train_loss:0.18550067182098115,train_acc:0.9365024566650391
node15 epoch3:node_model train_loss:0.1270761112017291,train_acc:0.9536452889442444
node15 epoch4:node_model train_loss:0.07232734933495522,train_acc:0.9807881116867065
node15_model on test-dataset: loss:0.927779201567173,acc:0.7739999890327454
node15 weight score:677.9630314384227
node16: train data size:877
node16 epoch0:node_model train_loss:0.5905211137400733,train_acc:0.8355700373649597
node16 epoch1:node_model train_loss:0.28061601354016197,train_acc:0.9068975448608398
node16 epoch2:node_model train_loss:0.1479370966553688,train_acc:0.959004282951355
node16 epoch3:node_model train_loss:0.10791956240104304,train_acc:0.9707792401313782
node16 epoch4:node_model train_loss:0.06260357569489214,train_acc:0.9826695322990417
node16_model on test-dataset: loss:0.7904126843065024,acc:0.7976000308990479
node16 weight score:1109.5469713640389
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3247926560945289,train_acc:0.8923543095588684
node19 epoch1:node_model train_loss:0.16345900713011277,train_acc:0.9420525431632996
node19 epoch2:node_model train_loss:0.09180404865291229,train_acc:0.9745966196060181
node19 epoch3:node_model train_loss:0.07704191459992597,train_acc:0.9774418473243713
node19 epoch4:node_model train_loss:0.04758575968011174,train_acc:0.9897128939628601
node19_model on test-dataset: loss:0.8008398117125034,acc:0.7970999479293823
node19 weight score:5345.63833789129
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6780380953848362,acc:0.8208999800682067
total cost energy:8.333838582813476 | all_enery_cp：6.3694999999999995 | all_enery_tp: 1.964338582813477
ef: 24.81117040299821
reward: 16.477331820184737
step 291:loss:11.812163352966309|running q:65.04287719726562
episode4,iteration51 selected nodes:[0, 19, 10, 4, 5],center node:10
################################################## episode4,iteration51 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.08697019857712664,train_acc:0.9733828902244568
node0 epoch1:node_model train_loss:0.055829632884034745,train_acc:0.9842308163642883
node0 epoch2:node_model train_loss:0.041602075941717394,train_acc:0.9896153807640076
node0 epoch3:node_model train_loss:0.03332241610265695,train_acc:0.9924607276916504
node0 epoch4:node_model train_loss:0.031031801866797302,train_acc:0.992845356464386
node0_model on test-dataset: loss:0.8480524247139692,acc:0.7932999730110168
node0 weight score:6111.650469896505
node4: train data size:2705
node4 epoch0:node_model train_loss:0.33797609426879455,train_acc:0.8903573155403137
node4 epoch1:node_model train_loss:0.190076927388353,train_acc:0.931071400642395
node4 epoch2:node_model train_loss:0.14816749787756375,train_acc:0.9521428942680359
node4 epoch3:node_model train_loss:0.16951783999268497,train_acc:0.9564284086227417
node4 epoch4:node_model train_loss:0.17566767601030214,train_acc:0.9478570818901062
node4_model on test-dataset: loss:0.9415313851833343,acc:0.7641000151634216
node4 weight score:2872.979108894266
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4701667224106036,train_acc:0.8557519316673279
node5 epoch1:node_model train_loss:0.2611964597905937,train_acc:0.9131954908370972
node5 epoch2:node_model train_loss:0.1879896831355597,train_acc:0.9356014132499695
node5 epoch3:node_model train_loss:0.12608573223023037,train_acc:0.9631578326225281
node5 epoch4:node_model train_loss:0.08391363516842064,train_acc:0.9758267998695374
node5_model on test-dataset: loss:0.7798027640581131,acc:0.7966001033782959
node5 weight score:4789.672686671392
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5732599511742592,train_acc:0.8356665968894958
node10 epoch1:node_model train_loss:0.29680960029363634,train_acc:0.9070000648498535
node10 epoch2:node_model train_loss:0.18691461775451898,train_acc:0.940333366394043
node10 epoch3:node_model train_loss:0.10095870587974787,train_acc:0.9676666259765625
node10 epoch4:node_model train_loss:0.07136597046628594,train_acc:0.9814998507499695
node10_model on test-dataset: loss:0.787992876842618,acc:0.7932998538017273
node10 weight score:2506.3678340768265
node19: train data size:4281
node19 epoch0:node_model train_loss:0.18593912467707036,train_acc:0.9350071549415588
node19 epoch1:node_model train_loss:0.09511804520044216,train_acc:0.9696580767631531
node19 epoch2:node_model train_loss:0.06161310917936092,train_acc:0.9844876527786255
node19 epoch3:node_model train_loss:0.04528259581258131,train_acc:0.9909302592277527
node19 epoch4:node_model train_loss:0.029435785617246184,train_acc:0.9953489303588867
node19_model on test-dataset: loss:0.8239902153611183,acc:0.7970001101493835
node19 weight score:5195.450043206919
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6563371056318283,acc:0.8224999815225601
total cost energy:11.279004485022874 | all_enery_cp：8.939499999999999 | all_enery_tp: 2.3395044850228754
ef: 25.098630850237857
reward: 13.819626365214983
step 292:loss:6.033868312835693|running q:66.05974578857422
episode4,iteration52 selected nodes:[4, 8, 7, 17, 11],center node:11
################################################## episode4,iteration52 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.29696660063096453,train_acc:0.904285728931427
node4 epoch1:node_model train_loss:0.15811910959226744,train_acc:0.949285626411438
node4 epoch2:node_model train_loss:0.10723967250669375,train_acc:0.9635714292526245
node4 epoch3:node_model train_loss:0.07916441812579121,train_acc:0.9753570556640625
node4 epoch4:node_model train_loss:0.042515880977069695,train_acc:0.9892856478691101
node4_model on test-dataset: loss:0.8746195307374001,acc:0.7863999009132385
node4 weight score:3092.773377378606
node7: train data size:1951
node7 epoch0:node_model train_loss:0.34818244725465775,train_acc:0.8841372728347778
node7 epoch1:node_model train_loss:0.14560881163924932,train_acc:0.9465392231941223
node7 epoch2:node_model train_loss:0.09848635848611594,train_acc:0.9680196046829224
node7 epoch3:node_model train_loss:0.05526416855864227,train_acc:0.98499995470047
node7 epoch4:node_model train_loss:0.0433747629635036,train_acc:0.9904999136924744
node7_model on test-dataset: loss:0.824309860765934,acc:0.7940998673439026
node7 weight score:2366.828413513294
node8: train data size:1798
node8 epoch0:node_model train_loss:0.38651494847403634,train_acc:0.873219907283783
node8 epoch1:node_model train_loss:0.18649890356593662,train_acc:0.9366325736045837
node8 epoch2:node_model train_loss:0.11117562941379017,train_acc:0.9683104753494263
node8 epoch3:node_model train_loss:0.06144014704558584,train_acc:0.9871880412101746
node8 epoch4:node_model train_loss:0.04420299527959691,train_acc:0.9894443154335022
node8_model on test-dataset: loss:0.784841170758009,acc:0.8005003333091736
node8 weight score:2290.909380127791
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5419459623448989,train_acc:0.8447920083999634
node11 epoch1:node_model train_loss:0.2826592256041134,train_acc:0.9115781188011169
node11 epoch2:node_model train_loss:0.15718367761548827,train_acc:0.9517073035240173
node11 epoch3:node_model train_loss:0.10835523881456431,train_acc:0.9681060910224915
node11 epoch4:node_model train_loss:0.06255001366576728,train_acc:0.985293984413147
node11_model on test-dataset: loss:0.8437733936309815,acc:0.787199854850769
node11 weight score:1993.4262121751744
node17: train data size:442
node17 epoch0:node_model train_loss:0.8028853416442872,train_acc:0.8114285469055176
node17 epoch1:node_model train_loss:0.371773886680603,train_acc:0.8977142572402954
node17 epoch2:node_model train_loss:0.22931392192840577,train_acc:0.9377142786979675
node17 epoch3:node_model train_loss:0.09144670069217682,train_acc:0.9732381105422974
node17 epoch4:node_model train_loss:0.058852876722812655,train_acc:0.9872381091117859
node17_model on test-dataset: loss:0.864771940112114,acc:0.7854000926017761
node17 weight score:511.11741662512384
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.66962074264884,acc:0.8239999842643738
total cost energy:5.721032791758287 | all_enery_cp：4.289000000000001 | all_enery_tp: 1.4320327917582856
ef: 24.77292173906955
reward: 19.051888947311262
step 293:loss:4.517006874084473|running q:67.09237670898438
episode4,iteration53 selected nodes:[3, 9, 15, 16, 8],center node:9
################################################## episode4,iteration53 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1736143447978552,train_acc:0.9357249140739441
node3 epoch1:node_model train_loss:0.08022147876232169,train_acc:0.9739534854888916
node3 epoch2:node_model train_loss:0.05130302726269462,train_acc:0.9855813384056091
node3 epoch3:node_model train_loss:0.040419681235974614,train_acc:0.9897377490997314
node3 epoch4:node_model train_loss:0.03302074251913054,train_acc:0.9925283193588257
node3_model on test-dataset: loss:0.8442922359704972,acc:0.8004001975059509
node3 weight score:5030.248791898646
node8: train data size:1798
node8 epoch0:node_model train_loss:0.25094301419125664,train_acc:0.9104421734809875
node8 epoch1:node_model train_loss:0.14401345700025558,train_acc:0.9560883045196533
node8 epoch2:node_model train_loss:0.09175222367048264,train_acc:0.9738547205924988
node8 epoch3:node_model train_loss:0.05912497158472737,train_acc:0.9833219647407532
node8 epoch4:node_model train_loss:0.04034247564979725,train_acc:0.9900000095367432
node8_model on test-dataset: loss:0.8470966532081365,acc:0.7833002805709839
node8 weight score:2122.544095990214
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5457497443023481,train_acc:0.8464266061782837
node9 epoch1:node_model train_loss:0.3111898597133787,train_acc:0.9011727571487427
node9 epoch2:node_model train_loss:0.15664250442856237,train_acc:0.9602584838867188
node9 epoch3:node_model train_loss:0.10853581973596622,train_acc:0.9680240154266357
node9 epoch4:node_model train_loss:0.06769511064416484,train_acc:0.9815788269042969
node9_model on test-dataset: loss:0.8004686976969242,acc:0.7955000400543213
node9 weight score:2319.89084063235
node15: train data size:629
node15 epoch0:node_model train_loss:0.5896711264337812,train_acc:0.8266502022743225
node15 epoch1:node_model train_loss:0.2712865493127278,train_acc:0.9058621525764465
node15 epoch2:node_model train_loss:0.14194205296891077,train_acc:0.9487192630767822
node15 epoch3:node_model train_loss:0.13209357112646103,train_acc:0.9693595767021179
node15 epoch4:node_model train_loss:0.06707601249217987,train_acc:0.9865024089813232
node15_model on test-dataset: loss:0.861226567029953,acc:0.790300190448761
node15 weight score:730.3536886573121
node16: train data size:877
node16 epoch0:node_model train_loss:0.5649815532896254,train_acc:0.8332467079162598
node16 epoch1:node_model train_loss:0.2855605764521493,train_acc:0.9071140289306641
node16 epoch2:node_model train_loss:0.12300015572044584,train_acc:0.9593361616134644
node16 epoch3:node_model train_loss:0.09931573188967174,train_acc:0.9711111187934875
node16 epoch4:node_model train_loss:0.0734558639427026,train_acc:0.9855555295944214
node16_model on test-dataset: loss:0.8105983817577362,acc:0.7875000834465027
node16 weight score:1081.9167910232877
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6727144423127175,acc:0.8259999805688858
total cost energy:6.706703444029738 | all_enery_cp：4.704000000000001 | all_enery_tp: 2.0027034440297378
ef: 24.55691370061565
reward: 17.85021025658591
step 294:loss:5.229222774505615|running q:68.08329772949219
episode4,iteration54 selected nodes:[6, 7, 15, 10, 0],center node:6
################################################## episode4,iteration54 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.10462984500023034,train_acc:0.9655721783638
node0 epoch1:node_model train_loss:0.07014218699903442,train_acc:0.9778059124946594
node0 epoch2:node_model train_loss:0.0338725782930851,train_acc:0.9926924109458923
node0 epoch3:node_model train_loss:0.022196864816718377,train_acc:0.995729923248291
node0 epoch4:node_model train_loss:0.01881488675896365,train_acc:0.9975001811981201
node0_model on test-dataset: loss:0.8226670657098293,acc:0.8030000925064087
node0 weight score:6300.2400558334075
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3935996409385435,train_acc:0.865391731262207
node6 epoch1:node_model train_loss:0.2234796600476388,train_acc:0.9289401173591614
node6 epoch2:node_model train_loss:0.17121373761385197,train_acc:0.9380643963813782
node6 epoch3:node_model train_loss:0.11244838702822885,train_acc:0.965806245803833
node6 epoch4:node_model train_loss:0.06094817862275147,train_acc:0.9867739677429199
node6_model on test-dataset: loss:0.8510986532270909,acc:0.7895000576972961
node6 weight score:3533.0804350335043
node7: train data size:1951
node7 epoch0:node_model train_loss:0.25707154236733915,train_acc:0.9170783162117004
node7 epoch1:node_model train_loss:0.12062635608017444,train_acc:0.9654998779296875
node7 epoch2:node_model train_loss:0.08765897154808044,train_acc:0.9735391736030579
node7 epoch3:node_model train_loss:0.05974959414452315,train_acc:0.9840194582939148
node7 epoch4:node_model train_loss:0.03636941001750529,train_acc:0.9909999966621399
node7_model on test-dataset: loss:0.8554207551479339,acc:0.7950001358985901
node7 weight score:2280.748962728406
node10: train data size:1975
node10 epoch0:node_model train_loss:0.49957427829504014,train_acc:0.8428334593772888
node10 epoch1:node_model train_loss:0.2776075005531311,train_acc:0.9125000238418579
node10 epoch2:node_model train_loss:0.1764982480555773,train_acc:0.9451665282249451
node10 epoch3:node_model train_loss:0.11207227390259504,train_acc:0.9711665511131287
node10 epoch4:node_model train_loss:0.06427024267613887,train_acc:0.9836665987968445
node10_model on test-dataset: loss:0.787802454829216,acc:0.7920998334884644
node10 weight score:2506.973655506254
node15: train data size:629
node15 epoch0:node_model train_loss:0.48617404273578096,train_acc:0.8482266664505005
node15 epoch1:node_model train_loss:0.2536093294620514,train_acc:0.9252216219902039
node15 epoch2:node_model train_loss:0.11447048240474292,train_acc:0.9614286422729492
node15 epoch3:node_model train_loss:0.11246335932186671,train_acc:0.9658620953559875
node15 epoch4:node_model train_loss:0.05815270436661584,train_acc:0.9879310131072998
node15_model on test-dataset: loss:0.9559986479580402,acc:0.7695000767707825
node15 weight score:657.9507213148354
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6635890554636716,acc:0.8285999816656112
total cost energy:7.903761144547348 | all_enery_cp：6.3725 | all_enery_tp: 1.531261144547348
ef: 24.835523404899078
reward: 16.931762260351732
step 295:loss:6.610196113586426|running q:69.09371948242188
episode4,iteration55 selected nodes:[6, 13, 19, 2, 12],center node:6
################################################## episode4,iteration55 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3754981237774094,train_acc:0.8767330050468445
node2 epoch1:node_model train_loss:0.18628815095871687,train_acc:0.9402462244033813
node2 epoch2:node_model train_loss:0.12967988708987832,train_acc:0.9580397605895996
node2 epoch3:node_model train_loss:0.09514325884326051,train_acc:0.9692612886428833
node2 epoch4:node_model train_loss:0.06587225062927853,train_acc:0.9825001955032349
node2_model on test-dataset: loss:0.7900732722878456,acc:0.7961000800132751
node2 weight score:6060.197411988389
node6: train data size:3007
node6 epoch0:node_model train_loss:0.25814494490623474,train_acc:0.9028110504150391
node6 epoch1:node_model train_loss:0.17106838572409847,train_acc:0.9379723072052002
node6 epoch2:node_model train_loss:0.13978288178482362,train_acc:0.9505528807640076
node6 epoch3:node_model train_loss:0.099432147678829,train_acc:0.9690319299697876
node6 epoch4:node_model train_loss:0.06005246192216873,train_acc:0.985483705997467
node6_model on test-dataset: loss:0.8690526022762061,acc:0.7857000827789307
node6 weight score:3460.0897484503503
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7308389076164791,train_acc:0.8114287257194519
node12 epoch1:node_model train_loss:0.3413748059953962,train_acc:0.892380952835083
node12 epoch2:node_model train_loss:0.19653556708778655,train_acc:0.9408731460571289
node12 epoch3:node_model train_loss:0.14670725060360773,train_acc:0.9551587700843811
node12 epoch4:node_model train_loss:0.10982114502361842,train_acc:0.9744443893432617
node12_model on test-dataset: loss:0.8373163998126983,acc:0.7875999212265015
node12 weight score:1595.5736688053091
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6674895013372103,train_acc:0.8199999332427979
node13 epoch1:node_model train_loss:0.30347836141784984,train_acc:0.901439368724823
node13 epoch2:node_model train_loss:0.1695230770856142,train_acc:0.9374241828918457
node13 epoch3:node_model train_loss:0.1062276146064202,train_acc:0.9708333015441895
node13 epoch4:node_model train_loss:0.07104084687307477,train_acc:0.9809848070144653
node13_model on test-dataset: loss:0.8813906019181013,acc:0.7804997563362122
node13 weight score:1310.42922114947
node19: train data size:4281
node19 epoch0:node_model train_loss:0.245224364794964,train_acc:0.913640558719635
node19 epoch1:node_model train_loss:0.1233608715755995,train_acc:0.9551305770874023
node19 epoch2:node_model train_loss:0.07191313075464825,train_acc:0.9794802069664001
node19 epoch3:node_model train_loss:0.05829164615377437,train_acc:0.9825035929679871
node19 epoch4:node_model train_loss:0.04460368760276672,train_acc:0.9888371229171753
node19_model on test-dataset: loss:0.8714122910797596,acc:0.7892998456954956
node19 weight score:4912.714731961663
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6681095884740352,acc:0.8265999788045884
total cost energy:9.724706086192473 | all_enery_cp：7.2835 | all_enery_tp: 2.4412060861924734
ef: 24.699568169283356
reward: 14.974862083090883
step 296:loss:3.429398775100708|running q:70.11367797851562
episode4,iteration56 selected nodes:[5, 3, 1, 11, 9],center node:9
################################################## episode4,iteration56 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.315003578486058,train_acc:0.8933822512626648
node1 epoch1:node_model train_loss:0.19606306590139866,train_acc:0.932647168636322
node1 epoch2:node_model train_loss:0.13974836545393748,train_acc:0.9532352685928345
node1 epoch3:node_model train_loss:0.1432447108809891,train_acc:0.95250004529953
node1 epoch4:node_model train_loss:0.08171934333136853,train_acc:0.9732354283332825
node1_model on test-dataset: loss:0.8535007651150227,acc:0.7879999876022339
node1 weight score:7859.395414948446
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1584094355619231,train_acc:0.9432260394096375
node3 epoch1:node_model train_loss:0.07665157617004804,train_acc:0.9740968942642212
node3 epoch2:node_model train_loss:0.053156885210164755,train_acc:0.987209141254425
node3 epoch3:node_model train_loss:0.028087561928428883,train_acc:0.9948837161064148
node3 epoch4:node_model train_loss:0.021788779429571574,train_acc:0.9976744651794434
node3_model on test-dataset: loss:0.805136597007513,acc:0.8009998202323914
node3 weight score:5274.881325460814
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4262663584790732,train_acc:0.862819492816925
node5 epoch1:node_model train_loss:0.2305668233648727,train_acc:0.9230075478553772
node5 epoch2:node_model train_loss:0.1577859252299133,train_acc:0.943270742893219
node5 epoch3:node_model train_loss:0.09705747526727225,train_acc:0.9715787172317505
node5 epoch4:node_model train_loss:0.06861390967510249,train_acc:0.981879472732544
node5_model on test-dataset: loss:0.8131752963364124,acc:0.7923999428749084
node5 weight score:4593.105590919012
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4597715845233516,train_acc:0.8604987263679504
node9 epoch1:node_model train_loss:0.230626364679713,train_acc:0.9261772036552429
node9 epoch2:node_model train_loss:0.14068295355690152,train_acc:0.9498614072799683
node9 epoch3:node_model train_loss:0.08478203298229921,train_acc:0.9723637104034424
node9 epoch4:node_model train_loss:0.05198172901413942,train_acc:0.9886794686317444
node9_model on test-dataset: loss:0.8653634930402041,acc:0.7881998419761658
node9 weight score:2145.919044349754
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4570822207366719,train_acc:0.8670731782913208
node11 epoch1:node_model train_loss:0.23528456118176966,train_acc:0.9258248805999756
node11 epoch2:node_model train_loss:0.11178917262484045,train_acc:0.9637302160263062
node11 epoch3:node_model train_loss:0.0811742144253324,train_acc:0.9804590940475464
node11 epoch4:node_model train_loss:0.0485115049078184,train_acc:0.9886942505836487
node11_model on test-dataset: loss:0.8223129466176033,acc:0.7934000492095947
node11 weight score:2045.4499797413177
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6652444271743297,acc:0.8266999834775924
total cost energy:10.325154957016753 | all_enery_cp：9.1145 | all_enery_tp: 1.210654957016754
ef: 25.27096369630152
reward: 14.945808739284768
step 297:loss:5.899075508117676|running q:71.11918640136719
episode4,iteration57 selected nodes:[13, 19, 11, 2, 18],center node:11
################################################## episode4,iteration57 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2555067245848477,train_acc:0.9098674058914185
node2 epoch1:node_model train_loss:0.12422119306089978,train_acc:0.9564108848571777
node2 epoch2:node_model train_loss:0.08052436646539718,train_acc:0.9749716520309448
node2 epoch3:node_model train_loss:0.05763724611218398,train_acc:0.9854166507720947
node2 epoch4:node_model train_loss:0.05025998693114767,train_acc:0.9889585375785828
node2_model on test-dataset: loss:0.7938485310226678,acc:0.7980002164840698
node2 weight score:6031.377287845963
node11: train data size:1682
node11 epoch0:node_model train_loss:0.39507897110546336,train_acc:0.8734146356582642
node11 epoch1:node_model train_loss:0.2070308638846173,train_acc:0.9291534423828125
node11 epoch2:node_model train_loss:0.09658338743097641,train_acc:0.9663413763046265
node11 epoch3:node_model train_loss:0.08107224358793567,train_acc:0.9728837013244629
node11 epoch4:node_model train_loss:0.04984197075314382,train_acc:0.9862122535705566
node11_model on test-dataset: loss:0.798398553431034,acc:0.8001000881195068
node11 weight score:2106.717243877486
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6636078357696533,train_acc:0.8256818652153015
node13 epoch1:node_model train_loss:0.32756097987294197,train_acc:0.8902272582054138
node13 epoch2:node_model train_loss:0.1723045160373052,train_acc:0.9457575678825378
node13 epoch3:node_model train_loss:0.10291403190543254,train_acc:0.9701515436172485
node13 epoch4:node_model train_loss:0.04541528690606356,train_acc:0.9933332800865173
node13_model on test-dataset: loss:0.814549014121294,acc:0.7930001020431519
node13 weight score:1417.962553482398
node18: train data size:472
node18 epoch0:node_model train_loss:0.6237276375293732,train_acc:0.8497778177261353
node18 epoch1:node_model train_loss:0.2922104328870773,train_acc:0.9181110262870789
node18 epoch2:node_model train_loss:0.14861727356910706,train_acc:0.9484444856643677
node18 epoch3:node_model train_loss:0.06221713125705719,train_acc:0.9844444394111633
node18 epoch4:node_model train_loss:0.04420877508819103,train_acc:0.9920000433921814
node18_model on test-dataset: loss:0.9357911013066769,acc:0.7796999216079712
node18 weight score:504.38607435028007
node19: train data size:4281
node19 epoch0:node_model train_loss:0.19104985563560975,train_acc:0.9362934231758118
node19 epoch1:node_model train_loss:0.09835389299794685,train_acc:0.9682628512382507
node19 epoch2:node_model train_loss:0.058752186639710914,train_acc:0.9838990569114685
node19 epoch3:node_model train_loss:0.04112946131634851,train_acc:0.9892477989196777
node19 epoch4:node_model train_loss:0.03296014778099434,train_acc:0.9934883117675781
node19_model on test-dataset: loss:0.8362370654195547,acc:0.797200083732605
node19 weight score:5119.361694225007
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6670139386504889,acc:0.827899979352951
total cost energy:8.264587376308517 | all_enery_cp：6.189 | all_enery_tp: 2.075587376308517
ef: 24.70258489248198
reward: 16.43799751617346
step 298:loss:5.015454292297363|running q:72.1139144897461
episode4,iteration58 selected nodes:[10, 1, 2, 15, 6],center node:6
################################################## episode4,iteration58 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.23501913534367785,train_acc:0.9183086156845093
node1 epoch1:node_model train_loss:0.15143564989899888,train_acc:0.9481617212295532
node1 epoch2:node_model train_loss:0.1283815237967407,train_acc:0.9575000405311584
node1 epoch3:node_model train_loss:0.08279343258918208,train_acc:0.9774265289306641
node1 epoch4:node_model train_loss:0.10066483997027664,train_acc:0.9680882692337036
node1_model on test-dataset: loss:0.8387016268074512,acc:0.7914999127388
node1 weight score:7998.07677199131
node2: train data size:4788
node2 epoch0:node_model train_loss:0.15141985363637409,train_acc:0.9465529918670654
node2 epoch1:node_model train_loss:0.09679487130294244,train_acc:0.9674431085586548
node2 epoch2:node_model train_loss:0.06444485341974844,train_acc:0.9834848642349243
node2 epoch3:node_model train_loss:0.052040893312854074,train_acc:0.9839015007019043
node2 epoch4:node_model train_loss:0.0506027666075776,train_acc:0.9872633218765259
node2_model on test-dataset: loss:0.8517169148474931,acc:0.7987000346183777
node2 weight score:5621.586135643826
node6: train data size:3007
node6 epoch0:node_model train_loss:0.30458266804775885,train_acc:0.8967739939689636
node6 epoch1:node_model train_loss:0.16111814792478277,train_acc:0.9419353604316711
node6 epoch2:node_model train_loss:0.09802168884104298,train_acc:0.965714156627655
node6 epoch3:node_model train_loss:0.10876661208608458,train_acc:0.9635481238365173
node6 epoch4:node_model train_loss:0.08205595888918446,train_acc:0.9757140278816223
node6_model on test-dataset: loss:0.8442784415185451,acc:0.7945998311042786
node6 weight score:3561.6212047195204
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5593394562602043,train_acc:0.8414999842643738
node10 epoch1:node_model train_loss:0.27666770592331885,train_acc:0.9025000929832458
node10 epoch2:node_model train_loss:0.14021906852722169,train_acc:0.9566665887832642
node10 epoch3:node_model train_loss:0.10220300033688545,train_acc:0.9733333587646484
node10 epoch4:node_model train_loss:0.05966229774057865,train_acc:0.9879999160766602
node10_model on test-dataset: loss:0.8069264079630375,acc:0.7949000597000122
node10 weight score:2447.559009731242
node15: train data size:629
node15 epoch0:node_model train_loss:0.5568161308765411,train_acc:0.8395073413848877
node15 epoch1:node_model train_loss:0.3221770663346563,train_acc:0.9080788493156433
node15 epoch2:node_model train_loss:0.1822233029774257,train_acc:0.9323645234107971
node15 epoch3:node_model train_loss:0.13811348910842622,train_acc:0.9530049562454224
node15 epoch4:node_model train_loss:0.05220858380198479,train_acc:0.9865024089813232
node15_model on test-dataset: loss:0.9425791847705841,acc:0.7706000208854675
node15 weight score:667.3179401400566
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6723630793392659,acc:0.8279999858140945
total cost energy:10.049132027572686 | all_enery_cp：8.5535 | all_enery_tp: 1.4956320275726869
ef: 24.901863661961077
reward: 14.852731634388391
step 299:loss:5.506609916687012|running q:73.14559936523438
episode4,iteration59 selected nodes:[5, 17, 8, 18, 3],center node:3
################################################## episode4,iteration59 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1746253157251103,train_acc:0.9371795654296875
node3 epoch1:node_model train_loss:0.08246207358532173,train_acc:0.9736613631248474
node3 epoch2:node_model train_loss:0.0590352980896484,train_acc:0.9848834872245789
node3 epoch3:node_model train_loss:0.04469638844128958,train_acc:0.9893024563789368
node3 epoch4:node_model train_loss:0.02572802031889211,train_acc:0.9941861033439636
node3_model on test-dataset: loss:0.8409731381386518,acc:0.8077998161315918
node3 weight score:5050.101849150614
node5: train data size:3735
node5 epoch0:node_model train_loss:0.40370757368050125,train_acc:0.8722558617591858
node5 epoch1:node_model train_loss:0.22185782872532545,train_acc:0.9211654663085938
node5 epoch2:node_model train_loss:0.13314836844801903,train_acc:0.9539849162101746
node5 epoch3:node_model train_loss:0.09590968014182229,train_acc:0.9747743010520935
node5 epoch4:node_model train_loss:0.07370487590761561,train_acc:0.9813156127929688
node5_model on test-dataset: loss:0.8238985557854176,acc:0.7944000363349915
node5 weight score:4533.325096606641
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4073304293884171,train_acc:0.8630726337432861
node8 epoch1:node_model train_loss:0.18237316028939354,train_acc:0.9410544633865356
node8 epoch2:node_model train_loss:0.11587021230823463,train_acc:0.9644103646278381
node8 epoch3:node_model train_loss:0.08256444397071998,train_acc:0.9771766662597656
node8 epoch4:node_model train_loss:0.05078884886784686,train_acc:0.9888661503791809
node8_model on test-dataset: loss:0.7884605048596859,acc:0.7974001169204712
node8 weight score:2280.3932332919726
node17: train data size:442
node17 epoch0:node_model train_loss:0.8791642904281616,train_acc:0.7713333368301392
node17 epoch1:node_model train_loss:0.4796185553073883,train_acc:0.8706666827201843
node17 epoch2:node_model train_loss:0.22666091173887254,train_acc:0.9437143206596375
node17 epoch3:node_model train_loss:0.15753839239478112,train_acc:0.9564762115478516
node17 epoch4:node_model train_loss:0.09496094435453414,train_acc:0.9799999594688416
node17_model on test-dataset: loss:0.9437887431681156,acc:0.7671999931335449
node17 weight score:468.32514500680713
node18: train data size:472
node18 epoch0:node_model train_loss:0.649609237909317,train_acc:0.8334444165229797
node18 epoch1:node_model train_loss:0.3026564300060272,train_acc:0.9070000052452087
node18 epoch2:node_model train_loss:0.1508222058415413,train_acc:0.9481111764907837
node18 epoch3:node_model train_loss:0.1092173382639885,train_acc:0.9772221446037292
node18 epoch4:node_model train_loss:0.07184200435876846,train_acc:0.9824444651603699
node18_model on test-dataset: loss:0.9460669005662203,acc:0.7757998704910278
node18 weight score:498.9076350916709
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6825996841490268,acc:0.828899986743927
total cost energy:7.70083749650909 | all_enery_cp：5.3469999999999995 | all_enery_tp: 2.35383749650909
ef: 24.321211511631596
reward: 16.620374015122508
step 300:loss:6.227250099182129|running q:74.09797668457031
episode4_cost time: 25346.81249666214
episode5,iteration0 selected nodes:[3, 8, 7, 0, 19],center node:7
################################################## episode5,iteration0 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:2.014159149848498,train_acc:0.30576229095458984
node0 epoch1:node_model train_loss:1.5278872618308434,train_acc:0.44641801714897156
node0 epoch2:node_model train_loss:1.3528427252402673,train_acc:0.5192724466323853
node0 epoch3:node_model train_loss:1.2797752779263716,train_acc:0.5485775470733643
node0 epoch4:node_model train_loss:1.1648442550347402,train_acc:0.582659900188446
node0_model on test-dataset: loss:1.449951967895031,acc:0.5298998355865479
node0 weight score:3574.6011693921314
node3: train data size:4247
node3 epoch0:node_model train_loss:1.9870655398036159,train_acc:0.31425532698631287
node3 epoch1:node_model train_loss:1.5207151978514915,train_acc:0.45108848810195923
node3 epoch2:node_model train_loss:1.3289297486460485,train_acc:0.5240277051925659
node3 epoch3:node_model train_loss:1.2618816494941711,train_acc:0.537803053855896
node3 epoch4:node_model train_loss:1.1082723722901455,train_acc:0.6038545370101929
node3_model on test-dataset: loss:1.3565857097506524,acc:0.5325999855995178
node3 weight score:3130.6536472219077
node7: train data size:1951
node7 epoch0:node_model train_loss:2.270758730173111,train_acc:0.24020588397979736
node7 epoch1:node_model train_loss:1.808300507068634,train_acc:0.35114702582359314
node7 epoch2:node_model train_loss:1.6176249384880066,train_acc:0.41799017786979675
node7 epoch3:node_model train_loss:1.4485411405563355,train_acc:0.4816078841686249
node7 epoch4:node_model train_loss:1.3378037929534912,train_acc:0.5129901766777039
node7_model on test-dataset: loss:1.4699679774045944,acc:0.46509993076324463
node7 weight score:1327.2397970496784
node8: train data size:1798
node8 epoch0:node_model train_loss:2.3670088251431785,train_acc:0.23378682136535645
node8 epoch1:node_model train_loss:1.7454483177926805,train_acc:0.3615872859954834
node8 epoch2:node_model train_loss:1.5905683173073664,train_acc:0.43333330750465393
node8 epoch3:node_model train_loss:1.4500199556350708,train_acc:0.4694557785987854
node8 epoch4:node_model train_loss:1.3284054928355746,train_acc:0.5311110615730286
node8_model on test-dataset: loss:1.559726920723915,acc:0.4515998661518097
node8 weight score:1152.7658951770193
node19: train data size:4281
node19 epoch0:node_model train_loss:1.9761299892913464,train_acc:0.310746431350708
node19 epoch1:node_model train_loss:1.5555918272151503,train_acc:0.43784675002098083
node19 epoch2:node_model train_loss:1.3531977492709493,train_acc:0.5153545141220093
node19 epoch3:node_model train_loss:1.2360890382944152,train_acc:0.556942343711853
node19 epoch4:node_model train_loss:1.1239368527434592,train_acc:0.6062445640563965
node19_model on test-dataset: loss:1.3544683429598807,acc:0.5413999557495117
node19 weight score:3160.649728176632
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.4359256970882415,acc:0.47909998886287214
total cost energy:10.500820393249938 | all_enery_cp：8.73 | all_enery_tp: 1.7708203932499371
ef: 23.81608910286855
reward: 13.31526870961861
step 301:loss:9.41667366027832|running q:1.5588799715042114
episode5,iteration1 selected nodes:[5, 8, 6, 0, 17],center node:6
################################################## episode5,iteration1 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:1.3802273754890149,train_acc:0.5238090753555298
node0 epoch1:node_model train_loss:1.143756498511021,train_acc:0.593892514705658
node0 epoch2:node_model train_loss:1.0376427150689638,train_acc:0.6339317560195923
node0 epoch3:node_model train_loss:0.9356081382586405,train_acc:0.6716288328170776
node0 epoch4:node_model train_loss:0.8639736622571945,train_acc:0.6977480053901672
node0_model on test-dataset: loss:1.258922906666994,acc:0.573699951171875
node0 weight score:4117.011432989192
node5: train data size:3735
node5 epoch0:node_model train_loss:1.5004355719214992,train_acc:0.46515029668807983
node5 epoch1:node_model train_loss:1.1533832926499217,train_acc:0.6020675897598267
node5 epoch2:node_model train_loss:1.038762843922565,train_acc:0.6300377249717712
node5 epoch3:node_model train_loss:1.0105665169264142,train_acc:0.6512029767036438
node5 epoch4:node_model train_loss:0.9007397560696853,train_acc:0.6881201863288879
node5_model on test-dataset: loss:1.1569183576107025,acc:0.6011000275611877
node5 weight score:3228.404126729926
node6: train data size:3007
node6 epoch0:node_model train_loss:1.5158053405823246,train_acc:0.46912434697151184
node6 epoch1:node_model train_loss:1.345407236006952,train_acc:0.5259907841682434
node6 epoch2:node_model train_loss:1.235783073209947,train_acc:0.5771428346633911
node6 epoch3:node_model train_loss:1.1063332423087089,train_acc:0.6123040914535522
node6 epoch4:node_model train_loss:1.0042141464448744,train_acc:0.6402764320373535
node6_model on test-dataset: loss:1.2341936480998994,acc:0.5696999430656433
node6 weight score:2436.4085851757713
node8: train data size:1798
node8 epoch0:node_model train_loss:1.5665514667828877,train_acc:0.4639115333557129
node8 epoch1:node_model train_loss:1.2266754772928026,train_acc:0.5728685259819031
node8 epoch2:node_model train_loss:1.1074352165063222,train_acc:0.6012018322944641
node8 epoch3:node_model train_loss:0.970347586605284,train_acc:0.6662924885749817
node8 epoch4:node_model train_loss:0.872548348373837,train_acc:0.692925214767456
node8_model on test-dataset: loss:1.325539492368698,acc:0.5522999167442322
node8 weight score:1356.428843011708
node17: train data size:442
node17 epoch0:node_model train_loss:1.8178774118423462,train_acc:0.4057142734527588
node17 epoch1:node_model train_loss:1.3654907464981079,train_acc:0.4947619140148163
node17 epoch2:node_model train_loss:1.254858922958374,train_acc:0.5427619218826294
node17 epoch3:node_model train_loss:1.0468692421913146,train_acc:0.6293333768844604
node17 epoch4:node_model train_loss:0.8895081639289856,train_acc:0.7156190276145935
node17_model on test-dataset: loss:1.4427251046895981,acc:0.49420011043548584
node17 weight score:306.3646695848522
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9725017976760865,acc:0.659699981212616
total cost energy:8.805655136217904 | all_enery_cp：7.0825 | all_enery_tp: 1.723155136217904
ef: 23.817715862030273
reward: 15.012060725812368
step 302:loss:7.75772762298584|running q:3.17769718170166
episode5,iteration2 selected nodes:[14, 16, 0, 18, 2],center node:14
################################################## episode5,iteration2 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:1.062234633244001,train_acc:0.632752537727356
node0 epoch1:node_model train_loss:0.9230450575168316,train_acc:0.6721314787864685
node0 epoch2:node_model train_loss:0.846948701601762,train_acc:0.6940546035766602
node0 epoch3:node_model train_loss:0.7662267329601141,train_acc:0.7357854247093201
node0 epoch4:node_model train_loss:0.719395314844755,train_acc:0.746253490447998
node0_model on test-dataset: loss:1.021956217586994,acc:0.6621999144554138
node0 weight score:5071.645840403918
node2: train data size:4788
node2 epoch0:node_model train_loss:1.2137127220630646,train_acc:0.5811173915863037
node2 epoch1:node_model train_loss:0.9316269407669703,train_acc:0.6784942150115967
node2 epoch2:node_model train_loss:0.8894835772613684,train_acc:0.6869413256645203
node2 epoch3:node_model train_loss:0.8194372355937958,train_acc:0.7103692889213562
node2 epoch4:node_model train_loss:0.7557146524389585,train_acc:0.7341099977493286
node2_model on test-dataset: loss:1.0411748629808426,acc:0.6506999731063843
node2 weight score:4598.65116825059
node14: train data size:1172
node14 epoch0:node_model train_loss:1.1986821840206783,train_acc:0.5992592573165894
node14 epoch1:node_model train_loss:0.9527478913466135,train_acc:0.6617128849029541
node14 epoch2:node_model train_loss:0.7546153416236242,train_acc:0.7346296310424805
node14 epoch3:node_model train_loss:0.6818971981604894,train_acc:0.7669907808303833
node14 epoch4:node_model train_loss:0.6118865584333738,train_acc:0.7766203880310059
node14_model on test-dataset: loss:1.215076802521944,acc:0.596299946308136
node14 weight score:964.5480825306382
node16: train data size:877
node16 epoch0:node_model train_loss:1.2752626803186204,train_acc:0.5724963545799255
node16 epoch1:node_model train_loss:0.9910494354036119,train_acc:0.65203458070755
node16 epoch2:node_model train_loss:0.8476594752735562,train_acc:0.7083693742752075
node16 epoch3:node_model train_loss:0.7329439785745409,train_acc:0.7613563537597656
node16 epoch4:node_model train_loss:0.6573073731528388,train_acc:0.7654688954353333
node16_model on test-dataset: loss:1.19734569221735,acc:0.5853000283241272
node16 weight score:732.4534641085102
node18: train data size:472
node18 epoch0:node_model train_loss:1.228574311733246,train_acc:0.5834444165229797
node18 epoch1:node_model train_loss:1.0172439813613892,train_acc:0.6621111631393433
node18 epoch2:node_model train_loss:0.7317490100860595,train_acc:0.7408888936042786
node18 epoch3:node_model train_loss:0.6150109231472015,train_acc:0.789555549621582
node18 epoch4:node_model train_loss:0.4730819284915924,train_acc:0.8619999885559082
node18_model on test-dataset: loss:1.2958010584115982,acc:0.572700023651123
node18 weight score:364.25344533873186
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8708089515566826,acc:0.7010999837517738
total cost energy:8.19605763057335 | all_enery_cp：6.246 | all_enery_tp: 1.9500576305733501
ef: 24.055582954901976
reward: 15.859525324328626
step 303:loss:5.097609519958496|running q:4.806129455566406
episode5,iteration3 selected nodes:[15, 6, 2, 13, 10],center node:6
################################################## episode5,iteration3 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.9380815836290518,train_acc:0.6804544925689697
node2 epoch1:node_model train_loss:0.8064328034718832,train_acc:0.719176173210144
node2 epoch2:node_model train_loss:0.6952855636676153,train_acc:0.7601797580718994
node2 epoch3:node_model train_loss:0.6278179840495189,train_acc:0.7842519283294678
node2 epoch4:node_model train_loss:0.5721293861667315,train_acc:0.8063446879386902
node2_model on test-dataset: loss:0.9690047729015351,acc:0.6724002957344055
node2 weight score:4941.152132474099
node6: train data size:3007
node6 epoch0:node_model train_loss:1.0708535179015128,train_acc:0.6454377174377441
node6 epoch1:node_model train_loss:0.9411469842157056,train_acc:0.6752994060516357
node6 epoch2:node_model train_loss:0.8164727841654131,train_acc:0.7164055109024048
node6 epoch3:node_model train_loss:0.809639823052191,train_acc:0.7112441658973694
node6 epoch4:node_model train_loss:0.7386769190911324,train_acc:0.7434561848640442
node6_model on test-dataset: loss:1.0200660583376884,acc:0.6611999869346619
node6 weight score:2947.848303962042
node10: train data size:1975
node10 epoch0:node_model train_loss:1.1489535719156265,train_acc:0.6340000629425049
node10 epoch1:node_model train_loss:0.8473299831151963,train_acc:0.7105000019073486
node10 epoch2:node_model train_loss:0.7287577360868454,train_acc:0.7388333678245544
node10 epoch3:node_model train_loss:0.6630113363265991,train_acc:0.7714999914169312
node10 epoch4:node_model train_loss:0.6218843966722488,train_acc:0.7898333668708801
node10_model on test-dataset: loss:1.04379983574152,acc:0.6590999960899353
node10 weight score:1892.1252259030596
node13: train data size:1155
node13 epoch0:node_model train_loss:1.1680376430352528,train_acc:0.6155303120613098
node13 epoch1:node_model train_loss:0.9283503939708074,train_acc:0.6750757098197937
node13 epoch2:node_model train_loss:0.7430311888456345,train_acc:0.747499942779541
node13 epoch3:node_model train_loss:0.5840352748831114,train_acc:0.7948484420776367
node13 epoch4:node_model train_loss:0.5095961168408394,train_acc:0.8224999904632568
node13_model on test-dataset: loss:1.0533652567863465,acc:0.6432998776435852
node13 weight score:1096.485756065019
node15: train data size:629
node15 epoch0:node_model train_loss:1.2042178426470076,train_acc:0.6234482526779175
node15 epoch1:node_model train_loss:0.8347876497677394,train_acc:0.7050246000289917
node15 epoch2:node_model train_loss:0.6865837318556649,train_acc:0.769655168056488
node15 epoch3:node_model train_loss:0.5800685882568359,train_acc:0.8112315535545349
node15 epoch4:node_model train_loss:0.4661033707005637,train_acc:0.8525123000144958
node15_model on test-dataset: loss:1.1461218631267547,acc:0.6246998310089111
node15 weight score:548.8072605857236
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8125674733519554,acc:0.7191999840736389
total cost energy:7.810602540409098 | all_enery_cp：5.776999999999999 | all_enery_tp: 2.033602540409099
ef: 24.08706268288073
reward: 16.276460142471635
step 304:loss:5.71492338180542|running q:6.3971476554870605
episode5,iteration4 selected nodes:[2, 7, 9, 4, 5],center node:9
################################################## episode5,iteration4 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.786777950823307,train_acc:0.7289395332336426
node2 epoch1:node_model train_loss:0.6250476321826378,train_acc:0.7803881168365479
node2 epoch2:node_model train_loss:0.5587199125438929,train_acc:0.8078124523162842
node2 epoch3:node_model train_loss:0.5450231234232584,train_acc:0.8118655681610107
node2 epoch4:node_model train_loss:0.5113834173729023,train_acc:0.8246022462844849
node2_model on test-dataset: loss:0.9539610913395882,acc:0.6920000910758972
node2 weight score:5019.07262619748
node4: train data size:2705
node4 epoch0:node_model train_loss:1.0797188899346761,train_acc:0.6482142210006714
node4 epoch1:node_model train_loss:0.8299822381564549,train_acc:0.7160714268684387
node4 epoch2:node_model train_loss:0.690906363938536,train_acc:0.7521429657936096
node4 epoch3:node_model train_loss:0.6571104994841984,train_acc:0.7685713171958923
node4 epoch4:node_model train_loss:0.6222537383437157,train_acc:0.7810714244842529
node4_model on test-dataset: loss:1.1363842751085758,acc:0.6341999769210815
node4 weight score:2380.3567677329493
node5: train data size:3735
node5 epoch0:node_model train_loss:1.0071668718990527,train_acc:0.6573683619499207
node5 epoch1:node_model train_loss:0.7766594510329398,train_acc:0.7327068448066711
node5 epoch2:node_model train_loss:0.6961340347402974,train_acc:0.7646616101264954
node5 epoch3:node_model train_loss:0.629048040038661,train_acc:0.7858272194862366
node5 epoch4:node_model train_loss:0.6397840443410372,train_acc:0.7776315212249756
node5_model on test-dataset: loss:1.002475104033947,acc:0.6651999354362488
node5 weight score:3725.778311072672
node7: train data size:1951
node7 epoch0:node_model train_loss:1.036854863166809,train_acc:0.6593334078788757
node7 epoch1:node_model train_loss:0.7926234424114227,train_acc:0.7096764445304871
node7 epoch2:node_model train_loss:0.6599211037158966,train_acc:0.774813711643219
node7 epoch3:node_model train_loss:0.5822553634643555,train_acc:0.8012155890464783
node7 epoch4:node_model train_loss:0.5232777163386345,train_acc:0.8246569037437439
node7_model on test-dataset: loss:0.9748947584629059,acc:0.6741999387741089
node7 weight score:2001.241655126033
node9: train data size:1857
node9 epoch0:node_model train_loss:1.0764366670658714,train_acc:0.6508771777153015
node9 epoch1:node_model train_loss:0.7667764833098963,train_acc:0.7361403107643127
node9 epoch2:node_model train_loss:0.6067372168365278,train_acc:0.7944413423538208
node9 epoch3:node_model train_loss:0.5261592425798115,train_acc:0.8136564493179321
node9 epoch4:node_model train_loss:0.516315620196493,train_acc:0.8261497020721436
node9_model on test-dataset: loss:1.2368412563949824,acc:0.621100127696991
node9 weight score:1501.4052857620486
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7603279039263725,acc:0.7390999826788902
total cost energy:8.786572788688026 | all_enery_cp：7.518 | all_enery_tp: 1.2685727886880274
ef: 24.625730135267034
reward: 15.839157346579007
step 305:loss:5.324184417724609|running q:7.974700927734375
episode5,iteration5 selected nodes:[16, 0, 10, 14, 12],center node:14
################################################## episode5,iteration5 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.8417284603302295,train_acc:0.7087836265563965
node0 epoch1:node_model train_loss:0.7120112644938322,train_acc:0.7519786357879639
node0 epoch2:node_model train_loss:0.6363993630959437,train_acc:0.7795921564102173
node0 epoch3:node_model train_loss:0.5593078852846072,train_acc:0.8069393038749695
node0 epoch4:node_model train_loss:0.5209864555643156,train_acc:0.8199862241744995
node0_model on test-dataset: loss:1.0408524166792632,acc:0.6745001077651978
node0 weight score:4979.572432118522
node10: train data size:1975
node10 epoch0:node_model train_loss:1.0678497165441514,train_acc:0.655500054359436
node10 epoch1:node_model train_loss:0.7227893218398094,train_acc:0.7558333277702332
node10 epoch2:node_model train_loss:0.6127998098731041,train_acc:0.7916666865348816
node10 epoch3:node_model train_loss:0.5138309881091118,train_acc:0.8248332142829895
node10 epoch4:node_model train_loss:0.4542077943682671,train_acc:0.8468332290649414
node10_model on test-dataset: loss:0.9484180122613907,acc:0.6875999569892883
node10 weight score:2082.415110707193
node12: train data size:1336
node12 epoch0:node_model train_loss:1.1327109336853027,train_acc:0.6330951452255249
node12 epoch1:node_model train_loss:0.8524888115269798,train_acc:0.7284127473831177
node12 epoch2:node_model train_loss:0.6098490676709584,train_acc:0.7933333516120911
node12 epoch3:node_model train_loss:0.49856152704783846,train_acc:0.8344445824623108
node12 epoch4:node_model train_loss:0.43022552664790836,train_acc:0.852777898311615
node12_model on test-dataset: loss:0.9792597949504852,acc:0.6811000108718872
node12 weight score:1364.2957741030843
node14: train data size:1172
node14 epoch0:node_model train_loss:1.0582062552372615,train_acc:0.6592592597007751
node14 epoch1:node_model train_loss:0.69746666153272,train_acc:0.7666666507720947
node14 epoch2:node_model train_loss:0.4996718242764473,train_acc:0.8286111354827881
node14 epoch3:node_model train_loss:0.39435581862926483,train_acc:0.8795833587646484
node14 epoch4:node_model train_loss:0.34621931488315266,train_acc:0.8899537324905396
node14_model on test-dataset: loss:1.015021475851536,acc:0.6829997897148132
node14 weight score:1154.6553722095086
node16: train data size:877
node16 epoch0:node_model train_loss:1.010518855518765,train_acc:0.6949206590652466
node16 epoch1:node_model train_loss:0.7105981707572937,train_acc:0.7719191312789917
node16 epoch2:node_model train_loss:0.6259411772092184,train_acc:0.796248197555542
node16 epoch3:node_model train_loss:0.4552973409493764,train_acc:0.8544588685035706
node16 epoch4:node_model train_loss:0.39256970749961007,train_acc:0.8803463578224182
node16_model on test-dataset: loss:0.9895111694931984,acc:0.6774000525474548
node16 weight score:886.2962107332011
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.78055047377944,acc:0.7358999779820442
total cost energy:7.096812160755487 | all_enery_cp：5.2715000000000005 | all_enery_tp: 1.8253121607554863
ef: 24.441371230877376
reward: 17.34455907012189
step 306:loss:3.333047866821289|running q:9.534253120422363
episode5,iteration6 selected nodes:[19, 2, 15, 11, 17],center node:11
################################################## episode5,iteration6 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.7166662216186523,train_acc:0.7536742687225342
node2 epoch1:node_model train_loss:0.5338291718314091,train_acc:0.8195075988769531
node2 epoch2:node_model train_loss:0.5185036702702442,train_acc:0.8208901286125183
node2 epoch3:node_model train_loss:0.4419674662252267,train_acc:0.8431724309921265
node2 epoch4:node_model train_loss:0.4169959121694167,train_acc:0.8571306467056274
node2_model on test-dataset: loss:0.881359469294548,acc:0.7177000045776367
node2 weight score:5432.516659555924
node11: train data size:1682
node11 epoch0:node_model train_loss:0.9553165190360126,train_acc:0.6757388710975647
node11 epoch1:node_model train_loss:0.6556325870401719,train_acc:0.782309889793396
node11 epoch2:node_model train_loss:0.5486590686966392,train_acc:0.8049211502075195
node11 epoch3:node_model train_loss:0.46195653431555805,train_acc:0.8486657738685608
node11 epoch4:node_model train_loss:0.38683007394566254,train_acc:0.8676040172576904
node11_model on test-dataset: loss:1.0107651561498643,acc:0.6747000217437744
node11 weight score:1664.0858559143019
node15: train data size:629
node15 epoch0:node_model train_loss:1.009449805532183,train_acc:0.6488177180290222
node15 epoch1:node_model train_loss:0.683479483638491,train_acc:0.7740886807441711
node15 epoch2:node_model train_loss:0.4858471964086805,train_acc:0.8375861644744873
node15 epoch3:node_model train_loss:0.4085928627422878,train_acc:0.8779309988021851
node15 epoch4:node_model train_loss:0.35271059615271433,train_acc:0.8801478147506714
node15_model on test-dataset: loss:1.1858864624798298,acc:0.6452999114990234
node15 weight score:530.4049079746522
node17: train data size:442
node17 epoch0:node_model train_loss:0.9535549998283386,train_acc:0.6883808970451355
node17 epoch1:node_model train_loss:0.6996981263160705,train_acc:0.755142867565155
node17 epoch2:node_model train_loss:0.49407785534858706,train_acc:0.8314285278320312
node17 epoch3:node_model train_loss:0.4277902007102966,train_acc:0.8801904916763306
node17 epoch4:node_model train_loss:0.33253697752952577,train_acc:0.8994285464286804
node17_model on test-dataset: loss:1.1162727870047093,acc:0.6536999344825745
node17 weight score:395.960561921443
node19: train data size:4281
node19 epoch0:node_model train_loss:0.9483660681303158,train_acc:0.691372275352478
node19 epoch1:node_model train_loss:0.7125033140182495,train_acc:0.7471029758453369
node19 epoch2:node_model train_loss:0.6194711119629616,train_acc:0.7858454585075378
node19 epoch3:node_model train_loss:0.5754320039305576,train_acc:0.7991816401481628
node19 epoch4:node_model train_loss:0.5180865242037662,train_acc:0.8273760080337524
node19_model on test-dataset: loss:0.9353879809379577,acc:0.6940999627113342
node19 weight score:4576.710506486559
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7187888622283936,acc:0.7578999835252762
total cost energy:7.7715551275463985 | all_enery_cp：5.911 | all_enery_tp: 1.860555127546399
ef: 24.21130407034907
reward: 16.43974894280267
step 307:loss:6.227527141571045|running q:11.102489471435547
episode5,iteration7 selected nodes:[1, 12, 18, 16, 3],center node:16
################################################## episode5,iteration7 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.880126602947712,train_acc:0.7155882120132446
node1 epoch1:node_model train_loss:0.7211766799583155,train_acc:0.7541911005973816
node1 epoch2:node_model train_loss:0.7533181734821376,train_acc:0.7477941513061523
node1 epoch3:node_model train_loss:0.6366940517635906,train_acc:0.7872794270515442
node1 epoch4:node_model train_loss:0.6097806008423076,train_acc:0.7856617569923401
node1_model on test-dataset: loss:0.7988776133954525,acc:0.7350000739097595
node1 weight score:8396.780542502787
node3: train data size:4247
node3 epoch0:node_model train_loss:0.8894157562144968,train_acc:0.711622953414917
node3 epoch1:node_model train_loss:0.6870179439699927,train_acc:0.7620880603790283
node3 epoch2:node_model train_loss:0.6058345326157504,train_acc:0.7838594913482666
node3 epoch3:node_model train_loss:0.5181185894234236,train_acc:0.8224096298217773
node3 epoch4:node_model train_loss:0.46220025212265725,train_acc:0.842201828956604
node3_model on test-dataset: loss:0.8575294044613838,acc:0.7222001552581787
node3 weight score:4952.599850109572
node12: train data size:1336
node12 epoch0:node_model train_loss:0.9496576317719051,train_acc:0.6871427297592163
node12 epoch1:node_model train_loss:0.6365562464509692,train_acc:0.7787302136421204
node12 epoch2:node_model train_loss:0.5104253036635262,train_acc:0.8325396776199341
node12 epoch3:node_model train_loss:0.35585165343114306,train_acc:0.903174638748169
node12 epoch4:node_model train_loss:0.33321833716971533,train_acc:0.8973809480667114
node12_model on test-dataset: loss:1.0035127639770507,acc:0.6754999160766602
node12 weight score:1331.3233752057715
node16: train data size:877
node16 epoch0:node_model train_loss:0.8920195500055949,train_acc:0.7184703946113586
node16 epoch1:node_model train_loss:0.5629673467742072,train_acc:0.8129004240036011
node16 epoch2:node_model train_loss:0.4305981993675232,train_acc:0.8527994751930237
node16 epoch3:node_model train_loss:0.33755614525742,train_acc:0.8910101056098938
node16 epoch4:node_model train_loss:0.26749282081921893,train_acc:0.925007164478302
node16_model on test-dataset: loss:0.8804140463471413,acc:0.7103999257087708
node16 weight score:996.122226398697
node18: train data size:472
node18 epoch0:node_model train_loss:0.8290268182754517,train_acc:0.7324444651603699
node18 epoch1:node_model train_loss:0.5117638170719147,train_acc:0.8166666030883789
node18 epoch2:node_model train_loss:0.36553633213043213,train_acc:0.8796667456626892
node18 epoch3:node_model train_loss:0.2722808659076691,train_acc:0.9288889169692993
node18 epoch4:node_model train_loss:0.20561543405056,train_acc:0.9453333020210266
node18_model on test-dataset: loss:1.0869410890340805,acc:0.6698000431060791
node18 weight score:434.2461654655514
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6961232208460569,acc:0.7648999816179276
total cost energy:9.244403979218955 | all_enery_cp：6.82 | all_enery_tp: 2.4244039792189556
ef: 24.510280658565406
reward: 15.265876679346452
step 308:loss:5.258251190185547|running q:12.65621566772461
episode5,iteration8 selected nodes:[16, 11, 10, 3, 1],center node:11
################################################## episode5,iteration8 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.6790122245164478,train_acc:0.7617645859718323
node1 epoch1:node_model train_loss:0.5715454286512207,train_acc:0.800735354423523
node1 epoch2:node_model train_loss:0.527622701928896,train_acc:0.8129411935806274
node1 epoch3:node_model train_loss:0.4838160267647575,train_acc:0.8270589113235474
node1 epoch4:node_model train_loss:0.4177042302839896,train_acc:0.857426643371582
node1_model on test-dataset: loss:0.8728501716256142,acc:0.7251999974250793
node1 weight score:7685.167761961807
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7453792310038279,train_acc:0.7469370365142822
node3 epoch1:node_model train_loss:0.5567448707514031,train_acc:0.8047056198120117
node3 epoch2:node_model train_loss:0.4683989944846131,train_acc:0.8354328870773315
node3 epoch3:node_model train_loss:0.4282500598319741,train_acc:0.8533400297164917
node3 epoch4:node_model train_loss:0.3825208371461824,train_acc:0.8688323497772217
node3_model on test-dataset: loss:0.9329748295247555,acc:0.7134998440742493
node3 weight score:4552.105657730726
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8947033643722534,train_acc:0.7091667652130127
node10 epoch1:node_model train_loss:0.599540664255619,train_acc:0.8021666407585144
node10 epoch2:node_model train_loss:0.5232160001993179,train_acc:0.8219999670982361
node10 epoch3:node_model train_loss:0.41850737780332564,train_acc:0.8683332800865173
node10 epoch4:node_model train_loss:0.34807365760207176,train_acc:0.890666663646698
node10_model on test-dataset: loss:0.8522105884552001,acc:0.7118999361991882
node10 weight score:2317.502301373745
node11: train data size:1682
node11 epoch0:node_model train_loss:0.8173535992117489,train_acc:0.7342897653579712
node11 epoch1:node_model train_loss:0.5575928354964537,train_acc:0.8149210214614868
node11 epoch2:node_model train_loss:0.4210110885255477,train_acc:0.8598852753639221
node11 epoch3:node_model train_loss:0.3618798764312969,train_acc:0.8842611312866211
node11 epoch4:node_model train_loss:0.29741184062817516,train_acc:0.9126254916191101
node11_model on test-dataset: loss:0.9378956328332424,acc:0.7043998837471008
node11 weight score:1793.37651346017
node16: train data size:877
node16 epoch0:node_model train_loss:0.8428878585497538,train_acc:0.7404761910438538
node16 epoch1:node_model train_loss:0.576205290026135,train_acc:0.7999134063720703
node16 epoch2:node_model train_loss:0.4439872470166948,train_acc:0.8487879037857056
node16 epoch3:node_model train_loss:0.30486422611607444,train_acc:0.9097834825515747
node16 epoch4:node_model train_loss:0.2230852875444624,train_acc:0.9412265419960022
node16_model on test-dataset: loss:0.969188387542963,acc:0.6846998333930969
node16 weight score:904.8808376907256
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6810264694690704,acc:0.7716999799013138
total cost energy:9.237510659580074 | all_enery_cp：7.7445 | all_enery_tp: 1.493010659580075
ef: 24.83775298636711
reward: 15.600242326787036
step 309:loss:5.005955696105957|running q:14.124746322631836
episode5,iteration9 selected nodes:[14, 18, 7, 2, 4],center node:7
################################################## episode5,iteration9 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6215263934185108,train_acc:0.785359799861908
node2 epoch1:node_model train_loss:0.47867905845244724,train_acc:0.8321307897567749
node2 epoch2:node_model train_loss:0.40992501222838956,train_acc:0.8623105883598328
node2 epoch3:node_model train_loss:0.34607607622941333,train_acc:0.8860604763031006
node2 epoch4:node_model train_loss:0.31546144808332127,train_acc:0.8958804607391357
node2_model on test-dataset: loss:0.8774376650154591,acc:0.727899968624115
node2 weight score:5456.797891068015
node4: train data size:2705
node4 epoch0:node_model train_loss:0.8495080747774669,train_acc:0.732499897480011
node4 epoch1:node_model train_loss:0.6219814834850175,train_acc:0.7832143306732178
node4 epoch2:node_model train_loss:0.5111928741846766,train_acc:0.82535719871521
node4 epoch3:node_model train_loss:0.4459631842161928,train_acc:0.8675000071525574
node4 epoch4:node_model train_loss:0.4812657098684992,train_acc:0.837142825126648
node4_model on test-dataset: loss:0.9733278468251229,acc:0.6985999941825867
node4 weight score:2779.1252544796507
node7: train data size:1951
node7 epoch0:node_model train_loss:0.8481304526329041,train_acc:0.7228922247886658
node7 epoch1:node_model train_loss:0.5534942656755447,train_acc:0.8081960678100586
node7 epoch2:node_model train_loss:0.4252419427037239,train_acc:0.8541176915168762
node7 epoch3:node_model train_loss:0.3223591364920139,train_acc:0.8990392684936523
node7 epoch4:node_model train_loss:0.2690611436963081,train_acc:0.9176176190376282
node7_model on test-dataset: loss:0.822194248586893,acc:0.7343003153800964
node7 weight score:2372.918569247094
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7615612248579661,train_acc:0.7393056154251099
node14 epoch1:node_model train_loss:0.4631261130174001,train_acc:0.8474074602127075
node14 epoch2:node_model train_loss:0.3378007387121518,train_acc:0.8860647678375244
node14 epoch3:node_model train_loss:0.2717805653810501,train_acc:0.9170370101928711
node14 epoch4:node_model train_loss:0.20288110896945,train_acc:0.9522222876548767
node14_model on test-dataset: loss:0.8345464210212231,acc:0.7346998453140259
node14 weight score:1404.3556721096945
node18: train data size:472
node18 epoch0:node_model train_loss:0.8096516966819763,train_acc:0.738444447517395
node18 epoch1:node_model train_loss:0.5087078869342804,train_acc:0.8391111493110657
node18 epoch2:node_model train_loss:0.3291554659605026,train_acc:0.8654444813728333
node18 epoch3:node_model train_loss:0.2503123313188553,train_acc:0.9256667494773865
node18 epoch4:node_model train_loss:0.16868146508932114,train_acc:0.9508889317512512
node18_model on test-dataset: loss:0.9621545988321304,acc:0.7120001912117004
node18 weight score:490.5656539738174
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6575681175291538,acc:0.7820999836921692
total cost energy:7.393648583671402 | all_enery_cp：5.544 | all_enery_tp: 1.849648583671402
ef: 24.558181434944167
reward: 17.164532851272767
step 310:loss:10.326298713684082|running q:15.588205337524414
episode5,iteration10 selected nodes:[9, 10, 6, 13, 3],center node:10
################################################## episode5,iteration10 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7094951868057251,train_acc:0.7657199501991272
node3 epoch1:node_model train_loss:0.46968427854914996,train_acc:0.8343838453292847
node3 epoch2:node_model train_loss:0.4463497531968494,train_acc:0.8490944504737854
node3 epoch3:node_model train_loss:0.3574238824982976,train_acc:0.8814201354980469
node3 epoch4:node_model train_loss:0.3291864225337672,train_acc:0.8877882957458496
node3_model on test-dataset: loss:0.8614040185511113,acc:0.7241999506950378
node3 weight score:4930.322947811979
node6: train data size:3007
node6 epoch0:node_model train_loss:0.8325918605250697,train_acc:0.7345622181892395
node6 epoch1:node_model train_loss:0.6238661831425082,train_acc:0.7878801822662354
node6 epoch2:node_model train_loss:0.5995947372528815,train_acc:0.8018894195556641
node6 epoch3:node_model train_loss:0.4791229199017248,train_acc:0.8448386788368225
node6 epoch4:node_model train_loss:0.38636607508505544,train_acc:0.871105968952179
node6_model on test-dataset: loss:0.8320063816010952,acc:0.7296000719070435
node6 weight score:3614.1549710392774
node9: train data size:1857
node9 epoch0:node_model train_loss:0.8020150943806297,train_acc:0.7540442943572998
node9 epoch1:node_model train_loss:0.5299653222686366,train_acc:0.8084025979042053
node9 epoch2:node_model train_loss:0.4282126497281225,train_acc:0.8541920185089111
node9 epoch3:node_model train_loss:0.3569681448371787,train_acc:0.8818283081054688
node9 epoch4:node_model train_loss:0.26505372320350845,train_acc:0.912622332572937
node9_model on test-dataset: loss:0.8554683020710945,acc:0.7331000566482544
node9 weight score:2170.740862641188
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7921931177377701,train_acc:0.7464998960494995
node10 epoch1:node_model train_loss:0.5295768052339553,train_acc:0.8236665725708008
node10 epoch2:node_model train_loss:0.4091630719602108,train_acc:0.859333336353302
node10 epoch3:node_model train_loss:0.3059182964265347,train_acc:0.9106665849685669
node10 epoch4:node_model train_loss:0.27370216622948645,train_acc:0.9169999957084656
node10_model on test-dataset: loss:0.8629023665189743,acc:0.7236998677253723
node10 weight score:2288.787325925791
node13: train data size:1155
node13 epoch0:node_model train_loss:0.9620961745580038,train_acc:0.7147727012634277
node13 epoch1:node_model train_loss:0.6386348977684975,train_acc:0.7787121534347534
node13 epoch2:node_model train_loss:0.4407844493786494,train_acc:0.8471211791038513
node13 epoch3:node_model train_loss:0.34087271988391876,train_acc:0.888257622718811
node13 epoch4:node_model train_loss:0.25930423599978286,train_acc:0.9247726798057556
node13_model on test-dataset: loss:0.8558331076800824,acc:0.7282001376152039
node13 weight score:1349.562186406732
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6367102114856243,acc:0.7876999813318253
total cost energy:7.636851461583877 | all_enery_cp：6.1205 | all_enery_tp: 1.5163514615838767
ef: 24.937672388744044
reward: 17.300820927160167
step 311:loss:10.188763618469238|running q:17.157791137695312
episode5,iteration11 selected nodes:[5, 12, 3, 11, 15],center node:11
################################################## episode5,iteration11 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5249401649763418,train_acc:0.8177585005760193
node3 epoch1:node_model train_loss:0.3888152110022168,train_acc:0.8637751936912537
node3 epoch2:node_model train_loss:0.32933626375919167,train_acc:0.889792263507843
node3 epoch3:node_model train_loss:0.2817200709914052,train_acc:0.9084562063217163
node3 epoch4:node_model train_loss:0.21419010190076607,train_acc:0.9385452270507812
node3_model on test-dataset: loss:0.7888051667809486,acc:0.7456000447273254
node3 weight score:5384.092522278564
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7753196492006904,train_acc:0.7483834028244019
node5 epoch1:node_model train_loss:0.5376508000649904,train_acc:0.813082754611969
node5 epoch2:node_model train_loss:0.47314253292585673,train_acc:0.8441352844238281
node5 epoch3:node_model train_loss:0.4116352898509879,train_acc:0.8569926023483276
node5 epoch4:node_model train_loss:0.34959024269329875,train_acc:0.8862404227256775
node5_model on test-dataset: loss:0.7976957562565804,acc:0.7387001514434814
node5 weight score:4682.236267029393
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7424709463820738,train_acc:0.7573027610778809
node11 epoch1:node_model train_loss:0.48284641784780163,train_acc:0.8389812707901001
node11 epoch2:node_model train_loss:0.3579549543997821,train_acc:0.8804734349250793
node11 epoch3:node_model train_loss:0.2861120788490071,train_acc:0.9133428335189819
node11 epoch4:node_model train_loss:0.22547103070160923,train_acc:0.9335437417030334
node11_model on test-dataset: loss:0.811135901659727,acc:0.7458998560905457
node11 weight score:2073.6352521918107
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8394485541752407,train_acc:0.7239682078361511
node12 epoch1:node_model train_loss:0.538145335657256,train_acc:0.8241270184516907
node12 epoch2:node_model train_loss:0.3728605019194739,train_acc:0.8784920573234558
node12 epoch3:node_model train_loss:0.27003805126462666,train_acc:0.9180158376693726
node12 epoch4:node_model train_loss:0.2266739723937852,train_acc:0.9325396418571472
node12_model on test-dataset: loss:0.824696424305439,acc:0.7415000796318054
node12 weight score:1619.9900480048545
node15: train data size:629
node15 epoch0:node_model train_loss:0.8443299787385123,train_acc:0.7283744215965271
node15 epoch1:node_model train_loss:0.5994149020739964,train_acc:0.8118719458580017
node15 epoch2:node_model train_loss:0.5169309760843005,train_acc:0.8239409327507019
node15 epoch3:node_model train_loss:0.2970874160528183,train_acc:0.8980787992477417
node15 epoch4:node_model train_loss:0.24561590807778494,train_acc:0.9258621335029602
node15_model on test-dataset: loss:0.919984335899353,acc:0.7137999534606934
node15 weight score:683.7072931085352
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6297561655938625,acc:0.7876999819278717
total cost energy:7.622268723046357 | all_enery_cp：5.8145 | all_enery_tp: 1.8077687230463568
ef: 24.822915730166027
reward: 17.20064700711967
step 312:loss:5.101413249969482|running q:18.622974395751953
episode5,iteration12 selected nodes:[14, 8, 15, 13, 11],center node:11
################################################## episode5,iteration12 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.8385583625899421,train_acc:0.7435374855995178
node8 epoch1:node_model train_loss:0.5933414697647095,train_acc:0.7975397109985352
node8 epoch2:node_model train_loss:0.4214188754558563,train_acc:0.8548524975776672
node8 epoch3:node_model train_loss:0.3151439767744806,train_acc:0.9014965891838074
node8 epoch4:node_model train_loss:0.28795838935507667,train_acc:0.9110202789306641
node8_model on test-dataset: loss:0.9056894150003791,acc:0.7254000902175903
node8 weight score:1985.2280154993832
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6853195463909822,train_acc:0.7688521146774292
node11 epoch1:node_model train_loss:0.43687344298643227,train_acc:0.8585795760154724
node11 epoch2:node_model train_loss:0.2996213173165041,train_acc:0.9036728739738464
node11 epoch3:node_model train_loss:0.2357933232012917,train_acc:0.9300142526626587
node11 epoch4:node_model train_loss:0.18289550349992864,train_acc:0.9554949402809143
node11_model on test-dataset: loss:0.7861918471753597,acc:0.7520000338554382
node11 weight score:2139.4269172888417
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7996094624201456,train_acc:0.7496969699859619
node13 epoch1:node_model train_loss:0.5160098622242609,train_acc:0.8249242305755615
node13 epoch2:node_model train_loss:0.3399873599410057,train_acc:0.882727324962616
node13 epoch3:node_model train_loss:0.26179444169004756,train_acc:0.9264394044876099
node13 epoch4:node_model train_loss:0.2148528192192316,train_acc:0.9397727847099304
node13_model on test-dataset: loss:0.842771438062191,acc:0.7326998114585876
node13 weight score:1370.4783382974217
node14: train data size:1172
node14 epoch0:node_model train_loss:0.702412873506546,train_acc:0.7717129588127136
node14 epoch1:node_model train_loss:0.4459406832853953,train_acc:0.84458327293396
node14 epoch2:node_model train_loss:0.2975073456764221,train_acc:0.8990740776062012
node14 epoch3:node_model train_loss:0.21984638522068659,train_acc:0.9381943941116333
node14 epoch4:node_model train_loss:0.14401445537805557,train_acc:0.9676851630210876
node14_model on test-dataset: loss:0.8362823785841464,acc:0.7376998066902161
node14 weight score:1401.440506236942
node15: train data size:629
node15 epoch0:node_model train_loss:0.7833365457398551,train_acc:0.7270936369895935
node15 epoch1:node_model train_loss:0.5481528426919665,train_acc:0.8167980313301086
node15 epoch2:node_model train_loss:0.33341490796634127,train_acc:0.8882266283035278
node15 epoch3:node_model train_loss:0.2320108562707901,train_acc:0.927290678024292
node15 epoch4:node_model train_loss:0.17332894248621805,train_acc:0.9571428894996643
node15_model on test-dataset: loss:0.9179977986216545,acc:0.7254999876022339
node15 weight score:685.1868282739067
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.635837514847517,acc:0.7933999818563461
total cost energy:4.875115546859236 | all_enery_cp：3.218 | all_enery_tp: 1.6571155468592365
ef: 24.53465773833053
reward: 19.659542191471296
step 313:loss:5.217353820800781|running q:20.0837459564209
episode5,iteration13 selected nodes:[12, 2, 3, 18, 1],center node:1
################################################## episode5,iteration13 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.6166083072038258,train_acc:0.7919116616249084
node1 epoch1:node_model train_loss:0.45860234209719825,train_acc:0.8442649245262146
node1 epoch2:node_model train_loss:0.4335479902870515,train_acc:0.8488970994949341
node1 epoch3:node_model train_loss:0.36890582784133796,train_acc:0.8702206611633301
node1 epoch4:node_model train_loss:0.3076597439015613,train_acc:0.8995588421821594
node1_model on test-dataset: loss:0.7913734316825867,acc:0.7485998868942261
node1 weight score:8476.40283517949
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5532775421937307,train_acc:0.8116192817687988
node2 epoch1:node_model train_loss:0.3792281976590554,train_acc:0.8664774298667908
node2 epoch2:node_model train_loss:0.29390086481968564,train_acc:0.9060510993003845
node2 epoch3:node_model train_loss:0.26191241449366015,train_acc:0.9175378084182739
node2 epoch4:node_model train_loss:0.237823318845282,train_acc:0.927177906036377
node2_model on test-dataset: loss:0.8333058744668961,acc:0.7447999119758606
node2 weight score:5745.789327434063
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4555376449296641,train_acc:0.8402572274208069
node3 epoch1:node_model train_loss:0.3184557096209637,train_acc:0.8904353380203247
node3 epoch2:node_model train_loss:0.25123452759066295,train_acc:0.9205194711685181
node3 epoch3:node_model train_loss:0.22891463755175126,train_acc:0.9300840497016907
node3 epoch4:node_model train_loss:0.1974989789169888,train_acc:0.9406383633613586
node3_model on test-dataset: loss:0.82178188174963,acc:0.7533998489379883
node3 weight score:5168.038009012617
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6705071819680077,train_acc:0.7797620296478271
node12 epoch1:node_model train_loss:0.508693373629025,train_acc:0.827380895614624
node12 epoch2:node_model train_loss:0.39452249131032396,train_acc:0.8592063784599304
node12 epoch3:node_model train_loss:0.3078819843275206,train_acc:0.8917460441589355
node12 epoch4:node_model train_loss:0.20090205009494508,train_acc:0.9515873789787292
node12_model on test-dataset: loss:0.8254476027190685,acc:0.7490999698638916
node12 weight score:1618.5158156606728
node18: train data size:472
node18 epoch0:node_model train_loss:0.6724753856658936,train_acc:0.7878888845443726
node18 epoch1:node_model train_loss:0.4114186704158783,train_acc:0.8414444327354431
node18 epoch2:node_model train_loss:0.2202308803796768,train_acc:0.9364444017410278
node18 epoch3:node_model train_loss:0.22108545303344726,train_acc:0.932888925075531
node18 epoch4:node_model train_loss:0.12927109748125076,train_acc:0.9784443974494934
node18_model on test-dataset: loss:0.9248112316429615,acc:0.7205999493598938
node18 weight score:510.37442436925693
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6321502414345741,acc:0.7948999816179275
total cost energy:11.12747627342636 | all_enery_cp：8.775500000000001 | all_enery_tp: 2.351976273426359
ef: 24.649672643699816
reward: 13.522196370273456
step 314:loss:6.360267162322998|running q:21.48150634765625
episode5,iteration14 selected nodes:[12, 13, 10, 7, 11],center node:10
################################################## episode5,iteration14 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7485407948493957,train_acc:0.7662352919578552
node7 epoch1:node_model train_loss:0.4599224030971527,train_acc:0.8476764559745789
node7 epoch2:node_model train_loss:0.31762688159942626,train_acc:0.8906764984130859
node7 epoch3:node_model train_loss:0.26174544021487234,train_acc:0.9226175546646118
node7 epoch4:node_model train_loss:0.19336906522512437,train_acc:0.9520195126533508
node7_model on test-dataset: loss:0.8047572301328182,acc:0.7497998476028442
node7 weight score:2424.3336088797914
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7180592507123947,train_acc:0.765500009059906
node10 epoch1:node_model train_loss:0.5243962451815605,train_acc:0.8191666603088379
node10 epoch2:node_model train_loss:0.40372133180499076,train_acc:0.8659999966621399
node10 epoch3:node_model train_loss:0.30115812867879865,train_acc:0.9019999504089355
node10 epoch4:node_model train_loss:0.22304572947323323,train_acc:0.9398332834243774
node10_model on test-dataset: loss:0.7806938689947128,acc:0.7571001052856445
node10 weight score:2529.800832870862
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5956317834994372,train_acc:0.8102869391441345
node11 epoch1:node_model train_loss:0.3985413926489213,train_acc:0.8743327856063843
node11 epoch2:node_model train_loss:0.2667330073959687,train_acc:0.9131419658660889
node11 epoch3:node_model train_loss:0.21928774171015797,train_acc:0.9352365732192993
node11 epoch4:node_model train_loss:0.1683996161993812,train_acc:0.9520373940467834
node11_model on test-dataset: loss:0.8050534768402576,acc:0.7543997168540955
node11 weight score:2089.302199652198
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7003082803317479,train_acc:0.7771428823471069
node12 epoch1:node_model train_loss:0.39781435685498373,train_acc:0.861587405204773
node12 epoch2:node_model train_loss:0.27539559666599545,train_acc:0.913730263710022
node12 epoch3:node_model train_loss:0.20199998787471227,train_acc:0.9401586651802063
node12 epoch4:node_model train_loss:0.1719066341008459,train_acc:0.953889012336731
node12_model on test-dataset: loss:0.8558238568902016,acc:0.7343000769615173
node12 weight score:1561.0688919734134
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7956903328498205,train_acc:0.7417424917221069
node13 epoch1:node_model train_loss:0.41081585238377255,train_acc:0.8525757193565369
node13 epoch2:node_model train_loss:0.30634448304772377,train_acc:0.8982574939727783
node13 epoch3:node_model train_loss:0.21382631113131842,train_acc:0.9347727298736572
node13 epoch4:node_model train_loss:0.19550803489983082,train_acc:0.9432575106620789
node13_model on test-dataset: loss:0.9059363994002342,acc:0.7268000841140747
node13 weight score:1274.9239359017429
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6154608626663685,acc:0.8003999799489975
total cost energy:5.188405042258275 | all_enery_cp：4.0495 | all_enery_tp: 1.138905042258274
ef: 24.984242636274114
reward: 19.79583759401584
step 315:loss:4.809406757354736|running q:22.903135299682617
episode5,iteration15 selected nodes:[10, 15, 9, 0, 19],center node:9
################################################## episode5,iteration15 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.7297710638779861,train_acc:0.7648332118988037
node0 epoch1:node_model train_loss:0.5095846698834345,train_acc:0.8214852809906006
node0 epoch2:node_model train_loss:0.4191046357154846,train_acc:0.8501042723655701
node0 epoch3:node_model train_loss:0.3546089567244053,train_acc:0.8772591352462769
node0 epoch4:node_model train_loss:0.3158830590546131,train_acc:0.8880236744880676
node0_model on test-dataset: loss:0.9535323444008827,acc:0.7166001200675964
node0 weight score:5435.578594092211
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7428934009451615,train_acc:0.7577378153800964
node9 epoch1:node_model train_loss:0.44471403721131775,train_acc:0.844856858253479
node9 epoch2:node_model train_loss:0.34282189137057256,train_acc:0.885115385055542
node9 epoch3:node_model train_loss:0.2727167865163402,train_acc:0.9078762531280518
node9 epoch4:node_model train_loss:0.20540487530984378,train_acc:0.9372298717498779
node9_model on test-dataset: loss:0.7778489740937948,acc:0.7564002275466919
node9 weight score:2387.3528947742475
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5841979041695595,train_acc:0.8024999499320984
node10 epoch1:node_model train_loss:0.3487797640264034,train_acc:0.8905000686645508
node10 epoch2:node_model train_loss:0.29363900497555734,train_acc:0.901166558265686
node10 epoch3:node_model train_loss:0.20236769914627076,train_acc:0.9340000152587891
node10 epoch4:node_model train_loss:0.17224684543907642,train_acc:0.953499972820282
node10_model on test-dataset: loss:0.8223542700707912,acc:0.746999979019165
node10 weight score:2401.641326468682
node15: train data size:629
node15 epoch0:node_model train_loss:0.7694478545870099,train_acc:0.7505910992622375
node15 epoch1:node_model train_loss:0.4472705125808716,train_acc:0.833152711391449
node15 epoch2:node_model train_loss:0.2903287687471935,train_acc:0.90443354845047
node15 epoch3:node_model train_loss:0.20951061589377268,train_acc:0.9393596649169922
node15 epoch4:node_model train_loss:0.13097684936864035,train_acc:0.9650738835334778
node15_model on test-dataset: loss:0.8966696691513062,acc:0.7278997898101807
node15 weight score:701.4846399291566
node19: train data size:4281
node19 epoch0:node_model train_loss:0.7316878354826639,train_acc:0.769988477230072
node19 epoch1:node_model train_loss:0.5429438484269519,train_acc:0.8208096623420715
node19 epoch2:node_model train_loss:0.4120476259741672,train_acc:0.8596324324607849
node19 epoch3:node_model train_loss:0.3453330546617508,train_acc:0.8868962526321411
node19 epoch4:node_model train_loss:0.29673488784668056,train_acc:0.8999196290969849
node19_model on test-dataset: loss:0.7307136952877045,acc:0.765999972820282
node19 weight score:5858.655760262491
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6032842709124089,acc:0.8033999803662301
total cost energy:8.902724255080033 | all_enery_cp：6.9625 | all_enery_tp: 1.9402242550800328
ef: 24.872324523014964
reward: 15.969600267934931
step 316:loss:5.4194512367248535|running q:24.283912658691406
episode5,iteration16 selected nodes:[1, 11, 0, 18, 17],center node:11
################################################## episode5,iteration16 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.547091408417775,train_acc:0.8104889988899231
node0 epoch1:node_model train_loss:0.40230599819467616,train_acc:0.8568003177642822
node0 epoch2:node_model train_loss:0.33285733140431917,train_acc:0.8883389234542847
node0 epoch3:node_model train_loss:0.27303766201321894,train_acc:0.9112975597381592
node0 epoch4:node_model train_loss:0.2561287051783158,train_acc:0.9183780550956726
node0_model on test-dataset: loss:0.772656144052744,acc:0.765099823474884
node0 weight score:6708.029231236129
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4960986060254714,train_acc:0.8275735974311829
node1 epoch1:node_model train_loss:0.36050987835316095,train_acc:0.8708088397979736
node1 epoch2:node_model train_loss:0.35036762475090866,train_acc:0.8807353377342224
node1 epoch3:node_model train_loss:0.28103714677340846,train_acc:0.9056618809700012
node1 epoch4:node_model train_loss:0.26270540716016993,train_acc:0.9126469492912292
node1_model on test-dataset: loss:0.8630026987195015,acc:0.7493001222610474
node1 weight score:7772.860977089802
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5594452023506165,train_acc:0.8105452656745911
node11 epoch1:node_model train_loss:0.3305819139761083,train_acc:0.8989095091819763
node11 epoch2:node_model train_loss:0.21440930752193227,train_acc:0.935035765171051
node11 epoch3:node_model train_loss:0.15485375593690312,train_acc:0.9600716829299927
node11 epoch4:node_model train_loss:0.11741751739207436,train_acc:0.9717647433280945
node11_model on test-dataset: loss:0.7729611213505269,acc:0.7679997682571411
node11 weight score:2176.047350300348
node17: train data size:442
node17 epoch0:node_model train_loss:0.786359989643097,train_acc:0.7600952386856079
node17 epoch1:node_model train_loss:0.466296511888504,train_acc:0.8581904768943787
node17 epoch2:node_model train_loss:0.3264377683401108,train_acc:0.907714307308197
node17 epoch3:node_model train_loss:0.2552785038948059,train_acc:0.9214286208152771
node17 epoch4:node_model train_loss:0.14288188368082047,train_acc:0.9724761843681335
node17_model on test-dataset: loss:0.9227362126111984,acc:0.7183001637458801
node17 weight score:479.01013741425567
node18: train data size:472
node18 epoch0:node_model train_loss:0.7109510898590088,train_acc:0.7771111130714417
node18 epoch1:node_model train_loss:0.3704980671405792,train_acc:0.8709999918937683
node18 epoch2:node_model train_loss:0.20358123183250426,train_acc:0.9341111183166504
node18 epoch3:node_model train_loss:0.13485640436410903,train_acc:0.965222179889679
node18 epoch4:node_model train_loss:0.11038341075181961,train_acc:0.9819999933242798
node18_model on test-dataset: loss:0.8814259526133538,acc:0.740899920463562
node18 weight score:535.4959184042173
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6336761884391308,acc:0.8036999821662902
total cost energy:9.235425977444876 | all_enery_cp：7.2435 | all_enery_tp: 1.9919259774448754
ef: 24.631471759678895
reward: 15.396045782234019
step 317:loss:5.537428379058838|running q:25.681211471557617
episode5,iteration17 selected nodes:[11, 2, 13, 7, 15],center node:11
################################################## episode5,iteration17 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5117445414264997,train_acc:0.8296021223068237
node2 epoch1:node_model train_loss:0.3199280795330803,train_acc:0.8948956727981567
node2 epoch2:node_model train_loss:0.24678950337693095,train_acc:0.9225093126296997
node2 epoch3:node_model train_loss:0.2148369235607485,train_acc:0.9299999475479126
node2 epoch4:node_model train_loss:0.19809337643285593,train_acc:0.9384943842887878
node2_model on test-dataset: loss:0.838543938100338,acc:0.7580999732017517
node2 weight score:5709.897576562149
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6055971384048462,train_acc:0.7927352786064148
node7 epoch1:node_model train_loss:0.3946741037070751,train_acc:0.8686373829841614
node7 epoch2:node_model train_loss:0.2954033650457859,train_acc:0.9025782942771912
node7 epoch3:node_model train_loss:0.19925351813435555,train_acc:0.9446176886558533
node7 epoch4:node_model train_loss:0.15576886311173438,train_acc:0.9575392603874207
node7_model on test-dataset: loss:0.7794372662901878,acc:0.7569998502731323
node7 weight score:2503.08791275273
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5206391636063071,train_acc:0.8224389553070068
node11 epoch1:node_model train_loss:0.33204657452947955,train_acc:0.8956959247589111
node11 epoch2:node_model train_loss:0.19933443253531175,train_acc:0.9392251968383789
node11 epoch3:node_model train_loss:0.15345028524889665,train_acc:0.9585652351379395
node11 epoch4:node_model train_loss:0.1205637490048128,train_acc:0.9760830998420715
node11_model on test-dataset: loss:0.8272712679207325,acc:0.7549999356269836
node11 weight score:2033.1903998401235
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7228088354070982,train_acc:0.7625000476837158
node13 epoch1:node_model train_loss:0.40289994701743126,train_acc:0.8628789186477661
node13 epoch2:node_model train_loss:0.28057645012935,train_acc:0.9089393615722656
node13 epoch3:node_model train_loss:0.17330611372987428,train_acc:0.9579545855522156
node13 epoch4:node_model train_loss:0.15356068996091685,train_acc:0.9634847640991211
node13_model on test-dataset: loss:0.8465948480367661,acc:0.7432999014854431
node13 weight score:1364.288954366328
node15: train data size:629
node15 epoch0:node_model train_loss:0.8223833867481777,train_acc:0.7356650829315186
node15 epoch1:node_model train_loss:0.42432630487850737,train_acc:0.8475862741470337
node15 epoch2:node_model train_loss:0.35219502449035645,train_acc:0.8745813369750977
node15 epoch3:node_model train_loss:0.23713036520140512,train_acc:0.9330049753189087
node15 epoch4:node_model train_loss:0.17658528579132898,train_acc:0.9566502571105957
node15_model on test-dataset: loss:0.8750232511758804,acc:0.7294998168945312
node15 weight score:718.838041337453
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6444930654019118,acc:0.8006999808549881
total cost energy:6.812401951359279 | all_enery_cp：5.1025 | all_enery_tp: 1.7099019513592784
ef: 24.759254371376418
reward: 17.946852420017137
step 318:loss:2.156376361846924|running q:27.10715675354004
episode5,iteration18 selected nodes:[2, 15, 12, 17, 8],center node:17
################################################## episode5,iteration18 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.33968711489190656,train_acc:0.8780208826065063
node2 epoch1:node_model train_loss:0.23822340896974006,train_acc:0.917982816696167
node2 epoch2:node_model train_loss:0.18600330781191587,train_acc:0.9444979429244995
node2 epoch3:node_model train_loss:0.15350511716678739,train_acc:0.954943060874939
node2 epoch4:node_model train_loss:0.12890893554625413,train_acc:0.9657194018363953
node2_model on test-dataset: loss:0.7769596080482006,acc:0.7760999202728271
node2 weight score:6162.482515697217
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7594934503237406,train_acc:0.7669160962104797
node8 epoch1:node_model train_loss:0.4534524182478587,train_acc:0.8399091958999634
node8 epoch2:node_model train_loss:0.33038925213946235,train_acc:0.8915079236030579
node8 epoch3:node_model train_loss:0.2275004577305582,train_acc:0.9332539439201355
node8 epoch4:node_model train_loss:0.16693099463979402,train_acc:0.9516099095344543
node8_model on test-dataset: loss:0.7736766092479229,acc:0.7583000659942627
node8 weight score:2323.9684107133644
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6694148927927017,train_acc:0.7952380776405334
node12 epoch1:node_model train_loss:0.42433854937553406,train_acc:0.854127049446106
node12 epoch2:node_model train_loss:0.2941215740782874,train_acc:0.9047619104385376
node12 epoch3:node_model train_loss:0.20042594096490315,train_acc:0.9408730268478394
node12 epoch4:node_model train_loss:0.14792483991810254,train_acc:0.9647619724273682
node12_model on test-dataset: loss:0.8385440135002136,acc:0.7451000809669495
node12 weight score:1593.2377770170078
node15: train data size:629
node15 epoch0:node_model train_loss:0.6958569543702262,train_acc:0.7628079056739807
node15 epoch1:node_model train_loss:0.40614196019513266,train_acc:0.8630049824714661
node15 epoch2:node_model train_loss:0.3154365824801581,train_acc:0.8837931156158447
node15 epoch3:node_model train_loss:0.18713653832674026,train_acc:0.9442856907844543
node15 epoch4:node_model train_loss:0.14288062176534108,train_acc:0.9523645639419556
node15_model on test-dataset: loss:0.8978101074695587,acc:0.7367000579833984
node15 weight score:700.5935829490836
node17: train data size:442
node17 epoch0:node_model train_loss:0.8781997561454773,train_acc:0.755809485912323
node17 epoch1:node_model train_loss:0.4960111379623413,train_acc:0.8474285006523132
node17 epoch2:node_model train_loss:0.2236226886510849,train_acc:0.942476212978363
node17 epoch3:node_model train_loss:0.2139456480741501,train_acc:0.9301905035972595
node17 epoch4:node_model train_loss:0.14882493317127227,train_acc:0.9612380862236023
node17_model on test-dataset: loss:0.9864623676240444,acc:0.7105000019073486
node17 weight score:448.06574939557436
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6356230181455612,acc:0.7991999793052673
total cost energy:6.907808686849236 | all_enery_cp：4.4965 | all_enery_tp: 2.411308686849236
ef: 24.492808351860816
reward: 17.58499966501158
step 319:loss:4.658688068389893|running q:28.501752853393555
episode5,iteration19 selected nodes:[5, 13, 2, 9, 7],center node:9
################################################## episode5,iteration19 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2205767446818451,train_acc:0.9193844795227051
node2 epoch1:node_model train_loss:0.16898436782260737,train_acc:0.9456628561019897
node2 epoch2:node_model train_loss:0.11770946152197818,train_acc:0.9686647653579712
node2 epoch3:node_model train_loss:0.09453134145587683,train_acc:0.9777743816375732
node2 epoch4:node_model train_loss:0.09431770925099651,train_acc:0.9740813970565796
node2_model on test-dataset: loss:0.880286023914814,acc:0.7586000561714172
node2 weight score:5439.141222198182
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6836772031689945,train_acc:0.7834961414337158
node5 epoch1:node_model train_loss:0.46632113660636704,train_acc:0.8448873162269592
node5 epoch2:node_model train_loss:0.34802297424328954,train_acc:0.8856390714645386
node5 epoch3:node_model train_loss:0.29813194274902344,train_acc:0.896616518497467
node5 epoch4:node_model train_loss:0.2486870590793459,train_acc:0.9242478609085083
node5_model on test-dataset: loss:0.7946983739733696,acc:0.7552000284194946
node5 weight score:4699.896366121368
node7: train data size:1951
node7 epoch0:node_model train_loss:0.617730550467968,train_acc:0.7971764802932739
node7 epoch1:node_model train_loss:0.36243064552545545,train_acc:0.877656877040863
node7 epoch2:node_model train_loss:0.24881460405886174,train_acc:0.9190391898155212
node7 epoch3:node_model train_loss:0.15388697274029256,train_acc:0.9560195207595825
node7 epoch4:node_model train_loss:0.12841983288526534,train_acc:0.9720194935798645
node7_model on test-dataset: loss:0.8038066782057285,acc:0.7547001242637634
node7 weight score:2427.2005357744188
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7693961701895061,train_acc:0.7733887434005737
node9 epoch1:node_model train_loss:0.446411099873091,train_acc:0.8416896462440491
node9 epoch2:node_model train_loss:0.2963644203386809,train_acc:0.8959094882011414
node9 epoch3:node_model train_loss:0.2169044111904345,train_acc:0.9399906992912292
node9 epoch4:node_model train_loss:0.17703742533922195,train_acc:0.949593722820282
node9_model on test-dataset: loss:0.7677878407388925,acc:0.7699998021125793
node9 weight score:2418.636896115582
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7089575628439585,train_acc:0.7896969318389893
node13 epoch1:node_model train_loss:0.461546899129947,train_acc:0.8387121558189392
node13 epoch2:node_model train_loss:0.24244106436769167,train_acc:0.9271211624145508
node13 epoch3:node_model train_loss:0.16982016588250795,train_acc:0.948939323425293
node13 epoch4:node_model train_loss:0.1316829559703668,train_acc:0.9669697284698486
node13_model on test-dataset: loss:0.8318636080622673,acc:0.7495998740196228
node13 weight score:1388.4487658865646
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6343740808963776,acc:0.8027999788522721
total cost energy:8.201067296902629 | all_enery_cp：6.743 | all_enery_tp: 1.458067296902629
ef: 25.111841576462545
reward: 16.910774279559917
step 320:loss:5.654852390289307|running q:29.86008644104004
episode5,iteration20 selected nodes:[8, 5, 9, 3, 7],center node:7
################################################## episode5,iteration20 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5797603982825612,train_acc:0.8103710412979126
node3 epoch1:node_model train_loss:0.34228464787782625,train_acc:0.8823503255844116
node3 epoch2:node_model train_loss:0.26742782495742623,train_acc:0.9070609211921692
node3 epoch3:node_model train_loss:0.21353098923383756,train_acc:0.9272933006286621
node3 epoch4:node_model train_loss:0.1903206278071847,train_acc:0.937847375869751
node3_model on test-dataset: loss:0.7781421148777008,acc:0.766499936580658
node3 weight score:5457.8719218500255
node5: train data size:3735
node5 epoch0:node_model train_loss:0.530632565680303,train_acc:0.8251504302024841
node5 epoch1:node_model train_loss:0.38671235622544037,train_acc:0.870864748954773
node5 epoch2:node_model train_loss:0.315682180617985,train_acc:0.8993609547615051
node5 epoch3:node_model train_loss:0.23131320743184342,train_acc:0.9245866537094116
node5 epoch4:node_model train_loss:0.1840773048369508,train_acc:0.9452632069587708
node5_model on test-dataset: loss:0.7938505087792873,acc:0.7673999071121216
node5 weight score:4704.9160499290365
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5444926023483276,train_acc:0.8122352957725525
node7 epoch1:node_model train_loss:0.2987068764865398,train_acc:0.8977157473564148
node7 epoch2:node_model train_loss:0.2035230178385973,train_acc:0.9390196204185486
node7 epoch3:node_model train_loss:0.14799292609095574,train_acc:0.9630588889122009
node7 epoch4:node_model train_loss:0.10523028746247291,train_acc:0.9764999747276306
node7_model on test-dataset: loss:0.7989252999424934,acc:0.7603000402450562
node7 weight score:2442.0305629830887
node8: train data size:1798
node8 epoch0:node_model train_loss:0.681788166364034,train_acc:0.7953288555145264
node8 epoch1:node_model train_loss:0.4794064462184906,train_acc:0.8408163189888
node8 epoch2:node_model train_loss:0.3062320765521791,train_acc:0.9004534482955933
node8 epoch3:node_model train_loss:0.20601400319072935,train_acc:0.9410317540168762
node8 epoch4:node_model train_loss:0.15815948653552267,train_acc:0.9644330143928528
node8_model on test-dataset: loss:0.7883888396620751,acc:0.7617999911308289
node8 weight score:2280.600522923018
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5619637777930812,train_acc:0.8189287781715393
node9 epoch1:node_model train_loss:0.32912711250154597,train_acc:0.8853831887245178
node9 epoch2:node_model train_loss:0.2448101067229321,train_acc:0.9239335656166077
node9 epoch3:node_model train_loss:0.19332588542448848,train_acc:0.9360480308532715
node9 epoch4:node_model train_loss:0.14727982996325745,train_acc:0.9632871747016907
node9_model on test-dataset: loss:0.8712991102039814,acc:0.7451001405715942
node9 weight score:2131.300236913193
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6004644800722599,acc:0.8069999808073044
total cost energy:8.041213595499958 | all_enery_cp：6.7940000000000005 | all_enery_tp: 1.247213595499958
ef: 25.243938088834426
reward: 17.20272449333447
step 321:loss:4.787964344024658|running q:31.2114200592041
episode5,iteration21 selected nodes:[0, 8, 11, 4, 7],center node:7
################################################## episode5,iteration21 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5538068390809573,train_acc:0.8163276314735413
node0 epoch1:node_model train_loss:0.35882905469490933,train_acc:0.8769137859344482
node0 epoch2:node_model train_loss:0.29639052656980663,train_acc:0.8961098194122314
node0 epoch3:node_model train_loss:0.24602587549732283,train_acc:0.9213809370994568
node0 epoch4:node_model train_loss:0.19858312778748,train_acc:0.9387232065200806
node0_model on test-dataset: loss:0.8034982186555862,acc:0.765999972820282
node0 weight score:6450.543236638656
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6949436467673097,train_acc:0.7824999690055847
node4 epoch1:node_model train_loss:0.4539525434374809,train_acc:0.8450000882148743
node4 epoch2:node_model train_loss:0.3804016996707235,train_acc:0.8735714554786682
node4 epoch3:node_model train_loss:0.3591205280806337,train_acc:0.8750000596046448
node4 epoch4:node_model train_loss:0.326468862593174,train_acc:0.8885713815689087
node4_model on test-dataset: loss:1.1405300498753785,acc:0.6847001314163208
node4 weight score:2371.704279335354
node7: train data size:1951
node7 epoch0:node_model train_loss:0.47180506885051726,train_acc:0.8441373705863953
node7 epoch1:node_model train_loss:0.2817232750356197,train_acc:0.9080783724784851
node7 epoch2:node_model train_loss:0.15944538451731205,train_acc:0.9555196166038513
node7 epoch3:node_model train_loss:0.12889099605381488,train_acc:0.9690195322036743
node7 epoch4:node_model train_loss:0.08544471822679042,train_acc:0.9840587973594666
node7_model on test-dataset: loss:0.8086911810934544,acc:0.7663999795913696
node7 weight score:2412.540220065214
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5886163877116309,train_acc:0.8013832569122314
node8 epoch1:node_model train_loss:0.36530129032002556,train_acc:0.8720747828483582
node8 epoch2:node_model train_loss:0.23914989663494957,train_acc:0.927108883857727
node8 epoch3:node_model train_loss:0.1693385500046942,train_acc:0.9515417814254761
node8 epoch4:node_model train_loss:0.12096512731578615,train_acc:0.9738548398017883
node8_model on test-dataset: loss:0.755730512291193,acc:0.771399974822998
node8 weight score:2379.1549643124727
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5483416248770321,train_acc:0.810028612613678
node11 epoch1:node_model train_loss:0.31557590645902295,train_acc:0.8919082283973694
node11 epoch2:node_model train_loss:0.2224094297956018,train_acc:0.923730194568634
node11 epoch3:node_model train_loss:0.1517865688485258,train_acc:0.9523528814315796
node11 epoch4:node_model train_loss:0.11175431924707749,train_acc:0.971707284450531
node11_model on test-dataset: loss:0.8325820389389992,acc:0.7655999064445496
node11 weight score:2020.221337159106
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6033696326613426,acc:0.8091999781131745
total cost energy:8.022941361516796 | all_enery_cp：6.6595 | all_enery_tp: 1.363441361516796
ef: 25.065447403381906
reward: 17.04250604186511
step 322:loss:5.424904823303223|running q:32.586891174316406
episode5,iteration22 selected nodes:[2, 15, 12, 19, 4],center node:4
################################################## episode5,iteration22 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2804982603217165,train_acc:0.9043845534324646
node2 epoch1:node_model train_loss:0.15269862844919166,train_acc:0.9507197141647339
node2 epoch2:node_model train_loss:0.10736188137282927,train_acc:0.9711363315582275
node2 epoch3:node_model train_loss:0.10086053027771413,train_acc:0.9734846949577332
node2 epoch4:node_model train_loss:0.06571335589978844,train_acc:0.9889299273490906
node2_model on test-dataset: loss:0.8038406072556973,acc:0.7797001004219055
node2 weight score:5956.404735941591
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6045484489628247,train_acc:0.8078572154045105
node4 epoch1:node_model train_loss:0.44700723460742403,train_acc:0.839285671710968
node4 epoch2:node_model train_loss:0.4088905176946095,train_acc:0.8617857098579407
node4 epoch3:node_model train_loss:0.31741273775696754,train_acc:0.8907144069671631
node4 epoch4:node_model train_loss:0.250479448054518,train_acc:0.9142857789993286
node4_model on test-dataset: loss:0.8466608621180057,acc:0.7539000511169434
node4 weight score:3194.90379327701
node12: train data size:1336
node12 epoch0:node_model train_loss:0.604405381849834,train_acc:0.8003968000411987
node12 epoch1:node_model train_loss:0.36221729112522943,train_acc:0.8683333992958069
node12 epoch2:node_model train_loss:0.25122494782720295,train_acc:0.9197620153427124
node12 epoch3:node_model train_loss:0.18129859545401164,train_acc:0.9419047832489014
node12 epoch4:node_model train_loss:0.128935307264328,train_acc:0.9579364657402039
node12_model on test-dataset: loss:0.8160823537409305,acc:0.7648000717163086
node12 weight score:1637.0896808094933
node15: train data size:629
node15 epoch0:node_model train_loss:0.6980336138180324,train_acc:0.7683743834495544
node15 epoch1:node_model train_loss:0.4120277762413025,train_acc:0.8475862741470337
node15 epoch2:node_model train_loss:0.21450564690998622,train_acc:0.9237930774688721
node15 epoch3:node_model train_loss:0.1675113035099847,train_acc:0.9507881999015808
node15 epoch4:node_model train_loss:0.11417684810502189,train_acc:0.9707881808280945
node15_model on test-dataset: loss:0.8813207185268402,acc:0.7461000084877014
node15 weight score:713.7015921416173
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6995462193045505,train_acc:0.7856531143188477
node19 epoch1:node_model train_loss:0.4329480022885079,train_acc:0.8530806303024292
node19 epoch2:node_model train_loss:0.33389211567335353,train_acc:0.8838874697685242
node19 epoch3:node_model train_loss:0.25121425993220753,train_acc:0.9211512804031372
node19 epoch4:node_model train_loss:0.20957179332888404,train_acc:0.936348021030426
node19_model on test-dataset: loss:0.7307803510129451,acc:0.7788998484611511
node19 weight score:5858.121382253977
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6249535009264946,acc:0.8095999819040298
total cost energy:9.70640994780744 | all_enery_cp：6.8694999999999995 | all_enery_tp: 2.836909947807441
ef: 24.688303660293805
reward: 14.981893712486364
step 323:loss:8.050832748413086|running q:33.82182693481445
episode5,iteration23 selected nodes:[4, 15, 14, 12, 3],center node:3
################################################## episode5,iteration23 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4649988149487695,train_acc:0.8355467915534973
node3 epoch1:node_model train_loss:0.2756575862335604,train_acc:0.9052596092224121
node3 epoch2:node_model train_loss:0.20253864866356516,train_acc:0.9384561777114868
node3 epoch3:node_model train_loss:0.18199430076881898,train_acc:0.938718318939209
node3 epoch4:node_model train_loss:0.14162898583467617,train_acc:0.9585155248641968
node3_model on test-dataset: loss:0.7959539033472538,acc:0.7754000425338745
node3 weight score:5335.73613012002
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5303842984139919,train_acc:0.8196427822113037
node4 epoch1:node_model train_loss:0.37921504357031416,train_acc:0.8732143640518188
node4 epoch2:node_model train_loss:0.3501072217311178,train_acc:0.8842857480049133
node4 epoch3:node_model train_loss:0.2616999595026885,train_acc:0.9200000762939453
node4 epoch4:node_model train_loss:0.18063999047236784,train_acc:0.9478570818901062
node4_model on test-dataset: loss:0.9165596672892571,acc:0.7473999857902527
node4 weight score:2951.25358068623
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5569513248545783,train_acc:0.8223016262054443
node12 epoch1:node_model train_loss:0.29449237350906643,train_acc:0.9081746339797974
node12 epoch2:node_model train_loss:0.2031046258551734,train_acc:0.939047634601593
node12 epoch3:node_model train_loss:0.1331347173878125,train_acc:0.966587245464325
node12 epoch4:node_model train_loss:0.09402448790413993,train_acc:0.9800000190734863
node12_model on test-dataset: loss:0.8093236035108566,acc:0.7657000422477722
node12 weight score:1650.7611963921652
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6919997334480286,train_acc:0.7981481552124023
node14 epoch1:node_model train_loss:0.38043725738922757,train_acc:0.8728703856468201
node14 epoch2:node_model train_loss:0.25924768050511676,train_acc:0.916388988494873
node14 epoch3:node_model train_loss:0.16229088480273882,train_acc:0.9497222900390625
node14 epoch4:node_model train_loss:0.092317926697433,train_acc:0.9799998998641968
node14_model on test-dataset: loss:0.7179667744040489,acc:0.7874999642372131
node14 weight score:1632.387516780039
node15: train data size:629
node15 epoch0:node_model train_loss:0.6490981153079441,train_acc:0.7860099077224731
node15 epoch1:node_model train_loss:0.376574239560536,train_acc:0.8625122904777527
node15 epoch2:node_model train_loss:0.25603768868105753,train_acc:0.919359564781189
node15 epoch3:node_model train_loss:0.20012787942375457,train_acc:0.942216694355011
node15 epoch4:node_model train_loss:0.1696928728904043,train_acc:0.950935959815979
node15_model on test-dataset: loss:0.934345123320818,acc:0.7394998073577881
node15 weight score:673.1987830839523
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6300924618542194,acc:0.8058999800682067
total cost energy:6.955808686849236 | all_enery_cp：5.0445 | all_enery_tp: 1.9113086868492362
ef: 24.59094419740019
reward: 17.635135510550953
step 324:loss:2.656575918197632|running q:35.21589660644531
episode5,iteration24 selected nodes:[16, 3, 11, 14, 7],center node:11
################################################## episode5,iteration24 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.31586603612400765,train_acc:0.8900246620178223
node3 epoch1:node_model train_loss:0.21055513966915218,train_acc:0.9304056167602539
node3 epoch2:node_model train_loss:0.14527452893035356,train_acc:0.9557544589042664
node3 epoch3:node_model train_loss:0.11060642139163128,train_acc:0.9697375297546387
node3 epoch4:node_model train_loss:0.10253269329320552,train_acc:0.9713358283042908
node3_model on test-dataset: loss:0.8155911764502526,acc:0.7713001370429993
node3 weight score:5207.2657510647405
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5048074007034302,train_acc:0.8306764960289001
node7 epoch1:node_model train_loss:0.29856095686554907,train_acc:0.8955980539321899
node7 epoch2:node_model train_loss:0.1905454456806183,train_acc:0.934539258480072
node7 epoch3:node_model train_loss:0.12614847384393216,train_acc:0.9690588116645813
node7 epoch4:node_model train_loss:0.10889656133949757,train_acc:0.9705392122268677
node7_model on test-dataset: loss:0.8039397530257701,acc:0.7680997848510742
node7 weight score:2426.798765276956
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5741923346238977,train_acc:0.8268148899078369
node11 epoch1:node_model train_loss:0.3203759070704965,train_acc:0.894332766532898
node11 epoch2:node_model train_loss:0.17233727434102228,train_acc:0.9477186799049377
node11 epoch3:node_model train_loss:0.10729225593454697,train_acc:0.97751784324646
node11 epoch4:node_model train_loss:0.09539820363416392,train_acc:0.9791534543037415
node11_model on test-dataset: loss:0.7632720926404,acc:0.7813998460769653
node11 weight score:2203.6702457984925
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4959670752286911,train_acc:0.834768533706665
node14 epoch1:node_model train_loss:0.31537659590442974,train_acc:0.8984259366989136
node14 epoch2:node_model train_loss:0.1689474197725455,train_acc:0.9456943869590759
node14 epoch3:node_model train_loss:0.14382929479082426,train_acc:0.9590277075767517
node14 epoch4:node_model train_loss:0.09798286389559507,train_acc:0.9771758317947388
node14_model on test-dataset: loss:0.8074747189879418,acc:0.7721001505851746
node14 weight score:1451.4386301393315
node16: train data size:877
node16 epoch0:node_model train_loss:0.7508212294843462,train_acc:0.7899134159088135
node16 epoch1:node_model train_loss:0.43661970065699685,train_acc:0.8634632229804993
node16 epoch2:node_model train_loss:0.2918595042493608,train_acc:0.9032322764396667
node16 epoch3:node_model train_loss:0.19380746533473334,train_acc:0.9474458694458008
node16 epoch4:node_model train_loss:0.1025353256199095,train_acc:0.9723376035690308
node16_model on test-dataset: loss:0.7796670383214951,acc:0.7676998972892761
node16 weight score:1124.8391388817051
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6305662371218205,acc:0.8073999792337417
total cost energy:6.0412828935632366 | all_enery_cp：4.964499999999999 | all_enery_tp: 1.0767828935632369
ef: 25.148363399589442
reward: 19.107080506026207
step 325:loss:7.185779094696045|running q:36.50367736816406
episode5,iteration25 selected nodes:[7, 5, 1, 0, 6],center node:6
################################################## episode5,iteration25 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.49860419504917586,train_acc:0.8302968144416809
node0 epoch1:node_model train_loss:0.2878948596234505,train_acc:0.9010265469551086
node0 epoch2:node_model train_loss:0.23476591219122595,train_acc:0.9245319962501526
node0 epoch3:node_model train_loss:0.17834582824546558,train_acc:0.9438413381576538
node0 epoch4:node_model train_loss:0.1547972597181797,train_acc:0.9517260789871216
node0_model on test-dataset: loss:0.7566470338404179,acc:0.7796998620033264
node0 weight score:6849.9574678741565
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5178978977834477,train_acc:0.8335294723510742
node1 epoch1:node_model train_loss:0.3485095852657276,train_acc:0.8805882334709167
node1 epoch2:node_model train_loss:0.2624967369963141,train_acc:0.9071323275566101
node1 epoch3:node_model train_loss:0.22345944855581312,train_acc:0.9224265217781067
node1 epoch4:node_model train_loss:0.21381573203731985,train_acc:0.9300734400749207
node1_model on test-dataset: loss:0.8106014357507229,acc:0.7678999900817871
node1 weight score:8275.33693397362
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5741039890992014,train_acc:0.8215413689613342
node5 epoch1:node_model train_loss:0.32694352300543533,train_acc:0.8911653757095337
node5 epoch2:node_model train_loss:0.2537313084069051,train_acc:0.9140223264694214
node5 epoch3:node_model train_loss:0.1880720987131721,train_acc:0.9374811053276062
node5 epoch4:node_model train_loss:0.16320618987083435,train_acc:0.9478947520256042
node5_model on test-dataset: loss:0.7401425583660602,acc:0.7827999591827393
node5 weight score:5046.325140720716
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6822279883969214,train_acc:0.7941935658454895
node6 epoch1:node_model train_loss:0.42420849155995155,train_acc:0.8596774339675903
node6 epoch2:node_model train_loss:0.3266810375836588,train_acc:0.8970045447349548
node6 epoch3:node_model train_loss:0.3121731242825908,train_acc:0.9021658897399902
node6 epoch4:node_model train_loss:0.24424829550327792,train_acc:0.920967698097229
node6_model on test-dataset: loss:0.7622138978540898,acc:0.7680999636650085
node6 weight score:3945.086816792245
node7: train data size:1951
node7 epoch0:node_model train_loss:0.42614382654428484,train_acc:0.8485783934593201
node7 epoch1:node_model train_loss:0.23303092755377292,train_acc:0.9190783500671387
node7 epoch2:node_model train_loss:0.15117459408938885,train_acc:0.9575392007827759
node7 epoch3:node_model train_loss:0.10353321395814419,train_acc:0.9680588841438293
node7 epoch4:node_model train_loss:0.07332569137215614,train_acc:0.9830392003059387
node7_model on test-dataset: loss:0.7889991852641106,acc:0.7740001678466797
node7 weight score:2472.7528702668046
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5893185083568097,acc:0.8180999785661698
total cost energy:11.198449510224599 | all_enery_cp：10.292 | all_enery_tp: 0.906449510224598
ef: 25.552008790014938
reward: 14.353559279790339
step 326:loss:7.590987205505371|running q:37.6978874206543
episode5,iteration26 selected nodes:[6, 17, 12, 19, 7],center node:17
################################################## episode5,iteration26 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.56135187946981,train_acc:0.825161337852478
node6 epoch1:node_model train_loss:0.35455981810246745,train_acc:0.8763593435287476
node6 epoch2:node_model train_loss:0.282377450937225,train_acc:0.9051612019538879
node6 epoch3:node_model train_loss:0.21109712484382814,train_acc:0.9348387122154236
node6 epoch4:node_model train_loss:0.1806816696639984,train_acc:0.9453915357589722
node6_model on test-dataset: loss:0.7892883951961994,acc:0.7727000117301941
node6 weight score:3809.7608153133015
node7: train data size:1951
node7 epoch0:node_model train_loss:0.38709731921553614,train_acc:0.8636372685432434
node7 epoch1:node_model train_loss:0.2303050834685564,train_acc:0.9165782928466797
node7 epoch2:node_model train_loss:0.14777427092194556,train_acc:0.9575587511062622
node7 epoch3:node_model train_loss:0.08873435948044062,train_acc:0.9765195250511169
node7 epoch4:node_model train_loss:0.07663387525826693,train_acc:0.9810194373130798
node7_model on test-dataset: loss:0.7663874618709088,acc:0.7781997323036194
node7 weight score:2545.7097056849148
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6221968638045448,train_acc:0.8041269779205322
node12 epoch1:node_model train_loss:0.3378855979868344,train_acc:0.8840477466583252
node12 epoch2:node_model train_loss:0.1897018514573574,train_acc:0.9451587200164795
node12 epoch3:node_model train_loss:0.13838465910937106,train_acc:0.9604762196540833
node12 epoch4:node_model train_loss:0.10861143523028918,train_acc:0.9749999642372131
node12_model on test-dataset: loss:0.7470088510215283,acc:0.7824997305870056
node12 weight score:1788.4660913629489
node17: train data size:442
node17 epoch0:node_model train_loss:0.820595645904541,train_acc:0.7751428484916687
node17 epoch1:node_model train_loss:0.4602385237812996,train_acc:0.8519046902656555
node17 epoch2:node_model train_loss:0.27732480466365816,train_acc:0.8989524245262146
node17 epoch3:node_model train_loss:0.20616510808467864,train_acc:0.9354285597801208
node17 epoch4:node_model train_loss:0.12850575596094133,train_acc:0.9724761843681335
node17_model on test-dataset: loss:0.9272807889431715,acc:0.7491000294685364
node17 weight score:476.6625225825616
node19: train data size:4281
node19 epoch0:node_model train_loss:0.5602762123873067,train_acc:0.8223829865455627
node19 epoch1:node_model train_loss:0.3539912645206895,train_acc:0.8801119327545166
node19 epoch2:node_model train_loss:0.27846065718074176,train_acc:0.906322181224823
node19 epoch3:node_model train_loss:0.2102651883696401,train_acc:0.9334480166435242
node19 epoch4:node_model train_loss:0.16619155937156013,train_acc:0.9504247903823853
node19_model on test-dataset: loss:0.7919609186053276,acc:0.7693001627922058
node19 weight score:5405.5697691989635
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6098283711075783,acc:0.8133999806642532
total cost energy:7.394435685643282 | all_enery_cp：5.5085 | all_enery_tp: 1.885935685643283
ef: 24.985977672240637
reward: 17.591541986597356
step 327:loss:5.940783977508545|running q:38.97702407836914
episode5,iteration27 selected nodes:[18, 16, 6, 13, 9],center node:9
################################################## episode5,iteration27 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5048295913204071,train_acc:0.8400461673736572
node6 epoch1:node_model train_loss:0.3926987941226652,train_acc:0.8711981773376465
node6 epoch2:node_model train_loss:0.28433245276251146,train_acc:0.9089400768280029
node6 epoch3:node_model train_loss:0.23459268769910258,train_acc:0.923870861530304
node6 epoch4:node_model train_loss:0.16862380985290773,train_acc:0.9460828304290771
node6_model on test-dataset: loss:0.7871045371890069,acc:0.7657998204231262
node6 weight score:3820.331173212296
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6271570968000513,train_acc:0.8078762292861938
node9 epoch1:node_model train_loss:0.38611449379669993,train_acc:0.8723545074462891
node9 epoch2:node_model train_loss:0.27170105905909286,train_acc:0.9114311933517456
node9 epoch3:node_model train_loss:0.15665897491731143,train_acc:0.9548659920692444
node9 epoch4:node_model train_loss:0.10699368738814403,train_acc:0.9749952554702759
node9_model on test-dataset: loss:0.7349942669272422,acc:0.7801002264022827
node9 weight score:2526.5503195874944
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6696498965223631,train_acc:0.7909848690032959
node13 epoch1:node_model train_loss:0.3705667008956273,train_acc:0.8693939447402954
node13 epoch2:node_model train_loss:0.23844033107161522,train_acc:0.916287899017334
node13 epoch3:node_model train_loss:0.16712312555561462,train_acc:0.9491666555404663
node13 epoch4:node_model train_loss:0.1089996841425697,train_acc:0.9776514768600464
node13_model on test-dataset: loss:0.7791232644021511,acc:0.7709000110626221
node13 weight score:1482.435517936013
node16: train data size:877
node16 epoch0:node_model train_loss:0.7278084225124783,train_acc:0.7963636517524719
node16 epoch1:node_model train_loss:0.40312162041664124,train_acc:0.8714573979377747
node16 epoch2:node_model train_loss:0.24221385684278277,train_acc:0.9208946228027344
node16 epoch3:node_model train_loss:0.14733166330390507,train_acc:0.9582250714302063
node16 epoch4:node_model train_loss:0.13245602117644417,train_acc:0.9696680903434753
node16_model on test-dataset: loss:0.748985399901867,acc:0.7775002717971802
node16 weight score:1170.917350478268
node18: train data size:472
node18 epoch0:node_model train_loss:0.5957425117492676,train_acc:0.8214444518089294
node18 epoch1:node_model train_loss:0.2916839063167572,train_acc:0.916111171245575
node18 epoch2:node_model train_loss:0.19680605828762054,train_acc:0.9413332939147949
node18 epoch3:node_model train_loss:0.14827173054218293,train_acc:0.9584444165229797
node18 epoch4:node_model train_loss:0.08765913546085358,train_acc:0.9716666340827942
node18_model on test-dataset: loss:0.8900874574482441,acc:0.7476000785827637
node18 weight score:530.2849692469072
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6015768010169268,acc:0.8115999841690064
total cost energy:5.550527469641502 | all_enery_cp：3.684 | all_enery_tp: 1.8665274696415022
ef: 24.61670142869513
reward: 19.06617395905363
step 328:loss:5.916109561920166|running q:40.32577133178711
episode5,iteration28 selected nodes:[15, 1, 8, 7, 11],center node:7
################################################## episode5,iteration28 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4420209281584796,train_acc:0.8438972234725952
node1 epoch1:node_model train_loss:0.33891189952983575,train_acc:0.882132351398468
node1 epoch2:node_model train_loss:0.23943106085062027,train_acc:0.919338047504425
node1 epoch3:node_model train_loss:0.20942059225019286,train_acc:0.9305880665779114
node1 epoch4:node_model train_loss:0.19290664123699947,train_acc:0.9340441226959229
node1_model on test-dataset: loss:0.7878733834624291,acc:0.7748000025749207
node1 weight score:8514.058401770952
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3721954099833965,train_acc:0.8681960105895996
node7 epoch1:node_model train_loss:0.23132669404149056,train_acc:0.9210587739944458
node7 epoch2:node_model train_loss:0.13780697397887706,train_acc:0.9590392112731934
node7 epoch3:node_model train_loss:0.10073602236807347,train_acc:0.9710783362388611
node7 epoch4:node_model train_loss:0.07161093559116125,train_acc:0.9859998822212219
node7_model on test-dataset: loss:0.7799817799776793,acc:0.7755999565124512
node7 weight score:2501.340480101768
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6872436321443982,train_acc:0.7909069657325745
node8 epoch1:node_model train_loss:0.3971942158208953,train_acc:0.8675963878631592
node8 epoch2:node_model train_loss:0.27873488267262775,train_acc:0.9060317277908325
node8 epoch3:node_model train_loss:0.17640817165374756,train_acc:0.9543764591217041
node8 epoch4:node_model train_loss:0.11969640043874581,train_acc:0.9710769653320312
node8_model on test-dataset: loss:0.7208319334685802,acc:0.7859001159667969
node8 weight score:2494.340104146304
node11: train data size:1682
node11 epoch0:node_model train_loss:0.47821970722254586,train_acc:0.8492971062660217
node11 epoch1:node_model train_loss:0.24571557605967803,train_acc:0.9231563210487366
node11 epoch2:node_model train_loss:0.17969794439918854,train_acc:0.9406599402427673
node11 epoch3:node_model train_loss:0.1418987635303946,train_acc:0.9593542814254761
node11 epoch4:node_model train_loss:0.08886649810216006,train_acc:0.9800716638565063
node11_model on test-dataset: loss:0.741863549798727,acc:0.7801001071929932
node11 weight score:2267.2632999105276
node15: train data size:629
node15 epoch0:node_model train_loss:0.6740500501223973,train_acc:0.7825123071670532
node15 epoch1:node_model train_loss:0.3538251050880977,train_acc:0.864729106426239
node15 epoch2:node_model train_loss:0.22579039207526616,train_acc:0.927290678024292
node15 epoch3:node_model train_loss:0.1207784850682531,train_acc:0.9665024280548096
node15 epoch4:node_model train_loss:0.09551911348743099,train_acc:0.9785714149475098
node15_model on test-dataset: loss:0.8143709082901478,acc:0.7666999101638794
node15 weight score:772.3753311874165
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6150245952606201,acc:0.8125999826192856
total cost energy:7.9848675512896845 | all_enery_cp：6.384 | all_enery_tp: 1.600867551289684
ef: 25.05877487676551
reward: 17.073907325475826
step 329:loss:8.222742080688477|running q:41.53504180908203
episode5,iteration29 selected nodes:[16, 13, 5, 6, 17],center node:16
################################################## episode5,iteration29 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5260669000838932,train_acc:0.8253007531166077
node5 epoch1:node_model train_loss:0.3231856564157887,train_acc:0.8913531303405762
node5 epoch2:node_model train_loss:0.23937427801521202,train_acc:0.9256014823913574
node5 epoch3:node_model train_loss:0.1862049814509718,train_acc:0.9458271861076355
node5 epoch4:node_model train_loss:0.13734576802112555,train_acc:0.9621050953865051
node5_model on test-dataset: loss:0.77035184815526,acc:0.7752997279167175
node5 weight score:4848.433879848669
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4255196519436375,train_acc:0.8601381778717041
node6 epoch1:node_model train_loss:0.3107698083885254,train_acc:0.8946543335914612
node6 epoch2:node_model train_loss:0.22784107875439427,train_acc:0.9211981892585754
node6 epoch3:node_model train_loss:0.16730143754712998,train_acc:0.9487095475196838
node6 epoch4:node_model train_loss:0.13407227156623716,train_acc:0.9737785458564758
node6_model on test-dataset: loss:0.7481450751423836,acc:0.7844999432563782
node6 weight score:4019.2739348417435
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6616113483905792,train_acc:0.8003031611442566
node13 epoch1:node_model train_loss:0.3121641141672929,train_acc:0.8949241638183594
node13 epoch2:node_model train_loss:0.2023429535329342,train_acc:0.9397727251052856
node13 epoch3:node_model train_loss:0.1542843555410703,train_acc:0.9547728300094604
node13 epoch4:node_model train_loss:0.09836020103345315,train_acc:0.9776513576507568
node13_model on test-dataset: loss:0.7907767552137375,acc:0.7763000130653381
node13 weight score:1460.5892148256398
node16: train data size:877
node16 epoch0:node_model train_loss:0.7226810786459181,train_acc:0.7724674940109253
node16 epoch1:node_model train_loss:0.42949682142999435,train_acc:0.8603463172912598
node16 epoch2:node_model train_loss:0.26277191440264386,train_acc:0.9154545664787292
node16 epoch3:node_model train_loss:0.15651030258999932,train_acc:0.9537805914878845
node16 epoch4:node_model train_loss:0.10630047114359008,train_acc:0.9737805128097534
node16_model on test-dataset: loss:0.7764588817954063,acc:0.7745000123977661
node16 weight score:1129.4867256487714
node17: train data size:442
node17 epoch0:node_model train_loss:0.7235068380832672,train_acc:0.7874285578727722
node17 epoch1:node_model train_loss:0.3841843277215958,train_acc:0.8864762187004089
node17 epoch2:node_model train_loss:0.2533173799514771,train_acc:0.9209524393081665
node17 epoch3:node_model train_loss:0.24938970655202866,train_acc:0.9306666254997253
node17 epoch4:node_model train_loss:0.1312831312417984,train_acc:0.968000054359436
node17_model on test-dataset: loss:0.9223167464137078,acc:0.73639976978302
node17 weight score:479.22798942841666
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6181644900888205,acc:0.809399978518486
total cost energy:6.445415566171036 | all_enery_cp：4.608 | all_enery_tp: 1.8374155661710359
ef: 24.938171009681117
reward: 18.49275544351008
step 330:loss:10.71515941619873|running q:42.710304260253906
episode5,iteration30 selected nodes:[11, 13, 9, 6, 12],center node:11
################################################## episode5,iteration30 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3017805382128685,train_acc:0.8873270153999329
node6 epoch1:node_model train_loss:0.205145345339852,train_acc:0.9311981201171875
node6 epoch2:node_model train_loss:0.1655947429037863,train_acc:0.9476498365402222
node6 epoch3:node_model train_loss:0.15621363227405854,train_acc:0.9485253095626831
node6 epoch4:node_model train_loss:0.13673831486413557,train_acc:0.9574192762374878
node6_model on test-dataset: loss:0.8001380920410156,acc:0.7750000357627869
node6 weight score:3758.101295152261
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5939878037101344,train_acc:0.8099722266197205
node9 epoch1:node_model train_loss:0.33581217417591497,train_acc:0.8918281197547913
node9 epoch2:node_model train_loss:0.20911836624145508,train_acc:0.93327796459198
node9 epoch3:node_model train_loss:0.1473350077867508,train_acc:0.956177294254303
node9 epoch4:node_model train_loss:0.1060885171356954,train_acc:0.9764450788497925
node9_model on test-dataset: loss:0.7260231500118971,acc:0.790399968624115
node9 weight score:2557.7696798918464
node11: train data size:1682
node11 epoch0:node_model train_loss:0.47294656318776745,train_acc:0.8456383943557739
node11 epoch1:node_model train_loss:0.24573948716416077,train_acc:0.9164131879806519
node11 epoch2:node_model train_loss:0.15947743680547266,train_acc:0.949153482913971
node11 epoch3:node_model train_loss:0.09441999302190893,train_acc:0.9743901491165161
node11 epoch4:node_model train_loss:0.07264215674470453,train_acc:0.9857531189918518
node11_model on test-dataset: loss:0.7484069530665874,acc:0.7881000638008118
node11 weight score:2247.440370653997
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5500358726297107,train_acc:0.8243650794029236
node12 epoch1:node_model train_loss:0.350328911628042,train_acc:0.880396842956543
node12 epoch2:node_model train_loss:0.17587369573967798,train_acc:0.9447618722915649
node12 epoch3:node_model train_loss:0.1488489162709032,train_acc:0.9546032547950745
node12 epoch4:node_model train_loss:0.09685834789914745,train_acc:0.9785715341567993
node12_model on test-dataset: loss:0.8149669343233108,acc:0.7654997706413269
node12 weight score:1639.3303135780807
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5664541547497114,train_acc:0.8263635635375977
node13 epoch1:node_model train_loss:0.3174019604921341,train_acc:0.9009090662002563
node13 epoch2:node_model train_loss:0.18104583149154982,train_acc:0.9421212077140808
node13 epoch3:node_model train_loss:0.09908998012542725,train_acc:0.9758333563804626
node13 epoch4:node_model train_loss:0.06694358463088672,train_acc:0.9883332252502441
node13_model on test-dataset: loss:0.7717005914449692,acc:0.7706997990608215
node13 weight score:1496.6944600072454
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6041892875730991,acc:0.8160999840497971
total cost energy:6.034851461583877 | all_enery_cp：4.5185 | all_enery_tp: 1.5163514615838767
ef: 25.06206135087768
reward: 19.027209889293804
step 331:loss:7.072078704833984|running q:43.88599395751953
episode5,iteration31 selected nodes:[8, 7, 4, 16, 14],center node:7
################################################## episode5,iteration31 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6536195874214172,train_acc:0.7953571677207947
node4 epoch1:node_model train_loss:0.39242573401757647,train_acc:0.8600001335144043
node4 epoch2:node_model train_loss:0.3338043532733406,train_acc:0.8907143473625183
node4 epoch3:node_model train_loss:0.3313749480460371,train_acc:0.8864285349845886
node4 epoch4:node_model train_loss:0.3352658578327724,train_acc:0.8910713791847229
node4_model on test-dataset: loss:0.7771539342403412,acc:0.7698997855186462
node4 weight score:3480.648917571402
node7: train data size:1951
node7 epoch0:node_model train_loss:0.43183726370334624,train_acc:0.8541765213012695
node7 epoch1:node_model train_loss:0.21732878275215625,train_acc:0.9246372580528259
node7 epoch2:node_model train_loss:0.13841890804469587,train_acc:0.9605194926261902
node7 epoch3:node_model train_loss:0.09661643616855145,train_acc:0.9760782122612
node7 epoch4:node_model train_loss:0.06010310892015695,train_acc:0.9900195002555847
node7_model on test-dataset: loss:0.7831125701963901,acc:0.78059983253479
node7 weight score:2491.3404205869474
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6116645352707969,train_acc:0.8109070658683777
node8 epoch1:node_model train_loss:0.3249614073170556,train_acc:0.895340085029602
node8 epoch2:node_model train_loss:0.202406188680066,train_acc:0.9382312893867493
node8 epoch3:node_model train_loss:0.12777758803632525,train_acc:0.9688208103179932
node8 epoch4:node_model train_loss:0.09965162558688058,train_acc:0.9766438603401184
node8_model on test-dataset: loss:0.7265468676388264,acc:0.7858999371528625
node8 weight score:2474.7199115223543
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5869165062904358,train_acc:0.8269907832145691
node14 epoch1:node_model train_loss:0.3155737469593684,train_acc:0.9048610925674438
node14 epoch2:node_model train_loss:0.19667742463449636,train_acc:0.942731499671936
node14 epoch3:node_model train_loss:0.14310988131910563,train_acc:0.9600462913513184
node14 epoch4:node_model train_loss:0.08212401655813058,train_acc:0.9796757698059082
node14_model on test-dataset: loss:0.7453797084093093,acc:0.784800112247467
node14 weight score:1572.3529722872752
node16: train data size:877
node16 epoch0:node_model train_loss:0.6324410372310214,train_acc:0.8021356463432312
node16 epoch1:node_model train_loss:0.30781560639540356,train_acc:0.8933477401733398
node16 epoch2:node_model train_loss:0.20641344040632248,train_acc:0.9340115785598755
node16 epoch3:node_model train_loss:0.1771797099047237,train_acc:0.954559862613678
node16 epoch4:node_model train_loss:0.09117591712209913,train_acc:0.9774459004402161
node16_model on test-dataset: loss:0.7635750006139278,acc:0.7737998962402344
node16 weight score:1148.544673797435
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5864013703167439,acc:0.8199999815225602
total cost energy:5.8776297173761165 | all_enery_cp：4.2515 | all_enery_tp: 1.6261297173761164
ef: 24.950750326192885
reward: 19.07312060881677
step 332:loss:5.331551551818848|running q:45.079750061035156
episode5,iteration32 selected nodes:[0, 17, 3, 2, 12],center node:3
################################################## episode5,iteration32 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.4806199434858102,train_acc:0.8385262489318848
node0 epoch1:node_model train_loss:0.27224021906463,train_acc:0.9064896106719971
node0 epoch2:node_model train_loss:0.19633660107277906,train_acc:0.9376482367515564
node0 epoch3:node_model train_loss:0.16649357630656317,train_acc:0.9458432793617249
node0 epoch4:node_model train_loss:0.14076464279339865,train_acc:0.9587279558181763
node0_model on test-dataset: loss:0.7222305398061871,acc:0.7934000492095947
node0 weight score:7176.378890583711
node2: train data size:4788
node2 epoch0:node_model train_loss:0.36912227142602205,train_acc:0.8709375262260437
node2 epoch1:node_model train_loss:0.2047188370488584,train_acc:0.9324430823326111
node2 epoch2:node_model train_loss:0.1443255931759874,train_acc:0.9525378942489624
node2 epoch3:node_model train_loss:0.10904003120958805,train_acc:0.9671496748924255
node2 epoch4:node_model train_loss:0.08540852561903496,train_acc:0.9801515936851501
node2_model on test-dataset: loss:0.7545225396752357,acc:0.7882001399993896
node2 weight score:6345.734882964355
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4120906504780747,train_acc:0.8566502332687378
node3 epoch1:node_model train_loss:0.2332610986953558,train_acc:0.9211328029632568
node3 epoch2:node_model train_loss:0.14553443948889888,train_acc:0.9554033279418945
node3 epoch3:node_model train_loss:0.11459237337112427,train_acc:0.9673231244087219
node3 epoch4:node_model train_loss:0.09478047030956246,train_acc:0.9734288454055786
node3_model on test-dataset: loss:0.7572792583703994,acc:0.7874998450279236
node3 weight score:5608.234945110186
node12: train data size:1336
node12 epoch0:node_model train_loss:0.501714614885194,train_acc:0.8219841718673706
node12 epoch1:node_model train_loss:0.24716594921691076,train_acc:0.9146031141281128
node12 epoch2:node_model train_loss:0.15880618670157023,train_acc:0.9510317444801331
node12 epoch3:node_model train_loss:0.110709726278271,train_acc:0.9715873003005981
node12 epoch4:node_model train_loss:0.08617205359041691,train_acc:0.9810317754745483
node12_model on test-dataset: loss:0.7916313154995441,acc:0.7782001495361328
node12 weight score:1687.6543080625129
node17: train data size:442
node17 epoch0:node_model train_loss:0.7740432500839234,train_acc:0.7782856822013855
node17 epoch1:node_model train_loss:0.4262839615345001,train_acc:0.8574285507202148
node17 epoch2:node_model train_loss:0.2514668762683868,train_acc:0.9224762320518494
node17 epoch3:node_model train_loss:0.10694440975785255,train_acc:0.9779999852180481
node17 epoch4:node_model train_loss:0.15392591208219528,train_acc:0.9617142677307129
node17_model on test-dataset: loss:0.9485999190807343,acc:0.7397999167442322
node17 weight score:465.949860535864
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6034494172036647,acc:0.8199999821186066
total cost energy:10.230540321741751 | all_enery_cp：7.998 | all_enery_tp: 2.2325403217417508
ef: 24.887432550738293
reward: 14.656892228996542
step 333:loss:8.8306303024292|running q:46.2751350402832
episode5,iteration33 selected nodes:[11, 1, 17, 4, 5],center node:11
################################################## episode5,iteration33 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.37269580802496743,train_acc:0.87161785364151
node1 epoch1:node_model train_loss:0.24902931798030348,train_acc:0.9109558463096619
node1 epoch2:node_model train_loss:0.174840641043642,train_acc:0.9394116997718811
node1 epoch3:node_model train_loss:0.14072510090601795,train_acc:0.9592647552490234
node1 epoch4:node_model train_loss:0.11589893369990237,train_acc:0.9637499451637268
node1_model on test-dataset: loss:0.7718038734793663,acc:0.7893001437187195
node1 weight score:8691.327201766544
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5221930252654212,train_acc:0.826071560382843
node4 epoch1:node_model train_loss:0.34252164885401726,train_acc:0.8814284801483154
node4 epoch2:node_model train_loss:0.23918038127677782,train_acc:0.9275001287460327
node4 epoch3:node_model train_loss:0.15379349807543413,train_acc:0.95035719871521
node4 epoch4:node_model train_loss:0.13506872632673808,train_acc:0.9560713768005371
node4_model on test-dataset: loss:0.7199471832811832,acc:0.7905999422073364
node4 weight score:3757.2200611604208
node5: train data size:3735
node5 epoch0:node_model train_loss:0.43098389553396327,train_acc:0.8591353893280029
node5 epoch1:node_model train_loss:0.2703999257401416,train_acc:0.9078195691108704
node5 epoch2:node_model train_loss:0.20837172336484255,train_acc:0.9283083081245422
node5 epoch3:node_model train_loss:0.13084070247254873,train_acc:0.9621425271034241
node5 epoch4:node_model train_loss:0.10683819847671609,train_acc:0.9708645343780518
node5_model on test-dataset: loss:0.7102306735515594,acc:0.789099931716919
node5 weight score:5258.854818706807
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4864821924882777,train_acc:0.8464275002479553
node11 epoch1:node_model train_loss:0.21080057410632863,train_acc:0.923012912273407
node11 epoch2:node_model train_loss:0.12153091150171616,train_acc:0.9638593196868896
node11 epoch3:node_model train_loss:0.08818015719161314,train_acc:0.9796124696731567
node11 epoch4:node_model train_loss:0.07134085148572922,train_acc:0.9863413572311401
node11_model on test-dataset: loss:0.7962927283346652,acc:0.7795000076293945
node11 weight score:2112.2885342902323
node17: train data size:442
node17 epoch0:node_model train_loss:0.6806832075119018,train_acc:0.7883809208869934
node17 epoch1:node_model train_loss:0.31549751460552217,train_acc:0.9104762077331543
node17 epoch2:node_model train_loss:0.26013838946819307,train_acc:0.9289523959159851
node17 epoch3:node_model train_loss:0.12865868210792542,train_acc:0.965238094329834
node17 epoch4:node_model train_loss:0.0820316269993782,train_acc:0.9860000014305115
node17_model on test-dataset: loss:0.8605311722308397,acc:0.7603998184204102
node17 weight score:513.6362449882668
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6019533780217171,acc:0.8197999829053879
total cost energy:9.368032791758285 | all_enery_cp：7.636 | all_enery_tp: 1.7320327917582854
ef: 25.12803321364968
reward: 15.760000421891394
step 334:loss:6.339465141296387|running q:47.42524337768555
episode5,iteration34 selected nodes:[18, 15, 8, 14, 10],center node:14
################################################## episode5,iteration34 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5511284867922465,train_acc:0.829784631729126
node8 epoch1:node_model train_loss:0.2937806107931667,train_acc:0.9066099524497986
node8 epoch2:node_model train_loss:0.18154493015673426,train_acc:0.9444103240966797
node8 epoch3:node_model train_loss:0.1241207950645023,train_acc:0.9688434600830078
node8 epoch4:node_model train_loss:0.09390067454013559,train_acc:0.9754988551139832
node8_model on test-dataset: loss:0.7029850621521473,acc:0.788899838924408
node8 weight score:2557.6645889110773
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7008280262351037,train_acc:0.7993334531784058
node10 epoch1:node_model train_loss:0.3981609158217907,train_acc:0.8698332905769348
node10 epoch2:node_model train_loss:0.2504456460475922,train_acc:0.9253333210945129
node10 epoch3:node_model train_loss:0.17348921503871678,train_acc:0.950499951839447
node10 epoch4:node_model train_loss:0.1202190414071083,train_acc:0.9668331146240234
node10_model on test-dataset: loss:0.7160546717047691,acc:0.7910001277923584
node10 weight score:2758.169282378897
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5053010905782381,train_acc:0.8340277671813965
node14 epoch1:node_model train_loss:0.2807435505092144,train_acc:0.8986110687255859
node14 epoch2:node_model train_loss:0.17827331212659678,train_acc:0.9468981027603149
node14 epoch3:node_model train_loss:0.1088962930565079,train_acc:0.9721758365631104
node14 epoch4:node_model train_loss:0.07889266265556216,train_acc:0.9840277433395386
node14_model on test-dataset: loss:0.7882737939059734,acc:0.777999997138977
node14 weight score:1486.7930521863802
node15: train data size:629
node15 epoch0:node_model train_loss:0.7309840534414563,train_acc:0.7818719744682312
node15 epoch1:node_model train_loss:0.4006672863449369,train_acc:0.8675863146781921
node15 epoch2:node_model train_loss:0.24259849105562484,train_acc:0.9193596839904785
node15 epoch3:node_model train_loss:0.16996379515954427,train_acc:0.942216694355011
node15 epoch4:node_model train_loss:0.1249459268791335,train_acc:0.960295557975769
node15_model on test-dataset: loss:0.8755774188041687,acc:0.7579997777938843
node15 weight score:718.3830766890551
node18: train data size:472
node18 epoch0:node_model train_loss:0.5620563566684723,train_acc:0.8378888964653015
node18 epoch1:node_model train_loss:0.2586802154779434,train_acc:0.9128889441490173
node18 epoch2:node_model train_loss:0.21984754055738448,train_acc:0.9448888897895813
node18 epoch3:node_model train_loss:0.0866897851228714,train_acc:0.9792222380638123
node18 epoch4:node_model train_loss:0.07129066623747349,train_acc:0.9892222285270691
node18_model on test-dataset: loss:0.9607214103639126,acc:0.7550999522209167
node18 weight score:491.2974717834285
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.588029090166092,acc:0.8213999819755554
total cost energy:4.87247365172289 | all_enery_cp：3.0229999999999997 | all_enery_tp: 1.8494736517228905
ef: 24.54750172173245
reward: 19.67502807000956
step 335:loss:5.046543121337891|running q:48.55725860595703
episode5,iteration35 selected nodes:[16, 19, 4, 8, 10],center node:10
################################################## episode5,iteration35 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4705947456615312,train_acc:0.8439285755157471
node4 epoch1:node_model train_loss:0.2910586329443114,train_acc:0.8871427178382874
node4 epoch2:node_model train_loss:0.22014886646398477,train_acc:0.9182142019271851
node4 epoch3:node_model train_loss:0.179186098543661,train_acc:0.9392856359481812
node4 epoch4:node_model train_loss:0.15515585782538568,train_acc:0.9510713815689087
node4_model on test-dataset: loss:0.7881261791288853,acc:0.7771998643875122
node4 weight score:3432.191534342677
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3568371327386962,train_acc:0.8793309330940247
node8 epoch1:node_model train_loss:0.20503830081886715,train_acc:0.9299206137657166
node8 epoch2:node_model train_loss:0.12326494852701823,train_acc:0.9655100703239441
node8 epoch3:node_model train_loss:0.0848219773421685,train_acc:0.9816100001335144
node8 epoch4:node_model train_loss:0.06941601965162489,train_acc:0.9861109852790833
node8_model on test-dataset: loss:0.7150586996972561,acc:0.7976998090744019
node8 weight score:2514.4788823088834
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4320700541138649,train_acc:0.8559999465942383
node10 epoch1:node_model train_loss:0.25657497718930244,train_acc:0.9171666502952576
node10 epoch2:node_model train_loss:0.15121047459542752,train_acc:0.9515000581741333
node10 epoch3:node_model train_loss:0.11910612620413304,train_acc:0.9754998087882996
node10 epoch4:node_model train_loss:0.0889419361948967,train_acc:0.9783332943916321
node10_model on test-dataset: loss:0.776542913839221,acc:0.7763999700546265
node10 weight score:2543.3237040765953
node16: train data size:877
node16 epoch0:node_model train_loss:0.5960327717992995,train_acc:0.8289177417755127
node16 epoch1:node_model train_loss:0.3297528790103065,train_acc:0.8938961029052734
node16 epoch2:node_model train_loss:0.22022510899437797,train_acc:0.9253390431404114
node16 epoch3:node_model train_loss:0.13142487737867567,train_acc:0.9578931927680969
node16 epoch4:node_model train_loss:0.0817643432981438,train_acc:0.9833332896232605
node16_model on test-dataset: loss:0.7337039470672607,acc:0.7881000638008118
node16 weight score:1195.3050048395105
node19: train data size:4281
node19 epoch0:node_model train_loss:0.536953039640604,train_acc:0.824298083782196
node19 epoch1:node_model train_loss:0.3028506882661997,train_acc:0.8978811502456665
node19 epoch2:node_model train_loss:0.2148529078031695,train_acc:0.9303848147392273
node19 epoch3:node_model train_loss:0.18023676858391874,train_acc:0.945076048374176
node19 epoch4:node_model train_loss:0.15175170070210167,train_acc:0.956169843673706
node19_model on test-dataset: loss:0.7751854638755321,acc:0.7755998969078064
node19 weight score:5522.549376244986
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5965781870484352,acc:0.8185999822616578
total cost energy:7.277455344987204 | all_enery_cp：5.818 | all_enery_tp: 1.4594553449872045
ef: 25.245801475139817
reward: 17.968346130152614
step 336:loss:7.390263080596924|running q:49.678550720214844
episode5,iteration36 selected nodes:[19, 0, 4, 17, 18],center node:17
################################################## episode5,iteration36 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.4044600773889285,train_acc:0.8595274090766907
node0 epoch1:node_model train_loss:0.2417201861166037,train_acc:0.9156811833381653
node0 epoch2:node_model train_loss:0.1664505858833973,train_acc:0.9479979276657104
node0 epoch3:node_model train_loss:0.12890509616297025,train_acc:0.9625348448753357
node0 epoch4:node_model train_loss:0.0938224419235037,train_acc:0.9752665162086487
node0_model on test-dataset: loss:0.7438323985040188,acc:0.7915001511573792
node0 weight score:6967.967529276687
node4: train data size:2705
node4 epoch0:node_model train_loss:0.3455843058015619,train_acc:0.8692858219146729
node4 epoch1:node_model train_loss:0.2478319542216403,train_acc:0.9078571200370789
node4 epoch2:node_model train_loss:0.17392675252631307,train_acc:0.9407143592834473
node4 epoch3:node_model train_loss:0.11866611162466663,train_acc:0.965714156627655
node4 epoch4:node_model train_loss:0.07424289839608329,train_acc:0.981428325176239
node4_model on test-dataset: loss:0.7967550203204155,acc:0.7797999382019043
node4 weight score:3395.0209675643873
node17: train data size:442
node17 epoch0:node_model train_loss:0.7662823259830475,train_acc:0.7748571634292603
node17 epoch1:node_model train_loss:0.501031619310379,train_acc:0.8374285101890564
node17 epoch2:node_model train_loss:0.2585048884153366,train_acc:0.9177142977714539
node17 epoch3:node_model train_loss:0.14834073334932327,train_acc:0.9399999976158142
node17 epoch4:node_model train_loss:0.10777812153100967,train_acc:0.9692381024360657
node17_model on test-dataset: loss:0.8486344142258168,acc:0.7643000483512878
node17 weight score:520.8367614966724
node18: train data size:472
node18 epoch0:node_model train_loss:0.591114753484726,train_acc:0.8234444856643677
node18 epoch1:node_model train_loss:0.30622989535331724,train_acc:0.9117777943611145
node18 epoch2:node_model train_loss:0.1657074883580208,train_acc:0.9584444165229797
node18 epoch3:node_model train_loss:0.11145560741424561,train_acc:0.9684444665908813
node18 epoch4:node_model train_loss:0.05929592549800873,train_acc:0.9864444732666016
node18_model on test-dataset: loss:0.8416214665025472,acc:0.7681999206542969
node18 weight score:560.8221971350721
node19: train data size:4281
node19 epoch0:node_model train_loss:0.328079589924147,train_acc:0.8833534121513367
node19 epoch1:node_model train_loss:0.19081313648196155,train_acc:0.9339678287506104
node19 epoch2:node_model train_loss:0.15127899393785832,train_acc:0.952913761138916
node19 epoch3:node_model train_loss:0.12117991587796877,train_acc:0.9669908881187439
node19 epoch4:node_model train_loss:0.09568364701645318,train_acc:0.97599196434021
node19_model on test-dataset: loss:0.7419499704241752,acc:0.7922998666763306
node19 weight score:5769.930818317222
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6129668387770653,acc:0.8222999823093414
total cost energy:8.962849664263551 | all_enery_cp：6.541499999999999 | all_enery_tp: 2.4213496642635515
ef: 24.887632867385257
reward: 15.924783203121706
step 337:loss:7.753951072692871|running q:50.84054183959961
episode5,iteration37 selected nodes:[0, 2, 10, 11, 4],center node:11
################################################## episode5,iteration37 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.24731135253722852,train_acc:0.9096060991287231
node0 epoch1:node_model train_loss:0.1620733907016424,train_acc:0.9443787932395935
node0 epoch2:node_model train_loss:0.11450963438703464,train_acc:0.9652268886566162
node0 epoch3:node_model train_loss:0.09784125099675013,train_acc:0.9703847169876099
node0 epoch4:node_model train_loss:0.07994424526651318,train_acc:0.9793443083763123
node0_model on test-dataset: loss:0.7901073257625103,acc:0.7878000736236572
node0 weight score:6559.8683001679965
node2: train data size:4788
node2 epoch0:node_model train_loss:0.33457790532459813,train_acc:0.882964015007019
node2 epoch1:node_model train_loss:0.1759281468888124,train_acc:0.9419979453086853
node2 epoch2:node_model train_loss:0.12043497160387535,train_acc:0.9647632837295532
node2 epoch3:node_model train_loss:0.09019439931338032,train_acc:0.9757766127586365
node2 epoch4:node_model train_loss:0.07336792012210935,train_acc:0.9794980883598328
node2_model on test-dataset: loss:0.806943769454956,acc:0.7854999899864197
node2 weight score:5933.498939131803
node4: train data size:2705
node4 epoch0:node_model train_loss:0.32034696728390244,train_acc:0.8885714411735535
node4 epoch1:node_model train_loss:0.19977453317759292,train_acc:0.9314284920692444
node4 epoch2:node_model train_loss:0.12042351279939924,train_acc:0.965714156627655
node4 epoch3:node_model train_loss:0.11733655066096357,train_acc:0.9628570675849915
node4 epoch4:node_model train_loss:0.08261767934475626,train_acc:0.9782140851020813
node4_model on test-dataset: loss:0.7539938567578792,acc:0.7882000803947449
node4 weight score:3587.5623862922575
node10: train data size:1975
node10 epoch0:node_model train_loss:0.47493614107370374,train_acc:0.8438333868980408
node10 epoch1:node_model train_loss:0.2633774571120739,train_acc:0.911500096321106
node10 epoch2:node_model train_loss:0.15774380695074797,train_acc:0.949833333492279
node10 epoch3:node_model train_loss:0.1169076632708311,train_acc:0.9714998602867126
node10 epoch4:node_model train_loss:0.0760003823786974,train_acc:0.9806665778160095
node10_model on test-dataset: loss:0.7862248334288597,acc:0.7794999480247498
node10 weight score:2512.004093519522
node11: train data size:1682
node11 epoch0:node_model train_loss:0.495522388640572,train_acc:0.8460975289344788
node11 epoch1:node_model train_loss:0.26396342936684103,train_acc:0.9079195857048035
node11 epoch2:node_model train_loss:0.13758467400775237,train_acc:0.9577187895774841
node11 epoch3:node_model train_loss:0.09735075691167046,train_acc:0.973730206489563
node11 epoch4:node_model train_loss:0.06911405143054093,train_acc:0.9839884638786316
node11_model on test-dataset: loss:0.748585968464613,acc:0.7970998287200928
node11 weight score:2246.9029221184383
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6095280127972365,acc:0.8226999831199646
total cost energy:9.972677259678004 | all_enery_cp：8.1665 | all_enery_tp: 1.8061772596780048
ef: 25.30016694950895
reward: 15.327489689830944
step 338:loss:4.483877182006836|running q:51.97710037231445
episode5,iteration38 selected nodes:[15, 13, 12, 10, 14],center node:10
################################################## episode5,iteration38 ##################################################
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4267542392015457,train_acc:0.8528333902359009
node10 epoch1:node_model train_loss:0.21176618933677674,train_acc:0.9264999628067017
node10 epoch2:node_model train_loss:0.1489260658621788,train_acc:0.9583333134651184
node10 epoch3:node_model train_loss:0.1079556941986084,train_acc:0.9694997668266296
node10 epoch4:node_model train_loss:0.0753801193088293,train_acc:0.9798330664634705
node10_model on test-dataset: loss:0.7814425668120384,acc:0.7824000120162964
node10 weight score:2527.3770381580066
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5754733756184578,train_acc:0.8239682912826538
node12 epoch1:node_model train_loss:0.2914051647697176,train_acc:0.9014286994934082
node12 epoch2:node_model train_loss:0.18092140768255507,train_acc:0.9449999332427979
node12 epoch3:node_model train_loss:0.10234687211258071,train_acc:0.9744445085525513
node12 epoch4:node_model train_loss:0.08452390932611056,train_acc:0.9799998998641968
node12_model on test-dataset: loss:0.7829749050736428,acc:0.7831999659538269
node12 weight score:1706.3126689537291
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6111533045768738,train_acc:0.8168182373046875
node13 epoch1:node_model train_loss:0.34187810743848485,train_acc:0.8829545378684998
node13 epoch2:node_model train_loss:0.18229148350656033,train_acc:0.9378031492233276
node13 epoch3:node_model train_loss:0.10946349551280339,train_acc:0.9711362719535828
node13 epoch4:node_model train_loss:0.0818629926070571,train_acc:0.9799999594688416
node13_model on test-dataset: loss:0.7951860359311104,acc:0.7779001593589783
node13 weight score:1452.4902951138613
node14: train data size:1172
node14 epoch0:node_model train_loss:0.49876074741284054,train_acc:0.8387500643730164
node14 epoch1:node_model train_loss:0.2542868380745252,train_acc:0.917546272277832
node14 epoch2:node_model train_loss:0.17748397402465343,train_acc:0.9462037086486816
node14 epoch3:node_model train_loss:0.09445897334565719,train_acc:0.9712036848068237
node14 epoch4:node_model train_loss:0.05821640882641077,train_acc:0.9841666221618652
node14_model on test-dataset: loss:0.7543713170289993,acc:0.7881999611854553
node14 weight score:1553.61156176481
node15: train data size:629
node15 epoch0:node_model train_loss:0.6038338414260319,train_acc:0.8217241764068604
node15 epoch1:node_model train_loss:0.3322192430496216,train_acc:0.8896552324295044
node15 epoch2:node_model train_loss:0.17947289454085485,train_acc:0.9436452984809875
node15 epoch3:node_model train_loss:0.1316388760294233,train_acc:0.9650738835334778
node15 epoch4:node_model train_loss:0.10983207555753845,train_acc:0.9693595767021179
node15_model on test-dataset: loss:0.8019802227616311,acc:0.7761000394821167
node15 weight score:784.308617778664
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5982035234570503,acc:0.8239999794960022
total cost energy:4.870439353766382 | all_enery_cp：3.1335 | all_enery_tp: 1.7369393537663815
ef: 24.684421559586674
reward: 19.81398220582029
step 339:loss:6.103002071380615|running q:53.093753814697266
episode5,iteration39 selected nodes:[2, 13, 10, 18, 11],center node:11
################################################## episode5,iteration39 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.252901381192108,train_acc:0.9110795855522156
node2 epoch1:node_model train_loss:0.14462867674107352,train_acc:0.9534943103790283
node2 epoch2:node_model train_loss:0.09805898178213586,train_acc:0.971193253993988
node2 epoch3:node_model train_loss:0.06912818535541494,train_acc:0.9840814471244812
node2 epoch4:node_model train_loss:0.050727175044206284,train_acc:0.9910134077072144
node2_model on test-dataset: loss:0.7824602892994881,acc:0.7884998917579651
node2 weight score:6119.160378460286
node10: train data size:1975
node10 epoch0:node_model train_loss:0.27676490917801855,train_acc:0.8969999551773071
node10 epoch1:node_model train_loss:0.15897987261414528,train_acc:0.9441664814949036
node10 epoch2:node_model train_loss:0.09880093522369862,train_acc:0.9745000004768372
node10 epoch3:node_model train_loss:0.05993013912811875,train_acc:0.9871665835380554
node10 epoch4:node_model train_loss:0.05091502284631133,train_acc:0.9915000200271606
node10_model on test-dataset: loss:0.7681789560616017,acc:0.7843998074531555
node10 weight score:2571.0154963443456
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4138033679303001,train_acc:0.8661549687385559
node11 epoch1:node_model train_loss:0.20278888853157268,train_acc:0.9315781593322754
node11 epoch2:node_model train_loss:0.13998984194853725,train_acc:0.9594117403030396
node11 epoch3:node_model train_loss:0.10945376907201375,train_acc:0.9692826271057129
node11 epoch4:node_model train_loss:0.0657235909910763,train_acc:0.9838593602180481
node11_model on test-dataset: loss:0.772133307904005,acc:0.7861998677253723
node11 weight score:2178.380316950546
node13: train data size:1155
node13 epoch0:node_model train_loss:0.4484829530119896,train_acc:0.850303053855896
node13 epoch1:node_model train_loss:0.21318804596861204,train_acc:0.9221212267875671
node13 epoch2:node_model train_loss:0.11708683334290981,train_acc:0.9671212434768677
node13 epoch3:node_model train_loss:0.08685487850258748,train_acc:0.9744696617126465
node13 epoch4:node_model train_loss:0.05973656428977847,train_acc:0.9859849214553833
node13_model on test-dataset: loss:0.8437799146771431,acc:0.7659000158309937
node13 weight score:1368.8403574312854
node18: train data size:472
node18 epoch0:node_model train_loss:0.5629704594612122,train_acc:0.8338889479637146
node18 epoch1:node_model train_loss:0.27703083455562594,train_acc:0.9053332209587097
node18 epoch2:node_model train_loss:0.138226655125618,train_acc:0.960444450378418
node18 epoch3:node_model train_loss:0.1226927250623703,train_acc:0.960444450378418
node18 epoch4:node_model train_loss:0.04597521498799324,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.8542236150801181,acc:0.7744000554084778
node18 weight score:552.5485267177153
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6222031883895397,acc:0.8176999813318253
total cost energy:6.927815142325356 | all_enery_cp：5.0360000000000005 | all_enery_tp: 1.8918151423253549
ef: 24.74997042279934
reward: 17.822155280473986
step 340:loss:6.565826892852783|running q:54.2172966003418
episode5,iteration40 selected nodes:[16, 14, 4, 3, 11],center node:11
################################################## episode5,iteration40 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4162602975618008,train_acc:0.8600545525550842
node3 epoch1:node_model train_loss:0.2096374764345413,train_acc:0.9300246834754944
node3 epoch2:node_model train_loss:0.15827991674805797,train_acc:0.9466846585273743
node3 epoch3:node_model train_loss:0.10039209097970364,train_acc:0.9729339480400085
node3 epoch4:node_model train_loss:0.07420210908491943,train_acc:0.9811328649520874
node3_model on test-dataset: loss:0.7606215137988329,acc:0.7898999452590942
node3 weight score:5583.591737747291
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4059950853032725,train_acc:0.8617857098579407
node4 epoch1:node_model train_loss:0.24714177979954652,train_acc:0.9110714793205261
node4 epoch2:node_model train_loss:0.17647358721920423,train_acc:0.9432142376899719
node4 epoch3:node_model train_loss:0.11374107681746994,train_acc:0.9685713052749634
node4 epoch4:node_model train_loss:0.0944359287885683,train_acc:0.9789283871650696
node4_model on test-dataset: loss:0.7877020953595638,acc:0.7821997404098511
node4 weight score:3434.0393607373153
node11: train data size:1682
node11 epoch0:node_model train_loss:0.316949444658616,train_acc:0.8885794878005981
node11 epoch1:node_model train_loss:0.17308854793801026,train_acc:0.941836416721344
node11 epoch2:node_model train_loss:0.11879730399917154,train_acc:0.9643183350563049
node11 epoch3:node_model train_loss:0.08222961754483335,train_acc:0.9804589748382568
node11 epoch4:node_model train_loss:0.052024632582769674,train_acc:0.9876468777656555
node11_model on test-dataset: loss:0.8045584116876126,acc:0.7829999923706055
node11 weight score:2090.587800172144
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4315865437189738,train_acc:0.8611111044883728
node14 epoch1:node_model train_loss:0.19217339158058167,train_acc:0.9305555820465088
node14 epoch2:node_model train_loss:0.14548091714580855,train_acc:0.9560184478759766
node14 epoch3:node_model train_loss:0.08192435652017593,train_acc:0.9815277457237244
node14 epoch4:node_model train_loss:0.04292392047743002,train_acc:0.9925000071525574
node14_model on test-dataset: loss:0.7633614954352379,acc:0.7917001247406006
node14 weight score:1535.3145357846126
node16: train data size:877
node16 epoch0:node_model train_loss:0.648791491985321,train_acc:0.8102453351020813
node16 epoch1:node_model train_loss:0.3291077133682039,train_acc:0.8913419842720032
node16 epoch2:node_model train_loss:0.18612466752529144,train_acc:0.9415584206581116
node16 epoch3:node_model train_loss:0.11697418118516605,train_acc:0.9701153635978699
node16 epoch4:node_model train_loss:0.08363516380389531,train_acc:0.985223650932312
node16_model on test-dataset: loss:0.7945033884048462,acc:0.7757001519203186
node16 weight score:1103.8341847235986
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6161756891012192,acc:0.8183999824523925
total cost energy:6.642546962275166 | all_enery_cp：5.3415 | all_enery_tp: 1.3010469622751655
ef: 25.13944689741179
reward: 18.496899935136625
step 341:loss:3.885080099105835|running q:55.27231979370117
episode5,iteration41 selected nodes:[1, 10, 5, 12, 13],center node:10
################################################## episode5,iteration41 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3764304211911033,train_acc:0.8716911673545837
node1 epoch1:node_model train_loss:0.23900838733157692,train_acc:0.9141175150871277
node1 epoch2:node_model train_loss:0.17061017497497447,train_acc:0.9459556341171265
node1 epoch3:node_model train_loss:0.1551635196322904,train_acc:0.9492647647857666
node1 epoch4:node_model train_loss:0.10635928973993834,train_acc:0.9674265384674072
node1_model on test-dataset: loss:0.7367130465060473,acc:0.7961999177932739
node1 weight score:9105.30909125815
node5: train data size:3735
node5 epoch0:node_model train_loss:0.45890981037365763,train_acc:0.8533457517623901
node5 epoch1:node_model train_loss:0.2573094999319629,train_acc:0.9146239757537842
node5 epoch2:node_model train_loss:0.17750658604659533,train_acc:0.9419549703598022
node5 epoch3:node_model train_loss:0.1314422068627257,train_acc:0.9624811410903931
node5 epoch4:node_model train_loss:0.10643069671564981,train_acc:0.9711275696754456
node5_model on test-dataset: loss:0.8393530005216598,acc:0.7782999873161316
node5 weight score:4449.856017287946
node10: train data size:1975
node10 epoch0:node_model train_loss:0.34163588657975197,train_acc:0.8813332915306091
node10 epoch1:node_model train_loss:0.1620318230241537,train_acc:0.9461666345596313
node10 epoch2:node_model train_loss:0.122032349742949,train_acc:0.9601665735244751
node10 epoch3:node_model train_loss:0.0779282858595252,train_acc:0.9823331832885742
node10 epoch4:node_model train_loss:0.053102356661111114,train_acc:0.9878332018852234
node10_model on test-dataset: loss:0.7650232423841953,acc:0.7877001166343689
node10 weight score:2581.62091107835
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5111639393227441,train_acc:0.8388095498085022
node12 epoch1:node_model train_loss:0.2865856524024691,train_acc:0.8947619199752808
node12 epoch2:node_model train_loss:0.1637082930122103,train_acc:0.9508729577064514
node12 epoch3:node_model train_loss:0.08886543634746756,train_acc:0.9764285683631897
node12 epoch4:node_model train_loss:0.06910815449165446,train_acc:0.9871428608894348
node12_model on test-dataset: loss:0.7300329393148423,acc:0.7948000431060791
node12 weight score:1830.054409947414
node13: train data size:1155
node13 epoch0:node_model train_loss:0.4895033712188403,train_acc:0.8402272462844849
node13 epoch1:node_model train_loss:0.2522411656876405,train_acc:0.9161363840103149
node13 epoch2:node_model train_loss:0.15882862731814384,train_acc:0.9511363506317139
node13 epoch3:node_model train_loss:0.09294065336386363,train_acc:0.9743181467056274
node13 epoch4:node_model train_loss:0.05627736526851853,train_acc:0.9858333468437195
node13_model on test-dataset: loss:0.7781839483976364,acc:0.7834998965263367
node13 weight score:1484.2249090054709
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6266970942914486,acc:0.8199999806284904
total cost energy:9.338876684661082 | all_enery_cp：7.4544999999999995 | all_enery_tp: 1.8843766846610825
ef: 25.213363977529415
reward: 15.874487292868332
step 342:loss:5.026296615600586|running q:56.34577178955078
episode5,iteration42 selected nodes:[5, 15, 9, 13, 6],center node:9
################################################## episode5,iteration42 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.326828916998286,train_acc:0.8837215900421143
node5 epoch1:node_model train_loss:0.22011172477351992,train_acc:0.921842098236084
node5 epoch2:node_model train_loss:0.1493195626688631,train_acc:0.9493233561515808
node5 epoch3:node_model train_loss:0.09228549378090783,train_acc:0.9763532876968384
node5 epoch4:node_model train_loss:0.07474968376520433,train_acc:0.9784585237503052
node5_model on test-dataset: loss:0.7327362257242203,acc:0.797999918460846
node5 weight score:5097.332258014689
node6: train data size:3007
node6 epoch0:node_model train_loss:0.45606332152120527,train_acc:0.85999995470047
node6 epoch1:node_model train_loss:0.27415934878010906,train_acc:0.9089400768280029
node6 epoch2:node_model train_loss:0.1855050911375832,train_acc:0.9399998188018799
node6 epoch3:node_model train_loss:0.1417493072728957,train_acc:0.9581103324890137
node6 epoch4:node_model train_loss:0.21741147771958383,train_acc:0.9332259297370911
node6_model on test-dataset: loss:0.8296378454566002,acc:0.7745997309684753
node6 weight score:3624.473035394214
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6120754276451311,train_acc:0.8220775723457336
node9 epoch1:node_model train_loss:0.2988331608082119,train_acc:0.8960386514663696
node9 epoch2:node_model train_loss:0.19310861473020755,train_acc:0.9369621276855469
node9 epoch3:node_model train_loss:0.11261853301211407,train_acc:0.9698615074157715
node9 epoch4:node_model train_loss:0.0819198602908536,train_acc:0.9817081093788147
node9_model on test-dataset: loss:0.7601828715205192,acc:0.785300076007843
node9 weight score:2442.833257063034
node13: train data size:1155
node13 epoch0:node_model train_loss:0.41070934136708576,train_acc:0.8570455312728882
node13 epoch1:node_model train_loss:0.23388136426607767,train_acc:0.9227272868156433
node13 epoch2:node_model train_loss:0.1356249147405227,train_acc:0.9579545259475708
node13 epoch3:node_model train_loss:0.10744513664394617,train_acc:0.9704544544219971
node13 epoch4:node_model train_loss:0.047150638264914356,train_acc:0.9909847974777222
node13_model on test-dataset: loss:0.8089730948209762,acc:0.7784001231193542
node13 weight score:1427.7359870115317
node15: train data size:629
node15 epoch0:node_model train_loss:0.6631195630346026,train_acc:0.7921674251556396
node15 epoch1:node_model train_loss:0.3369797191449574,train_acc:0.8817241787910461
node15 epoch2:node_model train_loss:0.20767452248505183,train_acc:0.9337931275367737
node15 epoch3:node_model train_loss:0.11980091780424118,train_acc:0.965073823928833
node15 epoch4:node_model train_loss:0.08573714537279946,train_acc:0.9744335412979126
node15_model on test-dataset: loss:0.8402669299393892,acc:0.7719999551773071
node15 weight score:748.5716473994419
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6062008528411389,acc:0.8191999816894531
total cost energy:6.690331382177989 | all_enery_cp：5.1915000000000004 | all_enery_tp: 1.4988313821779888
ef: 24.965324790287042
reward: 18.274993408109054
step 343:loss:5.672197341918945|running q:57.50712585449219
episode5,iteration43 selected nodes:[4, 6, 15, 19, 9],center node:9
################################################## episode5,iteration43 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4140725838286536,train_acc:0.8567857146263123
node4 epoch1:node_model train_loss:0.26445424450295313,train_acc:0.9099998474121094
node4 epoch2:node_model train_loss:0.27295328570263727,train_acc:0.919285774230957
node4 epoch3:node_model train_loss:0.22017530100752733,train_acc:0.9232142567634583
node4 epoch4:node_model train_loss:0.11222663787858826,train_acc:0.9603570699691772
node4_model on test-dataset: loss:0.780779595375061,acc:0.785899817943573
node4 weight score:3464.4860291214527
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3244886182008251,train_acc:0.8846542835235596
node6 epoch1:node_model train_loss:0.22447225439452356,train_acc:0.9280645847320557
node6 epoch2:node_model train_loss:0.1427042867627836,train_acc:0.9551612138748169
node6 epoch3:node_model train_loss:0.09296394439954911,train_acc:0.9770965576171875
node6 epoch4:node_model train_loss:0.09089931965835633,train_acc:0.9750688672065735
node6_model on test-dataset: loss:0.7651022526621819,acc:0.7904999852180481
node6 weight score:3930.1936303769985
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4596249696455504,train_acc:0.8570914268493652
node9 epoch1:node_model train_loss:0.280947271930544,train_acc:0.9027608633041382
node9 epoch2:node_model train_loss:0.15970300137996674,train_acc:0.9465743899345398
node9 epoch3:node_model train_loss:0.09748716024976027,train_acc:0.9678854942321777
node9 epoch4:node_model train_loss:0.07362170282163118,train_acc:0.9805261492729187
node9_model on test-dataset: loss:0.7541546079516411,acc:0.7865999341011047
node9 weight score:2462.359813783803
node15: train data size:629
node15 epoch0:node_model train_loss:0.5946047731808254,train_acc:0.8209359645843506
node15 epoch1:node_model train_loss:0.27143081171172007,train_acc:0.9065024852752686
node15 epoch2:node_model train_loss:0.18131107624088014,train_acc:0.9350739121437073
node15 epoch3:node_model train_loss:0.12670677208474704,train_acc:0.9644335508346558
node15 epoch4:node_model train_loss:0.0883996630353587,train_acc:0.9757142663002014
node15_model on test-dataset: loss:0.9319163745641709,acc:0.7574999928474426
node15 weight score:674.9532653014754
node19: train data size:4281
node19 epoch0:node_model train_loss:0.433949411608452,train_acc:0.8587711453437805
node19 epoch1:node_model train_loss:0.25363197922706604,train_acc:0.9126556515693665
node19 epoch2:node_model train_loss:0.1606244006475737,train_acc:0.9473872184753418
node19 epoch3:node_model train_loss:0.1287762410072393,train_acc:0.9590839743614197
node19 epoch4:node_model train_loss:0.0816601814139028,train_acc:0.9796582460403442
node19_model on test-dataset: loss:0.7483674132823944,acc:0.7904001474380493
node19 weight score:5720.452178994833
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.59525360442698,acc:0.821199979186058
total cost energy:7.957685608247084 | all_enery_cp：6.2395 | all_enery_tp: 1.7181856082470843
ef: 25.089599646997105
reward: 17.13191403875002
step 344:loss:6.111201763153076|running q:58.55467224121094
episode5,iteration44 selected nodes:[15, 10, 14, 9, 19],center node:14
################################################## episode5,iteration44 ##################################################
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4016825352844439,train_acc:0.8635271787643433
node9 epoch1:node_model train_loss:0.21571033173485807,train_acc:0.9216898083686829
node9 epoch2:node_model train_loss:0.14965825135770597,train_acc:0.9488087892532349
node9 epoch3:node_model train_loss:0.10829452680129754,train_acc:0.9710525870323181
node9 epoch4:node_model train_loss:0.06379880020885091,train_acc:0.983813464641571
node9_model on test-dataset: loss:0.744395654797554,acc:0.7912998795509338
node9 weight score:2494.6411065564726
node10: train data size:1975
node10 epoch0:node_model train_loss:0.37903919368982314,train_acc:0.8680000305175781
node10 epoch1:node_model train_loss:0.17980206608772278,train_acc:0.9406666159629822
node10 epoch2:node_model train_loss:0.13200664352625607,train_acc:0.9644999504089355
node10 epoch3:node_model train_loss:0.07234310507774352,train_acc:0.984666645526886
node10 epoch4:node_model train_loss:0.054939506482332943,train_acc:0.9904999136924744
node10_model on test-dataset: loss:0.7763849487900734,acc:0.7852999567985535
node10 weight score:2543.84117450739
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4984814152121544,train_acc:0.8420833945274353
node14 epoch1:node_model train_loss:0.2673019655048847,train_acc:0.9158796072006226
node14 epoch2:node_model train_loss:0.14503604856630167,train_acc:0.9465277194976807
node14 epoch3:node_model train_loss:0.08584641137470801,train_acc:0.9758332967758179
node14 epoch4:node_model train_loss:0.058269316175331674,train_acc:0.9846758246421814
node14_model on test-dataset: loss:0.7245241906493902,acc:0.7959998250007629
node14 weight score:1617.6133455937997
node15: train data size:629
node15 epoch0:node_model train_loss:0.5337327889033726,train_acc:0.8263054490089417
node15 epoch1:node_model train_loss:0.2366610552583422,train_acc:0.9123645424842834
node15 epoch2:node_model train_loss:0.17659419029951096,train_acc:0.9444335103034973
node15 epoch3:node_model train_loss:0.1267845013311931,train_acc:0.950935959815979
node15 epoch4:node_model train_loss:0.0777783915400505,train_acc:0.9744334816932678
node15_model on test-dataset: loss:0.8121633789688348,acc:0.7717999815940857
node15 weight score:774.4747132019315
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2507622598908668,train_acc:0.9087423086166382
node19 epoch1:node_model train_loss:0.15683346709539725,train_acc:0.9465805888175964
node19 epoch2:node_model train_loss:0.10837846327313157,train_acc:0.9677430987358093
node19 epoch3:node_model train_loss:0.08183883355800496,train_acc:0.980355978012085
node19 epoch4:node_model train_loss:0.06322644269743631,train_acc:0.9837897419929504
node19_model on test-dataset: loss:0.7419508973509074,acc:0.7994999289512634
node19 weight score:5769.923609884511
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6015289607644081,acc:0.8218999809026718
total cost energy:6.454129758820093 | all_enery_cp：4.957 | all_enery_tp: 1.4971297588200936
ef: 25.100564789548542
reward: 18.646435030728448
step 345:loss:9.480685234069824|running q:59.6105842590332
episode5,iteration45 selected nodes:[3, 11, 5, 18, 13],center node:11
################################################## episode5,iteration45 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.36913595781769865,train_acc:0.8740374445915222
node3 epoch1:node_model train_loss:0.18978451192378998,train_acc:0.9319148063659668
node3 epoch2:node_model train_loss:0.12473812058221462,train_acc:0.9597375392913818
node3 epoch3:node_model train_loss:0.0796543798561013,train_acc:0.9771794080734253
node3 epoch4:node_model train_loss:0.0717305232185957,train_acc:0.9818307757377625
node3_model on test-dataset: loss:0.7644808141887188,acc:0.7933999300003052
node3 weight score:5555.40429684556
node5: train data size:3735
node5 epoch0:node_model train_loss:0.32828356207985626,train_acc:0.8825187683105469
node5 epoch1:node_model train_loss:0.1715298800876266,train_acc:0.9417669177055359
node5 epoch2:node_model train_loss:0.10861787209777456,train_acc:0.9686840772628784
node5 epoch3:node_model train_loss:0.07762261788899961,train_acc:0.9789472222328186
node5 epoch4:node_model train_loss:0.05561415738377132,train_acc:0.9874435067176819
node5_model on test-dataset: loss:0.749028071463108,acc:0.7999998927116394
node5 weight score:4986.461979594793
node11: train data size:1682
node11 epoch0:node_model train_loss:0.41956813721095815,train_acc:0.8542037010192871
node11 epoch1:node_model train_loss:0.23337389616405263,train_acc:0.9205307960510254
node11 epoch2:node_model train_loss:0.13134362723897486,train_acc:0.9609181880950928
node11 epoch3:node_model train_loss:0.09311275994952987,train_acc:0.9723527431488037
node11 epoch4:node_model train_loss:0.07133282764869578,train_acc:0.9832710027694702
node11_model on test-dataset: loss:0.7616543713212013,acc:0.796500027179718
node11 weight score:2208.350747179885
node13: train data size:1155
node13 epoch0:node_model train_loss:0.48751585682233173,train_acc:0.8534091711044312
node13 epoch1:node_model train_loss:0.205289114266634,train_acc:0.9307575225830078
node13 epoch2:node_model train_loss:0.16756071460743746,train_acc:0.9378787875175476
node13 epoch3:node_model train_loss:0.0897694608817498,train_acc:0.976136326789856
node13 epoch4:node_model train_loss:0.05710653495043516,train_acc:0.9911363124847412
node13_model on test-dataset: loss:0.8566060890257359,acc:0.7724999785423279
node13 weight score:1348.344372982036
node18: train data size:472
node18 epoch0:node_model train_loss:0.6350177884101867,train_acc:0.8159999847412109
node18 epoch1:node_model train_loss:0.3479301750659943,train_acc:0.8905555605888367
node18 epoch2:node_model train_loss:0.17584928423166274,train_acc:0.9453333020210266
node18 epoch3:node_model train_loss:0.09851685166358948,train_acc:0.965222179889679
node18 epoch4:node_model train_loss:0.06431244611740113,train_acc:0.9796667098999023
node18_model on test-dataset: loss:0.9379556004703045,acc:0.7652000188827515
node18 weight score:503.22211388612897
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6199172130972147,acc:0.8266999804973603
total cost energy:7.528856099354873 | all_enery_cp：5.645499999999999 | all_enery_tp: 1.8833560993548737
ef: 24.823434697471008
reward: 17.294578598116136
step 346:loss:4.068780422210693|running q:60.718963623046875
episode5,iteration46 selected nodes:[1, 10, 6, 0, 9],center node:6
################################################## episode5,iteration46 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.37616505187291366,train_acc:0.8749120235443115
node0 epoch1:node_model train_loss:0.20316961579597914,train_acc:0.9324166774749756
node0 epoch2:node_model train_loss:0.136654331563757,train_acc:0.9587235450744629
node0 epoch3:node_model train_loss:0.08446959821650615,train_acc:0.9799606800079346
node0 epoch4:node_model train_loss:0.07318778139037582,train_acc:0.9804980754852295
node0_model on test-dataset: loss:0.7513242479413748,acc:0.7990002036094666
node0 weight score:6898.486258365011
node1: train data size:6708
node1 epoch0:node_model train_loss:0.3017117959611556,train_acc:0.8941912055015564
node1 epoch1:node_model train_loss:0.21528513872009866,train_acc:0.9243380427360535
node1 epoch2:node_model train_loss:0.14240453364875386,train_acc:0.9541177153587341
node1 epoch3:node_model train_loss:0.10303978238473921,train_acc:0.9675000309944153
node1 epoch4:node_model train_loss:0.08769830907968913,train_acc:0.9755149483680725
node1_model on test-dataset: loss:0.7396206451952457,acc:0.7995999455451965
node1 weight score:9069.51427542861
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3439275127264761,train_acc:0.8866819739341736
node6 epoch1:node_model train_loss:0.20970434935823565,train_acc:0.9309676289558411
node6 epoch2:node_model train_loss:0.13828416673406477,train_acc:0.9554836750030518
node6 epoch3:node_model train_loss:0.10710577156034208,train_acc:0.9654837846755981
node6 epoch4:node_model train_loss:0.07044503831815335,train_acc:0.9838706254959106
node6_model on test-dataset: loss:0.7226087090373039,acc:0.8050001263618469
node6 weight score:4161.311595602105
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4391722553654721,train_acc:0.868938148021698
node9 epoch1:node_model train_loss:0.23535684064814918,train_acc:0.9216989278793335
node9 epoch2:node_model train_loss:0.14964516813817777,train_acc:0.94973224401474
node9 epoch3:node_model train_loss:0.08660217276529263,train_acc:0.9786795377731323
node9 epoch4:node_model train_loss:0.05268947711508525,train_acc:0.9911818504333496
node9_model on test-dataset: loss:0.7569376468658447,acc:0.7959001660346985
node9 weight score:2453.3064350664063
node10: train data size:1975
node10 epoch0:node_model train_loss:0.38668220639228823,train_acc:0.872499942779541
node10 epoch1:node_model train_loss:0.20123157426714897,train_acc:0.93316650390625
node10 epoch2:node_model train_loss:0.10040430948138238,train_acc:0.9716665148735046
node10 epoch3:node_model train_loss:0.07366217225790024,train_acc:0.982166588306427
node10 epoch4:node_model train_loss:0.046148684713989495,train_acc:0.9923332333564758
node10_model on test-dataset: loss:0.7302908110618591,acc:0.7935996651649475
node10 weight score:2704.402095828518
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6005803615599871,acc:0.8285999804735183
total cost energy:10.481351461583875 | all_enery_cp：9.364999999999998 | all_enery_tp: 1.1163514615838765
ef: 25.55839997146467
reward: 15.077048509880795
step 347:loss:8.385876655578613|running q:61.808990478515625
episode5,iteration47 selected nodes:[11, 2, 12, 17, 6],center node:11
################################################## episode5,iteration47 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2998208937545617,train_acc:0.8993088006973267
node2 epoch1:node_model train_loss:0.1472422086323301,train_acc:0.9508709907531738
node2 epoch2:node_model train_loss:0.09760459008005758,train_acc:0.9709848165512085
node2 epoch3:node_model train_loss:0.06963987652367602,train_acc:0.9830681681632996
node2 epoch4:node_model train_loss:0.05165672542837759,train_acc:0.9858333468437195
node2_model on test-dataset: loss:0.7575448581576347,acc:0.799000084400177
node2 weight score:6320.417792346341
node6: train data size:3007
node6 epoch0:node_model train_loss:0.30541076679383555,train_acc:0.9038708209991455
node6 epoch1:node_model train_loss:0.1931427085111218,train_acc:0.9331335425376892
node6 epoch2:node_model train_loss:0.167807828755148,train_acc:0.9422580599784851
node6 epoch3:node_model train_loss:0.11982716247439384,train_acc:0.9631333947181702
node6 epoch4:node_model train_loss:0.11420830885969824,train_acc:0.9616127610206604
node6_model on test-dataset: loss:0.8088064520061016,acc:0.7854999899864197
node6 weight score:3717.823952246755
node11: train data size:1682
node11 epoch0:node_model train_loss:0.3382275849580765,train_acc:0.8832854628562927
node11 epoch1:node_model train_loss:0.14907405437792048,train_acc:0.9502007961273193
node11 epoch2:node_model train_loss:0.1044641961069668,train_acc:0.9738593697547913
node11 epoch3:node_model train_loss:0.06646927972050275,train_acc:0.9837301969528198
node11 epoch4:node_model train_loss:0.050721873145769626,train_acc:0.9922237396240234
node11_model on test-dataset: loss:0.7733912126719952,acc:0.7927999496459961
node11 weight score:2174.8372265426256
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5305706645761218,train_acc:0.8461111187934875
node12 epoch1:node_model train_loss:0.2717188894748688,train_acc:0.910793662071228
node12 epoch2:node_model train_loss:0.1678991631737777,train_acc:0.9467461109161377
node12 epoch3:node_model train_loss:0.09095379683588233,train_acc:0.9723016023635864
node12 epoch4:node_model train_loss:0.06235089006700686,train_acc:0.9878571033477783
node12_model on test-dataset: loss:0.7904941865801811,acc:0.7843001484870911
node12 weight score:1690.0820052577167
node17: train data size:442
node17 epoch0:node_model train_loss:0.7749619722366333,train_acc:0.7813333868980408
node17 epoch1:node_model train_loss:0.3814458906650543,train_acc:0.8829523324966431
node17 epoch2:node_model train_loss:0.18575195074081421,train_acc:0.9417142868041992
node17 epoch3:node_model train_loss:0.16004732847213746,train_acc:0.9552380442619324
node17 epoch4:node_model train_loss:0.0732221007347107,train_acc:0.9819999933242798
node17_model on test-dataset: loss:0.8729503701627255,acc:0.7724997401237488
node17 weight score:506.32889922207994
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6147980169206858,acc:0.8222999835014343
total cost energy:7.270897840021019 | all_enery_cp：5.6275 | all_enery_tp: 1.643397840021018
ef: 24.901848144012202
reward: 17.630950303991185
step 348:loss:8.592192649841309|running q:62.83034133911133
episode5,iteration48 selected nodes:[5, 14, 19, 11, 8],center node:11
################################################## episode5,iteration48 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.2963283191386022,train_acc:0.8998872637748718
node5 epoch1:node_model train_loss:0.16468237556124987,train_acc:0.9446239471435547
node5 epoch2:node_model train_loss:0.11434785482522689,train_acc:0.9658644795417786
node5 epoch3:node_model train_loss:0.07205014460180935,train_acc:0.9789847135543823
node5 epoch4:node_model train_loss:0.0568245976771179,train_acc:0.9884210824966431
node5_model on test-dataset: loss:0.7477370996028185,acc:0.8000999093055725
node5 weight score:4995.071131262511
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5120103855927786,train_acc:0.8376758098602295
node8 epoch1:node_model train_loss:0.2554707005620003,train_acc:0.9187527894973755
node8 epoch2:node_model train_loss:0.16317955694264835,train_acc:0.9482766389846802
node8 epoch3:node_model train_loss:0.10275238710972998,train_acc:0.9755440950393677
node8 epoch4:node_model train_loss:0.06368862847901052,train_acc:0.9827435612678528
node8_model on test-dataset: loss:0.7658425650000572,acc:0.7945000529289246
node8 weight score:2347.74101384645
node11: train data size:1682
node11 epoch0:node_model train_loss:0.2767345598515342,train_acc:0.9082494974136353
node11 epoch1:node_model train_loss:0.1704728936447817,train_acc:0.9454949498176575
node11 epoch2:node_model train_loss:0.10280516765573446,train_acc:0.9639884829521179
node11 epoch3:node_model train_loss:0.07503709503832985,train_acc:0.9770587682723999
node11 epoch4:node_model train_loss:0.048165388186188304,train_acc:0.9886943697929382
node11_model on test-dataset: loss:0.7846192885935307,acc:0.7978999018669128
node11 weight score:2143.714823803362
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4465550494690736,train_acc:0.8652315139770508
node14 epoch1:node_model train_loss:0.24887613207101822,train_acc:0.9163888692855835
node14 epoch2:node_model train_loss:0.14874475076794624,train_acc:0.9481944441795349
node14 epoch3:node_model train_loss:0.07525650955115755,train_acc:0.9788424968719482
node14 epoch4:node_model train_loss:0.051984100292126335,train_acc:0.9874999523162842
node14_model on test-dataset: loss:0.8088916375488043,acc:0.7867000699043274
node14 weight score:1448.8961754525342
node19: train data size:4281
node19 epoch0:node_model train_loss:0.32795130201550415,train_acc:0.8856789469718933
node19 epoch1:node_model train_loss:0.15485865241566368,train_acc:0.9518890380859375
node19 epoch2:node_model train_loss:0.08416279539639174,train_acc:0.9787279963493347
node19 epoch3:node_model train_loss:0.062012287896386416,train_acc:0.9871001839637756
node19 epoch4:node_model train_loss:0.046650436252009035,train_acc:0.9906430840492249
node19_model on test-dataset: loss:0.7263926510512829,acc:0.8022000193595886
node19 weight score:5893.506760846571
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6027593772113323,acc:0.8321999824047088
total cost energy:7.9284271909999156 | all_enery_cp：6.334 | all_enery_tp: 1.594427190999916
ef: 25.248028790528153
reward: 17.319601599528237
step 349:loss:10.163304328918457|running q:63.86756896972656
episode5,iteration49 selected nodes:[14, 15, 11, 2, 10],center node:11
################################################## episode5,iteration49 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.1930885201630493,train_acc:0.9326230883598328
node2 epoch1:node_model train_loss:0.10739046102389693,train_acc:0.967357873916626
node2 epoch2:node_model train_loss:0.07109007922311623,train_acc:0.9786931872367859
node2 epoch3:node_model train_loss:0.05638525031584626,train_acc:0.9853315353393555
node2 epoch4:node_model train_loss:0.043057884264271706,train_acc:0.990776538848877
node2_model on test-dataset: loss:0.7695756644010544,acc:0.7954002022743225
node2 weight score:6221.610455583215
node10: train data size:1975
node10 epoch0:node_model train_loss:0.37824383825063707,train_acc:0.8738332986831665
node10 epoch1:node_model train_loss:0.19648924618959426,train_acc:0.9264999628067017
node10 epoch2:node_model train_loss:0.12106751371175051,train_acc:0.9604999423027039
node10 epoch3:node_model train_loss:0.0777649249881506,train_acc:0.9773331880569458
node10 epoch4:node_model train_loss:0.05771306809037924,train_acc:0.9848331809043884
node10_model on test-dataset: loss:0.7306065820157528,acc:0.7963001132011414
node10 weight score:2703.233242918439
node11: train data size:1682
node11 epoch0:node_model train_loss:0.2569289128569996,train_acc:0.9070730805397034
node11 epoch1:node_model train_loss:0.13659694177262924,train_acc:0.9580487608909607
node11 epoch2:node_model train_loss:0.08425878612872433,train_acc:0.9723527431488037
node11 epoch3:node_model train_loss:0.07730592283255913,train_acc:0.9776470065116882
node11 epoch4:node_model train_loss:0.047777074882212806,train_acc:0.9879769682884216
node11_model on test-dataset: loss:0.7875132648646832,acc:0.7961999177932739
node11 weight score:2135.8370392516686
node14: train data size:1172
node14 epoch0:node_model train_loss:0.3631601482629776,train_acc:0.882361114025116
node14 epoch1:node_model train_loss:0.19210327292482057,train_acc:0.9365277290344238
node14 epoch2:node_model train_loss:0.1028867991020282,train_acc:0.9631943702697754
node14 epoch3:node_model train_loss:0.07195736033221085,train_acc:0.9805091619491577
node14 epoch4:node_model train_loss:0.04080441314727068,train_acc:0.9933333396911621
node14_model on test-dataset: loss:0.7704292203485966,acc:0.7912998199462891
node14 weight score:1521.2299443545307
node15: train data size:629
node15 epoch0:node_model train_loss:0.6092996554715293,train_acc:0.8202956318855286
node15 epoch1:node_model train_loss:0.2821792811155319,train_acc:0.9215763807296753
node15 epoch2:node_model train_loss:0.17492658538477762,train_acc:0.9437930583953857
node15 epoch3:node_model train_loss:0.10223876684904099,train_acc:0.965073823928833
node15 epoch4:node_model train_loss:0.0699289898787226,train_acc:0.977931022644043
node15_model on test-dataset: loss:0.832324471026659,acc:0.7804999947547913
node15 weight score:755.7148947262581
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6287250390648842,acc:0.8257999807596207
total cost energy:6.639227766016838 | all_enery_cp：5.123 | all_enery_tp: 1.516227766016838
ef: 25.023344089569328
reward: 18.38411632355249
step 350:loss:8.099444389343262|running q:64.92771911621094
episode5,iteration50 selected nodes:[17, 4, 3, 9, 16],center node:3
################################################## episode5,iteration50 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.311390689639158,train_acc:0.8949974179267883
node3 epoch1:node_model train_loss:0.16727331627246944,train_acc:0.9401434659957886
node3 epoch2:node_model train_loss:0.11532609719176624,train_acc:0.9640375375747681
node3 epoch3:node_model train_loss:0.09364637966419376,train_acc:0.9746511578559875
node3 epoch4:node_model train_loss:0.06254281679731469,train_acc:0.9850865006446838
node3_model on test-dataset: loss:0.8004086309671402,acc:0.7982000708580017
node3 weight score:5306.039734814348
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4328158555020179,train_acc:0.8667857646942139
node4 epoch1:node_model train_loss:0.2414892398353134,train_acc:0.910714328289032
node4 epoch2:node_model train_loss:0.15545913803258113,train_acc:0.9457142353057861
node4 epoch3:node_model train_loss:0.10824351897463202,train_acc:0.9610713124275208
node4 epoch4:node_model train_loss:0.1449476427265576,train_acc:0.9592857360839844
node4_model on test-dataset: loss:0.8217212164402008,acc:0.7851999402046204
node4 weight score:3291.8707049069494
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4970384491117377,train_acc:0.8433980345726013
node9 epoch1:node_model train_loss:0.25511897629813146,train_acc:0.9174883961677551
node9 epoch2:node_model train_loss:0.16447344618408302,train_acc:0.9428900480270386
node9 epoch3:node_model train_loss:0.08266476404510047,train_acc:0.9773682951927185
node9 epoch4:node_model train_loss:0.07206469676212261,train_acc:0.979732096195221
node9_model on test-dataset: loss:0.766899011284113,acc:0.7946001887321472
node9 weight score:2421.44007578077
node16: train data size:877
node16 epoch0:node_model train_loss:0.6658291849825118,train_acc:0.8173593282699585
node16 epoch1:node_model train_loss:0.3505238542954127,train_acc:0.8961184024810791
node16 epoch2:node_model train_loss:0.18333570824729073,train_acc:0.9391196370124817
node16 epoch3:node_model train_loss:0.16370344989829594,train_acc:0.9508946537971497
node16 epoch4:node_model train_loss:0.06071684923436907,train_acc:0.9830014109611511
node16_model on test-dataset: loss:0.820145037919283,acc:0.776699960231781
node16 weight score:1069.32305805929
node17: train data size:442
node17 epoch0:node_model train_loss:0.8409312963485718,train_acc:0.7733333110809326
node17 epoch1:node_model train_loss:0.3871548354625702,train_acc:0.8781904578208923
node17 epoch2:node_model train_loss:0.2710461437702179,train_acc:0.9157142639160156
node17 epoch3:node_model train_loss:0.15380093902349473,train_acc:0.9489523768424988
node17 epoch4:node_model train_loss:0.10158795937895775,train_acc:0.9604762196540833
node17_model on test-dataset: loss:0.9714078676700592,acc:0.7537999749183655
node17 weight score:455.00969748180614
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6278909362852574,acc:0.8246999806165696
total cost energy:6.819489848529781 | all_enery_cp：5.064 | all_enery_tp: 1.75548984852978
ef: 24.59538603085409
reward: 17.77589618232431
step 351:loss:13.191253662109375|running q:65.91133117675781
episode5,iteration51 selected nodes:[3, 12, 2, 17, 10],center node:10
################################################## episode5,iteration51 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.13019433858183524,train_acc:0.9544410705566406
node2 epoch1:node_model train_loss:0.07240258265907566,train_acc:0.9797065258026123
node2 epoch2:node_model train_loss:0.054196382445904114,train_acc:0.9870266318321228
node2 epoch3:node_model train_loss:0.0528505245068421,train_acc:0.9865814447402954
node2 epoch4:node_model train_loss:0.04927281142833332,train_acc:0.987083375453949
node2_model on test-dataset: loss:0.8369192178547382,acc:0.7907998561859131
node2 weight score:5720.982261911735
node3: train data size:4247
node3 epoch0:node_model train_loss:0.16002201930034993,train_acc:0.9408708810806274
node3 epoch1:node_model train_loss:0.09457338748629703,train_acc:0.9709004163742065
node3 epoch2:node_model train_loss:0.07406056906248248,train_acc:0.9767144918441772
node3 epoch3:node_model train_loss:0.0662836309868929,train_acc:0.982325553894043
node3 epoch4:node_model train_loss:0.05900399542825167,train_acc:0.9818010330200195
node3_model on test-dataset: loss:0.8856511035561562,acc:0.7783999443054199
node3 weight score:4795.3420742626695
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3265175767242908,train_acc:0.8851666450500488
node10 epoch1:node_model train_loss:0.1516162034124136,train_acc:0.9461666345596313
node10 epoch2:node_model train_loss:0.09264398608356714,train_acc:0.9731666445732117
node10 epoch3:node_model train_loss:0.05332039566710591,train_acc:0.9874998331069946
node10 epoch4:node_model train_loss:0.04095756495371461,train_acc:0.9908332824707031
node10_model on test-dataset: loss:0.7614294610917568,acc:0.790399968624115
node10 weight score:2593.8055997573238
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5565975627728871,train_acc:0.8368254899978638
node12 epoch1:node_model train_loss:0.2802160935742514,train_acc:0.9119841456413269
node12 epoch2:node_model train_loss:0.17786470374890737,train_acc:0.9438889026641846
node12 epoch3:node_model train_loss:0.11803556526345867,train_acc:0.9649206399917603
node12 epoch4:node_model train_loss:0.0920963997819594,train_acc:0.9744443893432617
node12_model on test-dataset: loss:0.8002277806401252,acc:0.7868000864982605
node12 weight score:1669.5246432600668
node17: train data size:442
node17 epoch0:node_model train_loss:0.6411649346351623,train_acc:0.8106666803359985
node17 epoch1:node_model train_loss:0.23127554208040238,train_acc:0.9319999814033508
node17 epoch2:node_model train_loss:0.1751101940870285,train_acc:0.9449524283409119
node17 epoch3:node_model train_loss:0.14783603847026824,train_acc:0.9577142596244812
node17 epoch4:node_model train_loss:0.07802249118685722,train_acc:0.9860000610351562
node17_model on test-dataset: loss:0.8643535055220127,acc:0.7737000584602356
node17 weight score:511.3648491921844
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6528013727068901,acc:0.8225999784469604
total cost energy:7.981535146989776 | all_enery_cp：6.394 | all_enery_tp: 1.5875351469897758
ef: 24.905446921731787
reward: 16.92391177474201
step 352:loss:12.422150611877441|running q:66.99482727050781
episode5,iteration52 selected nodes:[4, 16, 17, 0, 10],center node:10
################################################## episode5,iteration52 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.3864580556177176,train_acc:0.8706464171409607
node0 epoch1:node_model train_loss:0.19888208319361395,train_acc:0.9311097860336304
node0 epoch2:node_model train_loss:0.13282002945645496,train_acc:0.9591519832611084
node0 epoch3:node_model train_loss:0.10072152187617925,train_acc:0.9691913723945618
node0 epoch4:node_model train_loss:0.07301356998057319,train_acc:0.9816126823425293
node0_model on test-dataset: loss:0.7961699317395687,acc:0.792400062084198
node0 weight score:6509.9167820562525
node4: train data size:2705
node4 epoch0:node_model train_loss:0.32916773004191263,train_acc:0.8878570795059204
node4 epoch1:node_model train_loss:0.1992738978671176,train_acc:0.9253571629524231
node4 epoch2:node_model train_loss:0.2940981417362179,train_acc:0.9007143974304199
node4 epoch3:node_model train_loss:0.26195425380553516,train_acc:0.9100000858306885
node4 epoch4:node_model train_loss:0.158594503333526,train_acc:0.9474999904632568
node4_model on test-dataset: loss:0.9212482324242592,acc:0.7676999568939209
node4 weight score:2936.233584819814
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3060497883707285,train_acc:0.9008333086967468
node10 epoch1:node_model train_loss:0.13590358532965183,train_acc:0.9558331370353699
node10 epoch2:node_model train_loss:0.07655245307832956,train_acc:0.9773330688476562
node10 epoch3:node_model train_loss:0.06123921265825629,train_acc:0.9863333106040955
node10 epoch4:node_model train_loss:0.042616640031337735,train_acc:0.9893333315849304
node10_model on test-dataset: loss:0.7967906531691551,acc:0.7879001498222351
node10 weight score:2478.6937348532333
node16: train data size:877
node16 epoch0:node_model train_loss:0.672081235382292,train_acc:0.8309090733528137
node16 epoch1:node_model train_loss:0.2872770776351293,train_acc:0.9031168818473816
node16 epoch2:node_model train_loss:0.18289643029371896,train_acc:0.9535641074180603
node16 epoch3:node_model train_loss:0.11189543041918013,train_acc:0.967229425907135
node16 epoch4:node_model train_loss:0.07373028662469652,train_acc:0.9785569310188293
node16_model on test-dataset: loss:0.7935149447619915,acc:0.7867999076843262
node16 weight score:1105.2091782128302
node17: train data size:442
node17 epoch0:node_model train_loss:0.6922023355960846,train_acc:0.8071428537368774
node17 epoch1:node_model train_loss:0.2571547105908394,train_acc:0.9124761819839478
node17 epoch2:node_model train_loss:0.19440376311540603,train_acc:0.9404762387275696
node17 epoch3:node_model train_loss:0.08801808282732963,train_acc:0.977238118648529
node17 epoch4:node_model train_loss:0.10318588465452194,train_acc:0.9704761505126953
node17_model on test-dataset: loss:0.9244430968165398,acc:0.7680999636650085
node17 weight score:478.1256969975698
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6364337892830372,acc:0.8260999816656113
total cost energy:7.412101468648115 | all_enery_cp：5.591 | all_enery_tp: 1.8211014686481148
ef: 24.668676797594813
reward: 17.2565753289467
step 353:loss:10.107908248901367|running q:68.07718658447266
episode5,iteration53 selected nodes:[14, 15, 7, 1, 5],center node:5
################################################## episode5,iteration53 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.333209743942408,train_acc:0.8862499594688416
node1 epoch1:node_model train_loss:0.19166135546915672,train_acc:0.9341175556182861
node1 epoch2:node_model train_loss:0.1328202488229555,train_acc:0.9569116234779358
node1 epoch3:node_model train_loss:0.09476787657203044,train_acc:0.9716176390647888
node1 epoch4:node_model train_loss:0.08431565347949371,train_acc:0.9771325588226318
node1_model on test-dataset: loss:0.8655790389701724,acc:0.7775999903678894
node1 weight score:7749.725557103232
node5: train data size:3735
node5 epoch0:node_model train_loss:0.31943856160107414,train_acc:0.8938345909118652
node5 epoch1:node_model train_loss:0.17472309717222265,train_acc:0.9463531970977783
node5 epoch2:node_model train_loss:0.10929860988337743,train_acc:0.9650749564170837
node5 epoch3:node_model train_loss:0.07122066862096912,train_acc:0.9797742962837219
node5 epoch4:node_model train_loss:0.0617965487028031,train_acc:0.9832330346107483
node5_model on test-dataset: loss:0.7658472467958927,acc:0.796000063419342
node5 weight score:4876.95165795304
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5484584875404834,train_acc:0.8371959924697876
node7 epoch1:node_model train_loss:0.25654901266098024,train_acc:0.9100784659385681
node7 epoch2:node_model train_loss:0.13924405574798585,train_acc:0.9555392265319824
node7 epoch3:node_model train_loss:0.08794316593557597,train_acc:0.9760391116142273
node7 epoch4:node_model train_loss:0.06481902608647942,train_acc:0.9815390706062317
node7_model on test-dataset: loss:0.7721327155828476,acc:0.7953001260757446
node7 weight score:2526.767692426139
node14: train data size:1172
node14 epoch0:node_model train_loss:0.43645013868808746,train_acc:0.8604629039764404
node14 epoch1:node_model train_loss:0.20238851880033812,train_acc:0.941527783870697
node14 epoch2:node_model train_loss:0.15235047973692417,train_acc:0.9530092477798462
node14 epoch3:node_model train_loss:0.06206249042103688,train_acc:0.9843518733978271
node14 epoch4:node_model train_loss:0.04420148596788446,train_acc:0.9871758818626404
node14_model on test-dataset: loss:0.7845137643814087,acc:0.7898998260498047
node14 weight score:1493.9189765830624
node15: train data size:629
node15 epoch0:node_model train_loss:0.6081870666572026,train_acc:0.8020197153091431
node15 epoch1:node_model train_loss:0.27155643488679615,train_acc:0.9201478958129883
node15 epoch2:node_model train_loss:0.1637222979749952,train_acc:0.9537931084632874
node15 epoch3:node_model train_loss:0.10810064790504319,train_acc:0.9693595767021179
node15 epoch4:node_model train_loss:0.06061398983001709,train_acc:0.9900000095367432
node15_model on test-dataset: loss:0.8383536587655545,acc:0.7804999947547913
node15 weight score:750.2800201602028
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6419348162412644,acc:0.8231999826431274
total cost energy:8.786792222699217 | all_enery_cp：7.0975 | all_enery_tp: 1.6892922226992173
ef: 24.809077245165046
reward: 16.02228502246583
step 354:loss:6.074204444885254|running q:69.15718078613281
episode5,iteration54 selected nodes:[18, 9, 10, 4, 11],center node:11
################################################## episode5,iteration54 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.32958689065916197,train_acc:0.8889285922050476
node4 epoch1:node_model train_loss:0.21286860134984767,train_acc:0.9260714650154114
node4 epoch2:node_model train_loss:0.12569491578532116,train_acc:0.9624997973442078
node4 epoch3:node_model train_loss:0.12025411666503974,train_acc:0.9632141590118408
node4 epoch4:node_model train_loss:0.10165306862576731,train_acc:0.9678571224212646
node4_model on test-dataset: loss:0.8318315018713475,acc:0.788300096988678
node4 weight score:3251.8604956828867
node9: train data size:1857
node9 epoch0:node_model train_loss:0.46004783715072434,train_acc:0.8563064932823181
node9 epoch1:node_model train_loss:0.2818861690006758,train_acc:0.9068236351013184
node9 epoch2:node_model train_loss:0.13131603716235413,train_acc:0.9584209322929382
node9 epoch3:node_model train_loss:0.08716249701223876,train_acc:0.973416268825531
node9 epoch4:node_model train_loss:0.05626534464720048,train_acc:0.9848660826683044
node9_model on test-dataset: loss:0.8294587869942188,acc:0.7854999899864197
node9 weight score:2238.8092441932777
node10: train data size:1975
node10 epoch0:node_model train_loss:0.3015538178384304,train_acc:0.8865000009536743
node10 epoch1:node_model train_loss:0.13368328027427195,train_acc:0.961499810218811
node10 epoch2:node_model train_loss:0.08932227976620197,train_acc:0.9738332629203796
node10 epoch3:node_model train_loss:0.05580616472288966,train_acc:0.9850000739097595
node10 epoch4:node_model train_loss:0.030626612436026334,train_acc:0.9959999918937683
node10_model on test-dataset: loss:0.7499047760665417,acc:0.8034999370574951
node10 weight score:2633.6677175993227
node11: train data size:1682
node11 epoch0:node_model train_loss:0.346865036908318,train_acc:0.8803443312644958
node11 epoch1:node_model train_loss:0.1662632792311556,train_acc:0.945953905582428
node11 epoch2:node_model train_loss:0.08667581331203966,train_acc:0.975494921207428
node11 epoch3:node_model train_loss:0.05385659646023722,train_acc:0.9878478646278381
node11 epoch4:node_model train_loss:0.041315541767022186,train_acc:0.9903298616409302
node11_model on test-dataset: loss:0.8181315756589175,acc:0.7963001132011414
node11 weight score:2055.904025761784
node18: train data size:472
node18 epoch0:node_model train_loss:0.5358260154724122,train_acc:0.8510000109672546
node18 epoch1:node_model train_loss:0.23285608887672424,train_acc:0.9288889169692993
node18 epoch2:node_model train_loss:0.12084879279136658,train_acc:0.9676666259765625
node18 epoch3:node_model train_loss:0.08315706998109818,train_acc:0.972444474697113
node18 epoch4:node_model train_loss:0.049559012055397034,train_acc:0.9872221946716309
node18_model on test-dataset: loss:0.9254431587457657,acc:0.7719997763633728
node18 weight score:510.0259216780986
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6319117465615273,acc:0.8235999828577042
total cost energy:5.875284057427984 | all_enery_cp：4.3455 | all_enery_tp: 1.5297840574279835
ef: 24.820675491727766
reward: 18.945391434299783
step 355:loss:7.534696102142334|running q:70.15774536132812
episode5,iteration55 selected nodes:[14, 3, 2, 15, 7],center node:7
################################################## episode5,iteration55 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.16257614774319032,train_acc:0.9386362433433533
node2 epoch1:node_model train_loss:0.0817386278261741,train_acc:0.9740815162658691
node2 epoch2:node_model train_loss:0.05579453330331793,train_acc:0.9854167699813843
node2 epoch3:node_model train_loss:0.037759149117240064,train_acc:0.9920265674591064
node2 epoch4:node_model train_loss:0.027699184933832537,train_acc:0.9960417151451111
node2_model on test-dataset: loss:0.7736051267385483,acc:0.808699905872345
node2 weight score:6189.20407131451
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2251250296831131,train_acc:0.9194161295890808
node3 epoch1:node_model train_loss:0.12484628594545431,train_acc:0.9554921388626099
node3 epoch2:node_model train_loss:0.07815082003037598,train_acc:0.9748537540435791
node3 epoch3:node_model train_loss:0.05014034481935723,train_acc:0.989534854888916
node3 epoch4:node_model train_loss:0.03021910173688517,train_acc:0.9951162338256836
node3_model on test-dataset: loss:0.7652984808385372,acc:0.8060001730918884
node3 weight score:5549.46874498766
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4170741230249405,train_acc:0.8626372218132019
node7 epoch1:node_model train_loss:0.21931084860116243,train_acc:0.9265195727348328
node7 epoch2:node_model train_loss:0.14719868078827858,train_acc:0.9520196318626404
node7 epoch3:node_model train_loss:0.07720278054475785,train_acc:0.9815391898155212
node7 epoch4:node_model train_loss:0.044354640506207944,train_acc:0.9924999475479126
node7_model on test-dataset: loss:0.7710820516943931,acc:0.7964999079704285
node7 weight score:2530.2106250726865
node14: train data size:1172
node14 epoch0:node_model train_loss:0.34064386785030365,train_acc:0.880231499671936
node14 epoch1:node_model train_loss:0.1744401225199302,train_acc:0.9456943869590759
node14 epoch2:node_model train_loss:0.130770248050491,train_acc:0.9563888311386108
node14 epoch3:node_model train_loss:0.06875698051104943,train_acc:0.9785184860229492
node14 epoch4:node_model train_loss:0.043040932311366,train_acc:0.9888424873352051
node14_model on test-dataset: loss:0.7576743578910827,acc:0.797999918460846
node14 weight score:1546.8386752089048
node15: train data size:629
node15 epoch0:node_model train_loss:0.6266456118651799,train_acc:0.8126600980758667
node15 epoch1:node_model train_loss:0.3626490959099361,train_acc:0.8880788087844849
node15 epoch2:node_model train_loss:0.17823192370789392,train_acc:0.9430049061775208
node15 epoch3:node_model train_loss:0.12750650623014995,train_acc:0.9530049562454224
node15 epoch4:node_model train_loss:0.10693368901099477,train_acc:0.960295557975769
node15_model on test-dataset: loss:0.8146797288954258,acc:0.786300003528595
node15 weight score:772.0825469081235
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6457456497848034,acc:0.8241999810934066
total cost energy:8.069729784055031 | all_enery_cp：6.3935 | all_enery_tp: 1.67622978405503
ef: 25.00112307310893
reward: 16.931393289053897
step 356:loss:10.759750366210938|running q:71.09522247314453
episode5,iteration56 selected nodes:[3, 6, 11, 10, 12],center node:10
################################################## episode5,iteration56 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1343576947963515,train_acc:0.9530231356620789
node3 epoch1:node_model train_loss:0.07928803411507329,train_acc:0.9769766330718994
node3 epoch2:node_model train_loss:0.06507516734648583,train_acc:0.9806382656097412
node3 epoch3:node_model train_loss:0.049914646763787714,train_acc:0.9878178834915161
node3 epoch4:node_model train_loss:0.04956300424541845,train_acc:0.986976683139801
node3_model on test-dataset: loss:0.8595941736549139,acc:0.7894001007080078
node3 weight score:4940.7035670590385
node6: train data size:3007
node6 epoch0:node_model train_loss:0.40474798908877757,train_acc:0.8664515614509583
node6 epoch1:node_model train_loss:0.20181574936835997,train_acc:0.9328110814094543
node6 epoch2:node_model train_loss:0.14515813440084457,train_acc:0.9541933536529541
node6 epoch3:node_model train_loss:0.10002485290169716,train_acc:0.9699075818061829
node6 epoch4:node_model train_loss:0.10523883442604734,train_acc:0.9658061861991882
node6_model on test-dataset: loss:0.864597834199667,acc:0.7821999192237854
node6 weight score:3477.917571680586
node10: train data size:1975
node10 epoch0:node_model train_loss:0.27457063272595406,train_acc:0.9065000414848328
node10 epoch1:node_model train_loss:0.11361343730241061,train_acc:0.9620000720024109
node10 epoch2:node_model train_loss:0.06559199905022979,train_acc:0.9860000014305115
node10 epoch3:node_model train_loss:0.043555024266242984,train_acc:0.9934999346733093
node10 epoch4:node_model train_loss:0.03130200016312301,train_acc:0.9926665425300598
node10_model on test-dataset: loss:0.7635332620143891,acc:0.7991999387741089
node10 weight score:2586.6587590296494
node11: train data size:1682
node11 epoch0:node_model train_loss:0.30045893875991597,train_acc:0.8961549401283264
node11 epoch1:node_model train_loss:0.1456653889926041,train_acc:0.9524246454238892
node11 epoch2:node_model train_loss:0.0819739680737257,train_acc:0.9750357270240784
node11 epoch3:node_model train_loss:0.0467797538594288,train_acc:0.9881060123443604
node11 epoch4:node_model train_loss:0.03794609821018051,train_acc:0.9903298616409302
node11_model on test-dataset: loss:0.8174092382192611,acc:0.794299840927124
node11 weight score:2057.720810281351
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5383631054844175,train_acc:0.8474604487419128
node12 epoch1:node_model train_loss:0.3028035174523081,train_acc:0.9015079736709595
node12 epoch2:node_model train_loss:0.16264578061444418,train_acc:0.9470635056495667
node12 epoch3:node_model train_loss:0.09747879127306598,train_acc:0.9742856621742249
node12 epoch4:node_model train_loss:0.0728957796735423,train_acc:0.9803174138069153
node12_model on test-dataset: loss:0.7894491025805473,acc:0.7947998046875
node12 weight score:1692.3193599598628
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6527709735929966,acc:0.823799979686737
total cost energy:7.396843312876074 | all_enery_cp：6.1235 | all_enery_tp: 1.2733433128760745
ef: 25.150834244972796
reward: 17.75399093209672
step 357:loss:9.593852043151855|running q:72.08623504638672
episode5,iteration57 selected nodes:[9, 7, 18, 1, 10],center node:9
################################################## episode5,iteration57 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.24753695025163538,train_acc:0.9131619930267334
node1 epoch1:node_model train_loss:0.1690942595767624,train_acc:0.9390440583229065
node1 epoch2:node_model train_loss:0.12697692560579846,train_acc:0.956985354423523
node1 epoch3:node_model train_loss:0.1030962391592124,train_acc:0.9673529267311096
node1 epoch4:node_model train_loss:0.08081513695309267,train_acc:0.9777942299842834
node1_model on test-dataset: loss:0.8182011903077364,acc:0.7964998483657837
node1 weight score:8198.472551081271
node7: train data size:1951
node7 epoch0:node_model train_loss:0.40327144637703893,train_acc:0.877597987651825
node7 epoch1:node_model train_loss:0.20288649164140224,train_acc:0.9365197420120239
node7 epoch2:node_model train_loss:0.11371729709208012,train_acc:0.9670587778091431
node7 epoch3:node_model train_loss:0.08517039734870195,train_acc:0.9755783081054688
node7 epoch4:node_model train_loss:0.05956104332581162,train_acc:0.9844998717308044
node7_model on test-dataset: loss:0.831637474000454,acc:0.7879999279975891
node7 weight score:2345.974130548782
node9: train data size:1857
node9 epoch0:node_model train_loss:0.40957648267871455,train_acc:0.868005633354187
node9 epoch1:node_model train_loss:0.20490548838126033,train_acc:0.9278854727745056
node9 epoch2:node_model train_loss:0.13055618852376938,train_acc:0.9563064575195312
node9 epoch3:node_model train_loss:0.06730938898889642,train_acc:0.981181800365448
node9 epoch4:node_model train_loss:0.04861097410321236,train_acc:0.9898614287376404
node9_model on test-dataset: loss:0.7576258438825607,acc:0.8030998706817627
node9 weight score:2451.0779496163186
node10: train data size:1975
node10 epoch0:node_model train_loss:0.18933443874120712,train_acc:0.938166618347168
node10 epoch1:node_model train_loss:0.09576939474791288,train_acc:0.966166615486145
node10 epoch2:node_model train_loss:0.06150071593001485,train_acc:0.9851664900779724
node10 epoch3:node_model train_loss:0.03830248429439962,train_acc:0.9921666979789734
node10 epoch4:node_model train_loss:0.0254182152915746,train_acc:0.9954999089241028
node10_model on test-dataset: loss:0.7781640884280204,acc:0.7948000431060791
node10 weight score:2538.025114972503
node18: train data size:472
node18 epoch0:node_model train_loss:0.5233869552612305,train_acc:0.8689999580383301
node18 epoch1:node_model train_loss:0.2321651667356491,train_acc:0.9248889088630676
node18 epoch2:node_model train_loss:0.1656117245554924,train_acc:0.9496666789054871
node18 epoch3:node_model train_loss:0.0720370166003704,train_acc:0.9812221527099609
node18 epoch4:node_model train_loss:0.07348236441612244,train_acc:0.9879999160766602
node18_model on test-dataset: loss:0.9155919744074345,acc:0.7730000615119934
node18 weight score:515.5134745534173
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6540624675154686,acc:0.8216999834775924
total cost energy:8.059851044480267 | all_enery_cp：6.4815 | all_enery_tp: 1.5783510444802673
ef: 24.910795441283216
reward: 16.85094439680295
step 358:loss:9.649449348449707|running q:73.08595275878906
episode5,iteration58 selected nodes:[0, 8, 13, 18, 3],center node:3
################################################## episode5,iteration58 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.31886612767210376,train_acc:0.8871501088142395
node0 epoch1:node_model train_loss:0.1695841634646058,train_acc:0.9426876306533813
node0 epoch2:node_model train_loss:0.10488862293557479,train_acc:0.966149091720581
node0 epoch3:node_model train_loss:0.08719170620091833,train_acc:0.9760750532150269
node0 epoch4:node_model train_loss:0.05947872176049994,train_acc:0.9853453636169434
node0_model on test-dataset: loss:0.8113373444974422,acc:0.7962000370025635
node0 weight score:6388.218211760546
node3: train data size:4247
node3 epoch0:node_model train_loss:0.14791109638158664,train_acc:0.9443294405937195
node3 epoch1:node_model train_loss:0.07461926124470178,train_acc:0.9767143726348877
node3 epoch2:node_model train_loss:0.05548726814950621,train_acc:0.9834882020950317
node3 epoch3:node_model train_loss:0.042967981160726657,train_acc:0.9886046051979065
node3 epoch4:node_model train_loss:0.03224263619631529,train_acc:0.9925581216812134
node3_model on test-dataset: loss:0.8330603018403053,acc:0.7920998334884644
node3 weight score:5098.0703205014015
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5648923036124971,train_acc:0.8353627324104309
node8 epoch1:node_model train_loss:0.3559974216752582,train_acc:0.8926870822906494
node8 epoch2:node_model train_loss:0.1562139079388645,train_acc:0.9504874348640442
node8 epoch3:node_model train_loss:0.10210583296914895,train_acc:0.9682992100715637
node8 epoch4:node_model train_loss:0.07005559642695719,train_acc:0.983877420425415
node8_model on test-dataset: loss:0.7490413253009319,acc:0.7989000678062439
node8 weight score:2400.401605716003
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5672570765018463,train_acc:0.830833375453949
node13 epoch1:node_model train_loss:0.33527511606613797,train_acc:0.8875758051872253
node13 epoch2:node_model train_loss:0.19204821810126305,train_acc:0.9399242401123047
node13 epoch3:node_model train_loss:0.1068837580581506,train_acc:0.9654545187950134
node13 epoch4:node_model train_loss:0.05961658448601762,train_acc:0.9836363196372986
node13_model on test-dataset: loss:0.8436412553489209,acc:0.7833998799324036
node13 weight score:1369.0653375199206
node18: train data size:472
node18 epoch0:node_model train_loss:0.576238477230072,train_acc:0.8378888964653015
node18 epoch1:node_model train_loss:0.2730073660612106,train_acc:0.908111035823822
node18 epoch2:node_model train_loss:0.10709741860628127,train_acc:0.9664444327354431
node18 epoch3:node_model train_loss:0.07601439282298088,train_acc:0.9739999771118164
node18 epoch4:node_model train_loss:0.03482253421097994,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.8918863655626774,acc:0.7820998430252075
node18 weight score:529.2154003298643
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6644458530843258,acc:0.8239999800920487
total cost energy:8.681337496509089 | all_enery_cp：6.427499999999999 | all_enery_tp: 2.2538374965090897
ef: 24.64417706238849
reward: 15.962839565879401
step 359:loss:7.145014762878418|running q:74.07820129394531
episode5,iteration59 selected nodes:[16, 13, 7, 15, 8],center node:16
################################################## episode5,iteration59 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4008139245212078,train_acc:0.8751177191734314
node7 epoch1:node_model train_loss:0.20904719401150942,train_acc:0.9299999475479126
node7 epoch2:node_model train_loss:0.09889781968668103,train_acc:0.967499852180481
node7 epoch3:node_model train_loss:0.06319775683805347,train_acc:0.9815195202827454
node7 epoch4:node_model train_loss:0.05113092008978128,train_acc:0.988019585609436
node7_model on test-dataset: loss:0.7846505457907915,acc:0.7986000776290894
node7 weight score:2486.4572011910486
node8: train data size:1798
node8 epoch0:node_model train_loss:0.44109486871295506,train_acc:0.8681859374046326
node8 epoch1:node_model train_loss:0.20871931686997414,train_acc:0.9221088886260986
node8 epoch2:node_model train_loss:0.1216362344308032,train_acc:0.9605100750923157
node8 epoch3:node_model train_loss:0.0852451543841097,train_acc:0.9760883450508118
node8 epoch4:node_model train_loss:0.0791828939691186,train_acc:0.9799885153770447
node8_model on test-dataset: loss:0.8179129640758037,acc:0.7904999852180481
node8 weight score:2198.2779084956064
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5236436302463213,train_acc:0.8453788161277771
node13 epoch1:node_model train_loss:0.2115606435885032,train_acc:0.9231060743331909
node13 epoch2:node_model train_loss:0.15279737922052541,train_acc:0.9529545307159424
node13 epoch3:node_model train_loss:0.08685905300080776,train_acc:0.9774999022483826
node13 epoch4:node_model train_loss:0.04928355865801374,train_acc:0.988484799861908
node13_model on test-dataset: loss:0.8170191740989685,acc:0.7893999218940735
node13 weight score:1413.6755129079636
node15: train data size:629
node15 epoch0:node_model train_loss:0.6433026322296688,train_acc:0.795024573802948
node15 epoch1:node_model train_loss:0.28320438521248953,train_acc:0.9079310297966003
node15 epoch2:node_model train_loss:0.18191529703991755,train_acc:0.9344335198402405
node15 epoch3:node_model train_loss:0.11164178273507527,train_acc:0.9636452794075012
node15 epoch4:node_model train_loss:0.07314837723970413,train_acc:0.973004937171936
node15_model on test-dataset: loss:0.8630780391395092,acc:0.7854000926017761
node15 weight score:728.7869363784467
node16: train data size:877
node16 epoch0:node_model train_loss:0.5990431739224328,train_acc:0.8323520421981812
node16 epoch1:node_model train_loss:0.28491683138741386,train_acc:0.9074603319168091
node16 epoch2:node_model train_loss:0.1990901819533772,train_acc:0.9357864856719971
node16 epoch3:node_model train_loss:0.10129846922225422,train_acc:0.9748916625976562
node16 epoch4:node_model train_loss:0.057992970363961324,train_acc:0.9830014109611511
node16_model on test-dataset: loss:0.8247402742505073,acc:0.7861999273300171
node16 weight score:1063.365070654497
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6295217652618885,acc:0.8250999820232391
total cost energy:5.2452107363437666 | all_enery_cp：3.205 | all_enery_tp: 2.0402107363437665
ef: 24.52013964785386
reward: 19.274928911510095
step 360:loss:7.718479156494141|running q:74.99954986572266
episode5_cost time: 25743.766147375107
episode6,iteration0 selected nodes:[10, 13, 15, 2, 16],center node:16
################################################## episode6,iteration0 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:1.9083593438069026,train_acc:0.3191193640232086
node2 epoch1:node_model train_loss:1.4971510395407677,train_acc:0.4634753167629242
node2 epoch2:node_model train_loss:1.3504182050625484,train_acc:0.509128749370575
node2 epoch3:node_model train_loss:1.2401933297514915,train_acc:0.5653314590454102
node2 epoch4:node_model train_loss:1.0987622911731403,train_acc:0.608115553855896
node2_model on test-dataset: loss:1.324765704870224,acc:0.5400998592376709
node2 weight score:3614.223996286981
node10: train data size:1975
node10 epoch0:node_model train_loss:2.3033069729804994,train_acc:0.22983334958553314
node10 epoch1:node_model train_loss:1.722696101665497,train_acc:0.3871666491031647
node10 epoch2:node_model train_loss:1.5354314625263215,train_acc:0.4573333263397217
node10 epoch3:node_model train_loss:1.4177126824855804,train_acc:0.4871666133403778
node10 epoch4:node_model train_loss:1.3004109978675842,train_acc:0.543666660785675
node10_model on test-dataset: loss:1.5623572570085527,acc:0.4474999010562897
node10 weight score:1264.1154839204542
node13: train data size:1155
node13 epoch0:node_model train_loss:2.4148186445236206,train_acc:0.2030303180217743
node13 epoch1:node_model train_loss:1.907175342241923,train_acc:0.3171211779117584
node13 epoch2:node_model train_loss:1.6305822332700093,train_acc:0.40098485350608826
node13 epoch3:node_model train_loss:1.5046288470427196,train_acc:0.4480303227901459
node13 epoch4:node_model train_loss:1.3784784177939098,train_acc:0.5125757455825806
node13_model on test-dataset: loss:1.6470622932910919,acc:0.3887999355792999
node13 weight score:701.2485227210967
node15: train data size:629
node15 epoch0:node_model train_loss:2.6657496861049106,train_acc:0.17305418848991394
node15 epoch1:node_model train_loss:1.9820688962936401,train_acc:0.27433496713638306
node15 epoch2:node_model train_loss:1.7908167498452323,train_acc:0.37004929780960083
node15 epoch3:node_model train_loss:1.626243965966361,train_acc:0.4008867144584656
node15 epoch4:node_model train_loss:1.5218961068562098,train_acc:0.4475369453430176
node15_model on test-dataset: loss:2.087216722369194,acc:0.26339998841285706
node15 weight score:301.3582601456086
node16: train data size:877
node16 epoch0:node_model train_loss:2.4880692429012723,train_acc:0.18519480526447296
node16 epoch1:node_model train_loss:1.8833503193325467,train_acc:0.331414133310318
node16 epoch2:node_model train_loss:1.680216285917494,train_acc:0.4230591654777527
node16 epoch3:node_model train_loss:1.5473381413353815,train_acc:0.43916308879852295
node16 epoch4:node_model train_loss:1.4189507961273193,train_acc:0.483174592256546
node16_model on test-dataset: loss:1.8534729409217834,acc:0.3310999274253845
node16 weight score:473.16579629365594
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.4382950508594512,acc:0.4656999906152487
total cost energy:6.887452158590582 | all_enery_cp：4.712000000000001 | all_enery_tp: 2.1754521585905815
ef: 23.00563129790902
reward: 16.118179139318435
step 361:loss:13.669740676879883|running q:1.6022578477859497
episode6,iteration1 selected nodes:[4, 9, 2, 19, 7],center node:7
################################################## episode6,iteration1 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:1.2736244661112626,train_acc:0.5593940019607544
node2 epoch1:node_model train_loss:1.0717711262404919,train_acc:0.6145927906036377
node2 epoch2:node_model train_loss:0.9602505179742972,train_acc:0.6726988554000854
node2 epoch3:node_model train_loss:0.9011451502641042,train_acc:0.6848484873771667
node2 epoch4:node_model train_loss:0.7920224318901697,train_acc:0.7214111089706421
node2_model on test-dataset: loss:1.158327897787094,acc:0.6033000349998474
node2 weight score:4133.544576753392
node4: train data size:2705
node4 epoch0:node_model train_loss:1.5840887001582555,train_acc:0.4571428596973419
node4 epoch1:node_model train_loss:1.3091375955513544,train_acc:0.5414286255836487
node4 epoch2:node_model train_loss:1.088761887380055,train_acc:0.6157142519950867
node4 epoch3:node_model train_loss:1.0871130802801676,train_acc:0.6282142996788025
node4 epoch4:node_model train_loss:0.9775995250259127,train_acc:0.6549999713897705
node4_model on test-dataset: loss:1.416568382382393,acc:0.5279998183250427
node4 weight score:1909.544243427709
node7: train data size:1951
node7 epoch0:node_model train_loss:1.5685112714767455,train_acc:0.45297059416770935
node7 epoch1:node_model train_loss:1.2184355676174163,train_acc:0.5783922076225281
node7 epoch2:node_model train_loss:1.0831041246652604,train_acc:0.6220489740371704
node7 epoch3:node_model train_loss:0.9814680367708206,train_acc:0.6598921418190002
node7 epoch4:node_model train_loss:0.8432669132947922,train_acc:0.7052940726280212
node7_model on test-dataset: loss:1.288777306675911,acc:0.5575999617576599
node7 weight score:1513.8379531465619
node9: train data size:1857
node9 epoch0:node_model train_loss:1.6287959500362998,train_acc:0.46401655673980713
node9 epoch1:node_model train_loss:1.2398974958219027,train_acc:0.5671929717063904
node9 epoch2:node_model train_loss:1.0533436787755865,train_acc:0.6195475459098816
node9 epoch3:node_model train_loss:0.9558648216096979,train_acc:0.665743350982666
node9 epoch4:node_model train_loss:0.9323530040289226,train_acc:0.6790488958358765
node9_model on test-dataset: loss:1.4562283587455749,acc:0.5202999711036682
node9 weight score:1275.212083906715
node19: train data size:4281
node19 epoch0:node_model train_loss:1.4802263614743254,train_acc:0.48319265246391296
node19 epoch1:node_model train_loss:1.1444229733112246,train_acc:0.591045081615448
node19 epoch2:node_model train_loss:1.03535476950712,train_acc:0.6423168182373047
node19 epoch3:node_model train_loss:0.970935677373132,train_acc:0.6567355990409851
node19 epoch4:node_model train_loss:0.8677092291588007,train_acc:0.6949152946472168
node19_model on test-dataset: loss:1.123253006339073,acc:0.6207999587059021
node19 weight score:3811.251762372499
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9644516387581825,acc:0.6636999821662903
total cost energy:9.443145126328584 | all_enery_cp：7.791 | all_enery_tp: 1.6521451263285831
ef: 24.102986297719823
reward: 14.659841171391239
step 362:loss:11.604500770568848|running q:3.2583932876586914
episode6,iteration2 selected nodes:[18, 13, 12, 19, 15],center node:19
################################################## episode6,iteration2 ##################################################
node12: train data size:1336
node12 epoch0:node_model train_loss:1.3802125198500497,train_acc:0.5350000262260437
node12 epoch1:node_model train_loss:0.9375838509627751,train_acc:0.6670635342597961
node12 epoch2:node_model train_loss:0.7854945191315242,train_acc:0.7207935452461243
node12 epoch3:node_model train_loss:0.7123500108718872,train_acc:0.757460355758667
node12 epoch4:node_model train_loss:0.6592928213732583,train_acc:0.7646825909614563
node12_model on test-dataset: loss:1.3432070022821427,acc:0.5664000511169434
node12 weight score:994.6344813048935
node13: train data size:1155
node13 epoch0:node_model train_loss:1.4307513932387035,train_acc:0.5487878322601318
node13 epoch1:node_model train_loss:1.0288555920124054,train_acc:0.6385605931282043
node13 epoch2:node_model train_loss:0.8405207693576813,train_acc:0.7184091806411743
node13 epoch3:node_model train_loss:0.7339117477337519,train_acc:0.7568939924240112
node13 epoch4:node_model train_loss:0.6039055859049162,train_acc:0.8021969795227051
node13_model on test-dataset: loss:1.2746934661269187,acc:0.5672999620437622
node13 weight score:906.1001963941965
node15: train data size:629
node15 epoch0:node_model train_loss:1.494157348360334,train_acc:0.5059606432914734
node15 epoch1:node_model train_loss:1.1261858684676034,train_acc:0.5958127975463867
node15 epoch2:node_model train_loss:0.8967166883604867,train_acc:0.7004433274269104
node15 epoch3:node_model train_loss:0.7803810238838196,train_acc:0.7073891162872314
node15 epoch4:node_model train_loss:0.7313709131308964,train_acc:0.7407389879226685
node15_model on test-dataset: loss:1.3548845255374908,acc:0.5569000244140625
node15 weight score:464.24620559488045
node18: train data size:472
node18 epoch0:node_model train_loss:1.2400145292282105,train_acc:0.6309999823570251
node18 epoch1:node_model train_loss:1.0520406484603881,train_acc:0.6581110954284668
node18 epoch2:node_model train_loss:0.7211732864379883,train_acc:0.7512222528457642
node18 epoch3:node_model train_loss:0.6102177500724792,train_acc:0.8038889169692993
node18 epoch4:node_model train_loss:0.5281558573246002,train_acc:0.8258889317512512
node18_model on test-dataset: loss:1.2694904243946075,acc:0.5877000689506531
node18 weight score:371.80272566851903
node19: train data size:4281
node19 epoch0:node_model train_loss:1.0748588165571524,train_acc:0.6241085529327393
node19 epoch1:node_model train_loss:0.9054088911344839,train_acc:0.6809215545654297
node19 epoch2:node_model train_loss:0.8195779919624329,train_acc:0.7099626660346985
node19 epoch3:node_model train_loss:0.7372504237086274,train_acc:0.7349152565002441
node19 epoch4:node_model train_loss:0.7140698134899139,train_acc:0.7509071826934814
node19_model on test-dataset: loss:1.524145639836788,acc:0.5642001032829285
node19 weight score:2808.786698663802
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9945068125426769,acc:0.6626999823749066
total cost energy:6.143443362991556 | all_enery_cp：3.9364999999999997 | all_enery_tp: 2.2069433629915567
ef: 22.897987338876682
reward: 16.754543975885127
step 363:loss:13.632172584533691|running q:4.9842209815979
episode6,iteration3 selected nodes:[11, 18, 12, 19, 1],center node:11
################################################## episode6,iteration3 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:1.1058123357155745,train_acc:0.6365441679954529
node1 epoch1:node_model train_loss:0.8955375915941071,train_acc:0.6897791028022766
node1 epoch2:node_model train_loss:0.8295207882628721,train_acc:0.7174264192581177
node1 epoch3:node_model train_loss:0.7928047416841283,train_acc:0.7306617498397827
node1 epoch4:node_model train_loss:0.7731906096724903,train_acc:0.7355880737304688
node1_model on test-dataset: loss:0.9638190576434136,acc:0.672700047492981
node1 weight score:6959.812577685899
node11: train data size:1682
node11 epoch0:node_model train_loss:1.2183004477444817,train_acc:0.6100574135780334
node11 epoch1:node_model train_loss:0.9028431457631728,train_acc:0.6748206615447998
node11 epoch2:node_model train_loss:0.712009526350919,train_acc:0.7463700771331787
node11 epoch3:node_model train_loss:0.6359019139233757,train_acc:0.7759110331535339
node11 epoch4:node_model train_loss:0.5506349784486434,train_acc:0.8255093097686768
node11_model on test-dataset: loss:1.0483595058321953,acc:0.6536999344825745
node11 weight score:1604.4114548900059
node12: train data size:1336
node12 epoch0:node_model train_loss:1.1532054543495178,train_acc:0.6134126782417297
node12 epoch1:node_model train_loss:0.8243937364646367,train_acc:0.7103174328804016
node12 epoch2:node_model train_loss:0.7041743568011692,train_acc:0.7457936406135559
node12 epoch3:node_model train_loss:0.5359884543078286,train_acc:0.8242064714431763
node12 epoch4:node_model train_loss:0.4680786303111485,train_acc:0.851190447807312
node12_model on test-dataset: loss:1.1390830501914024,acc:0.6365000009536743
node12 weight score:1172.8732156759852
node18: train data size:472
node18 epoch0:node_model train_loss:1.1568689703941346,train_acc:0.6485555768013
node18 epoch1:node_model train_loss:0.8636234998703003,train_acc:0.7206666469573975
node18 epoch2:node_model train_loss:0.7117074608802796,train_acc:0.7427777647972107
node18 epoch3:node_model train_loss:0.5373971939086915,train_acc:0.8043333292007446
node18 epoch4:node_model train_loss:0.43440609574317934,train_acc:0.8648888468742371
node18_model on test-dataset: loss:1.3327972015738487,acc:0.5908000469207764
node18 weight score:354.14240024111206
node19: train data size:4281
node19 epoch0:node_model train_loss:0.922574988631315,train_acc:0.6811943650245667
node19 epoch1:node_model train_loss:0.6814300355523132,train_acc:0.7590180039405823
node19 epoch2:node_model train_loss:0.6969948972380439,train_acc:0.750427782535553
node19 epoch3:node_model train_loss:0.6441439944644307,train_acc:0.7706316709518433
node19 epoch4:node_model train_loss:0.5977451246838237,train_acc:0.790645956993103
node19_model on test-dataset: loss:1.0644315230846404,acc:0.6536999344825745
node19 weight score:4021.8651056049075
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8023707351088524,acc:0.7257999819517136
total cost energy:9.305185424949238 | all_enery_cp：7.2395 | all_enery_tp: 2.0656854249492382
ef: 24.126539341393386
reward: 14.821353916444147
step 364:loss:9.551247596740723|running q:6.586581230163574
episode6,iteration4 selected nodes:[13, 18, 9, 6, 16],center node:9
################################################## episode6,iteration4 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:1.0427140228209957,train_acc:0.6577879786491394
node6 epoch1:node_model train_loss:0.8201902739463314,train_acc:0.713686466217041
node6 epoch2:node_model train_loss:0.7410101525245174,train_acc:0.7486174702644348
node6 epoch3:node_model train_loss:0.6943626759513732,train_acc:0.7483410239219666
node6 epoch4:node_model train_loss:0.7309054684254431,train_acc:0.7540091276168823
node6_model on test-dataset: loss:0.9929932552576065,acc:0.6721001267433167
node6 weight score:3028.217950201395
node9: train data size:1857
node9 epoch0:node_model train_loss:0.9988620218477751,train_acc:0.6736472249031067
node9 epoch1:node_model train_loss:0.7113369420955056,train_acc:0.7626038789749146
node9 epoch2:node_model train_loss:0.6138550199960408,train_acc:0.8012927174568176
node9 epoch3:node_model train_loss:0.551041138799567,train_acc:0.8026039004325867
node9 epoch4:node_model train_loss:0.48852642900065374,train_acc:0.8359002470970154
node9_model on test-dataset: loss:0.9972727918624877,acc:0.6752001047134399
node9 weight score:1862.0782750243313
node13: train data size:1155
node13 epoch0:node_model train_loss:1.034377450744311,train_acc:0.6690151691436768
node13 epoch1:node_model train_loss:0.7275899400313696,train_acc:0.7371212840080261
node13 epoch2:node_model train_loss:0.629516897102197,train_acc:0.7822727560997009
node13 epoch3:node_model train_loss:0.5180338571468989,train_acc:0.8293182253837585
node13 epoch4:node_model train_loss:0.4331086625655492,train_acc:0.8556818962097168
node13_model on test-dataset: loss:1.0754267978668213,acc:0.6498998403549194
node13 weight score:1073.9922068996395
node16: train data size:877
node16 epoch0:node_model train_loss:1.0715674294365778,train_acc:0.6550360918045044
node16 epoch1:node_model train_loss:0.8326339589224921,train_acc:0.7187012434005737
node16 epoch2:node_model train_loss:0.6317666305436028,train_acc:0.7906926274299622
node16 epoch3:node_model train_loss:0.5631329086091783,train_acc:0.808571457862854
node16 epoch4:node_model train_loss:0.489966223637263,train_acc:0.8456854224205017
node16_model on test-dataset: loss:1.0636126166582107,acc:0.6508997678756714
node16 weight score:824.5483235761783
node18: train data size:472
node18 epoch0:node_model train_loss:1.0820034503936768,train_acc:0.6548888683319092
node18 epoch1:node_model train_loss:0.7083841979503631,train_acc:0.7634444236755371
node18 epoch2:node_model train_loss:0.6033130943775177,train_acc:0.7966666221618652
node18 epoch3:node_model train_loss:0.4115875482559204,train_acc:0.8709999322891235
node18 epoch4:node_model train_loss:0.32645761370658877,train_acc:0.902999997138977
node18_model on test-dataset: loss:1.0975419414043426,acc:0.6436998844146729
node18 weight score:430.051902523251
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.796735227406025,acc:0.7278999802470207
total cost energy:5.5505274696415015 | all_enery_cp：3.6839999999999997 | all_enery_tp: 1.8665274696415022
ef: 23.927141418382
reward: 18.3766139487405
step 365:loss:12.59923267364502|running q:8.170732498168945
episode6,iteration5 selected nodes:[19, 1, 8, 15, 5],center node:5
################################################## episode6,iteration5 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.8538696047137765,train_acc:0.7084558606147766
node1 epoch1:node_model train_loss:0.7045684943304342,train_acc:0.7563970685005188
node1 epoch2:node_model train_loss:0.6799336702508085,train_acc:0.7623529434204102
node1 epoch3:node_model train_loss:0.6311676664387479,train_acc:0.7780147194862366
node1 epoch4:node_model train_loss:0.5842513161547044,train_acc:0.7973530888557434
node1_model on test-dataset: loss:1.099837575852871,acc:0.6542001366615295
node1 weight score:6099.082398415302
node5: train data size:3735
node5 epoch0:node_model train_loss:0.9954380659680617,train_acc:0.6730450987815857
node5 epoch1:node_model train_loss:0.7874084460107904,train_acc:0.7276315689086914
node5 epoch2:node_model train_loss:0.6561482807523326,train_acc:0.7723307609558105
node5 epoch3:node_model train_loss:0.6242500198514838,train_acc:0.7897744178771973
node5 epoch4:node_model train_loss:0.5540241379486887,train_acc:0.8073306679725647
node5_model on test-dataset: loss:0.9762821531295777,acc:0.6744000315666199
node5 weight score:3825.738274562384
node8: train data size:1798
node8 epoch0:node_model train_loss:1.1064664158556197,train_acc:0.6513264775276184
node8 epoch1:node_model train_loss:0.7665028605196211,train_acc:0.7492063641548157
node8 epoch2:node_model train_loss:0.6211173567507002,train_acc:0.7892516255378723
node8 epoch3:node_model train_loss:0.49209318723943496,train_acc:0.8408957719802856
node8 epoch4:node_model train_loss:0.43469439612494576,train_acc:0.863152027130127
node8_model on test-dataset: loss:1.0276631891727448,acc:0.6588000059127808
node8 weight score:1749.6004711887813
node15: train data size:629
node15 epoch0:node_model train_loss:1.1497036388942175,train_acc:0.6172413229942322
node15 epoch1:node_model train_loss:0.7844823343413216,train_acc:0.7474384903907776
node15 epoch2:node_model train_loss:0.6264310053416661,train_acc:0.7825123071670532
node15 epoch3:node_model train_loss:0.4939133184296744,train_acc:0.8502956628799438
node15 epoch4:node_model train_loss:0.47341515336717876,train_acc:0.8445813655853271
node15_model on test-dataset: loss:1.0240729606151582,acc:0.6604000926017761
node15 weight score:614.2140493799985
node19: train data size:4281
node19 epoch0:node_model train_loss:0.831266719241475,train_acc:0.7155841588973999
node19 epoch1:node_model train_loss:0.6021861288436624,train_acc:0.794421374797821
node19 epoch2:node_model train_loss:0.5529661532058272,train_acc:0.8042579293251038
node19 epoch3:node_model train_loss:0.5443993411784949,train_acc:0.8083060383796692
node19 epoch4:node_model train_loss:0.5055621217849643,train_acc:0.8276772499084473
node19_model on test-dataset: loss:1.0306428197026252,acc:0.6696000695228577
node19 weight score:4153.718357282314
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7726170685887337,acc:0.7370999836921692
total cost energy:10.905332572579834 | all_enery_cp：8.5755 | all_enery_tp: 2.329832572579834
ef: 24.2202660324213
reward: 13.314933459841466
step 366:loss:11.781567573547363|running q:9.778446197509766
episode6,iteration6 selected nodes:[17, 13, 5, 0, 6],center node:6
################################################## episode6,iteration6 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.9463009031919333,train_acc:0.6879401803016663
node0 epoch1:node_model train_loss:0.7455149161127897,train_acc:0.747018039226532
node0 epoch2:node_model train_loss:0.6840765625238419,train_acc:0.7594786882400513
node0 epoch3:node_model train_loss:0.5944337134177868,train_acc:0.7943651676177979
node0 epoch4:node_model train_loss:0.5575316811983402,train_acc:0.8047938346862793
node0_model on test-dataset: loss:0.9638614788651466,acc:0.6902000904083252
node0 weight score:5377.3287071317345
node5: train data size:3735
node5 epoch0:node_model train_loss:0.8813778020833668,train_acc:0.7088720202445984
node5 epoch1:node_model train_loss:0.6906929212181192,train_acc:0.7550751566886902
node5 epoch2:node_model train_loss:0.54823678575064,train_acc:0.8121052384376526
node5 epoch3:node_model train_loss:0.4958000190948185,train_acc:0.8344359993934631
node5 epoch4:node_model train_loss:0.47364294607388346,train_acc:0.8373684287071228
node5_model on test-dataset: loss:0.9520855082571507,acc:0.6937000155448914
node5 weight score:3922.9669684156206
node6: train data size:3007
node6 epoch0:node_model train_loss:0.8148648873452218,train_acc:0.7389400005340576
node6 epoch1:node_model train_loss:0.6750092064180682,train_acc:0.7636865973472595
node6 epoch2:node_model train_loss:0.6347184286963555,train_acc:0.7876497507095337
node6 epoch3:node_model train_loss:0.5748440161828072,train_acc:0.8085253238677979
node6 epoch4:node_model train_loss:0.5335636811871682,train_acc:0.8186174631118774
node6_model on test-dataset: loss:0.9483521094918251,acc:0.6910001039505005
node6 weight score:3170.763232246409
node13: train data size:1155
node13 epoch0:node_model train_loss:0.9548810670773188,train_acc:0.6976515054702759
node13 epoch1:node_model train_loss:0.7318814744551977,train_acc:0.7471212148666382
node13 epoch2:node_model train_loss:0.5053572952747345,train_acc:0.8225001096725464
node13 epoch3:node_model train_loss:0.42036902407805127,train_acc:0.8603788614273071
node13 epoch4:node_model train_loss:0.3395997869471709,train_acc:0.8968939185142517
node13_model on test-dataset: loss:1.0474116468429566,acc:0.6642998456954956
node13 weight score:1102.7183089679493
node17: train data size:442
node17 epoch0:node_model train_loss:1.0447198510169984,train_acc:0.6793333888053894
node17 epoch1:node_model train_loss:0.7294889032840729,train_acc:0.7626667022705078
node17 epoch2:node_model train_loss:0.5151582539081574,train_acc:0.8106666803359985
node17 epoch3:node_model train_loss:0.3779554426670074,train_acc:0.8761904835700989
node17 epoch4:node_model train_loss:0.29195323288440705,train_acc:0.9097143411636353
node17_model on test-dataset: loss:0.9966494968533516,acc:0.6776999235153198
node17 weight score:443.48590090648133
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7140269374847412,acc:0.7582999807596207
total cost energy:8.645732446804296 | all_enery_cp：6.761 | all_enery_tp: 1.8847324468042947
ef: 24.31235216750412
reward: 15.666619720699824
step 367:loss:6.418807506561279|running q:11.379363059997559
episode6,iteration7 selected nodes:[10, 19, 9, 18, 11],center node:11
################################################## episode6,iteration7 ##################################################
node9: train data size:1857
node9 epoch0:node_model train_loss:0.8641462859354521,train_acc:0.7236565351486206
node9 epoch1:node_model train_loss:0.5935371376966175,train_acc:0.7949769496917725
node9 epoch2:node_model train_loss:0.47335725081594365,train_acc:0.8419576287269592
node9 epoch3:node_model train_loss:0.3929899593717174,train_acc:0.8624653220176697
node9 epoch4:node_model train_loss:0.35481441177819906,train_acc:0.8920776844024658
node9_model on test-dataset: loss:0.9120713302493095,acc:0.6999996900558472
node9 weight score:2036.0249669205145
node10: train data size:1975
node10 epoch0:node_model train_loss:0.9536929905414582,train_acc:0.7066666483879089
node10 epoch1:node_model train_loss:0.7173962622880936,train_acc:0.7496666312217712
node10 epoch2:node_model train_loss:0.543833202123642,train_acc:0.8033334016799927
node10 epoch3:node_model train_loss:0.4740253075957298,train_acc:0.8358333706855774
node10 epoch4:node_model train_loss:0.3768568605184555,train_acc:0.8833333253860474
node10_model on test-dataset: loss:0.9298294687271118,acc:0.6946999430656433
node10 weight score:2124.0453937254456
node11: train data size:1682
node11 epoch0:node_model train_loss:0.960244736250709,train_acc:0.6923242211341858
node11 epoch1:node_model train_loss:0.6293448104577906,train_acc:0.7976757287979126
node11 epoch2:node_model train_loss:0.499149574952967,train_acc:0.8440029621124268
node11 epoch3:node_model train_loss:0.4042813637677361,train_acc:0.8736873269081116
node11 epoch4:node_model train_loss:0.31580214991289024,train_acc:0.9101434350013733
node11_model on test-dataset: loss:0.8798591378331184,acc:0.7201998829841614
node11 weight score:1911.669638554146
node18: train data size:472
node18 epoch0:node_model train_loss:0.8678173780441284,train_acc:0.7272222638130188
node18 epoch1:node_model train_loss:0.5672444820404052,train_acc:0.8129999041557312
node18 epoch2:node_model train_loss:0.5536300659179687,train_acc:0.8337777256965637
node18 epoch3:node_model train_loss:0.346462619304657,train_acc:0.8706666827201843
node18 epoch4:node_model train_loss:0.2173431009054184,train_acc:0.9448888897895813
node18_model on test-dataset: loss:1.038598199337721,acc:0.6752001047134399
node18 weight score:454.45871204184493
node19: train data size:4281
node19 epoch0:node_model train_loss:0.7196057501227356,train_acc:0.7559258937835693
node19 epoch1:node_model train_loss:0.5448438975700113,train_acc:0.8040798902511597
node19 epoch2:node_model train_loss:0.48939853629400565,train_acc:0.8230115175247192
node19 epoch3:node_model train_loss:0.4153216696062753,train_acc:0.8566235303878784
node19 epoch4:node_model train_loss:0.3576763511397118,train_acc:0.8799340128898621
node19_model on test-dataset: loss:0.9201690818369389,acc:0.7101999521255493
node19 weight score:4652.405828995921
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7088183265924454,acc:0.763899984061718
total cost energy:6.739019988716055 | all_enery_cp：5.1335 | all_enery_tp: 1.605519988716055
ef: 24.564050670295448
reward: 17.825030681579392
step 368:loss:8.823654174804688|running q:12.942022323608398
episode6,iteration8 selected nodes:[17, 8, 13, 5, 0],center node:8
################################################## episode6,iteration8 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.8131319754398786,train_acc:0.7308293581008911
node0 epoch1:node_model train_loss:0.6145533180007567,train_acc:0.7882112860679626
node0 epoch2:node_model train_loss:0.555406456383375,train_acc:0.8040547370910645
node0 epoch3:node_model train_loss:0.5192428821554551,train_acc:0.8180189728736877
node0 epoch4:node_model train_loss:0.43867428600788116,train_acc:0.8479888439178467
node0_model on test-dataset: loss:0.8554377657175064,acc:0.7246001362800598
node0 weight score:6058.886113886626
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7441473320910805,train_acc:0.7516164779663086
node5 epoch1:node_model train_loss:0.5478781783267072,train_acc:0.8113532662391663
node5 epoch2:node_model train_loss:0.4729180932044983,train_acc:0.8373308181762695
node5 epoch3:node_model train_loss:0.4154428031883742,train_acc:0.8592481017112732
node5 epoch4:node_model train_loss:0.3796213037873569,train_acc:0.8790601491928101
node5_model on test-dataset: loss:0.929189965724945,acc:0.707599937915802
node5 weight score:4019.6301485950603
node8: train data size:1798
node8 epoch0:node_model train_loss:0.9188111590014564,train_acc:0.7068594098091125
node8 epoch1:node_model train_loss:0.6657513909869723,train_acc:0.7781292200088501
node8 epoch2:node_model train_loss:0.5233333905537924,train_acc:0.8226531147956848
node8 epoch3:node_model train_loss:0.4068654552102089,train_acc:0.8786168098449707
node8 epoch4:node_model train_loss:0.3487633499834273,train_acc:0.8943424224853516
node8_model on test-dataset: loss:0.8302134464681149,acc:0.7257000803947449
node8 weight score:2165.7081171703885
node13: train data size:1155
node13 epoch0:node_model train_loss:0.9280004501342773,train_acc:0.6932575702667236
node13 epoch1:node_model train_loss:0.6001341914137205,train_acc:0.7905303239822388
node13 epoch2:node_model train_loss:0.4775637040535609,train_acc:0.8388636112213135
node13 epoch3:node_model train_loss:0.47732754548390705,train_acc:0.8415910005569458
node13 epoch4:node_model train_loss:0.32832176486651105,train_acc:0.8932575583457947
node13_model on test-dataset: loss:0.9282795694470406,acc:0.7054998874664307
node13 weight score:1244.237229833694
node17: train data size:442
node17 epoch0:node_model train_loss:1.0483678579330444,train_acc:0.6960952281951904
node17 epoch1:node_model train_loss:0.7651579737663269,train_acc:0.7418095469474792
node17 epoch2:node_model train_loss:0.5647853970527649,train_acc:0.8028571009635925
node17 epoch3:node_model train_loss:0.3846722781658173,train_acc:0.8814286589622498
node17 epoch4:node_model train_loss:0.26281532645225525,train_acc:0.9209524393081665
node17_model on test-dataset: loss:0.9514538460969925,acc:0.7024000287055969
node17 weight score:464.5522237501596
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6891190496087074,acc:0.7693999835848808
total cost energy:8.635865371853821 | all_enery_cp：6.1565 | all_enery_tp: 2.4793653718538207
ef: 24.46873849366674
reward: 15.83287312181292
step 369:loss:5.612939834594727|running q:14.469074249267578
episode6,iteration9 selected nodes:[6, 8, 10, 9, 0],center node:6
################################################## episode6,iteration9 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5880757020070002,train_acc:0.7967215776443481
node0 epoch1:node_model train_loss:0.4660483283492235,train_acc:0.8365247845649719
node0 epoch2:node_model train_loss:0.42353975486296874,train_acc:0.8566381335258484
node0 epoch3:node_model train_loss:0.39045300306035924,train_acc:0.8658780455589294
node0 epoch4:node_model train_loss:0.34449353728156823,train_acc:0.885030210018158
node0_model on test-dataset: loss:0.9134943150728941,acc:0.7241001129150391
node0 weight score:5673.817466052224
node6: train data size:3007
node6 epoch0:node_model train_loss:0.8144564715123945,train_acc:0.7412903308868408
node6 epoch1:node_model train_loss:0.5472554213577702,train_acc:0.818709671497345
node6 epoch2:node_model train_loss:0.46650368167508033,train_acc:0.8376497030258179
node6 epoch3:node_model train_loss:0.4850481450557709,train_acc:0.8282949328422546
node6 epoch4:node_model train_loss:0.4116851293271588,train_acc:0.8564515709877014
node6_model on test-dataset: loss:0.9948882347345352,acc:0.6924997568130493
node6 weight score:3022.450055208819
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7870679464605119,train_acc:0.7313038110733032
node8 epoch1:node_model train_loss:0.522813038693534,train_acc:0.8325851559638977
node8 epoch2:node_model train_loss:0.398534980085161,train_acc:0.8682311773300171
node8 epoch3:node_model train_loss:0.351087995701366,train_acc:0.8921087384223938
node8 epoch4:node_model train_loss:0.3045648742053244,train_acc:0.9088094830513
node8_model on test-dataset: loss:0.8134095460176468,acc:0.732999861240387
node8 weight score:2210.4486095630264
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7583120057457372,train_acc:0.7645891308784485
node9 epoch1:node_model train_loss:0.49795221027575043,train_acc:0.8241827487945557
node9 epoch2:node_model train_loss:0.36596376017520305,train_acc:0.8875993490219116
node9 epoch3:node_model train_loss:0.30886699180853994,train_acc:0.9082733988761902
node9 epoch4:node_model train_loss:0.27559071112620204,train_acc:0.9195936322212219
node9_model on test-dataset: loss:0.8984536868333817,acc:0.7147998809814453
node9 weight score:2066.88450079718
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8466154724359513,train_acc:0.7266666293144226
node10 epoch1:node_model train_loss:0.5436651051044464,train_acc:0.8171667456626892
node10 epoch2:node_model train_loss:0.44629725962877276,train_acc:0.8560000658035278
node10 epoch3:node_model train_loss:0.3719897627830505,train_acc:0.8786666989326477
node10 epoch4:node_model train_loss:0.32187688574194906,train_acc:0.8944999575614929
node10_model on test-dataset: loss:0.9588081063330174,acc:0.7018998265266418
node10 weight score:2059.8490844569833
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6902545686811209,acc:0.7695999819040299
total cost energy:8.402744663833897 | all_enery_cp：6.909999999999999 | all_enery_tp: 1.4927446638338977
ef: 24.812196936676163
reward: 16.409452272842266
step 370:loss:4.5531325340271|running q:15.983953475952148
episode6,iteration10 selected nodes:[9, 2, 14, 7, 18],center node:9
################################################## episode6,iteration10 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.7652846810718378,train_acc:0.7551798224449158
node2 epoch1:node_model train_loss:0.6225021872669458,train_acc:0.7865151166915894
node2 epoch2:node_model train_loss:0.5508628369619449,train_acc:0.815018892288208
node2 epoch3:node_model train_loss:0.4366488438099623,train_acc:0.8479639291763306
node2 epoch4:node_model train_loss:0.38690319284796715,train_acc:0.8737026453018188
node2_model on test-dataset: loss:0.8521951930224896,acc:0.7285999655723572
node2 weight score:5618.4311284582
node7: train data size:1951
node7 epoch0:node_model train_loss:0.8787942558526993,train_acc:0.7277156114578247
node7 epoch1:node_model train_loss:0.5749910101294518,train_acc:0.8092156648635864
node7 epoch2:node_model train_loss:0.41777273267507553,train_acc:0.864715576171875
node7 epoch3:node_model train_loss:0.33426342308521273,train_acc:0.8926372528076172
node7 epoch4:node_model train_loss:0.2657073840498924,train_acc:0.9191372990608215
node7_model on test-dataset: loss:0.8995272189378738,acc:0.7139999866485596
node7 weight score:2168.9171366083438
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6913936200894808,train_acc:0.7732686400413513
node9 epoch1:node_model train_loss:0.468022335516779,train_acc:0.8498523235321045
node9 epoch2:node_model train_loss:0.3399040134329545,train_acc:0.8913019895553589
node9 epoch3:node_model train_loss:0.2568384754030328,train_acc:0.9215697050094604
node9 epoch4:node_model train_loss:0.19451019167900085,train_acc:0.9486702680587769
node9_model on test-dataset: loss:0.8386643013358116,acc:0.7357001304626465
node9 weight score:2214.235179728288
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7970219403505325,train_acc:0.7464815378189087
node14 epoch1:node_model train_loss:0.4644773254791896,train_acc:0.8387500643730164
node14 epoch2:node_model train_loss:0.3703638290365537,train_acc:0.8696295619010925
node14 epoch3:node_model train_loss:0.26515765984853107,train_acc:0.9283795356750488
node14 epoch4:node_model train_loss:0.21223152180512747,train_acc:0.9512037038803101
node14_model on test-dataset: loss:0.8999439413845539,acc:0.7227001190185547
node14 weight score:1302.3033392467655
node18: train data size:472
node18 epoch0:node_model train_loss:0.7908583998680114,train_acc:0.7591111063957214
node18 epoch1:node_model train_loss:0.3858192920684814,train_acc:0.8677777647972107
node18 epoch2:node_model train_loss:0.31943829655647277,train_acc:0.8953332901000977
node18 epoch3:node_model train_loss:0.215906497836113,train_acc:0.9493333101272583
node18 epoch4:node_model train_loss:0.1432918503880501,train_acc:0.9644444584846497
node18_model on test-dataset: loss:0.9621133941411972,acc:0.7153998613357544
node18 weight score:490.5866635619569
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7008604780584574,acc:0.7689999827742576
total cost energy:6.5255211184844475 | all_enery_cp：5.12 | all_enery_tp: 1.4055211184844474
ef: 24.65565476200976
reward: 18.130133643525312
step 371:loss:8.418427467346191|running q:17.478906631469727
episode6,iteration11 selected nodes:[7, 6, 2, 4, 16],center node:7
################################################## episode6,iteration11 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6137702266375223,train_acc:0.7915056943893433
node2 epoch1:node_model train_loss:0.4281392755607764,train_acc:0.858939528465271
node2 epoch2:node_model train_loss:0.3859531565879782,train_acc:0.8677271604537964
node2 epoch3:node_model train_loss:0.33586957926551503,train_acc:0.8851703405380249
node2 epoch4:node_model train_loss:0.2989396397024393,train_acc:0.9048862457275391
node2_model on test-dataset: loss:0.8546308149397374,acc:0.731499969959259
node2 weight score:5602.419098751567
node4: train data size:2705
node4 epoch0:node_model train_loss:0.8815696026597705,train_acc:0.7260714769363403
node4 epoch1:node_model train_loss:0.6872983681304115,train_acc:0.7700000405311584
node4 epoch2:node_model train_loss:0.6017120799848011,train_acc:0.802142858505249
node4 epoch3:node_model train_loss:0.5551582723855972,train_acc:0.8182142376899719
node4 epoch4:node_model train_loss:0.4616839215159416,train_acc:0.8407142162322998
node4_model on test-dataset: loss:0.9242900079488754,acc:0.7138999700546265
node4 weight score:2926.5706398825637
node6: train data size:3007
node6 epoch0:node_model train_loss:0.706230652909125,train_acc:0.7682026028633118
node6 epoch1:node_model train_loss:0.5000769774759969,train_acc:0.8290321826934814
node6 epoch2:node_model train_loss:0.3692125735744353,train_acc:0.8757142424583435
node6 epoch3:node_model train_loss:0.37217383855773556,train_acc:0.8748846650123596
node6 epoch4:node_model train_loss:0.41754042381240475,train_acc:0.8582949638366699
node6_model on test-dataset: loss:0.8812489545345307,acc:0.7180001735687256
node6 weight score:3412.2026296057006
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7375427633523941,train_acc:0.7558136582374573
node7 epoch1:node_model train_loss:0.4917582467198372,train_acc:0.8231765031814575
node7 epoch2:node_model train_loss:0.3811856657266617,train_acc:0.8701569437980652
node7 epoch3:node_model train_loss:0.299492933601141,train_acc:0.908098042011261
node7 epoch4:node_model train_loss:0.2436845064163208,train_acc:0.9285784959793091
node7_model on test-dataset: loss:0.9293934491276741,acc:0.7106000185012817
node7 weight score:2099.2185837238285
node16: train data size:877
node16 epoch0:node_model train_loss:0.8301905228032006,train_acc:0.7441413998603821
node16 epoch1:node_model train_loss:0.5456257826752133,train_acc:0.8156853914260864
node16 epoch2:node_model train_loss:0.39024465448326534,train_acc:0.881010115146637
node16 epoch3:node_model train_loss:0.2640748984283871,train_acc:0.9243434071540833
node16 epoch4:node_model train_loss:0.20004532734553018,train_acc:0.9438961148262024
node16_model on test-dataset: loss:0.8577653270959854,acc:0.7221999764442444
node16 weight score:1022.4241669561706
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6506957507133484,acc:0.7858999812602997
total cost energy:8.102440279937884 | all_enery_cp：6.664000000000001 | all_enery_tp: 1.4384402799378826
ef: 24.85095161542835
reward: 16.748511335490466
step 372:loss:5.277008056640625|running q:19.005216598510742
episode6,iteration12 selected nodes:[8, 7, 10, 9, 4],center node:10
################################################## episode6,iteration12 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7507389198456492,train_acc:0.7542857527732849
node4 epoch1:node_model train_loss:0.5410426972167832,train_acc:0.8217856884002686
node4 epoch2:node_model train_loss:0.4342454576066562,train_acc:0.8450000286102295
node4 epoch3:node_model train_loss:0.4384729138442448,train_acc:0.8471429347991943
node4 epoch4:node_model train_loss:0.39567929399865015,train_acc:0.8607142567634583
node4_model on test-dataset: loss:0.8732382182776928,acc:0.7356998920440674
node4 weight score:3097.665612179838
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6406514689326286,train_acc:0.7791961431503296
node7 epoch1:node_model train_loss:0.4194261595606804,train_acc:0.8521960377693176
node7 epoch2:node_model train_loss:0.32112298905849457,train_acc:0.8895784616470337
node7 epoch3:node_model train_loss:0.26479617357254026,train_acc:0.9181371927261353
node7 epoch4:node_model train_loss:0.21303796134889125,train_acc:0.9365588426589966
node7_model on test-dataset: loss:0.850230909883976,acc:0.7343001961708069
node7 weight score:2294.670750403837
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7699226902590858,train_acc:0.7524716258049011
node8 epoch1:node_model train_loss:0.5569101207786136,train_acc:0.8125737309455872
node8 epoch2:node_model train_loss:0.3988802714480294,train_acc:0.8582766652107239
node8 epoch3:node_model train_loss:0.3051540048585998,train_acc:0.898219883441925
node8 epoch4:node_model train_loss:0.23943350629674065,train_acc:0.9343423843383789
node8_model on test-dataset: loss:0.8794723020493984,acc:0.7280998826026917
node8 weight score:2044.4077611201558
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7005441424093748,train_acc:0.7691781520843506
node9 epoch1:node_model train_loss:0.47185078576991435,train_acc:0.8302400708198547
node9 epoch2:node_model train_loss:0.3163607136199349,train_acc:0.8942012786865234
node9 epoch3:node_model train_loss:0.21882456384207072,train_acc:0.9455124735832214
node9 epoch4:node_model train_loss:0.1641215627130709,train_acc:0.9613111615180969
node9_model on test-dataset: loss:0.8545655059814453,acc:0.7318000793457031
node9 weight score:2173.034117340467
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8191233813762665,train_acc:0.7393333315849304
node10 epoch1:node_model train_loss:0.511057460308075,train_acc:0.828833281993866
node10 epoch2:node_model train_loss:0.38025597035884856,train_acc:0.8729999661445618
node10 epoch3:node_model train_loss:0.30410471111536025,train_acc:0.9054999351501465
node10 epoch4:node_model train_loss:0.26325439363718034,train_acc:0.9246665239334106
node10_model on test-dataset: loss:0.8312403750419617,acc:0.7356000542640686
node10 weight score:2375.9673607051395
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6285909661650657,acc:0.7933999812602996
total cost energy:6.300649122254148 | all_enery_cp：5.143000000000001 | all_enery_tp: 1.1576491222541474
ef: 25.033143994025618
reward: 18.732494871771472
step 373:loss:3.893510103225708|running q:20.481979370117188
episode6,iteration13 selected nodes:[16, 1, 4, 8, 11],center node:11
################################################## episode6,iteration13 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.7020867883282549,train_acc:0.7677207589149475
node1 epoch1:node_model train_loss:0.5603223017909947,train_acc:0.8055146932601929
node1 epoch2:node_model train_loss:0.46472976707360325,train_acc:0.8402938842773438
node1 epoch3:node_model train_loss:0.4157220963936518,train_acc:0.8574998378753662
node1 epoch4:node_model train_loss:0.34103735174764604,train_acc:0.8870587348937988
node1_model on test-dataset: loss:0.7898779182136059,acc:0.7487000226974487
node1 weight score:8492.451612232515
node4: train data size:2705
node4 epoch0:node_model train_loss:0.6178441452128547,train_acc:0.7942855954170227
node4 epoch1:node_model train_loss:0.4730711983782904,train_acc:0.8275001645088196
node4 epoch2:node_model train_loss:0.37749204465321134,train_acc:0.8746428489685059
node4 epoch3:node_model train_loss:0.4224319489938872,train_acc:0.8478572368621826
node4 epoch4:node_model train_loss:0.2749350911804608,train_acc:0.9167858362197876
node4_model on test-dataset: loss:0.8048113423585892,acc:0.7481001019477844
node4 weight score:3361.036130619999
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6640643460883034,train_acc:0.7748185396194458
node8 epoch1:node_model train_loss:0.43671933975484634,train_acc:0.8548526167869568
node8 epoch2:node_model train_loss:0.34103936867581475,train_acc:0.8814738392829895
node8 epoch3:node_model train_loss:0.23541296190685695,train_acc:0.930476188659668
node8 epoch4:node_model train_loss:0.17968561376134554,train_acc:0.960453450679779
node8_model on test-dataset: loss:0.8449596910178662,acc:0.737899899482727
node8 weight score:2127.9121585481435
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7722266456660103,train_acc:0.7601004838943481
node11 epoch1:node_model train_loss:0.46661096811294556,train_acc:0.8308751583099365
node11 epoch2:node_model train_loss:0.3461608913014917,train_acc:0.8837302923202515
node11 epoch3:node_model train_loss:0.2743324725066914,train_acc:0.9120372533798218
node11 epoch4:node_model train_loss:0.23997321374276104,train_acc:0.9275896549224854
node11_model on test-dataset: loss:0.8392450304329395,acc:0.7370999455451965
node11 weight score:2004.1822578708752
node16: train data size:877
node16 epoch0:node_model train_loss:0.7378746933407254,train_acc:0.761688232421875
node16 epoch1:node_model train_loss:0.5363530980216132,train_acc:0.8241270184516907
node16 epoch2:node_model train_loss:0.34171397652890945,train_acc:0.880793571472168
node16 epoch3:node_model train_loss:0.2543151179949443,train_acc:0.925007164478302
node16 epoch4:node_model train_loss:0.2021484656466378,train_acc:0.9567820429801941
node16_model on test-dataset: loss:0.8772537781298161,acc:0.7228999733924866
node16 weight score:999.7107129816446
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6307748158276081,acc:0.7931999808549881
total cost energy:8.572705430228725 | all_enery_cp：6.885 | all_enery_tp: 1.6877054302287244
ef: 24.959550782891977
reward: 16.38684535266325
step 374:loss:4.625030040740967|running q:21.92500114440918
episode6,iteration14 selected nodes:[13, 15, 10, 7, 3],center node:10
################################################## episode6,iteration14 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7869649529457092,train_acc:0.7474071383476257
node3 epoch1:node_model train_loss:0.541849625665088,train_acc:0.820371150970459
node3 epoch2:node_model train_loss:0.49001587961995324,train_acc:0.8341810703277588
node3 epoch3:node_model train_loss:0.4015539842982625,train_acc:0.8600247502326965
node3 epoch4:node_model train_loss:0.34912435467853103,train_acc:0.8836613893508911
node3_model on test-dataset: loss:0.7531569950282574,acc:0.7612999677658081
node3 weight score:5638.930565652727
node7: train data size:1951
node7 epoch0:node_model train_loss:0.668666222691536,train_acc:0.7907745838165283
node7 epoch1:node_model train_loss:0.4008648008108139,train_acc:0.8675785064697266
node7 epoch2:node_model train_loss:0.26874904334545135,train_acc:0.9180195927619934
node7 epoch3:node_model train_loss:0.21147897765040397,train_acc:0.9380783438682556
node7 epoch4:node_model train_loss:0.1668596275150776,train_acc:0.9540783166885376
node7_model on test-dataset: loss:0.7703233188390732,acc:0.7528999447822571
node7 weight score:2532.7027655611964
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6965024948120118,train_acc:0.7788333296775818
node10 epoch1:node_model train_loss:0.4537964537739754,train_acc:0.8353332877159119
node10 epoch2:node_model train_loss:0.32296568155288696,train_acc:0.891166627407074
node10 epoch3:node_model train_loss:0.24431453943252562,train_acc:0.9278332591056824
node10 epoch4:node_model train_loss:0.1938391849398613,train_acc:0.9469999670982361
node10_model on test-dataset: loss:0.7914121761918068,acc:0.7506001591682434
node10 weight score:2495.539062215968
node13: train data size:1155
node13 epoch0:node_model train_loss:0.833873858054479,train_acc:0.7371970415115356
node13 epoch1:node_model train_loss:0.4959024141232173,train_acc:0.8374242782592773
node13 epoch2:node_model train_loss:0.37490441525975865,train_acc:0.888106107711792
node13 epoch3:node_model train_loss:0.27358846987287205,train_acc:0.9096211791038513
node13 epoch4:node_model train_loss:0.20602097238103548,train_acc:0.9461363554000854
node13_model on test-dataset: loss:0.81131282761693,acc:0.7414999008178711
node13 weight score:1423.6185607869443
node15: train data size:629
node15 epoch0:node_model train_loss:0.9186245288167681,train_acc:0.719950795173645
node15 epoch1:node_model train_loss:0.5667745981897626,train_acc:0.8152216076850891
node15 epoch2:node_model train_loss:0.3137976420777185,train_acc:0.9093595743179321
node15 epoch3:node_model train_loss:0.25589260884693693,train_acc:0.9258621335029602
node15 epoch4:node_model train_loss:0.16823352979762213,train_acc:0.9614285230636597
node15_model on test-dataset: loss:0.8507473227381707,acc:0.7353000640869141
node15 weight score:739.3499611324472
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5985768760740757,acc:0.8016999816894531
total cost energy:6.607403051071291 | all_enery_cp：4.9785 | all_enery_tp: 1.628903051071291
ef: 24.911577876779464
reward: 18.304174825708174
step 375:loss:4.446852684020996|running q:23.40499496459961
episode6,iteration15 selected nodes:[15, 12, 17, 1, 6],center node:6
################################################## episode6,iteration15 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.535067984724746,train_acc:0.8179410696029663
node1 epoch1:node_model train_loss:0.4340710788965225,train_acc:0.8460294604301453
node1 epoch2:node_model train_loss:0.37429411095731396,train_acc:0.8687499165534973
node1 epoch3:node_model train_loss:0.36554957882446404,train_acc:0.8756615519523621
node1 epoch4:node_model train_loss:0.3726772611193797,train_acc:0.8765438795089722
node1_model on test-dataset: loss:0.7814901641011238,acc:0.7594999670982361
node1 weight score:8583.60131469549
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6411478394462217,train_acc:0.7871428728103638
node6 epoch1:node_model train_loss:0.4965366514459733,train_acc:0.8316128849983215
node6 epoch2:node_model train_loss:0.36291671231869727,train_acc:0.8763593435287476
node6 epoch3:node_model train_loss:0.373345120299247,train_acc:0.874746561050415
node6 epoch4:node_model train_loss:0.28893074729750234,train_acc:0.8992624878883362
node6_model on test-dataset: loss:0.8960659117996692,acc:0.7265999913215637
node6 weight score:3355.7799269037096
node12: train data size:1336
node12 epoch0:node_model train_loss:0.820191924061094,train_acc:0.7509523034095764
node12 epoch1:node_model train_loss:0.4486011862754822,train_acc:0.8501587510108948
node12 epoch2:node_model train_loss:0.33928730870996204,train_acc:0.8856350183486938
node12 epoch3:node_model train_loss:0.2819267692310469,train_acc:0.9096032381057739
node12 epoch4:node_model train_loss:0.1977700548512595,train_acc:0.947460412979126
node12_model on test-dataset: loss:0.8594997158646583,acc:0.7400001287460327
node12 weight score:1554.392602277921
node15: train data size:629
node15 epoch0:node_model train_loss:0.8285280891827175,train_acc:0.7490147352218628
node15 epoch1:node_model train_loss:0.5542921338762555,train_acc:0.8018718957901001
node15 epoch2:node_model train_loss:0.46561541727610994,train_acc:0.843448281288147
node15 epoch3:node_model train_loss:0.2836433989661081,train_acc:0.9123645424842834
node15 epoch4:node_model train_loss:0.18329571187496185,train_acc:0.9487192034721375
node15_model on test-dataset: loss:0.9161805658042431,acc:0.7150996923446655
node15 weight score:686.5458878707499
node17: train data size:442
node17 epoch0:node_model train_loss:0.7903494596481323,train_acc:0.7616190314292908
node17 epoch1:node_model train_loss:0.5450991988182068,train_acc:0.8214285969734192
node17 epoch2:node_model train_loss:0.3957809090614319,train_acc:0.8814285397529602
node17 epoch3:node_model train_loss:0.2465811550617218,train_acc:0.9312381148338318
node17 epoch4:node_model train_loss:0.17458676546812057,train_acc:0.948476254940033
node17_model on test-dataset: loss:0.9022569152712822,acc:0.7271997928619385
node17 weight score:489.8826404307509
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6452456119656563,acc:0.7921999853849411
total cost energy:8.191446691134766 | all_enery_cp：6.061 | all_enery_tp: 2.1304466911347664
ef: 24.279381128994864
reward: 16.0879344378601
step 376:loss:4.32866907119751|running q:24.820484161376953
episode6,iteration16 selected nodes:[9, 5, 11, 15, 16],center node:9
################################################## episode6,iteration16 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6744501873066551,train_acc:0.7803006768226624
node5 epoch1:node_model train_loss:0.456030273908063,train_acc:0.8432331085205078
node5 epoch2:node_model train_loss:0.3889062169351076,train_acc:0.8696240782737732
node5 epoch3:node_model train_loss:0.31926078976769195,train_acc:0.8974058032035828
node5 epoch4:node_model train_loss:0.263030638035975,train_acc:0.9130451083183289
node5_model on test-dataset: loss:0.888832894563675,acc:0.7333999276161194
node5 weight score:4202.1397079745775
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6722417316938701,train_acc:0.7835087776184082
node9 epoch1:node_model train_loss:0.43494140003856857,train_acc:0.8551154136657715
node9 epoch2:node_model train_loss:0.24640609559259916,train_acc:0.9191966652870178
node9 epoch3:node_model train_loss:0.20667895400210431,train_acc:0.9373683929443359
node9 epoch4:node_model train_loss:0.15858254385621925,train_acc:0.9652631878852844
node9_model on test-dataset: loss:0.7941761179268361,acc:0.752000093460083
node9 weight score:2338.2722774988774
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7076945848324719,train_acc:0.7734863758087158
node11 epoch1:node_model train_loss:0.5002698424984428,train_acc:0.8233571648597717
node11 epoch2:node_model train_loss:0.34738752421210795,train_acc:0.8838163018226624
node11 epoch3:node_model train_loss:0.26584488942342643,train_acc:0.902108907699585
node11 epoch4:node_model train_loss:0.20122085292549693,train_acc:0.9417072534561157
node11_model on test-dataset: loss:0.8137411411106587,acc:0.7505000233650208
node11 weight score:2066.996388684825
node15: train data size:629
node15 epoch0:node_model train_loss:0.7465743039335523,train_acc:0.7425123453140259
node15 epoch1:node_model train_loss:0.4615726683820997,train_acc:0.8431527614593506
node15 epoch2:node_model train_loss:0.28670836772237507,train_acc:0.8915764093399048
node15 epoch3:node_model train_loss:0.2003025859594345,train_acc:0.942364513874054
node15 epoch4:node_model train_loss:0.17506860303027288,train_acc:0.9395073652267456
node15_model on test-dataset: loss:0.9551394449174404,acc:0.7133001685142517
node15 weight score:658.5425859512785
node16: train data size:877
node16 epoch0:node_model train_loss:0.7750034729639689,train_acc:0.7466955184936523
node16 epoch1:node_model train_loss:0.45803435643513996,train_acc:0.8407936692237854
node16 epoch2:node_model train_loss:0.34836528367466396,train_acc:0.8784559369087219
node16 epoch3:node_model train_loss:0.24775959882471296,train_acc:0.9234488010406494
node16 epoch4:node_model train_loss:0.147627597881688,train_acc:0.969336211681366
node16_model on test-dataset: loss:0.8075533729791641,acc:0.7476000189781189
node16 weight score:1085.9963308241024
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.624261396676302,acc:0.7959999829530716
total cost energy:5.784427190999915 | all_enery_cp：4.39 | all_enery_tp: 1.394427190999916
ef: 24.71135625873168
reward: 18.926929067731763
step 377:loss:5.008368968963623|running q:26.260282516479492
episode6,iteration17 selected nodes:[2, 7, 6, 9, 8],center node:6
################################################## episode6,iteration17 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.6008377807835737,train_acc:0.8016570806503296
node2 epoch1:node_model train_loss:0.4179881525536378,train_acc:0.8571875095367432
node2 epoch2:node_model train_loss:0.3186303259183963,train_acc:0.8950095176696777
node2 epoch3:node_model train_loss:0.2701690088336666,train_acc:0.9183427691459656
node2 epoch4:node_model train_loss:0.2221349406366547,train_acc:0.9276893138885498
node2_model on test-dataset: loss:0.8411839275062084,acc:0.746799886226654
node2 weight score:5691.977513401386
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5576819915684962,train_acc:0.8116130232810974
node6 epoch1:node_model train_loss:0.3843539621560804,train_acc:0.8735482692718506
node6 epoch2:node_model train_loss:0.36364665531343027,train_acc:0.880046010017395
node6 epoch3:node_model train_loss:0.3302301730840437,train_acc:0.8805530071258545
node6 epoch4:node_model train_loss:0.26508540275596804,train_acc:0.9112902879714966
node6_model on test-dataset: loss:0.9957184484414756,acc:0.7199996709823608
node6 weight score:3019.929985937927
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6302028760313988,train_acc:0.7892352342605591
node7 epoch1:node_model train_loss:0.3861601181328297,train_acc:0.8651764988899231
node7 epoch2:node_model train_loss:0.26965449899435046,train_acc:0.9126176238059998
node7 epoch3:node_model train_loss:0.19321017041802407,train_acc:0.9495195746421814
node7 epoch4:node_model train_loss:0.14001490473747252,train_acc:0.9645391702651978
node7_model on test-dataset: loss:0.7903926829993725,acc:0.7538000345230103
node7 weight score:2468.393296097288
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7251798974143134,train_acc:0.7651813626289368
node8 epoch1:node_model train_loss:0.43498999377091724,train_acc:0.8565306067466736
node8 epoch2:node_model train_loss:0.2745930610431565,train_acc:0.9137641191482544
node8 epoch3:node_model train_loss:0.2128695332341724,train_acc:0.9388321042060852
node8 epoch4:node_model train_loss:0.17238853375116983,train_acc:0.9521315097808838
node8_model on test-dataset: loss:0.7404630477726459,acc:0.7596999406814575
node8 weight score:2428.210300849562
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5069493918042434,train_acc:0.8327423930168152
node9 epoch1:node_model train_loss:0.30958786920497294,train_acc:0.889723002910614
node9 epoch2:node_model train_loss:0.19215422084456996,train_acc:0.9513111114501953
node9 epoch3:node_model train_loss:0.17493677256922974,train_acc:0.9447276592254639
node9 epoch4:node_model train_loss:0.12383687770680378,train_acc:0.9686795473098755
node9_model on test-dataset: loss:0.8609393006563186,acc:0.7391000390052795
node9 weight score:2156.9464869176677
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6043954204022884,acc:0.8045999783277512
total cost energy:7.8241067977499785 | all_enery_cp：6.7005 | all_enery_tp: 1.123606797749979
ef: 25.118871126439366
reward: 17.29476432868939
step 378:loss:4.909804344177246|running q:27.623071670532227
episode6,iteration18 selected nodes:[15, 13, 4, 18, 14],center node:14
################################################## episode6,iteration18 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7080547000680651,train_acc:0.7692857384681702
node4 epoch1:node_model train_loss:0.45887848309108187,train_acc:0.8424999713897705
node4 epoch2:node_model train_loss:0.3565864861011505,train_acc:0.8674999475479126
node4 epoch3:node_model train_loss:0.3328439625246184,train_acc:0.8807142972946167
node4 epoch4:node_model train_loss:0.3468950039574078,train_acc:0.8874998688697815
node4_model on test-dataset: loss:0.8680182860791683,acc:0.7425999045372009
node4 weight score:3116.2937963190425
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7500551044940948,train_acc:0.7784848213195801
node13 epoch1:node_model train_loss:0.42663028091192245,train_acc:0.8653788566589355
node13 epoch2:node_model train_loss:0.3141653910279274,train_acc:0.8994696140289307
node13 epoch3:node_model train_loss:0.21672973657647768,train_acc:0.9369696378707886
node13 epoch4:node_model train_loss:0.14418601617217064,train_acc:0.9584848284721375
node13_model on test-dataset: loss:0.7948423191905022,acc:0.7582001090049744
node13 weight score:1453.1184011141934
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6807955404122671,train_acc:0.7799999713897705
node14 epoch1:node_model train_loss:0.4035037234425545,train_acc:0.8695833683013916
node14 epoch2:node_model train_loss:0.26658249894777936,train_acc:0.9122222661972046
node14 epoch3:node_model train_loss:0.1971158105880022,train_acc:0.9478703737258911
node14 epoch4:node_model train_loss:0.15273176257809004,train_acc:0.9550461769104004
node14_model on test-dataset: loss:0.8156615170836449,acc:0.7539999485015869
node14 weight score:1436.8705344717314
node15: train data size:629
node15 epoch0:node_model train_loss:0.8062960079738072,train_acc:0.7596551775932312
node15 epoch1:node_model train_loss:0.4542472745691027,train_acc:0.833940863609314
node15 epoch2:node_model train_loss:0.34230049167360577,train_acc:0.8804434537887573
node15 epoch3:node_model train_loss:0.19316526821681432,train_acc:0.9358620643615723
node15 epoch4:node_model train_loss:0.17683551141193934,train_acc:0.9565024971961975
node15_model on test-dataset: loss:0.9084165999293328,acc:0.7347000241279602
node15 weight score:692.4135909107463
node18: train data size:472
node18 epoch0:node_model train_loss:0.6864261507987977,train_acc:0.7963333129882812
node18 epoch1:node_model train_loss:0.43508129119873046,train_acc:0.8546666502952576
node18 epoch2:node_model train_loss:0.25735015273094175,train_acc:0.9113332629203796
node18 epoch3:node_model train_loss:0.20540634989738465,train_acc:0.9441110491752625
node18 epoch4:node_model train_loss:0.13301232904195787,train_acc:0.9601110816001892
node18_model on test-dataset: loss:0.9806162375211716,acc:0.7299001216888428
node18 weight score:481.3299861249845
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6354614105075598,acc:0.7991999858617782
total cost energy:5.019021298905533 | all_enery_cp：3.0665 | all_enery_tp: 1.9525212989055327
ef: 24.25787445507715
reward: 19.23885315617162
step 379:loss:5.886932373046875|running q:29.058046340942383
episode6,iteration19 selected nodes:[12, 18, 14, 15, 13],center node:14
################################################## episode6,iteration19 ##################################################
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8066177304301944,train_acc:0.7504761815071106
node12 epoch1:node_model train_loss:0.44000672016824993,train_acc:0.8483334183692932
node12 epoch2:node_model train_loss:0.31216028864894596,train_acc:0.8981745839118958
node12 epoch3:node_model train_loss:0.21135188745600836,train_acc:0.9383333921432495
node12 epoch4:node_model train_loss:0.16538175408329284,train_acc:0.9534921646118164
node12_model on test-dataset: loss:0.7966286233067512,acc:0.7618999481201172
node12 weight score:1677.0675329921676
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6350669910510381,train_acc:0.7728030681610107
node13 epoch1:node_model train_loss:0.33883413920799893,train_acc:0.8887879848480225
node13 epoch2:node_model train_loss:0.25568531081080437,train_acc:0.9240908622741699
node13 epoch3:node_model train_loss:0.15925486696263155,train_acc:0.9583332538604736
node13 epoch4:node_model train_loss:0.1454787664115429,train_acc:0.9641666412353516
node13_model on test-dataset: loss:0.8127888268232346,acc:0.7503998875617981
node13 weight score:1421.0333137997104
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5533169036110243,train_acc:0.8039814829826355
node14 epoch1:node_model train_loss:0.32258238022526103,train_acc:0.8910648226737976
node14 epoch2:node_model train_loss:0.2326482410232226,train_acc:0.9080554842948914
node14 epoch3:node_model train_loss:0.16728500835597515,train_acc:0.9492129683494568
node14 epoch4:node_model train_loss:0.13189435998598734,train_acc:0.9651851654052734
node14_model on test-dataset: loss:0.7673497512936592,acc:0.761900007724762
node14 weight score:1527.334827468373
node15: train data size:629
node15 epoch0:node_model train_loss:0.6926167224134717,train_acc:0.7674384713172913
node15 epoch1:node_model train_loss:0.38009693367140635,train_acc:0.8744335770606995
node15 epoch2:node_model train_loss:0.22854859488351004,train_acc:0.9293596148490906
node15 epoch3:node_model train_loss:0.17910178112132208,train_acc:0.9500000476837158
node15 epoch4:node_model train_loss:0.10714966750570706,train_acc:0.9785714745521545
node15_model on test-dataset: loss:0.8778628706932068,acc:0.7433001399040222
node15 weight score:716.5128187997158
node18: train data size:472
node18 epoch0:node_model train_loss:0.7001308083534241,train_acc:0.7978888750076294
node18 epoch1:node_model train_loss:0.3780437558889389,train_acc:0.8661110997200012
node18 epoch2:node_model train_loss:0.208048215508461,train_acc:0.9436666369438171
node18 epoch3:node_model train_loss:0.13072070181369783,train_acc:0.9636666178703308
node18 epoch4:node_model train_loss:0.08060942962765694,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.8469200131297111,acc:0.7388998866081238
node18 weight score:557.3135510823148
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6017537131905556,acc:0.8058999782800674
total cost energy:4.289942590134453 | all_enery_cp：2.382 | all_enery_tp: 1.907942590134453
ef: 24.274757018846525
reward: 19.98481442871207
step 380:loss:7.394943714141846|running q:30.40303611755371
episode6,iteration20 selected nodes:[9, 18, 4, 16, 11],center node:11
################################################## episode6,iteration20 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5117931248886245,train_acc:0.8246428966522217
node4 epoch1:node_model train_loss:0.34045681902872665,train_acc:0.8824999928474426
node4 epoch2:node_model train_loss:0.23371180891990662,train_acc:0.9267857074737549
node4 epoch3:node_model train_loss:0.28853933406727655,train_acc:0.8999999761581421
node4 epoch4:node_model train_loss:0.288104599075658,train_acc:0.9050000309944153
node4_model on test-dataset: loss:0.9552354103326798,acc:0.7290999293327332
node4 weight score:2831.762695080504
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5606566369533539,train_acc:0.8207663893699646
node9 epoch1:node_model train_loss:0.29133457415982295,train_acc:0.8968327045440674
node9 epoch2:node_model train_loss:0.20074197571528585,train_acc:0.942234456539154
node9 epoch3:node_model train_loss:0.15398107782790535,train_acc:0.9605263471603394
node9 epoch4:node_model train_loss:0.11799949564431843,train_acc:0.9718374013900757
node9_model on test-dataset: loss:0.7734280110895634,acc:0.7626999616622925
node9 weight score:2400.999153604431
node11: train data size:1682
node11 epoch0:node_model train_loss:0.6594049457241508,train_acc:0.7980630993843079
node11 epoch1:node_model train_loss:0.3905591061886619,train_acc:0.8641319870948792
node11 epoch2:node_model train_loss:0.24788189898518956,train_acc:0.9241893291473389
node11 epoch3:node_model train_loss:0.2001565265304902,train_acc:0.9364132881164551
node11 epoch4:node_model train_loss:0.15068552204791238,train_acc:0.9668721556663513
node11_model on test-dataset: loss:0.7385329887270927,acc:0.7700997591018677
node11 weight score:2277.487973690967
node16: train data size:877
node16 epoch0:node_model train_loss:0.6533392303519778,train_acc:0.8046897053718567
node16 epoch1:node_model train_loss:0.4138702419069078,train_acc:0.8703462481498718
node16 epoch2:node_model train_loss:0.3018643276558982,train_acc:0.8998989462852478
node16 epoch3:node_model train_loss:0.19034455551041496,train_acc:0.9505627155303955
node16 epoch4:node_model train_loss:0.14074690143267313,train_acc:0.9645598530769348
node16_model on test-dataset: loss:0.8475593401491642,acc:0.7396001219749451
node16 weight score:1034.7358095843229
node18: train data size:472
node18 epoch0:node_model train_loss:0.5567948877811432,train_acc:0.8267777562141418
node18 epoch1:node_model train_loss:0.3225176274776459,train_acc:0.8830000162124634
node18 epoch2:node_model train_loss:0.24123817682266235,train_acc:0.9182222485542297
node18 epoch3:node_model train_loss:0.17161989510059356,train_acc:0.9516666531562805
node18 epoch4:node_model train_loss:0.08120278269052505,train_acc:0.9804444313049316
node18_model on test-dataset: loss:0.9486091537028551,acc:0.7237999439239502
node18 weight score:497.5705728303045
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6336825369298458,acc:0.7993999791145324
total cost energy:5.3262840574279835 | all_enery_cp：3.7965 | all_enery_tp: 1.5297840574279835
ef: 24.678088747430333
reward: 19.35180469000235
step 381:loss:8.17210865020752|running q:31.786231994628906
episode6,iteration21 selected nodes:[18, 13, 0, 6, 2],center node:6
################################################## episode6,iteration21 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.6346350088715553,train_acc:0.7935565114021301
node0 epoch1:node_model train_loss:0.4316761843287028,train_acc:0.8546061515808105
node0 epoch2:node_model train_loss:0.3369567156411134,train_acc:0.8885706663131714
node0 epoch3:node_model train_loss:0.28986992686986923,train_acc:0.9016776084899902
node0 epoch4:node_model train_loss:0.25360102836902326,train_acc:0.9238369464874268
node0_model on test-dataset: loss:0.8106651046872139,acc:0.7546001076698303
node0 weight score:6393.515608396396
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5041556243474284,train_acc:0.8301989436149597
node2 epoch1:node_model train_loss:0.3112318068742752,train_acc:0.8890624046325684
node2 epoch2:node_model train_loss:0.26792295432339114,train_acc:0.9110229015350342
node2 epoch3:node_model train_loss:0.21130006418873867,train_acc:0.9343844652175903
node2 epoch4:node_model train_loss:0.19529213042308888,train_acc:0.945511519908905
node2_model on test-dataset: loss:0.8014931832253933,acc:0.7585000395774841
node2 weight score:5973.849934358748
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5556497948784982,train_acc:0.8152994513511658
node6 epoch1:node_model train_loss:0.3724031347420908,train_acc:0.8729493618011475
node6 epoch2:node_model train_loss:0.31593383079574955,train_acc:0.8924884796142578
node6 epoch3:node_model train_loss:0.23995302521413372,train_acc:0.9241935610771179
node6 epoch4:node_model train_loss:0.18077283256476925,train_acc:0.9437787532806396
node6_model on test-dataset: loss:0.803595039844513,acc:0.7606000900268555
node6 weight score:3741.9344954914386
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6153632675608,train_acc:0.7923485040664673
node13 epoch1:node_model train_loss:0.3573509678244591,train_acc:0.8798485994338989
node13 epoch2:node_model train_loss:0.20839954788486162,train_acc:0.93825763463974
node13 epoch3:node_model train_loss:0.14126539851228395,train_acc:0.9618181586265564
node13 epoch4:node_model train_loss:0.10687515729417403,train_acc:0.9800000190734863
node13_model on test-dataset: loss:0.8110857526957989,acc:0.7572000026702881
node13 weight score:1424.0171229258267
node18: train data size:472
node18 epoch0:node_model train_loss:0.5050511121749878,train_acc:0.8461111187934875
node18 epoch1:node_model train_loss:0.3405058026313782,train_acc:0.8825556039810181
node18 epoch2:node_model train_loss:0.1877356141805649,train_acc:0.9524444937705994
node18 epoch3:node_model train_loss:0.12431274503469467,train_acc:0.9664444327354431
node18 epoch4:node_model train_loss:0.10520762652158737,train_acc:0.9656667113304138
node18_model on test-dataset: loss:0.9229145443439484,acc:0.7338999509811401
node18 weight score:511.42329795606383
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6147020280361175,acc:0.8044999814033509
total cost energy:9.202982352844664 | all_enery_cp：7.3025 | all_enery_tp: 1.9004823528446648
ef: 24.75653384286847
reward: 15.553551490023807
step 382:loss:3.3507168292999268|running q:33.16322708129883
episode6,iteration22 selected nodes:[3, 4, 5, 16, 13],center node:3
################################################## episode6,iteration22 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.685062134681746,train_acc:0.7775802612304688
node3 epoch1:node_model train_loss:0.4523944563643877,train_acc:0.8421177268028259
node3 epoch2:node_model train_loss:0.34370470636112743,train_acc:0.8785996437072754
node3 epoch3:node_model train_loss:0.2818977209024651,train_acc:0.9077288508415222
node3 epoch4:node_model train_loss:0.24057797845019852,train_acc:0.9230777025222778
node3_model on test-dataset: loss:0.750253566801548,acc:0.7724998593330383
node3 weight score:5660.752828014729
node4: train data size:2705
node4 epoch0:node_model train_loss:0.43927546590566635,train_acc:0.8578570485115051
node4 epoch1:node_model train_loss:0.29117648037416594,train_acc:0.9028573036193848
node4 epoch2:node_model train_loss:0.26475184889776365,train_acc:0.9089285731315613
node4 epoch3:node_model train_loss:0.24947313778102398,train_acc:0.9160714149475098
node4 epoch4:node_model train_loss:0.21063552530748503,train_acc:0.9335715174674988
node4_model on test-dataset: loss:0.823736741989851,acc:0.7624999284744263
node4 weight score:3283.8161297330203
node5: train data size:3735
node5 epoch0:node_model train_loss:0.577423524699713,train_acc:0.8075564503669739
node5 epoch1:node_model train_loss:0.3410232643547811,train_acc:0.8822556734085083
node5 epoch2:node_model train_loss:0.2659627148195317,train_acc:0.9169548749923706
node5 epoch3:node_model train_loss:0.22345667155949692,train_acc:0.9330451488494873
node5 epoch4:node_model train_loss:0.20208415663556048,train_acc:0.9349246621131897
node5_model on test-dataset: loss:0.9423794673383236,acc:0.7333998680114746
node5 weight score:3963.3715816720974
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5398383860786756,train_acc:0.8120454549789429
node13 epoch1:node_model train_loss:0.29566436012585956,train_acc:0.9014393091201782
node13 epoch2:node_model train_loss:0.21586374565958977,train_acc:0.9235605597496033
node13 epoch3:node_model train_loss:0.12778588756918907,train_acc:0.9626514911651611
node13 epoch4:node_model train_loss:0.09920827423532803,train_acc:0.9759848117828369
node13_model on test-dataset: loss:0.7808095259964466,acc:0.7691999673843384
node13 weight score:1479.2340020775518
node16: train data size:877
node16 epoch0:node_model train_loss:0.6479965415265825,train_acc:0.7966811656951904
node16 epoch1:node_model train_loss:0.43398266037305194,train_acc:0.854011595249176
node16 epoch2:node_model train_loss:0.26914648049407536,train_acc:0.9177921414375305
node16 epoch3:node_model train_loss:0.17240865197446612,train_acc:0.9471139907836914
node16 epoch4:node_model train_loss:0.10050359865029652,train_acc:0.9807791113853455
node16_model on test-dataset: loss:0.7787891446053982,acc:0.7665997743606567
node16 weight score:1126.1071190769667
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6005661225318909,acc:0.8139999830722808
total cost energy:8.176052506059644 | all_enery_cp：6.3595 | all_enery_tp: 1.8165525060596441
ef: 24.848978605670055
reward: 16.67292609961041
step 383:loss:3.4786288738250732|running q:34.53202819824219
episode6,iteration23 selected nodes:[10, 12, 19, 8, 11],center node:10
################################################## episode6,iteration23 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6554362956020567,train_acc:0.7870067954063416
node8 epoch1:node_model train_loss:0.39206751849916244,train_acc:0.8631632328033447
node8 epoch2:node_model train_loss:0.25261707769499886,train_acc:0.9115645289421082
node8 epoch3:node_model train_loss:0.17979422998097208,train_acc:0.9515872597694397
node8 epoch4:node_model train_loss:0.1475383159187105,train_acc:0.9633333683013916
node8_model on test-dataset: loss:0.7709830683469773,acc:0.7633998990058899
node8 weight score:2332.08753060556
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6627019882202149,train_acc:0.796500027179718
node10 epoch1:node_model train_loss:0.45154404640197754,train_acc:0.8541666269302368
node10 epoch2:node_model train_loss:0.28488778956234456,train_acc:0.9041666388511658
node10 epoch3:node_model train_loss:0.20501417964696883,train_acc:0.9421665072441101
node10 epoch4:node_model train_loss:0.1477710124105215,train_acc:0.9621666073799133
node10_model on test-dataset: loss:0.7565230563282966,acc:0.766800045967102
node10 weight score:2610.6276384826265
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5547498727546019,train_acc:0.8136154413223267
node11 epoch1:node_model train_loss:0.34907077866442066,train_acc:0.8781205415725708
node11 epoch2:node_model train_loss:0.2148088663816452,train_acc:0.9343901872634888
node11 epoch3:node_model train_loss:0.1616892634945757,train_acc:0.9530128240585327
node11 epoch4:node_model train_loss:0.11650712064960424,train_acc:0.9690243005752563
node11_model on test-dataset: loss:0.7016020457446576,acc:0.7792999148368835
node11 weight score:2397.3704327141463
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6729606624160495,train_acc:0.7789682149887085
node12 epoch1:node_model train_loss:0.3825879565307072,train_acc:0.8803174495697021
node12 epoch2:node_model train_loss:0.2933655817593847,train_acc:0.9033333659172058
node12 epoch3:node_model train_loss:0.18398475646972656,train_acc:0.9446031451225281
node12 epoch4:node_model train_loss:0.1489871335881097,train_acc:0.956031858921051
node12_model on test-dataset: loss:0.8093624737858772,acc:0.7589000463485718
node12 weight score:1650.6819172758542
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6316039985002473,train_acc:0.7960350513458252
node19 epoch1:node_model train_loss:0.4464367219181948,train_acc:0.8437640070915222
node19 epoch2:node_model train_loss:0.35959291458129883,train_acc:0.872067391872406
node19 epoch3:node_model train_loss:0.2760732083819633,train_acc:0.9119438529014587
node19 epoch4:node_model train_loss:0.23103093858375107,train_acc:0.9267727732658386
node19_model on test-dataset: loss:0.7268257921934128,acc:0.7800000309944153
node19 weight score:5889.994612162579
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.57780667796731,acc:0.8143999832868576
total cost energy:6.888076313254064 | all_enery_cp：5.5360000000000005 | all_enery_tp: 1.3520763132540636
ef: 25.33702651059147
reward: 18.448950197337407
step 384:loss:7.970315933227539|running q:35.909549713134766
episode6,iteration24 selected nodes:[5, 10, 19, 1, 11],center node:11
################################################## episode6,iteration24 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4900658174472697,train_acc:0.8300002217292786
node1 epoch1:node_model train_loss:0.3397340522531201,train_acc:0.8797793388366699
node1 epoch2:node_model train_loss:0.297319930704201,train_acc:0.8973527550697327
node1 epoch3:node_model train_loss:0.30836261458256664,train_acc:0.8933088779449463
node1 epoch4:node_model train_loss:0.2554287662839188,train_acc:0.9119850993156433
node1_model on test-dataset: loss:0.8238785933703184,acc:0.7546002864837646
node1 weight score:8141.9763226993755
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5052374479802031,train_acc:0.8268797397613525
node5 epoch1:node_model train_loss:0.30855506187991094,train_acc:0.8935338258743286
node5 epoch2:node_model train_loss:0.22133609417237735,train_acc:0.9243608713150024
node5 epoch3:node_model train_loss:0.198150474970278,train_acc:0.9379698038101196
node5 epoch4:node_model train_loss:0.18401126445908295,train_acc:0.9421427845954895
node5_model on test-dataset: loss:0.7358196074515582,acc:0.7764999866485596
node5 weight score:5075.972374446259
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5496622011065483,train_acc:0.8171667456626892
node10 epoch1:node_model train_loss:0.35739435628056526,train_acc:0.8861666917800903
node10 epoch2:node_model train_loss:0.20888839811086654,train_acc:0.9378334283828735
node10 epoch3:node_model train_loss:0.16149063222110271,train_acc:0.951166570186615
node10 epoch4:node_model train_loss:0.11409929711371661,train_acc:0.9783331155776978
node10_model on test-dataset: loss:0.7536565397679805,acc:0.7741999626159668
node10 weight score:2620.5571049751925
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5019355539013358,train_acc:0.829167902469635
node11 epoch1:node_model train_loss:0.2605679394567714,train_acc:0.9115781784057617
node11 epoch2:node_model train_loss:0.18469540687168345,train_acc:0.9420372247695923
node11 epoch3:node_model train_loss:0.12325952728005017,train_acc:0.9690243005752563
node11 epoch4:node_model train_loss:0.08898575297173332,train_acc:0.985164999961853
node11_model on test-dataset: loss:0.7281233789026738,acc:0.7847998738288879
node11 weight score:2310.0480615453885
node19: train data size:4281
node19 epoch0:node_model train_loss:0.4514869908953822,train_acc:0.8441747426986694
node19 epoch1:node_model train_loss:0.30015120984509935,train_acc:0.8961154222488403
node19 epoch2:node_model train_loss:0.23171822744053464,train_acc:0.9280590415000916
node19 epoch3:node_model train_loss:0.20913939663144046,train_acc:0.9355412125587463
node19 epoch4:node_model train_loss:0.1739762366511101,train_acc:0.9457046985626221
node19_model on test-dataset: loss:0.77533103518188,acc:0.7602999806404114
node19 weight score:5521.512496911397
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5766359850764274,acc:0.8187999802827836
total cost energy:10.953941361516794 | all_enery_cp：9.190499999999998 | all_enery_tp: 1.7634413615167959
ef: 25.394571768086518
reward: 14.440630406569724
step 385:loss:5.932306289672852|running q:37.27103042602539
episode6,iteration25 selected nodes:[6, 15, 18, 8, 3],center node:6
################################################## episode6,iteration25 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.5566143206385679,train_acc:0.8120584487915039
node3 epoch1:node_model train_loss:0.33174418051575505,train_acc:0.8868875503540039
node3 epoch2:node_model train_loss:0.27379571386547974,train_acc:0.9077882766723633
node3 epoch3:node_model train_loss:0.2051336406621822,train_acc:0.938575029373169
node3 epoch4:node_model train_loss:0.1774633611704028,train_acc:0.9436022043228149
node3_model on test-dataset: loss:0.7605096782743931,acc:0.7737998962402344
node3 weight score:5584.412823826912
node6: train data size:3007
node6 epoch0:node_model train_loss:0.5119953588131936,train_acc:0.8366820216178894
node6 epoch1:node_model train_loss:0.36359983922973754,train_acc:0.8785253167152405
node6 epoch2:node_model train_loss:0.26394514829641386,train_acc:0.908709704875946
node6 epoch3:node_model train_loss:0.1994484050139304,train_acc:0.9389401078224182
node6 epoch4:node_model train_loss:0.1973184594704259,train_acc:0.9349769949913025
node6_model on test-dataset: loss:0.8343974043428898,acc:0.7532002329826355
node6 weight score:3603.798363164963
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5767772181166543,train_acc:0.8170521855354309
node8 epoch1:node_model train_loss:0.35779975685808396,train_acc:0.8748412132263184
node8 epoch2:node_model train_loss:0.23897827085521486,train_acc:0.9193876385688782
node8 epoch3:node_model train_loss:0.14719632723265225,train_acc:0.9655328392982483
node8 epoch4:node_model train_loss:0.11969758570194244,train_acc:0.9721767902374268
node8_model on test-dataset: loss:0.7375901393592358,acc:0.7724000215530396
node8 weight score:2437.668162920359
node15: train data size:629
node15 epoch0:node_model train_loss:0.7441611119679042,train_acc:0.7585222125053406
node15 epoch1:node_model train_loss:0.404874718614987,train_acc:0.8596552014350891
node15 epoch2:node_model train_loss:0.2609884440898895,train_acc:0.9207881689071655
node15 epoch3:node_model train_loss:0.1725955850311688,train_acc:0.9465024471282959
node15 epoch4:node_model train_loss:0.10239901819399425,train_acc:0.9828571081161499
node15_model on test-dataset: loss:0.8118710243701934,acc:0.7623997926712036
node15 weight score:774.7536013961637
node18: train data size:472
node18 epoch0:node_model train_loss:0.5720464766025544,train_acc:0.8183333277702332
node18 epoch1:node_model train_loss:0.26009465754032135,train_acc:0.908111035823822
node18 epoch2:node_model train_loss:0.193195703625679,train_acc:0.935333251953125
node18 epoch3:node_model train_loss:0.0929448962211609,train_acc:0.9824444651603699
node18 epoch4:node_model train_loss:0.09209389537572861,train_acc:0.9776666760444641
node18_model on test-dataset: loss:0.8405707705020905,acc:0.7530999779701233
node18 weight score:561.5232132305345
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5921582639217376,acc:0.8080999803543091
total cost energy:7.2597825753088925 | all_enery_cp：5.0765 | all_enery_tp: 2.1832825753088922
ef: 24.496215125435185
reward: 17.236432550126292
step 386:loss:6.028398036956787|running q:38.6599006652832
episode6,iteration26 selected nodes:[19, 4, 3, 15, 12],center node:3
################################################## episode6,iteration26 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.33294147668882856,train_acc:0.8860167264938354
node3 epoch1:node_model train_loss:0.22744728087685828,train_acc:0.9231073260307312
node3 epoch2:node_model train_loss:0.19212621034577834,train_acc:0.9396781921386719
node3 epoch3:node_model train_loss:0.166974363978519,train_acc:0.9492132663726807
node3 epoch4:node_model train_loss:0.14004200194464173,train_acc:0.9599700570106506
node3_model on test-dataset: loss:0.9117094528675079,acc:0.7516998648643494
node3 weight score:4658.282292283291
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5089511552027294,train_acc:0.8321429491043091
node4 epoch1:node_model train_loss:0.3218656780996493,train_acc:0.8939285278320312
node4 epoch2:node_model train_loss:0.2275904720383031,train_acc:0.9246428608894348
node4 epoch3:node_model train_loss:0.20645619849009172,train_acc:0.9289286136627197
node4 epoch4:node_model train_loss:0.19215817696281842,train_acc:0.9382143616676331
node4_model on test-dataset: loss:0.7777116614580154,acc:0.7693000435829163
node4 weight score:3478.1528091385435
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6023513227701187,train_acc:0.8100793957710266
node12 epoch1:node_model train_loss:0.3538546498332705,train_acc:0.8807937502861023
node12 epoch2:node_model train_loss:0.23937534860202245,train_acc:0.9232538938522339
node12 epoch3:node_model train_loss:0.17553451763732092,train_acc:0.9501586556434631
node12 epoch4:node_model train_loss:0.12516910103814943,train_acc:0.971746027469635
node12_model on test-dataset: loss:0.8166434787213802,acc:0.76500004529953
node12 weight score:1635.9648179543135
node15: train data size:629
node15 epoch0:node_model train_loss:0.7040822335651943,train_acc:0.7764531970024109
node15 epoch1:node_model train_loss:0.3555580036980765,train_acc:0.8772907257080078
node15 epoch2:node_model train_loss:0.21125270958457673,train_acc:0.9457142353057861
node15 epoch3:node_model train_loss:0.13841618916818074,train_acc:0.9565024375915527
node15 epoch4:node_model train_loss:0.10574711433478765,train_acc:0.9800000190734863
node15_model on test-dataset: loss:0.8500199244916439,acc:0.7489998936653137
node15 weight score:739.9826543785721
node19: train data size:4281
node19 epoch0:node_model train_loss:0.4593755006790161,train_acc:0.8371433615684509
node19 epoch1:node_model train_loss:0.29512594536293385,train_acc:0.8951995372772217
node19 epoch2:node_model train_loss:0.22509297104768974,train_acc:0.9261154532432556
node19 epoch3:node_model train_loss:0.17359138695999635,train_acc:0.9471145272254944
node19 epoch4:node_model train_loss:0.14814026047324025,train_acc:0.9589059352874756
node19_model on test-dataset: loss:0.7933642435818911,acc:0.7652000188827515
node19 weight score:5396.008245433505
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6046088126301765,acc:0.8111999803781509
total cost energy:8.796413331259318 | all_enery_cp：6.599 | all_enery_tp: 2.197413331259318
ef: 24.726158255235447
reward: 15.929744923976129
step 387:loss:6.695873737335205|running q:39.99932861328125
episode6,iteration27 selected nodes:[8, 1, 15, 12, 19],center node:8
################################################## episode6,iteration27 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.42920100053443627,train_acc:0.8514705300331116
node1 epoch1:node_model train_loss:0.31877048795714097,train_acc:0.8936763405799866
node1 epoch2:node_model train_loss:0.23441898713217063,train_acc:0.9236763715744019
node1 epoch3:node_model train_loss:0.19929377888055408,train_acc:0.9354409575462341
node1 epoch4:node_model train_loss:0.16904535780058189,train_acc:0.9467647671699524
node1_model on test-dataset: loss:0.7591062857210636,acc:0.7767999172210693
node1 weight score:8836.707225560873
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5045411586761475,train_acc:0.8275850415229797
node8 epoch1:node_model train_loss:0.31522199428743786,train_acc:0.897675633430481
node8 epoch2:node_model train_loss:0.20825925552182728,train_acc:0.9332651495933533
node8 epoch3:node_model train_loss:0.14054985447890228,train_acc:0.9633218050003052
node8 epoch4:node_model train_loss:0.10888364807599121,train_acc:0.9699432849884033
node8_model on test-dataset: loss:0.7585571753978729,acc:0.7742000222206116
node8 weight score:2370.289357630723
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5327324782099042,train_acc:0.8252381086349487
node12 epoch1:node_model train_loss:0.3205718068139894,train_acc:0.8913493156433105
node12 epoch2:node_model train_loss:0.21328102690832956,train_acc:0.9336507320404053
node12 epoch3:node_model train_loss:0.1408128504242216,train_acc:0.9590476155281067
node12 epoch4:node_model train_loss:0.09255788076136794,train_acc:0.979603111743927
node12_model on test-dataset: loss:0.8127887225151063,acc:0.7680000066757202
node12 weight score:1643.72359383366
node15: train data size:629
node15 epoch0:node_model train_loss:0.6180990891797202,train_acc:0.8066502809524536
node15 epoch1:node_model train_loss:0.3136447029454367,train_acc:0.8915764689445496
node15 epoch2:node_model train_loss:0.20566129578011377,train_acc:0.9371429085731506
node15 epoch3:node_model train_loss:0.1412138534443719,train_acc:0.9565024971961975
node15 epoch4:node_model train_loss:0.09018002982650485,train_acc:0.9780787229537964
node15_model on test-dataset: loss:0.8290524551272392,acc:0.759399950504303
node15 weight score:758.6974697559564
node19: train data size:4281
node19 epoch0:node_model train_loss:0.32241230596636616,train_acc:0.8866092562675476
node19 epoch1:node_model train_loss:0.23363900271266005,train_acc:0.9221763610839844
node19 epoch2:node_model train_loss:0.17564747725115265,train_acc:0.9412314891815186
node19 epoch3:node_model train_loss:0.1261201075516468,train_acc:0.9623943567276001
node19 epoch4:node_model train_loss:0.10450686271800551,train_acc:0.9735572338104248
node19_model on test-dataset: loss:0.7622577968239784,acc:0.7785001397132874
node19 weight score:5616.2101822207715
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6137551081180572,acc:0.8128999841213226
total cost energy:10.077241674713544 | all_enery_cp：7.3759999999999994 | all_enery_tp: 2.701241674713546
ef: 24.897804397947823
reward: 14.820562723234278
step 388:loss:4.007580757141113|running q:41.363765716552734
episode6,iteration28 selected nodes:[7, 2, 11, 6, 3],center node:7
################################################## episode6,iteration28 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.5047185098131498,train_acc:0.8268749713897705
node2 epoch1:node_model train_loss:0.30382066654662293,train_acc:0.8975095748901367
node2 epoch2:node_model train_loss:0.22854132655387124,train_acc:0.9226608276367188
node2 epoch3:node_model train_loss:0.1801228739010791,train_acc:0.9429829120635986
node2 epoch4:node_model train_loss:0.1502580374168853,train_acc:0.9553312659263611
node2_model on test-dataset: loss:0.7723094055056572,acc:0.7724001407623291
node2 weight score:6199.587841177635
node3: train data size:4247
node3 epoch0:node_model train_loss:0.38474315924699914,train_acc:0.8667392134666443
node3 epoch1:node_model train_loss:0.2175970683957255,train_acc:0.9259871244430542
node3 epoch2:node_model train_loss:0.16997985212608827,train_acc:0.9426423907279968
node3 epoch3:node_model train_loss:0.12963236955016158,train_acc:0.963691234588623
node3 epoch4:node_model train_loss:0.11136193920013517,train_acc:0.9690102934837341
node3_model on test-dataset: loss:0.797440129518509,acc:0.775999903678894
node3 weight score:5325.791671111812
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4702197421462305,train_acc:0.8454839587211609
node6 epoch1:node_model train_loss:0.31045910523783776,train_acc:0.8973270654678345
node6 epoch2:node_model train_loss:0.22511795187188732,train_acc:0.9270967841148376
node6 epoch3:node_model train_loss:0.16368455944522733,train_acc:0.9506451487541199
node6 epoch4:node_model train_loss:0.1325740388204013,train_acc:0.9661288857460022
node6_model on test-dataset: loss:0.7804977409541607,acc:0.7763999700546265
node6 weight score:3852.6697032126376
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6565730169415473,train_acc:0.804617702960968
node7 epoch1:node_model train_loss:0.34113265573978424,train_acc:0.8870980143547058
node7 epoch2:node_model train_loss:0.2518161028623581,train_acc:0.909656822681427
node7 epoch3:node_model train_loss:0.1635787632316351,train_acc:0.9535978436470032
node7 epoch4:node_model train_loss:0.11843075305223465,train_acc:0.9690783619880676
node7_model on test-dataset: loss:0.7707887607812881,acc:0.7790001034736633
node7 weight score:2531.173389220705
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5740398151033065,train_acc:0.8225107789039612
node11 epoch1:node_model train_loss:0.3328616952194887,train_acc:0.890731692314148
node11 epoch2:node_model train_loss:0.21404889401267557,train_acc:0.9332711696624756
node11 epoch3:node_model train_loss:0.132321137277519,train_acc:0.9587659239768982
node11 epoch4:node_model train_loss:0.09727610581937958,train_acc:0.9763413667678833
node11_model on test-dataset: loss:0.7447356076538563,acc:0.7819998860359192
node11 weight score:2258.51964470829
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5836455461382866,acc:0.8196999806165696
total cost energy:8.873417360311745 | all_enery_cp：7.8375 | all_enery_tp: 1.035917360311745
ef: 25.473320804094126
reward: 16.59990344378238
step 389:loss:5.442355632781982|running q:42.68488693237305
episode6,iteration29 selected nodes:[11, 8, 15, 16, 13],center node:11
################################################## episode6,iteration29 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5145570461948713,train_acc:0.8220861554145813
node8 epoch1:node_model train_loss:0.31397659165991676,train_acc:0.8898866772651672
node8 epoch2:node_model train_loss:0.17671231552958488,train_acc:0.9427210688591003
node8 epoch3:node_model train_loss:0.126484591099951,train_acc:0.9665871858596802
node8 epoch4:node_model train_loss:0.10219281394448546,train_acc:0.9777210354804993
node8_model on test-dataset: loss:0.7449929971992969,acc:0.7799999117851257
node8 weight score:2413.445504534062
node11: train data size:1682
node11 epoch0:node_model train_loss:0.48562367172802196,train_acc:0.8423098921775818
node11 epoch1:node_model train_loss:0.27512405988048105,train_acc:0.9145193696022034
node11 epoch2:node_model train_loss:0.1781848809298347,train_acc:0.9374604821205139
node11 epoch3:node_model train_loss:0.13336200135595658,train_acc:0.9622237086296082
node11 epoch4:node_model train_loss:0.0851895866148612,train_acc:0.9786943793296814
node11_model on test-dataset: loss:0.7518364870548249,acc:0.7804998755455017
node11 weight score:2237.188576187506
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6493727813164393,train_acc:0.8050000071525574
node13 epoch1:node_model train_loss:0.3575028218328953,train_acc:0.8807575702667236
node13 epoch2:node_model train_loss:0.23720536256829897,train_acc:0.9272727370262146
node13 epoch3:node_model train_loss:0.15791935535768667,train_acc:0.9497727751731873
node13 epoch4:node_model train_loss:0.096759969368577,train_acc:0.973787784576416
node13_model on test-dataset: loss:0.7470806756615639,acc:0.7810999155044556
node13 weight score:1546.017769737131
node15: train data size:629
node15 epoch0:node_model train_loss:0.6624173692294529,train_acc:0.8026601076126099
node15 epoch1:node_model train_loss:0.4256530978849956,train_acc:0.8395073413848877
node15 epoch2:node_model train_loss:0.259248469557081,train_acc:0.9058620929718018
node15 epoch3:node_model train_loss:0.18480483016797475,train_acc:0.9465024471282959
node15 epoch4:node_model train_loss:0.11179397787366595,train_acc:0.964433491230011
node15_model on test-dataset: loss:0.9154549864679575,acc:0.7422999143600464
node15 weight score:687.0900364274939
node16: train data size:877
node16 epoch0:node_model train_loss:0.6711820628907945,train_acc:0.8016883134841919
node16 epoch1:node_model train_loss:0.34654249085320366,train_acc:0.8835641145706177
node16 epoch2:node_model train_loss:0.22755358616511026,train_acc:0.9247906804084778
node16 epoch3:node_model train_loss:0.1496331418553988,train_acc:0.9612264633178711
node16 epoch4:node_model train_loss:0.09436140457789104,train_acc:0.973780632019043
node16_model on test-dataset: loss:0.7823477213084697,acc:0.7707001566886902
node16 weight score:1120.9849228335772
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5898326389491558,acc:0.818599978685379
total cost energy:4.843843312876075 | all_enery_cp：3.0705 | all_enery_tp: 1.7733433128760745
ef: 24.62635132851262
reward: 19.782508015636544
step 390:loss:3.8480474948883057|running q:44.039363861083984
episode6,iteration30 selected nodes:[3, 10, 6, 17, 5],center node:6
################################################## episode6,iteration30 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.31734482077665105,train_acc:0.8875852823257446
node3 epoch1:node_model train_loss:0.1921576713753301,train_acc:0.9343888163566589
node3 epoch2:node_model train_loss:0.12140098231476407,train_acc:0.9692724943161011
node3 epoch3:node_model train_loss:0.10149930547489676,train_acc:0.973458468914032
node3 epoch4:node_model train_loss:0.0887001927855403,train_acc:0.9797674417495728
node3_model on test-dataset: loss:0.7705681012570857,acc:0.7845001816749573
node3 weight score:5511.518051514914
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5012087531779942,train_acc:0.8371427655220032
node5 epoch1:node_model train_loss:0.29143992536946345,train_acc:0.8984963297843933
node5 epoch2:node_model train_loss:0.23741623641628967,train_acc:0.9193609952926636
node5 epoch3:node_model train_loss:0.190567773031561,train_acc:0.9362406730651855
node5 epoch4:node_model train_loss:0.15604385596356893,train_acc:0.9537969827651978
node5_model on test-dataset: loss:0.7684839156270027,acc:0.7752999067306519
node5 weight score:4860.218833536197
node6: train data size:3007
node6 epoch0:node_model train_loss:0.44688631161566705,train_acc:0.8545621633529663
node6 epoch1:node_model train_loss:0.317124136272938,train_acc:0.889815628528595
node6 epoch2:node_model train_loss:0.2288531372624059,train_acc:0.9263595342636108
node6 epoch3:node_model train_loss:0.1749311635090459,train_acc:0.9464516639709473
node6 epoch4:node_model train_loss:0.1500494633951495,train_acc:0.9478799700737
node6_model on test-dataset: loss:0.7301736151427031,acc:0.7849998474121094
node6 weight score:4118.198655277787
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6003820858895779,train_acc:0.8128334283828735
node10 epoch1:node_model train_loss:0.3519229993224144,train_acc:0.888499915599823
node10 epoch2:node_model train_loss:0.21237155683338643,train_acc:0.9386665225028992
node10 epoch3:node_model train_loss:0.13734422363340854,train_acc:0.9648332595825195
node10 epoch4:node_model train_loss:0.12118839863687754,train_acc:0.9684999585151672
node10_model on test-dataset: loss:0.7605145896971226,acc:0.7760000228881836
node10 weight score:2596.9258535678455
node17: train data size:442
node17 epoch0:node_model train_loss:0.7598568797111511,train_acc:0.7548571825027466
node17 epoch1:node_model train_loss:0.3799399346113205,train_acc:0.8561905026435852
node17 epoch2:node_model train_loss:0.2729138255119324,train_acc:0.9121905565261841
node17 epoch3:node_model train_loss:0.11862211674451828,train_acc:0.977238118648529
node17 epoch4:node_model train_loss:0.11956966817378997,train_acc:0.9697142839431763
node17_model on test-dataset: loss:0.9095351108908654,acc:0.7514000535011292
node17 weight score:485.9625480176052
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.592074820548296,acc:0.816499981880188
total cost energy:8.465524937664329 | all_enery_cp：6.702999999999999 | all_enery_tp: 1.7625249376643295
ef: 25.006565671254215
reward: 16.541040733589888
step 391:loss:4.873592376708984|running q:45.28282928466797
episode6,iteration31 selected nodes:[12, 15, 7, 13, 16],center node:16
################################################## episode6,iteration31 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.562566514313221,train_acc:0.8261372447013855
node7 epoch1:node_model train_loss:0.31380981728434565,train_acc:0.8985783457756042
node7 epoch2:node_model train_loss:0.20956738516688347,train_acc:0.9301176071166992
node7 epoch3:node_model train_loss:0.14795787781476974,train_acc:0.9580194354057312
node7 epoch4:node_model train_loss:0.09643559511750936,train_acc:0.9729999899864197
node7_model on test-dataset: loss:0.7618582037091255,acc:0.7793999314308167
node7 weight score:2560.843987111392
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5577778369188309,train_acc:0.8238095045089722
node12 epoch1:node_model train_loss:0.2854705772229603,train_acc:0.8990476131439209
node12 epoch2:node_model train_loss:0.19321584914411818,train_acc:0.9340475797653198
node12 epoch3:node_model train_loss:0.14182139986327716,train_acc:0.9628571271896362
node12 epoch4:node_model train_loss:0.11512974915759903,train_acc:0.9715873599052429
node12_model on test-dataset: loss:0.8212924638390541,acc:0.7681000828742981
node12 weight score:1626.7043213266493
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5346315080920855,train_acc:0.8277272582054138
node13 epoch1:node_model train_loss:0.2759143002331257,train_acc:0.9086364507675171
node13 epoch2:node_model train_loss:0.16788912440339723,train_acc:0.9412879347801208
node13 epoch3:node_model train_loss:0.10817947455992301,train_acc:0.972802996635437
node13 epoch4:node_model train_loss:0.07535692180196445,train_acc:0.9858332872390747
node13_model on test-dataset: loss:0.8060760408639908,acc:0.7718999981880188
node13 weight score:1432.867299668175
node15: train data size:629
node15 epoch0:node_model train_loss:0.6299648433923721,train_acc:0.782660186290741
node15 epoch1:node_model train_loss:0.2970177297081266,train_acc:0.8944335579872131
node15 epoch2:node_model train_loss:0.18171463693891252,train_acc:0.9336453080177307
node15 epoch3:node_model train_loss:0.1211239690227168,train_acc:0.9614285230636597
node15 epoch4:node_model train_loss:0.06558223707335335,train_acc:0.9885714054107666
node15_model on test-dataset: loss:0.8370079056918621,acc:0.7647000551223755
node15 weight score:751.4863309207038
node16: train data size:877
node16 epoch0:node_model train_loss:0.5508803062968783,train_acc:0.830129861831665
node16 epoch1:node_model train_loss:0.2519637495279312,train_acc:0.9227849245071411
node16 epoch2:node_model train_loss:0.17093893140554428,train_acc:0.9550071358680725
node16 epoch3:node_model train_loss:0.10371954407956865,train_acc:0.9704473614692688
node16 epoch4:node_model train_loss:0.08061311559544669,train_acc:0.9778931736946106
node16_model on test-dataset: loss:0.7744067810475826,acc:0.770799994468689
node16 weight score:1132.4797528420836
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5966070123016834,acc:0.8186999779939651
total cost energy:4.931115546859237 | all_enery_cp：2.974 | all_enery_tp: 1.9571155468592365
ef: 24.570786955029618
reward: 19.63967140817038
step 392:loss:2.6062979698181152|running q:46.50663375854492
episode6,iteration32 selected nodes:[9, 10, 17, 16, 19],center node:16
################################################## episode6,iteration32 ##################################################
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5581402072781011,train_acc:0.8282641172409058
node9 epoch1:node_model train_loss:0.3209857924988395,train_acc:0.8890581727027893
node9 epoch2:node_model train_loss:0.18623037518639313,train_acc:0.9413111805915833
node9 epoch3:node_model train_loss:0.11707574482026853,train_acc:0.9677560925483704
node9 epoch4:node_model train_loss:0.08908774409639209,train_acc:0.979732096195221
node9_model on test-dataset: loss:0.7707269158959389,acc:0.7739999294281006
node9 weight score:2409.413712821113
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5426545381546021,train_acc:0.8293332457542419
node10 epoch1:node_model train_loss:0.309518626332283,train_acc:0.8971667289733887
node10 epoch2:node_model train_loss:0.20873136408627033,train_acc:0.9290000796318054
node10 epoch3:node_model train_loss:0.1452960751950741,train_acc:0.9616667032241821
node10 epoch4:node_model train_loss:0.10368022955954075,train_acc:0.9733331799507141
node10_model on test-dataset: loss:0.7787428673356771,acc:0.7720000147819519
node10 weight score:2536.1387986218
node16: train data size:877
node16 epoch0:node_model train_loss:0.4130438317855199,train_acc:0.8483549952507019
node16 epoch1:node_model train_loss:0.240310024884012,train_acc:0.9180086255073547
node16 epoch2:node_model train_loss:0.16889067656464046,train_acc:0.9494516849517822
node16 epoch3:node_model train_loss:0.09727344744735295,train_acc:0.96978360414505
node16 epoch4:node_model train_loss:0.05114682453374068,train_acc:0.9918903708457947
node16_model on test-dataset: loss:0.8089278966933489,acc:0.7667999267578125
node16 weight score:1084.1510146762266
node17: train data size:442
node17 epoch0:node_model train_loss:0.7034800469875335,train_acc:0.7631428837776184
node17 epoch1:node_model train_loss:0.4205350160598755,train_acc:0.8526666760444641
node17 epoch2:node_model train_loss:0.2866287037730217,train_acc:0.9089523553848267
node17 epoch3:node_model train_loss:0.14564867317676544,train_acc:0.9517143368721008
node17 epoch4:node_model train_loss:0.09079696610569954,train_acc:0.9704761505126953
node17_model on test-dataset: loss:0.8279135274887085,acc:0.7688002586364746
node17 weight score:533.8721802755279
node19: train data size:4281
node19 epoch0:node_model train_loss:0.39991789085920465,train_acc:0.8608643412590027
node19 epoch1:node_model train_loss:0.24633353012938833,train_acc:0.9155008792877197
node19 epoch2:node_model train_loss:0.1718824353328971,train_acc:0.9439132809638977
node19 epoch3:node_model train_loss:0.11131719794384269,train_acc:0.9710536003112793
node19 epoch4:node_model train_loss:0.0963974384547666,train_acc:0.9761698246002197
node19_model on test-dataset: loss:0.742400993257761,acc:0.7844001054763794
node19 weight score:5766.425474748308
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5827851109206676,acc:0.8173999840021133
total cost energy:5.986820393249936 | all_enery_cp：4.715999999999999 | all_enery_tp: 1.270820393249937
ef: 25.12616144014611
reward: 19.139341046896174
step 393:loss:5.993099212646484|running q:47.721439361572266
episode6,iteration33 selected nodes:[6, 15, 7, 4, 13],center node:7
################################################## episode6,iteration33 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.49115719326904844,train_acc:0.8400000333786011
node4 epoch1:node_model train_loss:0.37303695135882925,train_acc:0.8628571629524231
node4 epoch2:node_model train_loss:0.27016050262110575,train_acc:0.9082143306732178
node4 epoch3:node_model train_loss:0.24812736016299045,train_acc:0.9100000858306885
node4 epoch4:node_model train_loss:0.24770157704395906,train_acc:0.9192856550216675
node4_model on test-dataset: loss:0.8464141398668289,acc:0.7669999599456787
node4 weight score:3195.835079533989
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4084856260207392,train_acc:0.867327094078064
node6 epoch1:node_model train_loss:0.25703972722253493,train_acc:0.9137787818908691
node6 epoch2:node_model train_loss:0.19822355047349008,train_acc:0.9412903189659119
node6 epoch3:node_model train_loss:0.12201104769783636,train_acc:0.9664514660835266
node6 epoch4:node_model train_loss:0.1355545328509423,train_acc:0.9619814157485962
node6_model on test-dataset: loss:0.8219165519624948,acc:0.7741000056266785
node6 weight score:3658.52225851903
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4140329085290432,train_acc:0.8575782775878906
node7 epoch1:node_model train_loss:0.22144003063440323,train_acc:0.9260784387588501
node7 epoch2:node_model train_loss:0.1506588475778699,train_acc:0.9540979266166687
node7 epoch3:node_model train_loss:0.09551741015166045,train_acc:0.9749999046325684
node7 epoch4:node_model train_loss:0.0765654519200325,train_acc:0.9809998869895935
node7_model on test-dataset: loss:0.7609609469771386,acc:0.7825000882148743
node7 weight score:2563.8635040999197
node13: train data size:1155
node13 epoch0:node_model train_loss:0.4890078554550807,train_acc:0.8373485803604126
node13 epoch1:node_model train_loss:0.2450783227880796,train_acc:0.9189393520355225
node13 epoch2:node_model train_loss:0.15037212893366814,train_acc:0.9559848308563232
node13 epoch3:node_model train_loss:0.10822263360023499,train_acc:0.9693180918693542
node13 epoch4:node_model train_loss:0.07426099199801683,train_acc:0.9822726249694824
node13_model on test-dataset: loss:0.7681771744787693,acc:0.7820999026298523
node13 weight score:1503.5593849605089
node15: train data size:629
node15 epoch0:node_model train_loss:0.5588401257991791,train_acc:0.805221676826477
node15 epoch1:node_model train_loss:0.27684680053165983,train_acc:0.8925123810768127
node15 epoch2:node_model train_loss:0.26110719995839254,train_acc:0.9139409065246582
node15 epoch3:node_model train_loss:0.16937462772641862,train_acc:0.950788140296936
node15 epoch4:node_model train_loss:0.11969300785234996,train_acc:0.9642857313156128
node15_model on test-dataset: loss:0.9341458187252283,acc:0.7346999049186707
node15 weight score:673.3424133486545
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6136826379597187,acc:0.8146999806165696
total cost energy:6.463135379244653 | all_enery_cp：4.7235 | all_enery_tp: 1.739635379244653
ef: 24.769129720292835
reward: 18.305994341048184
step 394:loss:6.012078285217285|running q:48.967193603515625
episode6,iteration34 selected nodes:[0, 18, 2, 16, 4],center node:2
################################################## episode6,iteration34 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5568657849843686,train_acc:0.8231024146080017
node0 epoch1:node_model train_loss:0.3784610735109219,train_acc:0.8743746876716614
node0 epoch2:node_model train_loss:0.291388725145505,train_acc:0.9034128785133362
node0 epoch3:node_model train_loss:0.2162717984846005,train_acc:0.9323424100875854
node0 epoch4:node_model train_loss:0.17890541527706844,train_acc:0.9441519975662231
node0_model on test-dataset: loss:0.699917738288641,acc:0.7924001812934875
node0 weight score:7405.1559440011915
node2: train data size:4788
node2 epoch0:node_model train_loss:0.4764905956884225,train_acc:0.8459470272064209
node2 epoch1:node_model train_loss:0.28918994187066954,train_acc:0.9027179479598999
node2 epoch2:node_model train_loss:0.1956342744330565,train_acc:0.9360795021057129
node2 epoch3:node_model train_loss:0.14362965462108454,train_acc:0.960899293422699
node2 epoch4:node_model train_loss:0.12271145575990279,train_acc:0.9628596305847168
node2_model on test-dataset: loss:0.8241813066601753,acc:0.7710999846458435
node2 weight score:5809.401355391549
node4: train data size:2705
node4 epoch0:node_model train_loss:0.3736941048077175,train_acc:0.8617857694625854
node4 epoch1:node_model train_loss:0.27052931966526167,train_acc:0.9092857241630554
node4 epoch2:node_model train_loss:0.24868850942168916,train_acc:0.9196429252624512
node4 epoch3:node_model train_loss:0.22720921731420926,train_acc:0.9228571653366089
node4 epoch4:node_model train_loss:0.15389353515846388,train_acc:0.9603569507598877
node4_model on test-dataset: loss:0.8183739164471626,acc:0.7641000151634216
node4 weight score:3305.3350621722134
node16: train data size:877
node16 epoch0:node_model train_loss:0.4767482694652345,train_acc:0.8476768136024475
node16 epoch1:node_model train_loss:0.23317707578341165,train_acc:0.918455958366394
node16 epoch2:node_model train_loss:0.16406090060869852,train_acc:0.9478931427001953
node16 epoch3:node_model train_loss:0.09771905880835322,train_acc:0.9741125106811523
node16 epoch4:node_model train_loss:0.0755052301618788,train_acc:0.9823376536369324
node16_model on test-dataset: loss:0.7826160046458245,acc:0.7767001390457153
node16 weight score:1120.6006455194963
node18: train data size:472
node18 epoch0:node_model train_loss:0.5767524838447571,train_acc:0.8318888545036316
node18 epoch1:node_model train_loss:0.23452147245407104,train_acc:0.9221111536026001
node18 epoch2:node_model train_loss:0.16917022466659545,train_acc:0.9424444437026978
node18 epoch3:node_model train_loss:0.10299839526414871,train_acc:0.978444516658783
node18 epoch4:node_model train_loss:0.06837474927306175,train_acc:0.9852221608161926
node18_model on test-dataset: loss:0.9642153863608837,acc:0.7391000390052795
node18 weight score:489.5171832731378
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5931706629693508,acc:0.8174999791383744
total cost energy:9.293524967590667 | all_enery_cp：7.012500000000001 | all_enery_tp: 2.2810249675906658
ef: 24.634354618822492
reward: 15.340829651231825
step 395:loss:8.201105117797852|running q:50.276817321777344
episode6,iteration35 selected nodes:[1, 18, 15, 4, 8],center node:4
################################################## episode6,iteration35 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4129095058888197,train_acc:0.8604412078857422
node1 epoch1:node_model train_loss:0.24865988576236894,train_acc:0.9129411578178406
node1 epoch2:node_model train_loss:0.19392268041915753,train_acc:0.9373529553413391
node1 epoch3:node_model train_loss:0.17800233480246627,train_acc:0.9402204155921936
node1 epoch4:node_model train_loss:0.1631800668204532,train_acc:0.945294201374054
node1_model on test-dataset: loss:0.7355671596527099,acc:0.7911001443862915
node1 weight score:9119.493593443065
node4: train data size:2705
node4 epoch0:node_model train_loss:0.3532933656658445,train_acc:0.8728570938110352
node4 epoch1:node_model train_loss:0.25867545498268946,train_acc:0.9032142758369446
node4 epoch2:node_model train_loss:0.20563926840467112,train_acc:0.9271427989006042
node4 epoch3:node_model train_loss:0.17078946637255804,train_acc:0.9432141184806824
node4 epoch4:node_model train_loss:0.1599261648952961,train_acc:0.9517857432365417
node4_model on test-dataset: loss:0.757279053106904,acc:0.779600203037262
node4 weight score:3571.998973036613
node8: train data size:1798
node8 epoch0:node_model train_loss:0.521980399886767,train_acc:0.8336281180381775
node8 epoch1:node_model train_loss:0.24990751180383894,train_acc:0.9154648184776306
node8 epoch2:node_model train_loss:0.17943022607101333,train_acc:0.9460318088531494
node8 epoch3:node_model train_loss:0.10233638704650933,train_acc:0.9716325402259827
node8 epoch4:node_model train_loss:0.07489090350766976,train_acc:0.9838660359382629
node8_model on test-dataset: loss:0.6916263768076897,acc:0.7934000492095947
node8 weight score:2599.669504073792
node15: train data size:629
node15 epoch0:node_model train_loss:0.5445915843759265,train_acc:0.8358621001243591
node15 epoch1:node_model train_loss:0.31922476206507,train_acc:0.8909359574317932
node15 epoch2:node_model train_loss:0.21402887574263982,train_acc:0.9307882189750671
node15 epoch3:node_model train_loss:0.10686750710010529,train_acc:0.9707881808280945
node15 epoch4:node_model train_loss:0.09403814056089946,train_acc:0.9757143259048462
node15_model on test-dataset: loss:0.9005269932746888,acc:0.7620000243186951
node15 weight score:698.4798953251759
node18: train data size:472
node18 epoch0:node_model train_loss:0.5222097992897033,train_acc:0.84544438123703
node18 epoch1:node_model train_loss:0.28149680197238924,train_acc:0.9153333902359009
node18 epoch2:node_model train_loss:0.14379599690437317,train_acc:0.9636666178703308
node18 epoch3:node_model train_loss:0.0886858083307743,train_acc:0.9812221527099609
node18 epoch4:node_model train_loss:0.058856583386659625,train_acc:0.9899999499320984
node18_model on test-dataset: loss:0.9128332597017288,acc:0.7529000043869019
node18 weight score:517.071431155157
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.594181808233261,acc:0.8196999841928482
total cost energy:8.817601548657588 | all_enery_cp：6.156000000000001 | all_enery_tp: 2.661601548657587
ef: 24.6929088740692
reward: 15.875307325411612
step 396:loss:6.648262023925781|running q:51.55268096923828
episode6,iteration36 selected nodes:[10, 0, 11, 18, 7],center node:11
################################################## episode6,iteration36 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.47655573372657484,train_acc:0.8411052823066711
node0 epoch1:node_model train_loss:0.27543158542651397,train_acc:0.9015686511993408
node0 epoch2:node_model train_loss:0.2086769022907202,train_acc:0.9316865801811218
node0 epoch3:node_model train_loss:0.1632779514273772,train_acc:0.9486097693443298
node0 epoch4:node_model train_loss:0.13078370074240062,train_acc:0.9598422646522522
node0_model on test-dataset: loss:0.7198888702318073,acc:0.7927998304367065
node0 weight score:7199.722365941083
node7: train data size:1951
node7 epoch0:node_model train_loss:0.41472385376691817,train_acc:0.864656925201416
node7 epoch1:node_model train_loss:0.2633755460381508,train_acc:0.9151176810264587
node7 epoch2:node_model train_loss:0.16034516096115112,train_acc:0.9505588412284851
node7 epoch3:node_model train_loss:0.10771739650517702,train_acc:0.9715391397476196
node7 epoch4:node_model train_loss:0.07770794443786144,train_acc:0.9810196161270142
node7_model on test-dataset: loss:0.7236486245691777,acc:0.7875999212265015
node7 weight score:2696.059846947851
node10: train data size:1975
node10 epoch0:node_model train_loss:0.546885558962822,train_acc:0.825499951839447
node10 epoch1:node_model train_loss:0.26306256279349327,train_acc:0.9150000810623169
node10 epoch2:node_model train_loss:0.1821910697966814,train_acc:0.9459999203681946
node10 epoch3:node_model train_loss:0.12886159420013427,train_acc:0.9696664810180664
node10 epoch4:node_model train_loss:0.09052144791930913,train_acc:0.9801665544509888
node10_model on test-dataset: loss:0.7429275789856911,acc:0.7821999192237854
node10 weight score:2658.4017821716093
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5112273132099825,train_acc:0.8421234488487244
node11 epoch1:node_model train_loss:0.2759184346479528,train_acc:0.9023672342300415
node11 epoch2:node_model train_loss:0.17152765305603251,train_acc:0.9494834542274475
node11 epoch3:node_model train_loss:0.10300387332544607,train_acc:0.9696125984191895
node11 epoch4:node_model train_loss:0.06742043793201447,train_acc:0.9860832095146179
node11_model on test-dataset: loss:0.7431163690984249,acc:0.7872001528739929
node11 weight score:2263.4409225040513
node18: train data size:472
node18 epoch0:node_model train_loss:0.5590709626674653,train_acc:0.8449999690055847
node18 epoch1:node_model train_loss:0.2757048100233078,train_acc:0.9021110534667969
node18 epoch2:node_model train_loss:0.18583299219608307,train_acc:0.9524444937705994
node18 epoch3:node_model train_loss:0.08445652574300766,train_acc:0.9844444394111633
node18 epoch4:node_model train_loss:0.09342000037431716,train_acc:0.9744443893432617
node18_model on test-dataset: loss:0.9723265390098095,acc:0.7390000820159912
node18 weight score:485.4336285839445
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5859219918400049,acc:0.8249999797344207
total cost energy:7.279098615915315 | all_enery_cp：5.6315 | all_enery_tp: 1.6475986159153142
ef: 25.096147418906185
reward: 17.81704880299087
step 397:loss:5.913259983062744|running q:52.77582550048828
episode6,iteration37 selected nodes:[6, 0, 2, 19, 1],center node:2
################################################## episode6,iteration37 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.26686240173876286,train_acc:0.9077224135398865
node0 epoch1:node_model train_loss:0.18341620409717926,train_acc:0.9388808012008667
node0 epoch2:node_model train_loss:0.15128288260446146,train_acc:0.9510703086853027
node0 epoch3:node_model train_loss:0.11372661526099993,train_acc:0.9652663469314575
node0 epoch4:node_model train_loss:0.09046823875262187,train_acc:0.9758038520812988
node0_model on test-dataset: loss:0.759981831163168,acc:0.7926998734474182
node0 weight score:6819.899881115988
node1: train data size:6708
node1 epoch0:node_model train_loss:0.266384918571395,train_acc:0.9083823561668396
node1 epoch1:node_model train_loss:0.22101363725960255,train_acc:0.9237499237060547
node1 epoch2:node_model train_loss:0.19819688643602765,train_acc:0.9351469874382019
node1 epoch3:node_model train_loss:0.14077802259913264,train_acc:0.954485297203064
node1 epoch4:node_model train_loss:0.135804147733485,train_acc:0.9568381905555725
node1_model on test-dataset: loss:0.8416639643907547,acc:0.7716999650001526
node1 weight score:7969.926578543303
node2: train data size:4788
node2 epoch0:node_model train_loss:0.4108531105642517,train_acc:0.8592993021011353
node2 epoch1:node_model train_loss:0.2305183452554047,train_acc:0.9192046523094177
node2 epoch2:node_model train_loss:0.17034140462055802,train_acc:0.9457480907440186
node2 epoch3:node_model train_loss:0.12548343581147492,train_acc:0.9627744555473328
node2 epoch4:node_model train_loss:0.09932984768723448,train_acc:0.9743182063102722
node2_model on test-dataset: loss:0.7097703912854194,acc:0.7994999289512634
node2 weight score:6745.843527409987
node6: train data size:3007
node6 epoch0:node_model train_loss:0.39953610829768643,train_acc:0.8664516806602478
node6 epoch1:node_model train_loss:0.26540119801798173,train_acc:0.9124884605407715
node6 epoch2:node_model train_loss:0.2309875055666893,train_acc:0.9159446358680725
node6 epoch3:node_model train_loss:0.19728160601469777,train_acc:0.9389399290084839
node6 epoch4:node_model train_loss:0.12671546007115994,train_acc:0.9632256031036377
node6_model on test-dataset: loss:0.7540241308510304,acc:0.7813999056816101
node6 weight score:3987.936031445234
node19: train data size:4281
node19 epoch0:node_model train_loss:0.41430454406627387,train_acc:0.8587424755096436
node19 epoch1:node_model train_loss:0.21905064011035963,train_acc:0.9223542213439941
node19 epoch2:node_model train_loss:0.13701545906274817,train_acc:0.9594255685806274
node19 epoch3:node_model train_loss:0.10906313420381657,train_acc:0.9666348695755005
node19 epoch4:node_model train_loss:0.08056810515564541,train_acc:0.9817512631416321
node19_model on test-dataset: loss:0.7628826025128365,acc:0.7855001091957092
node19 weight score:5611.610470469428
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.574805096834898,acc:0.830899983048439
total cost energy:13.342955344987203 | all_enery_cp：11.9835 | all_enery_tp: 1.3594553449872044
ef: 25.53587344767404
reward: 12.192918102686836
step 398:loss:7.039064407348633|running q:53.993350982666016
episode6,iteration38 selected nodes:[7, 11, 9, 12, 1],center node:7
################################################## episode6,iteration38 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.22587388118400292,train_acc:0.9164705872535706
node1 epoch1:node_model train_loss:0.18419983577640617,train_acc:0.936397135257721
node1 epoch2:node_model train_loss:0.12956688673618963,train_acc:0.9614706635475159
node1 epoch3:node_model train_loss:0.12448072652606403,train_acc:0.9636764526367188
node1 epoch4:node_model train_loss:0.08873141266624718,train_acc:0.9715441465377808
node1_model on test-dataset: loss:0.7527946901321411,acc:0.7946998476982117
node1 weight score:8910.79611470495
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4035991981625557,train_acc:0.868156909942627
node7 epoch1:node_model train_loss:0.2504760041832924,train_acc:0.9150980114936829
node7 epoch2:node_model train_loss:0.13056917451322078,train_acc:0.9585393071174622
node7 epoch3:node_model train_loss:0.07498159315437078,train_acc:0.9834999442100525
node7 epoch4:node_model train_loss:0.056370215862989424,train_acc:0.9884998202323914
node7_model on test-dataset: loss:0.7203712041676045,acc:0.7943000793457031
node7 weight score:2708.325914074256
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5279769175931027,train_acc:0.8376085162162781
node9 epoch1:node_model train_loss:0.2660152708229266,train_acc:0.9109049439430237
node9 epoch2:node_model train_loss:0.18389771447369926,train_acc:0.938679575920105
node9 epoch3:node_model train_loss:0.11637401757271666,train_acc:0.964727520942688
node9 epoch4:node_model train_loss:0.07840730642017565,train_acc:0.9831578135490417
node9_model on test-dataset: loss:0.7605351348221302,acc:0.7849000096321106
node9 weight score:2441.701789930198
node11: train data size:1682
node11 epoch0:node_model train_loss:0.49744945939849405,train_acc:0.8419799208641052
node11 epoch1:node_model train_loss:0.23094375431537628,train_acc:0.9138594269752502
node11 epoch2:node_model train_loss:0.13693232133108027,train_acc:0.9587660431861877
node11 epoch3:node_model train_loss:0.09302816956358798,train_acc:0.9776468873023987
node11 epoch4:node_model train_loss:0.07037260668242679,train_acc:0.9844474792480469
node11_model on test-dataset: loss:0.7495096153020859,acc:0.7976998090744019
node11 weight score:2244.133985288606
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5399921110698155,train_acc:0.8342064023017883
node12 epoch1:node_model train_loss:0.27281911777598516,train_acc:0.9034920930862427
node12 epoch2:node_model train_loss:0.176809090588774,train_acc:0.9451586604118347
node12 epoch3:node_model train_loss:0.09699624430920396,train_acc:0.9758730530738831
node12 epoch4:node_model train_loss:0.059035786028419225,train_acc:0.9899999499320984
node12_model on test-dataset: loss:0.7839305570721626,acc:0.7892999649047852
node12 weight score:1704.2325853321954
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6178441506624222,acc:0.8226999831199646
total cost energy:8.089678406009828 | all_enery_cp：6.767 | all_enery_tp: 1.3226784060098282
ef: 25.327617650442793
reward: 17.237939244432965
step 399:loss:7.686586380004883|running q:55.22946548461914
episode6,iteration39 selected nodes:[10, 2, 7, 8, 6],center node:7
################################################## episode6,iteration39 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.38423588282118243,train_acc:0.8686457872390747
node2 epoch1:node_model train_loss:0.21037304168567061,train_acc:0.9262878894805908
node2 epoch2:node_model train_loss:0.1523604242441555,train_acc:0.9511929750442505
node2 epoch3:node_model train_loss:0.1057283627645423,train_acc:0.9720548987388611
node2 epoch4:node_model train_loss:0.08053754774543147,train_acc:0.9824715852737427
node2_model on test-dataset: loss:0.721510735526681,acc:0.8014998435974121
node2 weight score:6636.075894982914
node6: train data size:3007
node6 epoch0:node_model train_loss:0.44173041947426334,train_acc:0.8634561896324158
node6 epoch1:node_model train_loss:0.26364835784319907,train_acc:0.9118432402610779
node6 epoch2:node_model train_loss:0.16847807217028835,train_acc:0.9470044374465942
node6 epoch3:node_model train_loss:0.12395551151806308,train_acc:0.9645159244537354
node6 epoch4:node_model train_loss:0.07760197942655894,train_acc:0.9816126823425293
node6_model on test-dataset: loss:0.7646776261925697,acc:0.7867998480796814
node6 weight score:3932.376071956293
node7: train data size:1951
node7 epoch0:node_model train_loss:0.34395389035344126,train_acc:0.8777157068252563
node7 epoch1:node_model train_loss:0.19688891917467116,train_acc:0.9310588836669922
node7 epoch2:node_model train_loss:0.1168820858001709,train_acc:0.9660195708274841
node7 epoch3:node_model train_loss:0.06423901878297329,train_acc:0.985519528388977
node7 epoch4:node_model train_loss:0.05298913381993771,train_acc:0.991519570350647
node7_model on test-dataset: loss:0.7873341771215201,acc:0.7782997488975525
node7 weight score:2477.9821030160556
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5371398975451788,train_acc:0.8353402018547058
node8 epoch1:node_model train_loss:0.3117231809430652,train_acc:0.8954647779464722
node8 epoch2:node_model train_loss:0.19380778529577786,train_acc:0.938877522945404
node8 epoch3:node_model train_loss:0.11023041750821802,train_acc:0.9671881198883057
node8 epoch4:node_model train_loss:0.06890705165763696,train_acc:0.9872108101844788
node8_model on test-dataset: loss:0.7434544831514358,acc:0.7945998907089233
node8 weight score:2418.4399189825876
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5303663581609726,train_acc:0.831333339214325
node10 epoch1:node_model train_loss:0.270052594691515,train_acc:0.906166672706604
node10 epoch2:node_model train_loss:0.16432917825877666,train_acc:0.9450001120567322
node10 epoch3:node_model train_loss:0.10902044605463743,train_acc:0.9725000262260437
node10 epoch4:node_model train_loss:0.07774352096021175,train_acc:0.9843332171440125
node10_model on test-dataset: loss:0.7873838660120964,acc:0.7786999344825745
node10 weight score:2508.306412224172
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5893413735181093,acc:0.8284999787807464
total cost energy:8.088038328578603 | all_enery_cp：6.7595 | all_enery_tp: 1.328538328578604
ef: 25.359196997408123
reward: 17.27115866882952
step 400:loss:7.171590328216553|running q:56.44234085083008
episode6,iteration40 selected nodes:[5, 12, 15, 3, 1],center node:5
################################################## episode6,iteration40 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.17582000726286104,train_acc:0.9364704489707947
node1 epoch1:node_model train_loss:0.10699660722714137,train_acc:0.9691177010536194
node1 epoch2:node_model train_loss:0.09138097383958452,train_acc:0.9724999666213989
node1 epoch3:node_model train_loss:0.10046185799600447,train_acc:0.9716176390647888
node1 epoch4:node_model train_loss:0.11942906343542478,train_acc:0.9625735878944397
node1_model on test-dataset: loss:0.9079359702765941,acc:0.7754999995231628
node1 weight score:7388.186193302233
node3: train data size:4247
node3 epoch0:node_model train_loss:0.42840083601862883,train_acc:0.8567094206809998
node3 epoch1:node_model train_loss:0.24170701881480772,train_acc:0.9163927435874939
node3 epoch2:node_model train_loss:0.16050292984690778,train_acc:0.948980450630188
node3 epoch3:node_model train_loss:0.12117460127486739,train_acc:0.9624688625335693
node3 epoch4:node_model train_loss:0.08466804997865544,train_acc:0.9781098365783691
node3_model on test-dataset: loss:0.7491936768591404,acc:0.7948002219200134
node3 weight score:5668.761137713793
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5189432588062788,train_acc:0.8328947424888611
node5 epoch1:node_model train_loss:0.28893984462085526,train_acc:0.9030452370643616
node5 epoch2:node_model train_loss:0.21141599471631803,train_acc:0.9332330822944641
node5 epoch3:node_model train_loss:0.1451689158811381,train_acc:0.9504135847091675
node5 epoch4:node_model train_loss:0.13994834356402097,train_acc:0.9538345336914062
node5_model on test-dataset: loss:0.7442275606095791,acc:0.7925999164581299
node5 weight score:5018.626288094935
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5273510026080268,train_acc:0.8343651294708252
node12 epoch1:node_model train_loss:0.2908016964793205,train_acc:0.9054762125015259
node12 epoch2:node_model train_loss:0.17366267208542144,train_acc:0.9464285373687744
node12 epoch3:node_model train_loss:0.09803337923118047,train_acc:0.9658729434013367
node12 epoch4:node_model train_loss:0.0743217019896422,train_acc:0.9790475368499756
node12_model on test-dataset: loss:0.7566255976259708,acc:0.7946999073028564
node12 weight score:1765.7346039995282
node15: train data size:629
node15 epoch0:node_model train_loss:0.7370992430618831,train_acc:0.7786699533462524
node15 epoch1:node_model train_loss:0.37736302614212036,train_acc:0.8683744668960571
node15 epoch2:node_model train_loss:0.1726225252662386,train_acc:0.9372906684875488
node15 epoch3:node_model train_loss:0.12913811739001954,train_acc:0.9701477885246277
node15 epoch4:node_model train_loss:0.10084010979958943,train_acc:0.9728571176528931
node15_model on test-dataset: loss:0.9007543612271547,acc:0.7539998292922974
node15 weight score:698.3035853893325
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6241404430568218,acc:0.8266999822854996
total cost energy:10.58133749650909 | all_enery_cp：8.3275 | all_enery_tp: 2.2538374965090897
ef: 24.79853260275261
reward: 14.21719510624352
step 401:loss:7.227913856506348|running q:57.596981048583984
episode6,iteration41 selected nodes:[18, 2, 11, 12, 9],center node:9
################################################## episode6,iteration41 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3185180453583598,train_acc:0.8867992758750916
node2 epoch1:node_model train_loss:0.1655110095938047,train_acc:0.9414962530136108
node2 epoch2:node_model train_loss:0.11587477948827048,train_acc:0.9669697284698486
node2 epoch3:node_model train_loss:0.08967074821703136,train_acc:0.9753314256668091
node2 epoch4:node_model train_loss:0.06622117505564044,train_acc:0.9864583015441895
node2_model on test-dataset: loss:0.7392309708893299,acc:0.7936999201774597
node2 weight score:6477.001354853692
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5356936266547755,train_acc:0.8376177549362183
node9 epoch1:node_model train_loss:0.27763826282400833,train_acc:0.9076176881790161
node9 epoch2:node_model train_loss:0.17594891237585167,train_acc:0.9451245665550232
node9 epoch3:node_model train_loss:0.1151644453210266,train_acc:0.9665742516517639
node9 epoch4:node_model train_loss:0.06971727664533414,train_acc:0.9818374514579773
node9_model on test-dataset: loss:0.7358052912354469,acc:0.7940002083778381
node9 weight score:2523.7654881252915
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4533721997457392,train_acc:0.8656384348869324
node11 epoch1:node_model train_loss:0.23786687412682703,train_acc:0.9227545857429504
node11 epoch2:node_model train_loss:0.12097611102987738,train_acc:0.9637303352355957
node11 epoch3:node_model train_loss:0.08953269110882983,train_acc:0.9758822321891785
node11 epoch4:node_model train_loss:0.06027412765166339,train_acc:0.9875178933143616
node11_model on test-dataset: loss:0.7817265231907368,acc:0.7903002500534058
node11 weight score:2151.6476032240266
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5315936952829361,train_acc:0.826269805431366
node12 epoch1:node_model train_loss:0.26053070702723097,train_acc:0.908730149269104
node12 epoch2:node_model train_loss:0.15936936331646784,train_acc:0.9531745910644531
node12 epoch3:node_model train_loss:0.10766579263976642,train_acc:0.9660317301750183
node12 epoch4:node_model train_loss:0.05885320396295616,train_acc:0.9887301325798035
node12_model on test-dataset: loss:0.7896943646669388,acc:0.786799967288971
node12 weight score:1691.793761961909
node18: train data size:472
node18 epoch0:node_model train_loss:0.6100066423416137,train_acc:0.8391111493110657
node18 epoch1:node_model train_loss:0.24942079484462737,train_acc:0.9293333292007446
node18 epoch2:node_model train_loss:0.15701780170202256,train_acc:0.9524444937705994
node18 epoch3:node_model train_loss:0.09312254339456558,train_acc:0.9692221879959106
node18 epoch4:node_model train_loss:0.06632878184318543,train_acc:0.9832221865653992
node18_model on test-dataset: loss:0.9025961704552173,acc:0.7595000267028809
node18 weight score:522.935965662197
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.612681583315134,acc:0.8251999813318253
total cost energy:6.819572772124596 | all_enery_cp：5.0675 | all_enery_tp: 1.752072772124596
ef: 24.87330172928531
reward: 18.053728957160715
step 402:loss:5.407155990600586|running q:58.77410125732422
episode6,iteration42 selected nodes:[12, 4, 13, 16, 0],center node:12
################################################## episode6,iteration42 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.4174015103624417,train_acc:0.8608734607696533
node0 epoch1:node_model train_loss:0.24055975847519362,train_acc:0.920301079750061
node0 epoch2:node_model train_loss:0.15039125156517213,train_acc:0.9515337347984314
node0 epoch3:node_model train_loss:0.11567179428843352,train_acc:0.9697288870811462
node0 epoch4:node_model train_loss:0.0992917434957165,train_acc:0.9714992046356201
node0_model on test-dataset: loss:0.6871268007159234,acc:0.8024000525474548
node0 weight score:7543.003699753505
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5024370412741389,train_acc:0.837142825126648
node4 epoch1:node_model train_loss:0.3095441026879208,train_acc:0.8953570127487183
node4 epoch2:node_model train_loss:0.18930234819916741,train_acc:0.9392858147621155
node4 epoch3:node_model train_loss:0.11548993550240993,train_acc:0.9649998545646667
node4 epoch4:node_model train_loss:0.09188395539032561,train_acc:0.9721426963806152
node4_model on test-dataset: loss:0.7585574889928103,acc:0.7918000817298889
node4 weight score:3565.9788997556625
node12: train data size:1336
node12 epoch0:node_model train_loss:0.39307015921388355,train_acc:0.867063581943512
node12 epoch1:node_model train_loss:0.2182292171886989,train_acc:0.9279365539550781
node12 epoch2:node_model train_loss:0.12157111907643932,train_acc:0.9592857360839844
node12 epoch3:node_model train_loss:0.07360702433756419,train_acc:0.9871427416801453
node12 epoch4:node_model train_loss:0.05224556090044124,train_acc:0.991428554058075
node12_model on test-dataset: loss:0.7850830206274986,acc:0.7922999262809753
node12 weight score:1701.7308550784428
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5598491107424101,train_acc:0.823863685131073
node13 epoch1:node_model train_loss:0.2984681638578574,train_acc:0.9000757932662964
node13 epoch2:node_model train_loss:0.1974814763913552,train_acc:0.941969633102417
node13 epoch3:node_model train_loss:0.11505620026340087,train_acc:0.969318151473999
node13 epoch4:node_model train_loss:0.08155749800304572,train_acc:0.9818181395530701
node13_model on test-dataset: loss:0.7675789774954319,acc:0.7902998924255371
node13 weight score:1504.7311532276478
node16: train data size:877
node16 epoch0:node_model train_loss:0.5643038319216834,train_acc:0.8434632420539856
node16 epoch1:node_model train_loss:0.3155837373601066,train_acc:0.9016738533973694
node16 epoch2:node_model train_loss:0.1446512449118826,train_acc:0.9526694416999817
node16 epoch3:node_model train_loss:0.11293036821815702,train_acc:0.9694517254829407
node16 epoch4:node_model train_loss:0.06452618973950545,train_acc:0.9833332896232605
node16_model on test-dataset: loss:0.8114254719018936,acc:0.776299774646759
node16 weight score:1080.8139876905843
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5887751308083534,acc:0.8273999828100205
total cost energy:7.57344090772601 | all_enery_cp：5.628 | all_enery_tp: 1.9454409077260095
ef: 25.146585251286268
reward: 17.57314434356026
step 403:loss:11.539252281188965|running q:59.90862274169922
episode6,iteration43 selected nodes:[5, 19, 17, 9, 15],center node:9
################################################## episode6,iteration43 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.45467593873801987,train_acc:0.8469923734664917
node5 epoch1:node_model train_loss:0.26292171564541367,train_acc:0.9129698872566223
node5 epoch2:node_model train_loss:0.15225478340136378,train_acc:0.9503006339073181
node5 epoch3:node_model train_loss:0.13716843890908517,train_acc:0.9574434161186218
node5 epoch4:node_model train_loss:0.1030106163142543,train_acc:0.9703006744384766
node5_model on test-dataset: loss:0.7700901620835066,acc:0.7857998609542847
node5 weight score:4850.081437081112
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4932733523218255,train_acc:0.8520867228507996
node9 epoch1:node_model train_loss:0.2538352475354546,train_acc:0.9162973165512085
node9 epoch2:node_model train_loss:0.14627081547912799,train_acc:0.9519667625427246
node9 epoch3:node_model train_loss:0.08592110264458154,train_acc:0.9799999594688416
node9 epoch4:node_model train_loss:0.06573975615595516,train_acc:0.9874976277351379
node9_model on test-dataset: loss:0.7209525494277478,acc:0.7957997918128967
node9 weight score:2575.75897536389
node15: train data size:629
node15 epoch0:node_model train_loss:0.629786742585046,train_acc:0.8167980313301086
node15 epoch1:node_model train_loss:0.32219110216413227,train_acc:0.8980787992477417
node15 epoch2:node_model train_loss:0.20136814883777074,train_acc:0.935862123966217
node15 epoch3:node_model train_loss:0.1278742253780365,train_acc:0.9480788111686707
node15 epoch4:node_model train_loss:0.07494359250579562,train_acc:0.9836453199386597
node15_model on test-dataset: loss:0.8650794222950935,acc:0.7648997902870178
node15 weight score:727.100869341263
node17: train data size:442
node17 epoch0:node_model train_loss:0.6306579947471619,train_acc:0.815142810344696
node17 epoch1:node_model train_loss:0.3315786838531494,train_acc:0.9157142639160156
node17 epoch2:node_model train_loss:0.1973970875144005,train_acc:0.9477142691612244
node17 epoch3:node_model train_loss:0.1664041444659233,train_acc:0.9569523930549622
node17 epoch4:node_model train_loss:0.10720301792025566,train_acc:0.9657142758369446
node17_model on test-dataset: loss:0.9614641192555428,acc:0.7494997978210449
node17 weight score:459.71554335510575
node19: train data size:4281
node19 epoch0:node_model train_loss:0.43861042448254517,train_acc:0.8553230166435242
node19 epoch1:node_model train_loss:0.23425878792308097,train_acc:0.9199597239494324
node19 epoch2:node_model train_loss:0.15929136191343152,train_acc:0.9497673511505127
node19 epoch3:node_model train_loss:0.11436359515023786,train_acc:0.9667040109634399
node19 epoch4:node_model train_loss:0.07724186487842438,train_acc:0.9799453616142273
node19_model on test-dataset: loss:0.793206882327795,acc:0.7915999293327332
node19 weight score:5397.078738697661
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6079292629659175,acc:0.8247999811172485
total cost energy:7.340961350232851 | all_enery_cp：5.4719999999999995 | all_enery_tp: 1.8689613502328513
ef: 24.736736212219906
reward: 17.395774861987057
step 404:loss:5.360227584838867|running q:61.00960159301758
episode6,iteration44 selected nodes:[13, 18, 3, 1, 17],center node:17
################################################## episode6,iteration44 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.21385755083140204,train_acc:0.9258822202682495
node1 epoch1:node_model train_loss:0.12585211232961976,train_acc:0.9572796821594238
node1 epoch2:node_model train_loss:0.10969214919297134,train_acc:0.965441107749939
node1 epoch3:node_model train_loss:0.07807867196114625,train_acc:0.9776471257209778
node1 epoch4:node_model train_loss:0.05910954983247554,train_acc:0.9870591759681702
node1_model on test-dataset: loss:0.768104909658432,acc:0.7931999564170837
node1 weight score:8733.182037572153
node3: train data size:4247
node3 epoch0:node_model train_loss:0.40641121600949487,train_acc:0.8623800873756409
node3 epoch1:node_model train_loss:0.2014203903286956,train_acc:0.9268282651901245
node3 epoch2:node_model train_loss:0.13414836614284403,train_acc:0.9567142724990845
node3 epoch3:node_model train_loss:0.10600087878315947,train_acc:0.9690400958061218
node3 epoch4:node_model train_loss:0.07921501708238624,train_acc:0.9787777662277222
node3_model on test-dataset: loss:0.7185720421373845,acc:0.8028001189231873
node3 weight score:5910.332925516203
node13: train data size:1155
node13 epoch0:node_model train_loss:0.47510065386692685,train_acc:0.8510605692863464
node13 epoch1:node_model train_loss:0.2780492330590884,train_acc:0.9060606360435486
node13 epoch2:node_model train_loss:0.14280639899273714,train_acc:0.9551514983177185
node13 epoch3:node_model train_loss:0.10854909693201382,train_acc:0.9709848761558533
node13 epoch4:node_model train_loss:0.0815078029409051,train_acc:0.9791666865348816
node13_model on test-dataset: loss:0.8152094385027886,acc:0.7835999727249146
node13 weight score:1416.8138216373818
node17: train data size:442
node17 epoch0:node_model train_loss:0.753121840953827,train_acc:0.7688571214675903
node17 epoch1:node_model train_loss:0.4061329305171967,train_acc:0.8781904578208923
node17 epoch2:node_model train_loss:0.2451218381524086,train_acc:0.9429523348808289
node17 epoch3:node_model train_loss:0.15704929679632187,train_acc:0.9672380685806274
node17 epoch4:node_model train_loss:0.0834859348833561,train_acc:0.981238067150116
node17_model on test-dataset: loss:0.8430038315057754,acc:0.7737998962402344
node17 weight score:524.3155291601684
node18: train data size:472
node18 epoch0:node_model train_loss:0.5342565953731537,train_acc:0.8574444651603699
node18 epoch1:node_model train_loss:0.2339260071516037,train_acc:0.932888925075531
node18 epoch2:node_model train_loss:0.14972880855202675,train_acc:0.9564444422721863
node18 epoch3:node_model train_loss:0.07659091129899025,train_acc:0.9812221527099609
node18 epoch4:node_model train_loss:0.04837638549506664,train_acc:0.9920000433921814
node18_model on test-dataset: loss:0.9113415150344372,acc:0.7653998136520386
node18 weight score:517.9178082128349
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6061862853169441,acc:0.8291999804973602
total cost energy:8.941063907280483 | all_enery_cp：6.5120000000000005 | all_enery_tp: 2.429063907280484
ef: 24.782480691542574
reward: 15.84141678426209
step 405:loss:19.972047805786133|running q:62.127899169921875
episode6,iteration45 selected nodes:[16, 8, 13, 19, 14],center node:16
################################################## episode6,iteration45 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5372044791777929,train_acc:0.8403968214988708
node8 epoch1:node_model train_loss:0.28616711290346253,train_acc:0.9054761528968811
node8 epoch2:node_model train_loss:0.1704864593015777,train_acc:0.9465987086296082
node8 epoch3:node_model train_loss:0.09707181942131785,train_acc:0.9761109948158264
node8 epoch4:node_model train_loss:0.07903111974398296,train_acc:0.9777664542198181
node8_model on test-dataset: loss:0.7371517779678106,acc:0.7987997531890869
node8 weight score:2439.1177688762405
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5065091463426749,train_acc:0.846969723701477
node13 epoch1:node_model train_loss:0.27197064956029254,train_acc:0.8999242782592773
node13 epoch2:node_model train_loss:0.13335721070567766,train_acc:0.9521211981773376
node13 epoch3:node_model train_loss:0.09783956998338302,train_acc:0.9743181467056274
node13 epoch4:node_model train_loss:0.06332024807731311,train_acc:0.9866666197776794
node13_model on test-dataset: loss:0.7937188053131103,acc:0.7881999611854553
node13 weight score:1455.1752991972635
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6297520647446314,train_acc:0.8352777361869812
node14 epoch1:node_model train_loss:0.2846154806514581,train_acc:0.9117592573165894
node14 epoch2:node_model train_loss:0.18076273798942566,train_acc:0.939536988735199
node14 epoch3:node_model train_loss:0.1086252462118864,train_acc:0.9724999666213989
node14 epoch4:node_model train_loss:0.07468323471645515,train_acc:0.9826852083206177
node14_model on test-dataset: loss:0.7584919603168965,acc:0.7904001474380493
node14 weight score:1545.171288974956
node16: train data size:877
node16 epoch0:node_model train_loss:0.5683859090010325,train_acc:0.8332467079162598
node16 epoch1:node_model train_loss:0.25498829119735295,train_acc:0.9235641956329346
node16 epoch2:node_model train_loss:0.16480152060588202,train_acc:0.9457863569259644
node16 epoch3:node_model train_loss:0.12287655762500233,train_acc:0.9612264633178711
node16 epoch4:node_model train_loss:0.0754254139545891,train_acc:0.9796680808067322
node16_model on test-dataset: loss:0.756000165194273,acc:0.7921000719070435
node16 weight score:1160.05265656871
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3388804218103719,train_acc:0.8843928575515747
node19 epoch1:node_model train_loss:0.17529859896316086,train_acc:0.9411770105361938
node19 epoch2:node_model train_loss:0.11382212481179903,train_acc:0.9674561023712158
node19 epoch3:node_model train_loss:0.08335605714210244,train_acc:0.9757047295570374
node19 epoch4:node_model train_loss:0.0626462691918362,train_acc:0.9839535355567932
node19_model on test-dataset: loss:0.7482731191813946,acc:0.8014000654220581
node19 weight score:5721.17304532252
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6134439569711685,acc:0.8250999808311462
total cost energy:6.036836938971777 | all_enery_cp：4.641500000000001 | all_enery_tp: 1.3953369389717765
ef: 25.182315830879247
reward: 19.14547889190747
step 406:loss:6.885036945343018|running q:63.147186279296875
episode6,iteration46 selected nodes:[10, 19, 17, 16, 2],center node:16
################################################## episode6,iteration46 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2955072526820004,train_acc:0.9057860374450684
node2 epoch1:node_model train_loss:0.16702023272713026,train_acc:0.9425661563873291
node2 epoch2:node_model train_loss:0.10922045384844144,train_acc:0.9674431681632996
node2 epoch3:node_model train_loss:0.08399624191224575,train_acc:0.9769982099533081
node2 epoch4:node_model train_loss:0.06350025728655358,train_acc:0.9841666221618652
node2_model on test-dataset: loss:0.7665220341086387,acc:0.7958998680114746
node2 weight score:6246.395781130799
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5735094428062439,train_acc:0.8248333930969238
node10 epoch1:node_model train_loss:0.31435649171471597,train_acc:0.9004999995231628
node10 epoch2:node_model train_loss:0.19055803399533033,train_acc:0.9394999742507935
node10 epoch3:node_model train_loss:0.12548734471201897,train_acc:0.9628332257270813
node10 epoch4:node_model train_loss:0.0728137637488544,train_acc:0.9839999079704285
node10_model on test-dataset: loss:0.7195693597197532,acc:0.7965001463890076
node10 weight score:2744.697190510158
node16: train data size:877
node16 epoch0:node_model train_loss:0.47209425767262775,train_acc:0.8577921986579895
node16 epoch1:node_model train_loss:0.260526062713729,train_acc:0.9129003882408142
node16 epoch2:node_model train_loss:0.14075051786171067,train_acc:0.9574459195137024
node16 epoch3:node_model train_loss:0.06233351305127144,train_acc:0.9863348007202148
node16 epoch4:node_model train_loss:0.05233926408820682,train_acc:0.9918903708457947
node16_model on test-dataset: loss:0.7463168114423752,acc:0.790600061416626
node16 weight score:1175.1041736619318
node17: train data size:442
node17 epoch0:node_model train_loss:0.7300937354564667,train_acc:0.8006666302680969
node17 epoch1:node_model train_loss:0.38698639869689944,train_acc:0.8777143359184265
node17 epoch2:node_model train_loss:0.19312104880809783,train_acc:0.94647616147995
node17 epoch3:node_model train_loss:0.12502875030040742,train_acc:0.9592380523681641
node17 epoch4:node_model train_loss:0.09002211838960647,train_acc:0.9697142839431763
node17_model on test-dataset: loss:0.8249076907336712,acc:0.7788997292518616
node17 weight score:535.8175283914327
node19: train data size:4281
node19 epoch0:node_model train_loss:0.16741418569933536,train_acc:0.940192461013794
node19 epoch1:node_model train_loss:0.10773189963642942,train_acc:0.964774489402771
node19 epoch2:node_model train_loss:0.08546513535602149,train_acc:0.9758138656616211
node19 epoch3:node_model train_loss:0.05361062413904556,train_acc:0.9876198768615723
node19 epoch4:node_model train_loss:0.04720093023984931,train_acc:0.9892478585243225
node19_model on test-dataset: loss:0.8364430790394545,acc:0.7848997116088867
node19 weight score:5118.100809580694
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6300491663068533,acc:0.8219999819993973
total cost energy:7.733345360840602 | all_enery_cp：6.1815 | all_enery_tp: 1.5518453608406024
ef: 25.216211742806845
reward: 17.482866381966243
step 407:loss:11.514678955078125|running q:64.11257934570312
episode6,iteration47 selected nodes:[9, 18, 6, 17, 0],center node:9
################################################## episode6,iteration47 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.3693894013189353,train_acc:0.8721850514411926
node0 epoch1:node_model train_loss:0.18536810533931622,train_acc:0.9351873993873596
node0 epoch2:node_model train_loss:0.11506145511968778,train_acc:0.9631509184837341
node0 epoch3:node_model train_loss:0.08715372840658976,train_acc:0.9769578576087952
node0 epoch4:node_model train_loss:0.07152759477209586,train_acc:0.982421338558197
node0_model on test-dataset: loss:0.7469063057005405,acc:0.8019000291824341
node0 weight score:6939.290725546554
node6: train data size:3007
node6 epoch0:node_model train_loss:0.43079916267625745,train_acc:0.8641011714935303
node6 epoch1:node_model train_loss:0.27416113018989563,train_acc:0.9108755588531494
node6 epoch2:node_model train_loss:0.18132168619382766,train_acc:0.9399999380111694
node6 epoch3:node_model train_loss:0.1226676861124654,train_acc:0.9637786149978638
node6 epoch4:node_model train_loss:0.13754813036611002,train_acc:0.9562670588493347
node6_model on test-dataset: loss:0.8002871207892894,acc:0.7866004109382629
node6 weight score:3757.4014649071482
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4589020429473174,train_acc:0.8547183871269226
node9 epoch1:node_model train_loss:0.2123943771186628,train_acc:0.9269713163375854
node9 epoch2:node_model train_loss:0.1331974893416229,train_acc:0.9607847929000854
node9 epoch3:node_model train_loss:0.0893414226409636,train_acc:0.9771004915237427
node9 epoch4:node_model train_loss:0.06099701976697696,train_acc:0.9859186410903931
node9_model on test-dataset: loss:0.7703200420737266,acc:0.7901999354362488
node9 weight score:2410.6863362932836
node17: train data size:442
node17 epoch0:node_model train_loss:0.6283607959747315,train_acc:0.8169523477554321
node17 epoch1:node_model train_loss:0.3288181871175766,train_acc:0.8877143859863281
node17 epoch2:node_model train_loss:0.1418841600418091,train_acc:0.9532381296157837
node17 epoch3:node_model train_loss:0.09423726797103882,train_acc:0.9657142758369446
node17 epoch4:node_model train_loss:0.12012341693043709,train_acc:0.977238118648529
node17_model on test-dataset: loss:0.8970638233423233,acc:0.7717002034187317
node17 weight score:492.7185652779702
node18: train data size:472
node18 epoch0:node_model train_loss:0.6234056293964386,train_acc:0.8215555548667908
node18 epoch1:node_model train_loss:0.22515957653522492,train_acc:0.9102222323417664
node18 epoch2:node_model train_loss:0.13823679685592652,train_acc:0.9584444165229797
node18 epoch3:node_model train_loss:0.13951702415943146,train_acc:0.9576666951179504
node18 epoch4:node_model train_loss:0.03956876918673515,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.9638849568367004,acc:0.7585000395774841
node18 weight score:489.6849947207604
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.62296984359622,acc:0.824999977350235
total cost energy:7.045257033209087 | all_enery_cp：5.4805 | all_enery_tp: 1.5647570332090874
ef: 24.72808853160048
reward: 17.682831498391394
step 408:loss:8.560463905334473|running q:65.11337280273438
episode6,iteration48 selected nodes:[4, 12, 17, 19, 3],center node:17
################################################## episode6,iteration48 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.3560814729263616,train_acc:0.8756653666496277
node3 epoch1:node_model train_loss:0.18307266713574874,train_acc:0.9357841610908508
node3 epoch2:node_model train_loss:0.12982936056200847,train_acc:0.9590399265289307
node3 epoch3:node_model train_loss:0.0955642142094845,train_acc:0.9711330533027649
node3 epoch4:node_model train_loss:0.0668524153703867,train_acc:0.9836912155151367
node3_model on test-dataset: loss:0.7601952785253525,acc:0.7974001169204712
node3 weight score:5586.722411955052
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4590206359113966,train_acc:0.8507143259048462
node4 epoch1:node_model train_loss:0.2638462305601154,train_acc:0.9117857813835144
node4 epoch2:node_model train_loss:0.15043815657762544,train_acc:0.9517857432365417
node4 epoch3:node_model train_loss:0.0882397957611829,train_acc:0.9771426916122437
node4 epoch4:node_model train_loss:0.09133889100381307,train_acc:0.9728570580482483
node4_model on test-dataset: loss:0.7672121428698301,acc:0.7919999361038208
node4 weight score:3525.7523295730043
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5270229897328785,train_acc:0.8266667127609253
node12 epoch1:node_model train_loss:0.24518565194947378,train_acc:0.9165873527526855
node12 epoch2:node_model train_loss:0.12425172461995057,train_acc:0.9651587009429932
node12 epoch3:node_model train_loss:0.07109422795474529,train_acc:0.98499995470047
node12 epoch4:node_model train_loss:0.07391520775854588,train_acc:0.9810317158699036
node12_model on test-dataset: loss:0.7408576735854149,acc:0.8025998473167419
node12 weight score:1803.3153298316615
node17: train data size:442
node17 epoch0:node_model train_loss:0.6327097773551941,train_acc:0.8154285550117493
node17 epoch1:node_model train_loss:0.3362129062414169,train_acc:0.8699046969413757
node17 epoch2:node_model train_loss:0.1715679496526718,train_acc:0.9524762034416199
node17 epoch3:node_model train_loss:0.11231662184000016,train_acc:0.9684762358665466
node17 epoch4:node_model train_loss:0.07371729910373688,train_acc:0.9819999933242798
node17_model on test-dataset: loss:0.867158041447401,acc:0.7679999470710754
node17 weight score:509.71100869023104
node19: train data size:4281
node19 epoch0:node_model train_loss:0.18468147013769592,train_acc:0.9348980188369751
node19 epoch1:node_model train_loss:0.10556804518713507,train_acc:0.9650759100914001
node19 epoch2:node_model train_loss:0.06909362633907518,train_acc:0.9830231666564941
node19 epoch3:node_model train_loss:0.05734193013157955,train_acc:0.986046552658081
node19 epoch4:node_model train_loss:0.04261120740150989,train_acc:0.992503821849823
node19_model on test-dataset: loss:0.794784801453352,acc:0.7937998175621033
node19 weight score:5386.363695143286
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6282351550459861,acc:0.8291999787092209
total cost energy:8.42088303421637 | all_enery_cp：6.5055 | all_enery_tp: 1.9153830342163698
ef: 25.12626872335907
reward: 16.705385689142698
step 409:loss:17.87236785888672|running q:66.1518325805664
episode6,iteration49 selected nodes:[0, 2, 15, 14, 19],center node:14
################################################## episode6,iteration49 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.22956565452309755,train_acc:0.9179540276527405
node0 epoch1:node_model train_loss:0.1297822379005643,train_acc:0.9564201235771179
node0 epoch2:node_model train_loss:0.09314293328386086,train_acc:0.9720759391784668
node0 epoch3:node_model train_loss:0.06201732409401582,train_acc:0.9860753417015076
node0 epoch4:node_model train_loss:0.05378386245753903,train_acc:0.9886538982391357
node0_model on test-dataset: loss:0.7444291752576828,acc:0.8027998805046082
node0 weight score:6962.381610320302
node2: train data size:4788
node2 epoch0:node_model train_loss:0.25019305168340605,train_acc:0.9092897772789001
node2 epoch1:node_model train_loss:0.12617731505694488,train_acc:0.9589961767196655
node2 epoch2:node_model train_loss:0.09029826095017295,train_acc:0.9732198119163513
node2 epoch3:node_model train_loss:0.0814331091241911,train_acc:0.9758050441741943
node2 epoch4:node_model train_loss:0.05701840044154475,train_acc:0.9841382503509521
node2_model on test-dataset: loss:0.7811596681177616,acc:0.7925999164581299
node2 weight score:6129.348704774909
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5959239055713018,train_acc:0.8343056440353394
node14 epoch1:node_model train_loss:0.2931853582461675,train_acc:0.9017130136489868
node14 epoch2:node_model train_loss:0.16922553752859434,train_acc:0.9490277171134949
node14 epoch3:node_model train_loss:0.08618239375452201,train_acc:0.9788424968719482
node14 epoch4:node_model train_loss:0.054171370497594275,train_acc:0.9916666746139526
node14_model on test-dataset: loss:0.7702754215523601,acc:0.7916998863220215
node14 weight score:1521.5336841957542
node15: train data size:629
node15 epoch0:node_model train_loss:0.6048681693417686,train_acc:0.819507360458374
node15 epoch1:node_model train_loss:0.26193054978336605,train_acc:0.9023645520210266
node15 epoch2:node_model train_loss:0.2471197886126382,train_acc:0.9210837483406067
node15 epoch3:node_model train_loss:0.1732859813741275,train_acc:0.9360098838806152
node15 epoch4:node_model train_loss:0.09457912615367345,train_acc:0.9765024781227112
node15_model on test-dataset: loss:1.0018260037899018,acc:0.752599835395813
node15 weight score:627.8535370618218
node19: train data size:4281
node19 epoch0:node_model train_loss:0.11567434564579365,train_acc:0.9609445929527283
node19 epoch1:node_model train_loss:0.08254862204194069,train_acc:0.9742547869682312
node19 epoch2:node_model train_loss:0.06999277838960637,train_acc:0.979767382144928
node19 epoch3:node_model train_loss:0.05173637999524904,train_acc:0.9881395697593689
node19 epoch4:node_model train_loss:0.034509449655753234,train_acc:0.9938989877700806
node19_model on test-dataset: loss:0.8091109551489353,acc:0.7885999083518982
node19 weight score:5290.992505733636
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6146076077222824,acc:0.832199981212616
total cost energy:10.100233241397849 | all_enery_cp：8.0265 | all_enery_tp: 2.073733241397849
ef: 25.055377615665922
reward: 14.955144374268073
step 410:loss:10.329710960388184|running q:67.06927490234375
episode6,iteration50 selected nodes:[18, 17, 19, 7, 3],center node:17
################################################## episode6,iteration50 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.28084106985912766,train_acc:0.9008954763412476
node3 epoch1:node_model train_loss:0.14926374331116676,train_acc:0.9464225769042969
node3 epoch2:node_model train_loss:0.07615817879694839,train_acc:0.9795052409172058
node3 epoch3:node_model train_loss:0.06415330956495086,train_acc:0.9834288358688354
node3 epoch4:node_model train_loss:0.051784144779450675,train_acc:0.9874418377876282
node3_model on test-dataset: loss:0.7571457861363888,acc:0.8009000420570374
node3 weight score:5609.22358383827
node7: train data size:1951
node7 epoch0:node_model train_loss:0.49627319872379305,train_acc:0.8407352566719055
node7 epoch1:node_model train_loss:0.2609801098704338,train_acc:0.914078414440155
node7 epoch2:node_model train_loss:0.14517733063548804,train_acc:0.951539158821106
node7 epoch3:node_model train_loss:0.0862247516401112,train_acc:0.9759999513626099
node7 epoch4:node_model train_loss:0.04434503437951207,train_acc:0.9895195364952087
node7_model on test-dataset: loss:0.737989796102047,acc:0.8011001348495483
node7 weight score:2643.6679887782916
node17: train data size:442
node17 epoch0:node_model train_loss:0.6685679078102111,train_acc:0.8159047365188599
node17 epoch1:node_model train_loss:0.401841402053833,train_acc:0.8829523324966431
node17 epoch2:node_model train_loss:0.20050489008426667,train_acc:0.9384762048721313
node17 epoch3:node_model train_loss:0.15627763122320176,train_acc:0.955714225769043
node17 epoch4:node_model train_loss:0.06803307309746742,train_acc:0.9804762005805969
node17_model on test-dataset: loss:0.8455859030038119,acc:0.7748000025749207
node17 weight score:522.7144852224523
node18: train data size:472
node18 epoch0:node_model train_loss:0.589112663269043,train_acc:0.8311110734939575
node18 epoch1:node_model train_loss:0.20297116339206694,train_acc:0.928444504737854
node18 epoch2:node_model train_loss:0.11181620508432388,train_acc:0.9568888545036316
node18 epoch3:node_model train_loss:0.06600415445864201,train_acc:0.9819999933242798
node18 epoch4:node_model train_loss:0.052497129142284396,train_acc:0.9892221689224243
node18_model on test-dataset: loss:1.0246591656655073,acc:0.7612997889518738
node18 weight score:460.6409778157208
node19: train data size:4281
node19 epoch0:node_model train_loss:0.12077940316047779,train_acc:0.9613407850265503
node19 epoch1:node_model train_loss:0.06788409644261349,train_acc:0.9784954786300659
node19 epoch2:node_model train_loss:0.059341201168853185,train_acc:0.9832558035850525
node19 epoch3:node_model train_loss:0.057853852636938874,train_acc:0.9845966100692749
node19 epoch4:node_model train_loss:0.04019965439341789,train_acc:0.9918603897094727
node19_model on test-dataset: loss:0.8232560732960701,acc:0.7879999279975891
node19 weight score:5200.083107629151
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.651545981913805,acc:0.8243999791145324
total cost energy:7.72613544621789 | all_enery_cp：5.6964999999999995 | all_enery_tp: 2.0296354462178914
ef: 24.817321211371492
reward: 17.091185765153604
step 411:loss:9.762848854064941|running q:68.02429962158203
episode6,iteration51 selected nodes:[1, 13, 7, 3, 17],center node:7
################################################## episode6,iteration51 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.25368917130810376,train_acc:0.9120588302612305
node1 epoch1:node_model train_loss:0.15026042374837048,train_acc:0.9499998688697815
node1 epoch2:node_model train_loss:0.0954921792863923,train_acc:0.9714706540107727
node1 epoch3:node_model train_loss:0.06215172387002146,train_acc:0.9825001955032349
node1 epoch4:node_model train_loss:0.047370633521281624,train_acc:0.9914708733558655
node1_model on test-dataset: loss:0.7706135867536068,acc:0.8073001503944397
node1 weight score:8704.751791697636
node3: train data size:4247
node3 epoch0:node_model train_loss:0.1621816276117813,train_acc:0.9450567364692688
node3 epoch1:node_model train_loss:0.10595104020348815,train_acc:0.9664224982261658
node3 epoch2:node_model train_loss:0.08302704673693624,train_acc:0.9734883904457092
node3 epoch3:node_model train_loss:0.05167756190653457,train_acc:0.9865116477012634
node3 epoch4:node_model train_loss:0.045371943435003594,train_acc:0.9879068732261658
node3_model on test-dataset: loss:0.8284169083833695,acc:0.7972001433372498
node3 weight score:5126.645722729021
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4132221594452858,train_acc:0.8711568117141724
node7 epoch1:node_model train_loss:0.18296166472136974,train_acc:0.9415000081062317
node7 epoch2:node_model train_loss:0.11341407764703035,train_acc:0.9640588164329529
node7 epoch3:node_model train_loss:0.06599000655114651,train_acc:0.9849998354911804
node7 epoch4:node_model train_loss:0.05911314561963081,train_acc:0.9850391745567322
node7_model on test-dataset: loss:0.8092473904043436,acc:0.793299674987793
node7 weight score:2410.8820406886643
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6089349215229353,train_acc:0.8216667175292969
node13 epoch1:node_model train_loss:0.2990346935888131,train_acc:0.901439368724823
node13 epoch2:node_model train_loss:0.13808948639780283,train_acc:0.9553029537200928
node13 epoch3:node_model train_loss:0.08583691002180178,train_acc:0.9724999666213989
node13 epoch4:node_model train_loss:0.04827858600765467,train_acc:0.9918181300163269
node13_model on test-dataset: loss:0.8025419762730599,acc:0.7922999262809753
node13 weight score:1439.1770575836129
node17: train data size:442
node17 epoch0:node_model train_loss:0.7185654520988465,train_acc:0.8096190690994263
node17 epoch1:node_model train_loss:0.33493642807006835,train_acc:0.8726666569709778
node17 epoch2:node_model train_loss:0.24408303797245026,train_acc:0.9257143139839172
node17 epoch3:node_model train_loss:0.13496995717287064,train_acc:0.968000054359436
node17 epoch4:node_model train_loss:0.07779553160071373,train_acc:0.9819999933242798
node17_model on test-dataset: loss:0.9459951144456863,acc:0.7632997035980225
node17 weight score:467.2328569677589
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6767482240498066,acc:0.8243999820947647
total cost energy:8.957273595494359 | all_enery_cp：7.2515 | all_enery_tp: 1.7057735954943585
ef: 24.815063878300037
reward: 15.857790282805679
step 412:loss:10.715243339538574|running q:69.034912109375
episode6,iteration52 selected nodes:[4, 3, 6, 2, 12],center node:3
################################################## episode6,iteration52 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.26172477736448246,train_acc:0.9099242687225342
node2 epoch1:node_model train_loss:0.13237729280566177,train_acc:0.9563540816307068
node2 epoch2:node_model train_loss:0.08646182521867256,train_acc:0.9741100072860718
node2 epoch3:node_model train_loss:0.05831435687529544,train_acc:0.9862217307090759
node2 epoch4:node_model train_loss:0.044060274958610535,train_acc:0.9891382455825806
node2_model on test-dataset: loss:0.8100108431279659,acc:0.7975000739097595
node2 weight score:5911.0319826219775
node3: train data size:4247
node3 epoch0:node_model train_loss:0.142836214238128,train_acc:0.9480503797531128
node3 epoch1:node_model train_loss:0.09619361396099246,train_acc:0.9687480330467224
node3 epoch2:node_model train_loss:0.06337711548562659,train_acc:0.9816278219223022
node3 epoch3:node_model train_loss:0.04941612018575502,train_acc:0.9874120354652405
node3 epoch4:node_model train_loss:0.040866577781217044,train_acc:0.9904650449752808
node3_model on test-dataset: loss:0.8472355064749718,acc:0.7926998734474182
node3 weight score:5012.773859856476
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4265336846666677,train_acc:0.868571400642395
node4 epoch1:node_model train_loss:0.2390326117830617,train_acc:0.9264285564422607
node4 epoch2:node_model train_loss:0.13401714117831684,train_acc:0.9567856788635254
node4 epoch3:node_model train_loss:0.11446926982275077,train_acc:0.9653571248054504
node4 epoch4:node_model train_loss:0.1359063195330756,train_acc:0.9610713124275208
node4_model on test-dataset: loss:0.9026500575244427,acc:0.7729997634887695
node4 weight score:2996.7316541457726
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4016156713328054,train_acc:0.875806450843811
node6 epoch1:node_model train_loss:0.23375204683191353,train_acc:0.9232257604598999
node6 epoch2:node_model train_loss:0.1307443885072585,train_acc:0.9587095975875854
node6 epoch3:node_model train_loss:0.0902272185250636,train_acc:0.9745157957077026
node6 epoch4:node_model train_loss:0.08176147709450414,train_acc:0.977096438407898
node6_model on test-dataset: loss:0.8076561398804187,acc:0.7941001057624817
node6 weight score:3723.1190992310358
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5353984960487911,train_acc:0.8318254351615906
node12 epoch1:node_model train_loss:0.27152335324457716,train_acc:0.9146032333374023
node12 epoch2:node_model train_loss:0.14388655658279145,train_acc:0.946587324142456
node12 epoch3:node_model train_loss:0.12235083750316075,train_acc:0.964206337928772
node12 epoch4:node_model train_loss:0.0679960542225412,train_acc:0.979999840259552
node12_model on test-dataset: loss:0.8275179982185363,acc:0.790300190448761
node12 weight score:1614.4663957474197
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6412896525859832,acc:0.8278999811410904
total cost energy:9.578074631273694 | all_enery_cp：8.0415 | all_enery_tp: 1.5365746312736948
ef: 25.07315790315788
reward: 15.495083271884187
step 413:loss:11.049811363220215|running q:69.98143768310547
episode6,iteration53 selected nodes:[19, 8, 1, 9, 16],center node:9
################################################## episode6,iteration53 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.16533172996166876,train_acc:0.9409558176994324
node1 epoch1:node_model train_loss:0.10822123979382656,train_acc:0.9627206325531006
node1 epoch2:node_model train_loss:0.09560121023370062,train_acc:0.9697059392929077
node1 epoch3:node_model train_loss:0.07909767863833729,train_acc:0.9763234257698059
node1 epoch4:node_model train_loss:0.053621670200407284,train_acc:0.9866180419921875
node1_model on test-dataset: loss:0.808436943590641,acc:0.8016999363899231
node1 weight score:8297.493147958678
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5099370827277502,train_acc:0.8514512777328491
node8 epoch1:node_model train_loss:0.3178003169596195,train_acc:0.8982199430465698
node8 epoch2:node_model train_loss:0.19224821320838398,train_acc:0.9327322840690613
node8 epoch3:node_model train_loss:0.11678539319998688,train_acc:0.9627323150634766
node8 epoch4:node_model train_loss:0.06047523383878999,train_acc:0.9821881055831909
node8_model on test-dataset: loss:0.8165633250772953,acc:0.786500096321106
node8 weight score:2201.911284504239
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4885456389502475,train_acc:0.8570822477340698
node9 epoch1:node_model train_loss:0.24823229250154996,train_acc:0.9103878736495972
node9 epoch2:node_model train_loss:0.15880240107837476,train_acc:0.9427516460418701
node9 epoch3:node_model train_loss:0.09560730053406012,train_acc:0.9693350791931152
node9 epoch4:node_model train_loss:0.058740541534988505,train_acc:0.9848660230636597
node9_model on test-dataset: loss:0.7930265781283379,acc:0.7929003238677979
node9 weight score:2341.661744027293
node16: train data size:877
node16 epoch0:node_model train_loss:0.5659543640083737,train_acc:0.8426839709281921
node16 epoch1:node_model train_loss:0.2509726219707065,train_acc:0.9234488010406494
node16 epoch2:node_model train_loss:0.14869710927208266,train_acc:0.9574459195137024
node16 epoch3:node_model train_loss:0.08809676559434997,train_acc:0.9752236604690552
node16 epoch4:node_model train_loss:0.07130100785030259,train_acc:0.988556981086731
node16_model on test-dataset: loss:0.8293696500360965,acc:0.7839999198913574
node16 weight score:1057.4295791530717
node19: train data size:4281
node19 epoch0:node_model train_loss:0.16262032628752465,train_acc:0.9400142431259155
node19 epoch1:node_model train_loss:0.07919412574102712,train_acc:0.9751850366592407
node19 epoch2:node_model train_loss:0.051313320283106596,train_acc:0.9883720874786377
node19 epoch3:node_model train_loss:0.03297341980993054,train_acc:0.9932556748390198
node19 epoch4:node_model train_loss:0.027891961467820546,train_acc:0.9957594871520996
node19_model on test-dataset: loss:0.8319058331847191,acc:0.7938998937606812
node19 weight score:5146.015124826553
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6664420503377915,acc:0.8234999793767929
total cost energy:9.817459551080336 | all_enery_cp：7.7604999999999995 | all_enery_tp: 2.056959551080336
ef: 24.917349300775662
reward: 15.099889749695325
step 414:loss:16.975658416748047|running q:70.97401428222656
episode6,iteration54 selected nodes:[18, 13, 2, 16, 1],center node:16
################################################## episode6,iteration54 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.09689258767620605,train_acc:0.9691913723945618
node1 epoch1:node_model train_loss:0.10479555013315643,train_acc:0.9652942419052124
node1 epoch2:node_model train_loss:0.06627908547628014,train_acc:0.9808825254440308
node1 epoch3:node_model train_loss:0.05205522798111334,train_acc:0.9869121313095093
node1 epoch4:node_model train_loss:0.04014584026299417,train_acc:0.9897061586380005
node1_model on test-dataset: loss:0.9172445846349001,acc:0.78659987449646
node1 weight score:7313.207526507285
node2: train data size:4788
node2 epoch0:node_model train_loss:0.20689648979653916,train_acc:0.928428053855896
node2 epoch1:node_model train_loss:0.10375825230342646,train_acc:0.9646779298782349
node2 epoch2:node_model train_loss:0.07366053061559796,train_acc:0.977888286113739
node2 epoch3:node_model train_loss:0.05085981412169834,train_acc:0.9883334636688232
node2 epoch4:node_model train_loss:0.03764084695527951,train_acc:0.9918751120567322
node2_model on test-dataset: loss:0.7958389112353325,acc:0.8012000322341919
node2 weight score:6016.29291104638
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5792482433219751,train_acc:0.835606038570404
node13 epoch1:node_model train_loss:0.2634336973230044,train_acc:0.9121970534324646
node13 epoch2:node_model train_loss:0.15559913342197737,train_acc:0.9507575035095215
node13 epoch3:node_model train_loss:0.08728831633925438,train_acc:0.9741665720939636
node13 epoch4:node_model train_loss:0.08032441822191079,train_acc:0.9786362648010254
node13_model on test-dataset: loss:0.7913055089116097,acc:0.7982998490333557
node13 weight score:1459.6132429162903
node16: train data size:877
node16 epoch0:node_model train_loss:0.5234839816888174,train_acc:0.8355699777603149
node16 epoch1:node_model train_loss:0.23026074137952593,train_acc:0.92311692237854
node16 epoch2:node_model train_loss:0.16735181212425232,train_acc:0.9421210885047913
node16 epoch3:node_model train_loss:0.10156117131312688,train_acc:0.9707792401313782
node16 epoch4:node_model train_loss:0.0733110975060198,train_acc:0.97822505235672
node16_model on test-dataset: loss:0.8458497866988182,acc:0.7879999279975891
node16 weight score:1036.827122015074
node18: train data size:472
node18 epoch0:node_model train_loss:0.6306881010532379,train_acc:0.82833331823349
node18 epoch1:node_model train_loss:0.2715449661016464,train_acc:0.9281110763549805
node18 epoch2:node_model train_loss:0.14017608314752578,train_acc:0.9517777562141418
node18 epoch3:node_model train_loss:0.08729836940765381,train_acc:0.9772222638130188
node18 epoch4:node_model train_loss:0.06515159271657467,train_acc:0.9899999499320984
node18_model on test-dataset: loss:0.9729351451992989,acc:0.7634000182151794
node18 weight score:485.12997225864854
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6943949896097183,acc:0.8234999823570252
total cost energy:9.544366289279758 | all_enery_cp：7.0 | all_enery_tp: 2.5443662892797567
ef: 24.592682377355153
reward: 15.048316088075396
step 415:loss:11.15087604522705|running q:71.97257995605469
episode6,iteration55 selected nodes:[18, 12, 17, 4, 19],center node:17
################################################## episode6,iteration55 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4350236728787422,train_acc:0.8639285564422607
node4 epoch1:node_model train_loss:0.23916106604571855,train_acc:0.9185714721679688
node4 epoch2:node_model train_loss:0.15241038080837047,train_acc:0.9464285373687744
node4 epoch3:node_model train_loss:0.15771533588745765,train_acc:0.9432142972946167
node4 epoch4:node_model train_loss:0.17596481893477695,train_acc:0.9417858719825745
node4_model on test-dataset: loss:0.9427122865617276,acc:0.7760999202728271
node4 weight score:2869.380232505202
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5516795430864606,train_acc:0.8339682817459106
node12 epoch1:node_model train_loss:0.3287346065044403,train_acc:0.8927779197692871
node12 epoch2:node_model train_loss:0.16236361382263048,train_acc:0.9487302303314209
node12 epoch3:node_model train_loss:0.08271139607365642,train_acc:0.9773015975952148
node12 epoch4:node_model train_loss:0.05671723160360541,train_acc:0.983015775680542
node12_model on test-dataset: loss:0.853486974388361,acc:0.7859999537467957
node12 weight score:1565.3431629198853
node17: train data size:442
node17 epoch0:node_model train_loss:0.6912642002105713,train_acc:0.8059048056602478
node17 epoch1:node_model train_loss:0.2964469462633133,train_acc:0.9161904454231262
node17 epoch2:node_model train_loss:0.2102295383810997,train_acc:0.9504761695861816
node17 epoch3:node_model train_loss:0.16288510710000992,train_acc:0.971714198589325
node17 epoch4:node_model train_loss:0.07622745409607887,train_acc:0.9824762344360352
node17_model on test-dataset: loss:1.0719731893762947,acc:0.7459000945091248
node17 weight score:412.32374501564584
node18: train data size:472
node18 epoch0:node_model train_loss:0.6018541812896728,train_acc:0.8431110382080078
node18 epoch1:node_model train_loss:0.2247572958469391,train_acc:0.9352222681045532
node18 epoch2:node_model train_loss:0.1432066209614277,train_acc:0.9512222409248352
node18 epoch3:node_model train_loss:0.12220260873436928,train_acc:0.9644444584846497
node18 epoch4:node_model train_loss:0.07305695824325084,train_acc:0.9788889288902283
node18_model on test-dataset: loss:1.1132318952679634,acc:0.7463000416755676
node18 weight score:423.9907264661924
node19: train data size:4281
node19 epoch0:node_model train_loss:0.1826441364232884,train_acc:0.9353631734848022
node19 epoch1:node_model train_loss:0.08477489866836127,train_acc:0.9726816415786743
node19 epoch2:node_model train_loss:0.046541422367269214,train_acc:0.9888371229171753
node19 epoch3:node_model train_loss:0.03185637046171482,train_acc:0.9929141402244568
node19 epoch4:node_model train_loss:0.022212415825315687,train_acc:0.9972093105316162
node19_model on test-dataset: loss:0.7823741771280766,acc:0.8068000078201294
node19 weight score:5471.8063621611445
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6680261462926864,acc:0.8253999793529511
total cost energy:6.541659287246192 | all_enery_cp：4.618 | all_enery_tp: 1.9236592872461917
ef: 24.413807642781336
reward: 17.872148355535145
step 416:loss:12.0916166305542|running q:72.89415740966797
episode6,iteration56 selected nodes:[10, 11, 2, 12, 18],center node:11
################################################## episode6,iteration56 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.16869523601296046,train_acc:0.9395265579223633
node2 epoch1:node_model train_loss:0.09070122971509893,train_acc:0.9705963134765625
node2 epoch2:node_model train_loss:0.07155342842452228,train_acc:0.9774717688560486
node2 epoch3:node_model train_loss:0.03907972591696307,train_acc:0.9922916889190674
node2 epoch4:node_model train_loss:0.028155847219750285,train_acc:0.995180070400238
node2_model on test-dataset: loss:0.782118016332388,acc:0.8050001859664917
node2 weight score:6121.838264834415
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5837968453764916,train_acc:0.824833333492279
node10 epoch1:node_model train_loss:0.29949417635798453,train_acc:0.9103332757949829
node10 epoch2:node_model train_loss:0.17783540971577166,train_acc:0.9469999670982361
node10 epoch3:node_model train_loss:0.120606804266572,train_acc:0.9629999399185181
node10 epoch4:node_model train_loss:0.07881595138460398,train_acc:0.9813331961631775
node10_model on test-dataset: loss:0.8245967523753643,acc:0.7836999297142029
node10 weight score:2395.1100878226125
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5685674434199053,train_acc:0.8346055150032043
node11 epoch1:node_model train_loss:0.26513928941944065,train_acc:0.9100143313407898
node11 epoch2:node_model train_loss:0.1708901997874765,train_acc:0.9519654512405396
node11 epoch3:node_model train_loss:0.1010616604677018,train_acc:0.9693542718887329
node11 epoch4:node_model train_loss:0.06133456133744296,train_acc:0.9881061315536499
node11_model on test-dataset: loss:0.8329434570670128,acc:0.7871999144554138
node11 weight score:2019.3447535114956
node12: train data size:1336
node12 epoch0:node_model train_loss:0.43165547187839237,train_acc:0.8556350469589233
node12 epoch1:node_model train_loss:0.24370554941041128,train_acc:0.9176190495491028
node12 epoch2:node_model train_loss:0.14135913896773541,train_acc:0.9531746506690979
node12 epoch3:node_model train_loss:0.09702069485293967,train_acc:0.977142870426178
node12 epoch4:node_model train_loss:0.04344806181532996,train_acc:0.9885713458061218
node12_model on test-dataset: loss:0.8979807803034783,acc:0.7822001576423645
node12 weight score:1487.7823994723924
node18: train data size:472
node18 epoch0:node_model train_loss:0.6003072082996368,train_acc:0.8494443893432617
node18 epoch1:node_model train_loss:0.206692436337471,train_acc:0.925000011920929
node18 epoch2:node_model train_loss:0.1844327539205551,train_acc:0.9441110491752625
node18 epoch3:node_model train_loss:0.09114577844738961,train_acc:0.972444474697113
node18 epoch4:node_model train_loss:0.04352129064500332,train_acc:0.9912222027778625
node18_model on test-dataset: loss:0.9094690164923668,acc:0.7743997573852539
node18 weight score:518.9841450788572
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6587919689714908,acc:0.8246999782323837
total cost energy:7.008413190966076 | all_enery_cp：5.1265 | all_enery_tp: 1.8819131909660762
ef: 24.64764998643049
reward: 17.639236795464413
step 417:loss:15.416632652282715|running q:73.71697998046875
episode6,iteration57 selected nodes:[18, 16, 10, 14, 11],center node:14
################################################## episode6,iteration57 ##################################################
node10: train data size:1975
node10 epoch0:node_model train_loss:0.4683265514671803,train_acc:0.8555000424385071
node10 epoch1:node_model train_loss:0.22345542013645173,train_acc:0.9236665964126587
node10 epoch2:node_model train_loss:0.15158616192638874,train_acc:0.9511666297912598
node10 epoch3:node_model train_loss:0.10395392570644617,train_acc:0.9701666235923767
node10 epoch4:node_model train_loss:0.06950462851673364,train_acc:0.9809999465942383
node10_model on test-dataset: loss:0.8334749326109886,acc:0.7871995568275452
node10 weight score:2369.597360070576
node11: train data size:1682
node11 epoch0:node_model train_loss:0.40189601305653067,train_acc:0.8732855319976807
node11 epoch1:node_model train_loss:0.19563023000955582,train_acc:0.9351076483726501
node11 epoch2:node_model train_loss:0.11867758697446655,train_acc:0.9596126675605774
node11 epoch3:node_model train_loss:0.07339721698971356,train_acc:0.9781060814857483
node11 epoch4:node_model train_loss:0.043338176520431745,train_acc:0.9910473823547363
node11_model on test-dataset: loss:0.8225253535807133,acc:0.7953999042510986
node11 weight score:2044.92176767284
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6087484980622927,train_acc:0.838101863861084
node14 epoch1:node_model train_loss:0.3136648175617059,train_acc:0.8872684836387634
node14 epoch2:node_model train_loss:0.1860376043866078,train_acc:0.9380555152893066
node14 epoch3:node_model train_loss:0.10514281162371238,train_acc:0.9735184907913208
node14 epoch4:node_model train_loss:0.0902892304584384,train_acc:0.9799998998641968
node14_model on test-dataset: loss:0.7933890432119369,acc:0.7998001575469971
node14 weight score:1477.2071911345079
node16: train data size:877
node16 epoch0:node_model train_loss:0.5688133570883009,train_acc:0.8511255383491516
node16 epoch1:node_model train_loss:0.26594921118683285,train_acc:0.9151226878166199
node16 epoch2:node_model train_loss:0.15962729354699454,train_acc:0.9524529576301575
node16 epoch3:node_model train_loss:0.07402957127326065,train_acc:0.9799999594688416
node16 epoch4:node_model train_loss:0.04333325371974044,train_acc:0.9900000095367432
node16_model on test-dataset: loss:0.856273138821125,acc:0.7827001214027405
node16 weight score:1024.205899074927
node18: train data size:472
node18 epoch0:node_model train_loss:0.5204678177833557,train_acc:0.8403332829475403
node18 epoch1:node_model train_loss:0.2867660939693451,train_acc:0.9308889508247375
node18 epoch2:node_model train_loss:0.08570835813879966,train_acc:0.97044438123703
node18 epoch3:node_model train_loss:0.08714468106627464,train_acc:0.9696666598320007
node18 epoch4:node_model train_loss:0.02484250795096159,train_acc:0.9959999918937683
node18_model on test-dataset: loss:1.080487274453044,acc:0.7532997727394104
node18 weight score:436.83994356984186
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6522107049822807,acc:0.8233999788761139
total cost energy:4.301899020449196 | all_enery_cp：3.0889999999999995 | all_enery_tp: 1.212899020449196
ef: 24.733975734272498
reward: 20.432076713823303
step 418:loss:11.956012725830078|running q:74.67738342285156
episode6,iteration58 selected nodes:[17, 19, 14, 3, 2],center node:14
################################################## episode6,iteration58 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.11225636396557093,train_acc:0.9642897844314575
node2 epoch1:node_model train_loss:0.06406119029270485,train_acc:0.9830967783927917
node2 epoch2:node_model train_loss:0.04882507877967631,train_acc:0.987708330154419
node2 epoch3:node_model train_loss:0.04780375313324233,train_acc:0.9880966544151306
node2 epoch4:node_model train_loss:0.04042361302223677,train_acc:0.9904168844223022
node2_model on test-dataset: loss:0.8500806438922882,acc:0.7900999188423157
node2 weight score:5632.4068009324965
node3: train data size:4247
node3 epoch0:node_model train_loss:0.2760122778803803,train_acc:0.9036318063735962
node3 epoch1:node_model train_loss:0.14351967826139095,train_acc:0.9527016282081604
node3 epoch2:node_model train_loss:0.08216023133244625,train_acc:0.9748243093490601
node3 epoch3:node_model train_loss:0.06190098674837933,train_acc:0.9843889474868774
node3 epoch4:node_model train_loss:0.03604746791859006,train_acc:0.9934884905815125
node3_model on test-dataset: loss:0.7724310219287872,acc:0.8053998351097107
node3 weight score:5498.2255753984255
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4196941914657752,train_acc:0.866759181022644
node14 epoch1:node_model train_loss:0.19627284879485765,train_acc:0.9290277361869812
node14 epoch2:node_model train_loss:0.1349644878258308,train_acc:0.960509181022644
node14 epoch3:node_model train_loss:0.06217762532954415,train_acc:0.9849998950958252
node14 epoch4:node_model train_loss:0.06909762788563967,train_acc:0.9793518781661987
node14_model on test-dataset: loss:0.8723068153858184,acc:0.7834001183509827
node14 weight score:1343.563960900189
node17: train data size:442
node17 epoch0:node_model train_loss:0.6876029431819916,train_acc:0.8008571863174438
node17 epoch1:node_model train_loss:0.3208609759807587,train_acc:0.9046667218208313
node17 epoch2:node_model train_loss:0.16508527994155883,train_acc:0.949238121509552
node17 epoch3:node_model train_loss:0.10268522948026657,train_acc:0.9644762277603149
node17 epoch4:node_model train_loss:0.056573516130447386,train_acc:0.9879999160766602
node17_model on test-dataset: loss:0.8409178087115288,acc:0.7900998592376709
node17 weight score:525.616172497573
node19: train data size:4281
node19 epoch0:node_model train_loss:0.1272796371648478,train_acc:0.9562387466430664
node19 epoch1:node_model train_loss:0.06504381686275781,train_acc:0.9789606928825378
node19 epoch2:node_model train_loss:0.05447637510680875,train_acc:0.9846512675285339
node19 epoch3:node_model train_loss:0.039993459488763365,train_acc:0.9897130131721497
node19 epoch4:node_model train_loss:0.03305239908310563,train_acc:0.9916279911994934
node19_model on test-dataset: loss:0.8965365462750197,acc:0.7868001461029053
node19 weight score:4775.042375893028
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6678651863336563,acc:0.826499980688095
total cost energy:9.167435702206713 | all_enery_cp：7.465 | all_enery_tp: 1.7024357022067143
ef: 24.951472484103803
reward: 15.78403678189709
step 419:loss:14.178789138793945|running q:75.6044692993164
episode6,iteration59 selected nodes:[1, 13, 15, 16, 19],center node:16
################################################## episode6,iteration59 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.14368637103368254,train_acc:0.9502207636833191
node1 epoch1:node_model train_loss:0.09560841113767203,train_acc:0.9660295844078064
node1 epoch2:node_model train_loss:0.10617601545527577,train_acc:0.9618383049964905
node1 epoch3:node_model train_loss:0.06935944999842082,train_acc:0.9795588254928589
node1 epoch4:node_model train_loss:0.06677253252607496,train_acc:0.9810296297073364
node1_model on test-dataset: loss:0.8215192194283009,acc:0.8055000305175781
node1 weight score:8165.359788743749
node13: train data size:1155
node13 epoch0:node_model train_loss:0.597208579381307,train_acc:0.8353788256645203
node13 epoch1:node_model train_loss:0.2775070828696092,train_acc:0.92090904712677
node13 epoch2:node_model train_loss:0.1818401279548804,train_acc:0.9457575678825378
node13 epoch3:node_model train_loss:0.08664361387491226,train_acc:0.9750000238418579
node13 epoch4:node_model train_loss:0.06848275288939476,train_acc:0.9818181395530701
node13_model on test-dataset: loss:0.8027286197245121,acc:0.8016999363899231
node13 weight score:1438.842432697097
node15: train data size:629
node15 epoch0:node_model train_loss:0.7057833884443555,train_acc:0.8033004999160767
node15 epoch1:node_model train_loss:0.2938855801309858,train_acc:0.905862033367157
node15 epoch2:node_model train_loss:0.16904710565826722,train_acc:0.9457142949104309
node15 epoch3:node_model train_loss:0.1262990034052304,train_acc:0.9572907090187073
node15 epoch4:node_model train_loss:0.07816749119332858,train_acc:0.9750738739967346
node15_model on test-dataset: loss:0.8697055606544017,acc:0.780299961566925
node15 weight score:723.2332739447071
node16: train data size:877
node16 epoch0:node_model train_loss:0.4489517841074202,train_acc:0.8703463673591614
node16 epoch1:node_model train_loss:0.291984510090616,train_acc:0.9253391623497009
node16 epoch2:node_model train_loss:0.14557931323846182,train_acc:0.9515584111213684
node16 epoch3:node_model train_loss:0.0854513868689537,train_acc:0.9733332395553589
node16 epoch4:node_model train_loss:0.04443239006731245,train_acc:0.9918902516365051
node16_model on test-dataset: loss:0.8317156785726547,acc:0.7918999195098877
node16 weight score:1054.4468772129674
node19: train data size:4281
node19 epoch0:node_model train_loss:0.10905479882345644,train_acc:0.9583175182342529
node19 epoch1:node_model train_loss:0.06704008674552274,train_acc:0.9803559184074402
node19 epoch2:node_model train_loss:0.037439348371049695,train_acc:0.9904651045799255
node19 epoch3:node_model train_loss:0.02777885279596545,train_acc:0.99395352602005
node19 epoch4:node_model train_loss:0.020116307285376066,train_acc:0.9972093105316162
node19_model on test-dataset: loss:0.8413742551207543,acc:0.7972999215126038
node19 weight score:5088.104341136026
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6998931936919689,acc:0.8266999799013138
total cost energy:8.802046168079793 | all_enery_cp：6.825000000000001 | all_enery_tp: 1.9770461680797917
ef: 24.803102605409812
reward: 16.00105643733002
step 420:loss:14.292512893676758|running q:76.54939270019531
episode6_cost time: 28332.821993350983
episode7,iteration0 selected nodes:[5, 19, 8, 9, 4],center node:9
################################################## episode7,iteration0 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:2.1172444777829305,train_acc:0.268928587436676
node4 epoch1:node_model train_loss:1.6972663147108895,train_acc:0.37607136368751526
node4 epoch2:node_model train_loss:1.6324895364897591,train_acc:0.43142858147621155
node4 epoch3:node_model train_loss:1.4418728947639465,train_acc:0.48392853140830994
node4 epoch4:node_model train_loss:1.4242411340985979,train_acc:0.4992856979370117
node4_model on test-dataset: loss:1.733626935482025,acc:0.434300035238266
node4 weight score:1560.3126281883074
node5: train data size:3735
node5 epoch0:node_model train_loss:2.0043163236818815,train_acc:0.29499998688697815
node5 epoch1:node_model train_loss:1.5931939388576306,train_acc:0.4247743785381317
node5 epoch2:node_model train_loss:1.4229455527506376,train_acc:0.49090224504470825
node5 epoch3:node_model train_loss:1.348591540989123,train_acc:0.5071428418159485
node5 epoch4:node_model train_loss:1.2121225391563617,train_acc:0.5626690983772278
node5_model on test-dataset: loss:1.514769239127636,acc:0.47140005230903625
node5 weight score:2465.7221070524292
node8: train data size:1798
node8 epoch0:node_model train_loss:2.2949495911598206,train_acc:0.22815190255641937
node8 epoch1:node_model train_loss:1.7551061775949266,train_acc:0.3660544157028198
node8 epoch2:node_model train_loss:1.5863556265830994,train_acc:0.4294217526912689
node8 epoch3:node_model train_loss:1.4406711988978915,train_acc:0.4800339937210083
node8 epoch4:node_model train_loss:1.3586836126115587,train_acc:0.5105555653572083
node8_model on test-dataset: loss:1.5267018979787828,acc:0.4588000476360321
node8 weight score:1177.7020794828327
node9: train data size:1857
node9 epoch0:node_model train_loss:2.1141008138656616,train_acc:0.25872576236724854
node9 epoch1:node_model train_loss:1.6301744046964144,train_acc:0.4161033630371094
node9 epoch2:node_model train_loss:1.496820280426427,train_acc:0.4529455602169037
node9 epoch3:node_model train_loss:1.4005621169742786,train_acc:0.49322250485420227
node9 epoch4:node_model train_loss:1.2624326505159076,train_acc:0.5357340574264526
node9_model on test-dataset: loss:1.5788390350341797,acc:0.4603000581264496
node9 weight score:1176.1806991045155
node19: train data size:4281
node19 epoch0:node_model train_loss:2.043239166570264,train_acc:0.29652026295661926
node19 epoch1:node_model train_loss:1.5868233885875969,train_acc:0.4221964180469513
node19 epoch2:node_model train_loss:1.4126598197360372,train_acc:0.4939993917942047
node19 epoch3:node_model train_loss:1.3005477439525515,train_acc:0.5332213640213013
node19 epoch4:node_model train_loss:1.1538319837215334,train_acc:0.5921533107757568
node19_model on test-dataset: loss:1.5091047683358192,acc:0.487499862909317
node19 weight score:2836.7811763797663
start merge all node model param
merge model finish!
global-model on test-dataset:loss:1.5752889424562455,acc:0.4263999912515283
total cost energy:9.190855063526927 | all_enery_cp：7.188 | all_enery_tp: 2.002855063526927
ef: 23.471825205109734
reward: 14.280970141582808
step 421:loss:13.507113456726074|running q:1.5721023082733154
episode7,iteration1 selected nodes:[4, 17, 13, 1, 16],center node:17
################################################## episode7,iteration1 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:1.4238573277697844,train_acc:0.49779415130615234
node1 epoch1:node_model train_loss:1.2188035986002754,train_acc:0.5755146741867065
node1 epoch2:node_model train_loss:1.0868328739615047,train_acc:0.6154411435127258
node1 epoch3:node_model train_loss:1.012304558473475,train_acc:0.6463969349861145
node1 epoch4:node_model train_loss:0.9119571280830047,train_acc:0.6838234663009644
node1_model on test-dataset: loss:1.0973926925659179,acc:0.6330000758171082
node1 weight score:6112.670555802034
node4: train data size:2705
node4 epoch0:node_model train_loss:1.553095634494509,train_acc:0.4571428596973419
node4 epoch1:node_model train_loss:1.3598622466836656,train_acc:0.5067856907844543
node4 epoch2:node_model train_loss:1.2632470173495156,train_acc:0.541785717010498
node4 epoch3:node_model train_loss:1.1810352099793298,train_acc:0.5867857336997986
node4 epoch4:node_model train_loss:1.0189100972243719,train_acc:0.6446428298950195
node4_model on test-dataset: loss:1.432646942436695,acc:0.5217000246047974
node4 weight score:1888.1134771413
node13: train data size:1155
node13 epoch0:node_model train_loss:1.8637755711873372,train_acc:0.3799242675304413
node13 epoch1:node_model train_loss:1.3938156167666118,train_acc:0.5105302929878235
node13 epoch2:node_model train_loss:1.1975001593430836,train_acc:0.5625
node13 epoch3:node_model train_loss:1.0584230323632557,train_acc:0.6273484230041504
node13 epoch4:node_model train_loss:0.984691709280014,train_acc:0.6535606384277344
node13_model on test-dataset: loss:1.393818769454956,acc:0.5034999847412109
node13 weight score:828.6586644629957
node16: train data size:877
node16 epoch0:node_model train_loss:1.8047988414764404,train_acc:0.39639249444007874
node16 epoch1:node_model train_loss:1.4459520843293932,train_acc:0.4796103835105896
node16 epoch2:node_model train_loss:1.242061244116889,train_acc:0.5824964046478271
node16 epoch3:node_model train_loss:1.0693016979429457,train_acc:0.5981529951095581
node16 epoch4:node_model train_loss:0.978588879108429,train_acc:0.6632612347602844
node16_model on test-dataset: loss:1.4133762246370316,acc:0.48769986629486084
node16 weight score:620.5000372248528
node17: train data size:442
node17 epoch0:node_model train_loss:2.0131720542907714,train_acc:0.3743809163570404
node17 epoch1:node_model train_loss:1.5089759826660156,train_acc:0.43019047379493713
node17 epoch2:node_model train_loss:1.3302355766296388,train_acc:0.5174285769462585
node17 epoch3:node_model train_loss:1.1202713251113892,train_acc:0.6067619323730469
node17 epoch4:node_model train_loss:1.057319450378418,train_acc:0.6330475807189941
node17_model on test-dataset: loss:1.4828416812419891,acc:0.4720999002456665
node17 weight score:298.07632574085216
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.9944717064499855,acc:0.6544999861717224
total cost energy:7.872563907280485 | all_enery_cp：5.943500000000001 | all_enery_tp: 1.9290639072804838
ef: 23.786285405709204
reward: 15.91372149842872
step 422:loss:12.48023509979248|running q:3.2600510120391846
episode7,iteration2 selected nodes:[11, 7, 18, 6, 2],center node:6
################################################## episode7,iteration2 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:1.14528742680947,train_acc:0.6125379204750061
node2 epoch1:node_model train_loss:0.9647553836305937,train_acc:0.6649526357650757
node2 epoch2:node_model train_loss:0.8620105162262917,train_acc:0.7002178430557251
node2 epoch3:node_model train_loss:0.7890909872949123,train_acc:0.7252461910247803
node2 epoch4:node_model train_loss:0.7688591455419859,train_acc:0.7314013838768005
node2_model on test-dataset: loss:1.0725492697954178,acc:0.6421001553535461
node2 weight score:4464.130585733634
node6: train data size:3007
node6 epoch0:node_model train_loss:1.2041786197693116,train_acc:0.5880184173583984
node6 epoch1:node_model train_loss:1.0578170860967329,train_acc:0.6287556290626526
node6 epoch2:node_model train_loss:0.8991642748155901,train_acc:0.6950690746307373
node6 epoch3:node_model train_loss:0.8099871431627581,train_acc:0.7169123291969299
node6 epoch4:node_model train_loss:0.8128819734819474,train_acc:0.7252073287963867
node6_model on test-dataset: loss:1.3285709816217421,acc:0.5736998915672302
node6 weight score:2263.3340947500265
node7: train data size:1951
node7 epoch0:node_model train_loss:1.3229287952184676,train_acc:0.5663922429084778
node7 epoch1:node_model train_loss:0.9937377512454987,train_acc:0.6577548980712891
node7 epoch2:node_model train_loss:0.8820661932229996,train_acc:0.6823136806488037
node7 epoch3:node_model train_loss:0.7469458490610122,train_acc:0.7442744970321655
node7 epoch4:node_model train_loss:0.6974841803312302,train_acc:0.7662352919578552
node7_model on test-dataset: loss:1.0330985110998154,acc:0.6370999813079834
node7 weight score:1888.493671259874
node11: train data size:1682
node11 epoch0:node_model train_loss:1.244620498488931,train_acc:0.5833286046981812
node11 epoch1:node_model train_loss:0.9973877633319181,train_acc:0.6599138975143433
node11 epoch2:node_model train_loss:0.8245178741567275,train_acc:0.710688591003418
node11 epoch3:node_model train_loss:0.7486422657966614,train_acc:0.7424532771110535
node11 epoch4:node_model train_loss:0.6603143390487222,train_acc:0.7705022096633911
node11_model on test-dataset: loss:1.187628617733717,acc:0.613800048828125
node11 weight score:1416.2676571482955
node18: train data size:472
node18 epoch0:node_model train_loss:1.307019019126892,train_acc:0.5512222647666931
node18 epoch1:node_model train_loss:1.0286695837974549,train_acc:0.64211106300354
node18 epoch2:node_model train_loss:0.8547138690948486,train_acc:0.7232222557067871
node18 epoch3:node_model train_loss:0.7737707614898681,train_acc:0.7363333106040955
node18 epoch4:node_model train_loss:0.5685957133769989,train_acc:0.8229999542236328
node18_model on test-dataset: loss:1.2863318192958832,acc:0.5688998103141785
node18 weight score:366.9348708627646
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8725786736607551,acc:0.7018999832868577
total cost energy:7.288905042258274 | all_enery_cp：5.95 | all_enery_tp: 1.338905042258274
ef: 24.05978526196055
reward: 16.770880219702274
step 423:loss:11.77961254119873|running q:4.920262336730957
episode7,iteration3 selected nodes:[15, 8, 12, 4, 11],center node:8
################################################## episode7,iteration3 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:1.0831310259444373,train_acc:0.6285713911056519
node4 epoch1:node_model train_loss:0.8872091967080321,train_acc:0.6892858147621155
node4 epoch2:node_model train_loss:0.7408841358763831,train_acc:0.7528571486473083
node4 epoch3:node_model train_loss:0.6676470637321472,train_acc:0.756428599357605
node4 epoch4:node_model train_loss:0.7910864757640022,train_acc:0.7292857766151428
node4_model on test-dataset: loss:1.1198986998200418,acc:0.6293999552726746
node4 weight score:2415.397035852145
node8: train data size:1798
node8 epoch0:node_model train_loss:1.1034113764762878,train_acc:0.6423469185829163
node8 epoch1:node_model train_loss:0.8224101232157813,train_acc:0.7063718438148499
node8 epoch2:node_model train_loss:0.7433668871720632,train_acc:0.7386167645454407
node8 epoch3:node_model train_loss:0.6856258544656966,train_acc:0.7585600018501282
node8 epoch4:node_model train_loss:0.612891384296947,train_acc:0.7786054611206055
node8_model on test-dataset: loss:1.00817945048213,acc:0.6630998849868774
node8 weight score:1783.4126644221553
node11: train data size:1682
node11 epoch0:node_model train_loss:1.1111340487704557,train_acc:0.624160647392273
node11 epoch1:node_model train_loss:0.7912775102783652,train_acc:0.7121376991271973
node11 epoch2:node_model train_loss:0.6962845150162192,train_acc:0.755523681640625
node11 epoch3:node_model train_loss:0.579572716180016,train_acc:0.7963701486587524
node11 epoch4:node_model train_loss:0.526579828823314,train_acc:0.8185222744941711
node11_model on test-dataset: loss:1.0313535305857657,acc:0.6624000668525696
node11 weight score:1630.8665749606678
node12: train data size:1336
node12 epoch0:node_model train_loss:1.1120440661907196,train_acc:0.6219047904014587
node12 epoch1:node_model train_loss:0.8413074910640717,train_acc:0.7057936191558838
node12 epoch2:node_model train_loss:0.6984003441674369,train_acc:0.7661111354827881
node12 epoch3:node_model train_loss:0.5848432098116193,train_acc:0.8044444918632507
node12 epoch4:node_model train_loss:0.5396643557718822,train_acc:0.8199207186698914
node12_model on test-dataset: loss:1.041182279586792,acc:0.6543999910354614
node12 weight score:1283.1566827378301
node15: train data size:629
node15 epoch0:node_model train_loss:1.15254259960992,train_acc:0.6094581484794617
node15 epoch1:node_model train_loss:0.882886256490435,train_acc:0.6770936250686646
node15 epoch2:node_model train_loss:0.8048337868281773,train_acc:0.6963053941726685
node15 epoch3:node_model train_loss:0.7363143903868539,train_acc:0.7596551775932312
node15 epoch4:node_model train_loss:0.631468972989491,train_acc:0.7869458794593811
node15_model on test-dataset: loss:1.1572619476914405,acc:0.6140000224113464
node15 weight score:543.5243086103005
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.825245703458786,acc:0.7216999837756157
total cost energy:5.830639862692907 | all_enery_cp：4.075 | all_enery_tp: 1.7556398626929068
ef: 24.146375505954634
reward: 18.315735643261725
step 424:loss:16.026365280151367|running q:6.5675249099731445
episode7,iteration4 selected nodes:[14, 1, 13, 11, 19],center node:14
################################################## episode7,iteration4 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.9256902801639894,train_acc:0.6861029267311096
node1 epoch1:node_model train_loss:0.7974831383017933,train_acc:0.7303677201271057
node1 epoch2:node_model train_loss:0.753652588409536,train_acc:0.742205798625946
node1 epoch3:node_model train_loss:0.6942648786832305,train_acc:0.7594853639602661
node1 epoch4:node_model train_loss:0.6718048011774526,train_acc:0.7657352089881897
node1_model on test-dataset: loss:1.022618474960327,acc:0.6744999289512634
node1 weight score:6559.631147149224
node11: train data size:1682
node11 epoch0:node_model train_loss:0.8911386868532967,train_acc:0.693572461605072
node11 epoch1:node_model train_loss:0.728220241911271,train_acc:0.7404160499572754
node11 epoch2:node_model train_loss:0.5937688245492823,train_acc:0.805781900882721
node11 epoch3:node_model train_loss:0.5149659847511965,train_acc:0.8217359781265259
node11 epoch4:node_model train_loss:0.4366614380303551,train_acc:0.8518507480621338
node11_model on test-dataset: loss:1.1163745325803758,acc:0.6417001485824585
node11 weight score:1506.662818715726
node13: train data size:1155
node13 epoch0:node_model train_loss:1.1007404526074727,train_acc:0.640984833240509
node13 epoch1:node_model train_loss:0.8210875143607458,train_acc:0.7184090614318848
node13 epoch2:node_model train_loss:0.6394595702489217,train_acc:0.7930302619934082
node13 epoch3:node_model train_loss:0.5255039234956106,train_acc:0.8296970129013062
node13 epoch4:node_model train_loss:0.5057392170031866,train_acc:0.8318938612937927
node13_model on test-dataset: loss:1.1200336086750031,acc:0.6285001039505005
node13 weight score:1031.21905544099
node14: train data size:1172
node14 epoch0:node_model train_loss:1.0703962296247482,train_acc:0.6427314877510071
node14 epoch1:node_model train_loss:0.7695656965176264,train_acc:0.7206944227218628
node14 epoch2:node_model train_loss:0.624691978096962,train_acc:0.7776389122009277
node14 epoch3:node_model train_loss:0.5271095111966133,train_acc:0.8121294975280762
node14 epoch4:node_model train_loss:0.44103402892748517,train_acc:0.8472685217857361
node14_model on test-dataset: loss:1.005680673122406,acc:0.6664998531341553
node14 weight score:1165.3798579634736
node19: train data size:4281
node19 epoch0:node_model train_loss:1.0115475557571234,train_acc:0.654504656791687
node19 epoch1:node_model train_loss:0.8040401311807854,train_acc:0.7121246457099915
node19 epoch2:node_model train_loss:0.7504612789597622,train_acc:0.7329859137535095
node19 epoch3:node_model train_loss:0.6921814000883768,train_acc:0.7606602907180786
node19 epoch4:node_model train_loss:0.683761189843333,train_acc:0.7626299262046814
node19_model on test-dataset: loss:1.2068345963954925,acc:0.6157999634742737
node19 weight score:3547.296384099575
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.8284350894391537,acc:0.722599980533123
total cost energy:9.179722344609216 | all_enery_cp：7.4990000000000006 | all_enery_tp: 1.6807223446092154
ef: 24.395992098325326
reward: 15.21626975371611
step 425:loss:7.791304588317871|running q:8.118444442749023
episode7,iteration5 selected nodes:[8, 3, 14, 4, 13],center node:8
################################################## episode7,iteration5 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.9545757950738419,train_acc:0.6844680905342102
node3 epoch1:node_model train_loss:0.7903724090997563,train_acc:0.7220878601074219
node3 epoch2:node_model train_loss:0.6687600529471109,train_acc:0.7610390186309814
node3 epoch3:node_model train_loss:0.6106689163418704,train_acc:0.7850223183631897
node3 epoch4:node_model train_loss:0.5466872051704762,train_acc:0.8034536838531494
node3_model on test-dataset: loss:0.9741065513342619,acc:0.6819999814033508
node3 weight score:4359.892656694241
node4: train data size:2705
node4 epoch0:node_model train_loss:0.8886392137834004,train_acc:0.6949999332427979
node4 epoch1:node_model train_loss:0.7874357955796378,train_acc:0.73499995470047
node4 epoch2:node_model train_loss:0.6637259221502713,train_acc:0.7642858028411865
node4 epoch3:node_model train_loss:0.5507297494581768,train_acc:0.8124999403953552
node4 epoch4:node_model train_loss:0.539354777761868,train_acc:0.8100000619888306
node4_model on test-dataset: loss:0.990569935888052,acc:0.6750000715255737
node4 weight score:2730.7511584984163
node8: train data size:1798
node8 epoch0:node_model train_loss:0.9813815156618754,train_acc:0.6768366694450378
node8 epoch1:node_model train_loss:0.6961610317230225,train_acc:0.7575282454490662
node8 epoch2:node_model train_loss:0.5884763697783152,train_acc:0.8019500374794006
node8 epoch3:node_model train_loss:0.5250990241765976,train_acc:0.8248412013053894
node8 epoch4:node_model train_loss:0.4756615095668369,train_acc:0.8387415409088135
node8_model on test-dataset: loss:0.9240391999483109,acc:0.6973000168800354
node8 weight score:1945.8048966976476
node13: train data size:1155
node13 epoch0:node_model train_loss:0.9522844304641088,train_acc:0.6863636374473572
node13 epoch1:node_model train_loss:0.7536372244358063,train_acc:0.7369697093963623
node13 epoch2:node_model train_loss:0.5783202797174454,train_acc:0.809015154838562
node13 epoch3:node_model train_loss:0.5191712925831476,train_acc:0.8242425322532654
node13 epoch4:node_model train_loss:0.4097887674967448,train_acc:0.8664394617080688
node13_model on test-dataset: loss:1.1095833073556423,acc:0.652100145816803
node13 weight score:1040.931304881104
node14: train data size:1172
node14 epoch0:node_model train_loss:0.9215228507916132,train_acc:0.6744444370269775
node14 epoch1:node_model train_loss:0.672621821363767,train_acc:0.7636574506759644
node14 epoch2:node_model train_loss:0.5282344669103622,train_acc:0.8093055486679077
node14 epoch3:node_model train_loss:0.41603564222653705,train_acc:0.8638889193534851
node14 epoch4:node_model train_loss:0.329317385951678,train_acc:0.909398078918457
node14_model on test-dataset: loss:0.9700232952833175,acc:0.684299886226654
node14 weight score:1208.2184064019725
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7405618238449097,acc:0.7527999794483184
total cost energy:6.785441344953364 | all_enery_cp：5.5385 | all_enery_tp: 1.2469413449533646
ef: 24.584042247405204
reward: 17.798600902451838
step 426:loss:9.464040756225586|running q:9.658760070800781
episode7,iteration6 selected nodes:[7, 14, 5, 15, 6],center node:6
################################################## episode7,iteration6 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.9708548517603623,train_acc:0.687406063079834
node5 epoch1:node_model train_loss:0.7224674318966112,train_acc:0.7522180080413818
node5 epoch2:node_model train_loss:0.6673113437075364,train_acc:0.7745487689971924
node5 epoch3:node_model train_loss:0.5475709367739526,train_acc:0.8082330226898193
node5 epoch4:node_model train_loss:0.5341900689037222,train_acc:0.8157895803451538
node5_model on test-dataset: loss:0.8755138392746449,acc:0.7077000141143799
node5 weight score:4266.066203013323
node6: train data size:3007
node6 epoch0:node_model train_loss:0.961748073177953,train_acc:0.6852995157241821
node6 epoch1:node_model train_loss:0.7698924195381903,train_acc:0.7386635541915894
node6 epoch2:node_model train_loss:0.6486646037909293,train_acc:0.7776497006416321
node6 epoch3:node_model train_loss:0.601756711159983,train_acc:0.7987557053565979
node6 epoch4:node_model train_loss:0.6317850687811452,train_acc:0.7729493975639343
node6_model on test-dataset: loss:1.155441467165947,acc:0.6407999396324158
node6 weight score:2602.4684810521244
node7: train data size:1951
node7 epoch0:node_model train_loss:1.0185035914182663,train_acc:0.6757352352142334
node7 epoch1:node_model train_loss:0.6889521092176437,train_acc:0.758254885673523
node7 epoch2:node_model train_loss:0.5964271932840347,train_acc:0.7997745871543884
node7 epoch3:node_model train_loss:0.4857826232910156,train_acc:0.8357157111167908
node7 epoch4:node_model train_loss:0.45993090867996217,train_acc:0.848656952381134
node7_model on test-dataset: loss:0.9023902840912342,acc:0.7008002400398254
node7 weight score:2162.0356894298616
node14: train data size:1172
node14 epoch0:node_model train_loss:0.8192508121331533,train_acc:0.7153703570365906
node14 epoch1:node_model train_loss:0.5662373875578245,train_acc:0.8081481456756592
node14 epoch2:node_model train_loss:0.4620621899763743,train_acc:0.842962920665741
node14 epoch3:node_model train_loss:0.37576062356432277,train_acc:0.8772222399711609
node14 epoch4:node_model train_loss:0.30890529851118725,train_acc:0.9043980836868286
node14_model on test-dataset: loss:0.9107589790225029,acc:0.7025999426841736
node14 weight score:1286.8388091631896
node15: train data size:629
node15 epoch0:node_model train_loss:0.9637021507535662,train_acc:0.6847290396690369
node15 epoch1:node_model train_loss:0.8151309064456395,train_acc:0.7229557633399963
node15 epoch2:node_model train_loss:0.6275483029229301,train_acc:0.7782266139984131
node15 epoch3:node_model train_loss:0.5152323118277958,train_acc:0.828226625919342
node15 epoch4:node_model train_loss:0.40361128960336956,train_acc:0.8698030114173889
node15_model on test-dataset: loss:1.0115288183093072,acc:0.6705998778343201
node15 weight score:621.8310231154118
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7389398007094861,acc:0.7512999829649926
total cost energy:6.6327300762134085 | all_enery_cp：5.247 | all_enery_tp: 1.3857300762134084
ef: 24.441465137426125
reward: 17.808735061212715
step 427:loss:9.752250671386719|running q:11.157955169677734
episode7,iteration7 selected nodes:[16, 11, 9, 15, 14],center node:14
################################################## episode7,iteration7 ##################################################
node9: train data size:1857
node9 epoch0:node_model train_loss:0.9544701231153387,train_acc:0.6868051886558533
node9 epoch1:node_model train_loss:0.6911683113951432,train_acc:0.7632687091827393
node9 epoch2:node_model train_loss:0.5494165624442854,train_acc:0.8075992465019226
node9 epoch3:node_model train_loss:0.449180311278293,train_acc:0.8487904071807861
node9 epoch4:node_model train_loss:0.3867013626977017,train_acc:0.8785319328308105
node9_model on test-dataset: loss:0.8963799777626992,acc:0.7056999802589417
node9 weight score:2071.6660858879736
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7792033237569472,train_acc:0.7250645756721497
node11 epoch1:node_model train_loss:0.5930726703475503,train_acc:0.7897704839706421
node11 epoch2:node_model train_loss:0.5168700954493355,train_acc:0.8182640075683594
node11 epoch3:node_model train_loss:0.42434710088898153,train_acc:0.8540746569633484
node11 epoch4:node_model train_loss:0.3426154825617285,train_acc:0.892238199710846
node11_model on test-dataset: loss:0.8847674210369587,acc:0.7141000032424927
node11 weight score:1901.0645735900566
node14: train data size:1172
node14 epoch0:node_model train_loss:0.7989065746466318,train_acc:0.7238425612449646
node14 epoch1:node_model train_loss:0.5204277584950129,train_acc:0.8187499046325684
node14 epoch2:node_model train_loss:0.4135349591573079,train_acc:0.8589351177215576
node14 epoch3:node_model train_loss:0.3132476086417834,train_acc:0.9042128920555115
node14 epoch4:node_model train_loss:0.26031363755464554,train_acc:0.9279167056083679
node14_model on test-dataset: loss:0.9016524070501327,acc:0.7116000056266785
node14 weight score:1299.8357136696866
node15: train data size:629
node15 epoch0:node_model train_loss:0.9963137337139675,train_acc:0.6672413945198059
node15 epoch1:node_model train_loss:0.6659438695226397,train_acc:0.7707388997077942
node15 epoch2:node_model train_loss:0.5426787648882184,train_acc:0.8233005404472351
node15 epoch3:node_model train_loss:0.406072735786438,train_acc:0.8712315559387207
node15 epoch4:node_model train_loss:0.33972759757723126,train_acc:0.90093594789505
node15_model on test-dataset: loss:0.9861779624223709,acc:0.679999828338623
node15 weight score:637.8159155523748
node16: train data size:877
node16 epoch0:node_model train_loss:0.9386206931538053,train_acc:0.6993650794029236
node16 epoch1:node_model train_loss:0.6669411063194275,train_acc:0.774357795715332
node16 epoch2:node_model train_loss:0.49411947197384304,train_acc:0.8419047594070435
node16 epoch3:node_model train_loss:0.3943207926220364,train_acc:0.8821212649345398
node16 epoch4:node_model train_loss:0.32771146959728664,train_acc:0.8973448276519775
node16_model on test-dataset: loss:0.9759227186441422,acc:0.6796001195907593
node16 weight score:898.6367293697433
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7114386492967606,acc:0.7591999807953834
total cost energy:4.222787046345474 | all_enery_cp：3.1085 | all_enery_tp: 1.1142870463454744
ef: 24.588588736689754
reward: 20.36580169034428
step 428:loss:8.458357810974121|running q:12.705769538879395
episode7,iteration8 selected nodes:[7, 10, 0, 9, 12],center node:7
################################################## episode7,iteration8 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.9035400702403142,train_acc:0.6954795122146606
node0 epoch1:node_model train_loss:0.7127772002266004,train_acc:0.7580931186676025
node0 epoch2:node_model train_loss:0.6394920016710575,train_acc:0.7785614728927612
node0 epoch3:node_model train_loss:0.5632728045949569,train_acc:0.8047499060630798
node0 epoch4:node_model train_loss:0.5485078531962174,train_acc:0.8094045519828796
node0_model on test-dataset: loss:0.953721648901701,acc:0.6955999732017517
node0 weight score:5434.499684440116
node7: train data size:1951
node7 epoch0:node_model train_loss:0.8701769679784774,train_acc:0.7197940945625305
node7 epoch1:node_model train_loss:0.5919936075806618,train_acc:0.7881569266319275
node7 epoch2:node_model train_loss:0.47212544530630113,train_acc:0.843195915222168
node7 epoch3:node_model train_loss:0.42728276997804643,train_acc:0.8566372990608215
node7 epoch4:node_model train_loss:0.3538831859827042,train_acc:0.8926568031311035
node7_model on test-dataset: loss:0.9293497714400292,acc:0.708899736404419
node7 weight score:2099.3172430407144
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7499722712918332,train_acc:0.7443027496337891
node9 epoch1:node_model train_loss:0.5552361293842918,train_acc:0.8060294985771179
node9 epoch2:node_model train_loss:0.4314205873953669,train_acc:0.8599815368652344
node9 epoch3:node_model train_loss:0.36614564218019185,train_acc:0.881163477897644
node9 epoch4:node_model train_loss:0.3263695632156573,train_acc:0.9002493619918823
node9_model on test-dataset: loss:0.9617352998256683,acc:0.7015999555587769
node9 weight score:1930.8847250762394
node10: train data size:1975
node10 epoch0:node_model train_loss:0.9560082495212555,train_acc:0.6889999508857727
node10 epoch1:node_model train_loss:0.7029469966888428,train_acc:0.760499894618988
node10 epoch2:node_model train_loss:0.5230342254042626,train_acc:0.827833354473114
node10 epoch3:node_model train_loss:0.4777670130133629,train_acc:0.8468332290649414
node10 epoch4:node_model train_loss:0.40698416978120805,train_acc:0.8661665916442871
node10_model on test-dataset: loss:0.8819018894433975,acc:0.7106000185012817
node10 weight score:2239.478136560632
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8680091542857034,train_acc:0.7053174376487732
node12 epoch1:node_model train_loss:0.5910345699105944,train_acc:0.7996824383735657
node12 epoch2:node_model train_loss:0.4768608106034143,train_acc:0.8273809552192688
node12 epoch3:node_model train_loss:0.3967038797480719,train_acc:0.8760318756103516
node12 epoch4:node_model train_loss:0.32320839379514965,train_acc:0.8963491916656494
node12_model on test-dataset: loss:0.9407993727922439,acc:0.7047998905181885
node12 weight score:1420.0689739352408
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7081795872747898,acc:0.769099982380867
total cost energy:7.676564639980225 | all_enery_cp：6.151 | all_enery_tp: 1.5255646399802254
ef: 24.71718103120016
reward: 17.040616391219935
step 429:loss:8.287385940551758|running q:14.178876876831055
episode7,iteration9 selected nodes:[5, 18, 8, 10, 4],center node:10
################################################## episode7,iteration9 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7647519079702241,train_acc:0.7442857623100281
node4 epoch1:node_model train_loss:0.6285103186964989,train_acc:0.780357301235199
node4 epoch2:node_model train_loss:0.6020328636680331,train_acc:0.783928632736206
node4 epoch3:node_model train_loss:0.4954392537474632,train_acc:0.8264285922050476
node4 epoch4:node_model train_loss:0.4174350468175752,train_acc:0.8514286875724792
node4_model on test-dataset: loss:0.9288923788070679,acc:0.705500066280365
node4 weight score:2912.0703988054056
node5: train data size:3735
node5 epoch0:node_model train_loss:0.7701463942465029,train_acc:0.7394360303878784
node5 epoch1:node_model train_loss:0.5665058579884077,train_acc:0.8031579256057739
node5 epoch2:node_model train_loss:0.49236248825725754,train_acc:0.8337217569351196
node5 epoch3:node_model train_loss:0.4816760844305942,train_acc:0.8367670178413391
node5 epoch4:node_model train_loss:0.40029760175629664,train_acc:0.8635714650154114
node5_model on test-dataset: loss:0.8710008609294891,acc:0.7192999124526978
node5 weight score:4288.170273464704
node8: train data size:1798
node8 epoch0:node_model train_loss:0.8657530744870504,train_acc:0.7186167240142822
node8 epoch1:node_model train_loss:0.6108496288458506,train_acc:0.782619059085846
node8 epoch2:node_model train_loss:0.5057018647591273,train_acc:0.8264852166175842
node8 epoch3:node_model train_loss:0.4150921089781655,train_acc:0.8587868809700012
node8 epoch4:node_model train_loss:0.3486872265736262,train_acc:0.885906994342804
node8_model on test-dataset: loss:0.8470031048357487,acc:0.7227998971939087
node8 weight score:2122.778523165708
node10: train data size:1975
node10 epoch0:node_model train_loss:0.8571976691484451,train_acc:0.7143332958221436
node10 epoch1:node_model train_loss:0.5891658738255501,train_acc:0.7956666350364685
node10 epoch2:node_model train_loss:0.46111216247081754,train_acc:0.8445000648498535
node10 epoch3:node_model train_loss:0.3888652384281158,train_acc:0.8773333430290222
node10 epoch4:node_model train_loss:0.37955253571271896,train_acc:0.8786665797233582
node10_model on test-dataset: loss:0.8492832756042481,acc:0.7243999242782593
node10 weight score:2325.4902771926445
node18: train data size:472
node18 epoch0:node_model train_loss:0.838865339756012,train_acc:0.7144444584846497
node18 epoch1:node_model train_loss:0.495670759677887,train_acc:0.8466666340827942
node18 epoch2:node_model train_loss:0.36665194630622866,train_acc:0.8709999918937683
node18 epoch3:node_model train_loss:0.2439839243888855,train_acc:0.9456667304039001
node18 epoch4:node_model train_loss:0.1767931491136551,train_acc:0.9692221879959106
node18_model on test-dataset: loss:0.993812018930912,acc:0.6884999871253967
node18 weight score:474.93891300263357
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6935691247880459,acc:0.7677999830245972
total cost energy:7.25126066412812 | all_enery_cp：5.3425 | all_enery_tp: 1.9087606641281198
ef: 24.675807134466293
reward: 17.42454647033817
step 430:loss:4.936347484588623|running q:15.664706230163574
episode7,iteration10 selected nodes:[14, 18, 5, 0, 10],center node:5
################################################## episode7,iteration10 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.7499450009602767,train_acc:0.750095009803772
node0 epoch1:node_model train_loss:0.5756246499144114,train_acc:0.80333411693573
node0 epoch2:node_model train_loss:0.5270569954927151,train_acc:0.8167910575866699
node0 epoch3:node_model train_loss:0.47605359038481343,train_acc:0.8350648283958435
node0 epoch4:node_model train_loss:0.4209584610966536,train_acc:0.8537975549697876
node0_model on test-dataset: loss:0.9157488095760346,acc:0.7155998945236206
node0 weight score:5659.84901732996
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6104561153211092,train_acc:0.7910526394844055
node5 epoch1:node_model train_loss:0.4727462310540049,train_acc:0.8431578278541565
node5 epoch2:node_model train_loss:0.42009143648963226,train_acc:0.8574060201644897
node5 epoch3:node_model train_loss:0.36794284339013855,train_acc:0.8718044757843018
node5 epoch4:node_model train_loss:0.32834193267320333,train_acc:0.8962406516075134
node5_model on test-dataset: loss:0.8548657363653183,acc:0.7306000590324402
node5 weight score:4369.107148779075
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7680483400821686,train_acc:0.7389999628067017
node10 epoch1:node_model train_loss:0.5363232895731926,train_acc:0.8098333477973938
node10 epoch2:node_model train_loss:0.41267538666725156,train_acc:0.8646666407585144
node10 epoch3:node_model train_loss:0.36806935891509057,train_acc:0.8816667795181274
node10 epoch4:node_model train_loss:0.3042986445128918,train_acc:0.9066665768623352
node10_model on test-dataset: loss:0.9203553969413042,acc:0.706899881362915
node10 weight score:2145.9101631431577
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6995322778820992,train_acc:0.7599536776542664
node14 epoch1:node_model train_loss:0.5049579714735349,train_acc:0.8252777457237244
node14 epoch2:node_model train_loss:0.3185124695301056,train_acc:0.888888955116272
node14 epoch3:node_model train_loss:0.2598920712868373,train_acc:0.9217129945755005
node14 epoch4:node_model train_loss:0.24204717576503754,train_acc:0.9318980574607849
node14_model on test-dataset: loss:1.0950622256845235,acc:0.6857000589370728
node14 weight score:1070.258814988694
node18: train data size:472
node18 epoch0:node_model train_loss:0.776639747619629,train_acc:0.7291111350059509
node18 epoch1:node_model train_loss:0.490074360370636,train_acc:0.8203333020210266
node18 epoch2:node_model train_loss:0.32579981684684756,train_acc:0.8709999322891235
node18 epoch3:node_model train_loss:0.22551786303520202,train_acc:0.9384444355964661
node18 epoch4:node_model train_loss:0.1360781565308571,train_acc:0.9692221879959106
node18_model on test-dataset: loss:0.9713850244879723,acc:0.6969999670982361
node18 weight score:485.90413492198564
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6809763224422931,acc:0.7784999805688858
total cost energy:8.341292206135785 | all_enery_cp：6.2684999999999995 | all_enery_tp: 2.072792206135786
ef: 24.288106042884934
reward: 15.946813836749149
step 431:loss:6.734745502471924|running q:17.16720962524414
episode7,iteration11 selected nodes:[17, 9, 1, 16, 7],center node:9
################################################## episode7,iteration11 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.726797367720043,train_acc:0.7513970136642456
node1 epoch1:node_model train_loss:0.6142289743703955,train_acc:0.7881618142127991
node1 epoch2:node_model train_loss:0.5761978867299417,train_acc:0.8032352924346924
node1 epoch3:node_model train_loss:0.5068629703977529,train_acc:0.8251472115516663
node1 epoch4:node_model train_loss:0.46369413220707106,train_acc:0.835367739200592
node1_model on test-dataset: loss:0.9190170910954475,acc:0.7089999914169312
node1 weight score:7299.102557498922
node7: train data size:1951
node7 epoch0:node_model train_loss:0.7823676466941833,train_acc:0.7583529949188232
node7 epoch1:node_model train_loss:0.5274198442697525,train_acc:0.8162745833396912
node7 epoch2:node_model train_loss:0.4249607689678669,train_acc:0.855137288570404
node7 epoch3:node_model train_loss:0.3502545453608036,train_acc:0.8796567916870117
node7 epoch4:node_model train_loss:0.2755053550004959,train_acc:0.9210980534553528
node7_model on test-dataset: loss:0.8743820998072624,acc:0.7226998805999756
node7 weight score:2231.289959423979
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7554061334384116,train_acc:0.7624561190605164
node9 epoch1:node_model train_loss:0.5021379480236455,train_acc:0.8293259143829346
node9 epoch2:node_model train_loss:0.4012448207328194,train_acc:0.8603785634040833
node9 epoch3:node_model train_loss:0.3408751581844531,train_acc:0.8955125212669373
node9 epoch4:node_model train_loss:0.28186516071620743,train_acc:0.9145983457565308
node9_model on test-dataset: loss:0.8696030026674271,acc:0.7261001467704773
node9 weight score:2135.4572078337164
node16: train data size:877
node16 epoch0:node_model train_loss:0.8553063670794169,train_acc:0.7434776425361633
node16 epoch1:node_model train_loss:0.5436147650082906,train_acc:0.8090187311172485
node16 epoch2:node_model train_loss:0.3766940136750539,train_acc:0.886681079864502
node16 epoch3:node_model train_loss:0.2877062227990892,train_acc:0.9150071740150452
node16 epoch4:node_model train_loss:0.21538474327988094,train_acc:0.9394516348838806
node16_model on test-dataset: loss:0.8472639864683151,acc:0.7254002094268799
node16 weight score:1035.096515379622
node17: train data size:442
node17 epoch0:node_model train_loss:0.9035459399223328,train_acc:0.6973333954811096
node17 epoch1:node_model train_loss:0.6184033930301667,train_acc:0.7711428999900818
node17 epoch2:node_model train_loss:0.41139854192733766,train_acc:0.8721904754638672
node17 epoch3:node_model train_loss:0.33699791431427,train_acc:0.8846666216850281
node17 epoch4:node_model train_loss:0.22031960189342498,train_acc:0.9384762048721313
node17_model on test-dataset: loss:0.9007000008225441,acc:0.7148000001907349
node17 weight score:490.72943221533626
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6683047188818455,acc:0.7762999802827835
total cost energy:7.523019988716055 | all_enery_cp：5.9175 | all_enery_tp: 1.605519988716055
ef: 24.5262628620636
reward: 17.003242873347546
step 432:loss:8.951094627380371|running q:18.63071060180664
episode7,iteration12 selected nodes:[7, 19, 10, 13, 6],center node:10
################################################## episode7,iteration12 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7822084436493535,train_acc:0.7401381134986877
node6 epoch1:node_model train_loss:0.5648502594520969,train_acc:0.8122580051422119
node6 epoch2:node_model train_loss:0.43485072062861535,train_acc:0.8579723238945007
node6 epoch3:node_model train_loss:0.42991254046078653,train_acc:0.8529033660888672
node6 epoch4:node_model train_loss:0.3505923276947391,train_acc:0.8832718133926392
node6_model on test-dataset: loss:1.0197233545780182,acc:0.6902998685836792
node6 weight score:2948.8390027551704
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6795671194791794,train_acc:0.7671176195144653
node7 epoch1:node_model train_loss:0.44958207607269285,train_acc:0.8510980606079102
node7 epoch2:node_model train_loss:0.38359856978058815,train_acc:0.8655783534049988
node7 epoch3:node_model train_loss:0.25203694999217985,train_acc:0.9290391802787781
node7 epoch4:node_model train_loss:0.2270053207874298,train_acc:0.9360783696174622
node7_model on test-dataset: loss:0.8812200346589089,acc:0.7251001000404358
node7 weight score:2213.9759915412815
node10: train data size:1975
node10 epoch0:node_model train_loss:0.7377410769462586,train_acc:0.7586667537689209
node10 epoch1:node_model train_loss:0.5049176424741745,train_acc:0.8220000267028809
node10 epoch2:node_model train_loss:0.39598040133714674,train_acc:0.8700000047683716
node10 epoch3:node_model train_loss:0.3283520482480526,train_acc:0.9013332724571228
node10 epoch4:node_model train_loss:0.27249049246311186,train_acc:0.9236665964126587
node10_model on test-dataset: loss:0.827180468738079,acc:0.7349998950958252
node10 weight score:2387.628908855886
node13: train data size:1155
node13 epoch0:node_model train_loss:0.8916736791531245,train_acc:0.7296969890594482
node13 epoch1:node_model train_loss:0.5826896304885546,train_acc:0.8073484897613525
node13 epoch2:node_model train_loss:0.4127876063187917,train_acc:0.8705303072929382
node13 epoch3:node_model train_loss:0.294182687997818,train_acc:0.9169697165489197
node13 epoch4:node_model train_loss:0.24720694248874983,train_acc:0.9271211624145508
node13_model on test-dataset: loss:0.8683869352936745,acc:0.7224997282028198
node13 weight score:1330.052253272785
node19: train data size:4281
node19 epoch0:node_model train_loss:0.8183167209458906,train_acc:0.7373212575912476
node19 epoch1:node_model train_loss:0.5865374256012051,train_acc:0.7987166047096252
node19 epoch2:node_model train_loss:0.5110766000525896,train_acc:0.8196468949317932
node19 epoch3:node_model train_loss:0.46345925816269806,train_acc:0.8425609469413757
node19 epoch4:node_model train_loss:0.4071236796850382,train_acc:0.8626011610031128
node19_model on test-dataset: loss:0.8010425534844399,acc:0.7453000545501709
node19 weight score:5344.285370830999
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6531658437848091,acc:0.7839999818801879
total cost energy:7.9642928231006715 | all_enery_cp：6.184499999999999 | all_enery_tp: 1.7797928231006725
ef: 24.86755065690212
reward: 16.90325783380145
step 433:loss:9.93796443939209|running q:20.12691307067871
episode7,iteration13 selected nodes:[11, 2, 6, 0, 1],center node:1
################################################## episode7,iteration13 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.6632082387804985,train_acc:0.7687882781028748
node0 epoch1:node_model train_loss:0.49428049990764034,train_acc:0.8263325095176697
node0 epoch2:node_model train_loss:0.40934666332144004,train_acc:0.8661004304885864
node0 epoch3:node_model train_loss:0.36383920535445213,train_acc:0.8771408200263977
node0 epoch4:node_model train_loss:0.3329708238060658,train_acc:0.8863715529441833
node0_model on test-dataset: loss:0.8346189744770527,acc:0.7369999289512634
node0 weight score:6210.0193723099965
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5478750971310279,train_acc:0.8080146908760071
node1 epoch1:node_model train_loss:0.463518149274237,train_acc:0.8380147814750671
node1 epoch2:node_model train_loss:0.4478559581672444,train_acc:0.8472793698310852
node1 epoch3:node_model train_loss:0.4193612113595009,train_acc:0.8540441393852234
node1 epoch4:node_model train_loss:0.32690833311747103,train_acc:0.8911029696464539
node1_model on test-dataset: loss:0.9931877154111862,acc:0.6993000507354736
node1 weight score:6754.0102398697545
node2: train data size:4788
node2 epoch0:node_model train_loss:0.7374962189545234,train_acc:0.7571970224380493
node2 epoch1:node_model train_loss:0.568503382926186,train_acc:0.810236930847168
node2 epoch2:node_model train_loss:0.4797306042164564,train_acc:0.8370170593261719
node2 epoch3:node_model train_loss:0.40994500927627087,train_acc:0.8621876239776611
node2 epoch4:node_model train_loss:0.3741074378291766,train_acc:0.8741571307182312
node2_model on test-dataset: loss:0.8373467585444451,acc:0.7307000756263733
node2 weight score:5718.061186888634
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7079305475757968,train_acc:0.759078323841095
node6 epoch1:node_model train_loss:0.5035571561705682,train_acc:0.8240092992782593
node6 epoch2:node_model train_loss:0.4092749049586634,train_acc:0.8629032373428345
node6 epoch3:node_model train_loss:0.3362773055991819,train_acc:0.8916127681732178
node6 epoch4:node_model train_loss:0.3388105702977027,train_acc:0.8882948160171509
node6_model on test-dataset: loss:0.9204876980185509,acc:0.7218998670578003
node6 weight score:3266.7465371594776
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7505695749731625,train_acc:0.7463700771331787
node11 epoch1:node_model train_loss:0.46073264584821816,train_acc:0.8456383347511292
node11 epoch2:node_model train_loss:0.3287610008436091,train_acc:0.8952366709709167
node11 epoch3:node_model train_loss:0.2727738592554541,train_acc:0.9204733967781067
node11 epoch4:node_model train_loss:0.23754219623172984,train_acc:0.9285078048706055
node11_model on test-dataset: loss:0.8574648506939411,acc:0.734799861907959
node11 weight score:1961.596441695269
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6608745121210814,acc:0.7853999805450439
total cost energy:11.64902815398729 | all_enery_cp：10.684000000000001 | all_enery_tp: 0.9650281539872885
ef: 25.101352300430964
reward: 13.452324146443674
step 434:loss:5.778477191925049|running q:21.559127807617188
episode7,iteration14 selected nodes:[7, 10, 16, 4, 18],center node:10
################################################## episode7,iteration14 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.7830814272165298,train_acc:0.7524999380111694
node4 epoch1:node_model train_loss:0.507191362125533,train_acc:0.821428656578064
node4 epoch2:node_model train_loss:0.42184436853442875,train_acc:0.8521430492401123
node4 epoch3:node_model train_loss:0.3166423828474113,train_acc:0.8985714912414551
node4 epoch4:node_model train_loss:0.3122895232268742,train_acc:0.9007142186164856
node4_model on test-dataset: loss:0.778443001061678,acc:0.7513999938964844
node4 weight score:3474.885118513226
node7: train data size:1951
node7 epoch0:node_model train_loss:0.6830213457345963,train_acc:0.7886568307876587
node7 epoch1:node_model train_loss:0.4769846469163895,train_acc:0.8286569714546204
node7 epoch2:node_model train_loss:0.33438519984483717,train_acc:0.8900589346885681
node7 epoch3:node_model train_loss:0.25107317566871645,train_acc:0.9250392913818359
node7 epoch4:node_model train_loss:0.2095842830836773,train_acc:0.9405784010887146
node7_model on test-dataset: loss:0.8462483873963356,acc:0.7384999394416809
node7 weight score:2305.4696813103174
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6793337285518646,train_acc:0.7719999551773071
node10 epoch1:node_model train_loss:0.4378533437848091,train_acc:0.8488332629203796
node10 epoch2:node_model train_loss:0.3181550197303295,train_acc:0.893166720867157
node10 epoch3:node_model train_loss:0.2587781623005867,train_acc:0.9281665682792664
node10 epoch4:node_model train_loss:0.19774138331413268,train_acc:0.948333203792572
node10_model on test-dataset: loss:0.8104544197767973,acc:0.7412001490592957
node10 weight score:2436.904472115685
node16: train data size:877
node16 epoch0:node_model train_loss:0.7618687053521475,train_acc:0.770028829574585
node16 epoch1:node_model train_loss:0.4595005777147081,train_acc:0.8524531126022339
node16 epoch2:node_model train_loss:0.3063296261760924,train_acc:0.9106782078742981
node16 epoch3:node_model train_loss:0.2017340428299374,train_acc:0.9483405351638794
node16 epoch4:node_model train_loss:0.16189166572358873,train_acc:0.9597835540771484
node16_model on test-dataset: loss:0.8272077795863152,acc:0.7324000597000122
node16 weight score:1060.193123955611
node18: train data size:472
node18 epoch0:node_model train_loss:0.7460434198379516,train_acc:0.7672222256660461
node18 epoch1:node_model train_loss:0.4040654629468918,train_acc:0.8678889274597168
node18 epoch2:node_model train_loss:0.26688076853752135,train_acc:0.9228889346122742
node18 epoch3:node_model train_loss:0.18631274104118348,train_acc:0.9377778172492981
node18 epoch4:node_model train_loss:0.15292635709047317,train_acc:0.9536666870117188
node18_model on test-dataset: loss:0.9907190045714378,acc:0.6941998600959778
node18 weight score:476.42166731643175
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6158579482138157,acc:0.795999983549118
total cost energy:5.813673888221059 | all_enery_cp：3.99 | all_enery_tp: 1.8236738882210588
ef: 24.601885060359066
reward: 18.788211172138006
step 435:loss:6.491724014282227|running q:23.045618057250977
episode7,iteration15 selected nodes:[11, 8, 4, 12, 9],center node:8
################################################## episode7,iteration15 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5529211099658694,train_acc:0.8100000619888306
node4 epoch1:node_model train_loss:0.3722483922860452,train_acc:0.8767857551574707
node4 epoch2:node_model train_loss:0.30553376036030905,train_acc:0.8985713720321655
node4 epoch3:node_model train_loss:0.3342804248843874,train_acc:0.8849999308586121
node4 epoch4:node_model train_loss:0.34245947375893593,train_acc:0.8821428418159485
node4_model on test-dataset: loss:0.9881114339828492,acc:0.7080000638961792
node4 weight score:2737.5454902862216
node8: train data size:1798
node8 epoch0:node_model train_loss:0.7462941292259428,train_acc:0.7552607655525208
node8 epoch1:node_model train_loss:0.49944305751058793,train_acc:0.8181518912315369
node8 epoch2:node_model train_loss:0.34802023155821693,train_acc:0.8860317468643188
node8 epoch3:node_model train_loss:0.2830920186307695,train_acc:0.9065533876419067
node8 epoch4:node_model train_loss:0.2438998950852288,train_acc:0.9259864091873169
node8_model on test-dataset: loss:0.8047843280434609,acc:0.7481000423431396
node8 weight score:2234.138933061955
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7441482700799641,train_acc:0.774570643901825
node9 epoch1:node_model train_loss:0.48220969971857575,train_acc:0.8377469778060913
node9 epoch2:node_model train_loss:0.3301859937216106,train_acc:0.8985410928726196
node9 epoch3:node_model train_loss:0.24531670306858264,train_acc:0.9260387420654297
node9 epoch4:node_model train_loss:0.1952617325280842,train_acc:0.9406371116638184
node9_model on test-dataset: loss:0.8691862869262695,acc:0.7283998727798462
node9 weight score:2136.4810144059757
node11: train data size:1682
node11 epoch0:node_model train_loss:0.7030580937862396,train_acc:0.7762983441352844
node11 epoch1:node_model train_loss:0.41735274476163525,train_acc:0.8504160642623901
node11 epoch2:node_model train_loss:0.2919548609677483,train_acc:0.9151793718338013
node11 epoch3:node_model train_loss:0.24064263701438904,train_acc:0.9230846166610718
node11 epoch4:node_model train_loss:0.17053788681240642,train_acc:0.9557532072067261
node11_model on test-dataset: loss:0.8354930850863457,acc:0.7312998175621033
node11 weight score:2013.1824308589823
node12: train data size:1336
node12 epoch0:node_model train_loss:0.8590930487428393,train_acc:0.7404761910438538
node12 epoch1:node_model train_loss:0.4987213271004813,train_acc:0.8307143449783325
node12 epoch2:node_model train_loss:0.38745507172175814,train_acc:0.8665080070495605
node12 epoch3:node_model train_loss:0.312872083059379,train_acc:0.8973808884620667
node12 epoch4:node_model train_loss:0.22507323218243463,train_acc:0.9306349754333496
node12_model on test-dataset: loss:0.8098011019825936,acc:0.7492998242378235
node12 weight score:1649.7878265775896
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6303281939029693,acc:0.7939999848604202
total cost energy:6.109518002517069 | all_enery_cp：4.689 | all_enery_tp: 1.4205180025170685
ef: 24.888803911885496
reward: 18.779285909368426
step 436:loss:8.853968620300293|running q:24.471839904785156
episode7,iteration16 selected nodes:[12, 10, 15, 11, 2],center node:11
################################################## episode7,iteration16 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.662318085009853,train_acc:0.7792234420776367
node2 epoch1:node_model train_loss:0.4885454544176658,train_acc:0.8304638862609863
node2 epoch2:node_model train_loss:0.39689271338284016,train_acc:0.865909218788147
node2 epoch3:node_model train_loss:0.3433732620129983,train_acc:0.8847160339355469
node2 epoch4:node_model train_loss:0.2985631466532747,train_acc:0.9021213054656982
node2_model on test-dataset: loss:0.7838841877877712,acc:0.7490999698638916
node2 weight score:6108.045135484099
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6053205996751785,train_acc:0.7861667275428772
node10 epoch1:node_model train_loss:0.37630380541086195,train_acc:0.872833251953125
node10 epoch2:node_model train_loss:0.2625629559159279,train_acc:0.9200000166893005
node10 epoch3:node_model train_loss:0.19071463719010354,train_acc:0.94816654920578
node10 epoch4:node_model train_loss:0.1512064564973116,train_acc:0.9629999399185181
node10_model on test-dataset: loss:0.7565756683051587,acc:0.7592998147010803
node10 weight score:2610.4460964549544
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5485691989169401,train_acc:0.8162983655929565
node11 epoch1:node_model train_loss:0.3645962143645567,train_acc:0.8768149018287659
node11 epoch2:node_model train_loss:0.2608520747984157,train_acc:0.9219080805778503
node11 epoch3:node_model train_loss:0.19822153887327978,train_acc:0.941707193851471
node11 epoch4:node_model train_loss:0.15198505056255004,train_acc:0.9600715637207031
node11_model on test-dataset: loss:0.8363030192255974,acc:0.7494999766349792
node11 weight score:2011.2327246618147
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6622601790087563,train_acc:0.778253972530365
node12 epoch1:node_model train_loss:0.4118424015385764,train_acc:0.8529364466667175
node12 epoch2:node_model train_loss:0.3067446233970778,train_acc:0.8950792551040649
node12 epoch3:node_model train_loss:0.23438544039215362,train_acc:0.9283333420753479
node12 epoch4:node_model train_loss:0.1833455397614411,train_acc:0.9519047737121582
node12_model on test-dataset: loss:0.8182369981706142,acc:0.7455999851226807
node12 weight score:1632.7787706825557
node15: train data size:629
node15 epoch0:node_model train_loss:0.8650826386043003,train_acc:0.7404432892799377
node15 epoch1:node_model train_loss:0.5297996146338326,train_acc:0.8091624975204468
node15 epoch2:node_model train_loss:0.32594507719789234,train_acc:0.9002955555915833
node15 epoch3:node_model train_loss:0.2850650517003877,train_acc:0.9166501760482788
node15 epoch4:node_model train_loss:0.21125503735882895,train_acc:0.9409359097480774
node15_model on test-dataset: loss:0.8934464958310128,acc:0.7217999696731567
node15 weight score:704.0152968700764
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6319268821179866,acc:0.7940999805927277
total cost energy:7.021227766016838 | all_enery_cp：5.205 | all_enery_tp: 1.816227766016838
ef: 24.790387749766694
reward: 17.769159983749855
step 437:loss:7.009694576263428|running q:25.881290435791016
episode7,iteration17 selected nodes:[15, 16, 8, 13, 2],center node:16
################################################## episode7,iteration17 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.4736937861889601,train_acc:0.8278504610061646
node2 epoch1:node_model train_loss:0.3396853891511758,train_acc:0.8834659457206726
node2 epoch2:node_model train_loss:0.28963400330394506,train_acc:0.9032859206199646
node2 epoch3:node_model train_loss:0.2481939218317469,train_acc:0.9208710193634033
node2 epoch4:node_model train_loss:0.21094710302228728,train_acc:0.9351041316986084
node2_model on test-dataset: loss:0.8207149765640497,acc:0.753800094127655
node2 weight score:5833.937647933659
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6666875018013848,train_acc:0.7802382111549377
node8 epoch1:node_model train_loss:0.4431495550605986,train_acc:0.8459184169769287
node8 epoch2:node_model train_loss:0.30925578872362774,train_acc:0.9037300944328308
node8 epoch3:node_model train_loss:0.24130289422141182,train_acc:0.9277550578117371
node8 epoch4:node_model train_loss:0.16870959723989168,train_acc:0.9610657095909119
node8_model on test-dataset: loss:0.8009226350486278,acc:0.7464998960494995
node8 weight score:2244.9109580862764
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7635481556256613,train_acc:0.7618181705474854
node13 epoch1:node_model train_loss:0.4861387486259143,train_acc:0.8512121438980103
node13 epoch2:node_model train_loss:0.3441782866915067,train_acc:0.884621262550354
node13 epoch3:node_model train_loss:0.23328179866075516,train_acc:0.9387878775596619
node13 epoch4:node_model train_loss:0.18689964463313422,train_acc:0.9477272033691406
node13_model on test-dataset: loss:0.8170362935960293,acc:0.7501998543739319
node13 weight score:1413.645891930318
node15: train data size:629
node15 epoch0:node_model train_loss:0.7790503416742597,train_acc:0.7456650137901306
node15 epoch1:node_model train_loss:0.4981380190168108,train_acc:0.8231527209281921
node15 epoch2:node_model train_loss:0.3347666348729815,train_acc:0.8867980241775513
node15 epoch3:node_model train_loss:0.23910243596349443,train_acc:0.9295074343681335
node15 epoch4:node_model train_loss:0.2103778613465173,train_acc:0.9436452984809875
node15_model on test-dataset: loss:0.8994485965371132,acc:0.7267999649047852
node15 weight score:699.3173400032607
node16: train data size:877
node16 epoch0:node_model train_loss:0.7522068288591173,train_acc:0.7658007740974426
node16 epoch1:node_model train_loss:0.4455563525358836,train_acc:0.8420202732086182
node16 epoch2:node_model train_loss:0.31078660322560203,train_acc:0.8946752548217773
node16 epoch3:node_model train_loss:0.19672992494371203,train_acc:0.9408945441246033
node16 epoch4:node_model train_loss:0.1580620656410853,train_acc:0.9574459195137024
node16_model on test-dataset: loss:0.8237272278964519,acc:0.7453998327255249
node16 weight score:1064.6728313686929
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6499857091903687,acc:0.7931999790668488
total cost energy:6.9348337525751536 | all_enery_cp：4.6235 | all_enery_tp: 2.3113337525751536
ef: 24.570161294070203
reward: 17.63532754149505
step 438:loss:8.004704475402832|running q:27.370433807373047
episode7,iteration18 selected nodes:[2, 15, 5, 13, 4],center node:5
################################################## episode7,iteration18 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3272931429867943,train_acc:0.8842425346374512
node2 epoch1:node_model train_loss:0.25587094357858103,train_acc:0.9183711409568787
node2 epoch2:node_model train_loss:0.2287973122050365,train_acc:0.9267327785491943
node2 epoch3:node_model train_loss:0.1714904016504685,train_acc:0.9510511755943298
node2 epoch4:node_model train_loss:0.14441584252441922,train_acc:0.9609942436218262
node2_model on test-dataset: loss:0.78086225181818,acc:0.767099916934967
node2 weight score:6131.683262766892
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5984943881630898,train_acc:0.7992857694625854
node4 epoch1:node_model train_loss:0.40163493901491165,train_acc:0.8667857050895691
node4 epoch2:node_model train_loss:0.3768638585294996,train_acc:0.8692858219146729
node4 epoch3:node_model train_loss:0.4087297495986734,train_acc:0.8582143783569336
node4 epoch4:node_model train_loss:0.37420561111399103,train_acc:0.8732143640518188
node4_model on test-dataset: loss:1.0286814805865288,acc:0.7097999453544617
node4 weight score:2629.579759186173
node5: train data size:3735
node5 epoch0:node_model train_loss:0.6809616951565993,train_acc:0.7713157534599304
node5 epoch1:node_model train_loss:0.42212144089372533,train_acc:0.8592106699943542
node5 epoch2:node_model train_loss:0.3806746594215694,train_acc:0.8756390810012817
node5 epoch3:node_model train_loss:0.2931814178040153,train_acc:0.9077067375183105
node5 epoch4:node_model train_loss:0.253626238750784,train_acc:0.917744517326355
node5_model on test-dataset: loss:0.7415756395459175,acc:0.7685999274253845
node5 weight score:5036.573210909436
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6812019447485606,train_acc:0.7679545283317566
node13 epoch1:node_model train_loss:0.43272601813077927,train_acc:0.8543939590454102
node13 epoch2:node_model train_loss:0.261537520835797,train_acc:0.9182575941085815
node13 epoch3:node_model train_loss:0.23251096407572427,train_acc:0.9281060695648193
node13 epoch4:node_model train_loss:0.15949333645403385,train_acc:0.9609848260879517
node13_model on test-dataset: loss:0.8428627535700798,acc:0.7437000870704651
node13 weight score:1370.3298610690924
node15: train data size:629
node15 epoch0:node_model train_loss:0.705207279750279,train_acc:0.7733004689216614
node15 epoch1:node_model train_loss:0.38089162749903543,train_acc:0.8836452960968018
node15 epoch2:node_model train_loss:0.24641374392168863,train_acc:0.917931079864502
node15 epoch3:node_model train_loss:0.19430300380502427,train_acc:0.9458620548248291
node15 epoch4:node_model train_loss:0.1654168250305312,train_acc:0.9593596458435059
node15_model on test-dataset: loss:0.8935972890257835,acc:0.7334999442100525
node15 weight score:703.8964953505483
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6328267037123442,acc:0.8007999813556671
total cost energy:8.76179007923706 | all_enery_cp：6.505999999999999 | all_enery_tp: 2.2557900792370615
ef: 24.623597517176464
reward: 15.861807437939403
step 439:loss:10.387069702148438|running q:28.864120483398438
episode7,iteration19 selected nodes:[18, 2, 11, 6, 19],center node:11
################################################## episode7,iteration19 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2750902945796649,train_acc:0.903702437877655
node2 epoch1:node_model train_loss:0.20363136691351733,train_acc:0.9308144450187683
node2 epoch2:node_model train_loss:0.18610418432702622,train_acc:0.9441381692886353
node2 epoch3:node_model train_loss:0.13155806980406246,train_acc:0.9641095995903015
node2 epoch4:node_model train_loss:0.13179808552376926,train_acc:0.9640812873840332
node2_model on test-dataset: loss:0.8986413033306598,acc:0.7510000467300415
node2 weight score:5328.043550028358
node6: train data size:3007
node6 epoch0:node_model train_loss:0.7110650337511494,train_acc:0.7752072215080261
node6 epoch1:node_model train_loss:0.5322144646798411,train_acc:0.82198166847229
node6 epoch2:node_model train_loss:0.43225817430403923,train_acc:0.8479722738265991
node6 epoch3:node_model train_loss:0.37654672851485593,train_acc:0.8719816207885742
node6 epoch4:node_model train_loss:0.3434507805974253,train_acc:0.8853915929794312
node6_model on test-dataset: loss:0.8597086615860462,acc:0.7423998117446899
node6 weight score:3497.696527161296
node11: train data size:1682
node11 epoch0:node_model train_loss:0.585448286112617,train_acc:0.8047346472740173
node11 epoch1:node_model train_loss:0.35327472493929024,train_acc:0.8812482357025146
node11 epoch2:node_model train_loss:0.23491890114896438,train_acc:0.9353658556938171
node11 epoch3:node_model train_loss:0.17586170005447724,train_acc:0.9501433968544006
node11 epoch4:node_model train_loss:0.12507725156405392,train_acc:0.9745767116546631
node11_model on test-dataset: loss:0.8458882586658001,acc:0.7545000314712524
node11 weight score:1988.4423063785985
node18: train data size:472
node18 epoch0:node_model train_loss:0.7204124331474304,train_acc:0.7814444899559021
node18 epoch1:node_model train_loss:0.340872648358345,train_acc:0.8862223029136658
node18 epoch2:node_model train_loss:0.22789525985717773,train_acc:0.9433333277702332
node18 epoch3:node_model train_loss:0.16288137286901475,train_acc:0.9644444584846497
node18 epoch4:node_model train_loss:0.10667123347520828,train_acc:0.9736666679382324
node18_model on test-dataset: loss:0.8678085896372795,acc:0.7386000752449036
node18 weight score:543.8987417689461
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6835650254127591,train_acc:0.7765833735466003
node19 epoch1:node_model train_loss:0.4777513243431269,train_acc:0.8419723510742188
node19 epoch2:node_model train_loss:0.38533976327541264,train_acc:0.8734624981880188
node19 epoch3:node_model train_loss:0.3083645753389181,train_acc:0.8993452787399292
node19 epoch4:node_model train_loss:0.3023333254941674,train_acc:0.8990294933319092
node19_model on test-dataset: loss:0.8354367095604539,acc:0.7446998953819275
node19 weight score:5124.266088633275
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6478293231129646,acc:0.8038999831676483
total cost energy:8.963528137423857 | all_enery_cp：7.115 | all_enery_tp: 1.8485281374238571
ef: 24.794883383504295
reward: 15.831355246080438
step 440:loss:6.979471206665039|running q:30.29796028137207
episode7,iteration20 selected nodes:[12, 0, 5, 3, 4],center node:3
################################################## episode7,iteration20 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.6443163827061653,train_acc:0.784719705581665
node0 epoch1:node_model train_loss:0.4186813057615207,train_acc:0.8591034412384033
node0 epoch2:node_model train_loss:0.36011954970084703,train_acc:0.8788323998451233
node0 epoch3:node_model train_loss:0.3044684036419942,train_acc:0.8960703611373901
node0 epoch4:node_model train_loss:0.2657514438033104,train_acc:0.9104933142662048
node0_model on test-dataset: loss:0.8545154795050621,acc:0.7481000423431396
node0 weight score:6065.42552394956
node3: train data size:4247
node3 epoch0:node_model train_loss:0.7508522791917934,train_acc:0.7615634799003601
node3 epoch1:node_model train_loss:0.5047941602939783,train_acc:0.8233053684234619
node3 epoch2:node_model train_loss:0.4061161037101302,train_acc:0.8619444966316223
node3 epoch3:node_model train_loss:0.34049354736195053,train_acc:0.8853140473365784
node3 epoch4:node_model train_loss:0.277609902758931,train_acc:0.908604621887207
node3_model on test-dataset: loss:0.8332550762593747,acc:0.7563000321388245
node3 weight score:5096.878640170442
node4: train data size:2705
node4 epoch0:node_model train_loss:0.5977490342089108,train_acc:0.8067857623100281
node4 epoch1:node_model train_loss:0.35803493431636263,train_acc:0.877500057220459
node4 epoch2:node_model train_loss:0.2755314180893557,train_acc:0.9021428823471069
node4 epoch3:node_model train_loss:0.2548152758473797,train_acc:0.9160714745521545
node4 epoch4:node_model train_loss:0.1758920502449785,train_acc:0.9424999952316284
node4_model on test-dataset: loss:0.8024142649769783,acc:0.7586997747421265
node4 weight score:3371.076659607501
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5664858096524289,train_acc:0.819060206413269
node5 epoch1:node_model train_loss:0.38103793365390676,train_acc:0.8667294383049011
node5 epoch2:node_model train_loss:0.2817411885449761,train_acc:0.9059022665023804
node5 epoch3:node_model train_loss:0.2345542484208157,train_acc:0.9256389141082764
node5 epoch4:node_model train_loss:0.21092881772078967,train_acc:0.9313534498214722
node5_model on test-dataset: loss:0.7869893111288547,acc:0.760200023651123
node5 weight score:4745.934852205971
node12: train data size:1336
node12 epoch0:node_model train_loss:0.7258617409637996,train_acc:0.7757936120033264
node12 epoch1:node_model train_loss:0.3961124143430165,train_acc:0.8683333992958069
node12 epoch2:node_model train_loss:0.27977625812803,train_acc:0.9027777910232544
node12 epoch3:node_model train_loss:0.2152510987860816,train_acc:0.9340475797653198
node12 epoch4:node_model train_loss:0.17976691360984529,train_acc:0.9420635104179382
node12_model on test-dataset: loss:0.7864746299386024,acc:0.7697000503540039
node12 weight score:1698.7197668465126
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6161006037890912,acc:0.8084999793767929
total cost energy:10.343816574771573 | all_enery_cp：8.603 | all_enery_tp: 1.7408165747715727
ef: 25.14109584988551
reward: 14.797279275113937
step 441:loss:7.375872611999512|running q:31.759002685546875
episode7,iteration21 selected nodes:[0, 10, 7, 12, 11],center node:7
################################################## episode7,iteration21 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.5226721878235157,train_acc:0.819443941116333
node0 epoch1:node_model train_loss:0.3347805907519964,train_acc:0.8798726201057434
node0 epoch2:node_model train_loss:0.28757112415937275,train_acc:0.9062231779098511
node0 epoch3:node_model train_loss:0.24023516948979634,train_acc:0.9173677563667297
node0 epoch4:node_model train_loss:0.2127479213074996,train_acc:0.9304540753364563
node0_model on test-dataset: loss:0.7786485639214515,acc:0.7720997929573059
node0 weight score:6656.404750683969
node7: train data size:1951
node7 epoch0:node_model train_loss:0.622351185977459,train_acc:0.7957548499107361
node7 epoch1:node_model train_loss:0.4324119187891483,train_acc:0.8440588116645813
node7 epoch2:node_model train_loss:0.2514072313904762,train_acc:0.9265783429145813
node7 epoch3:node_model train_loss:0.18876672573387623,train_acc:0.9404999613761902
node7 epoch4:node_model train_loss:0.1568969998508692,train_acc:0.9610391855239868
node7_model on test-dataset: loss:0.7420876543223858,acc:0.7795000076293945
node7 weight score:2629.069475332392
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6218468606472015,train_acc:0.7929999828338623
node10 epoch1:node_model train_loss:0.3522341623902321,train_acc:0.8800000548362732
node10 epoch2:node_model train_loss:0.26389172449707987,train_acc:0.9219999313354492
node10 epoch3:node_model train_loss:0.20120387487113475,train_acc:0.9414998888969421
node10 epoch4:node_model train_loss:0.13773138485848904,train_acc:0.9608331918716431
node10_model on test-dataset: loss:0.7709348204731942,acc:0.7583000063896179
node10 weight score:2561.8248748808096
node11: train data size:1682
node11 epoch0:node_model train_loss:0.535371159805971,train_acc:0.8219225406646729
node11 epoch1:node_model train_loss:0.3152427138651119,train_acc:0.8928264379501343
node11 epoch2:node_model train_loss:0.20788592713720658,train_acc:0.9341318607330322
node11 epoch3:node_model train_loss:0.1472183571142309,train_acc:0.955623984336853
node11 epoch4:node_model train_loss:0.10408837172914953,train_acc:0.9779768586158752
node11_model on test-dataset: loss:0.8105412529408932,acc:0.7629998326301575
node11 weight score:2075.1565622319486
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6192984964166369,train_acc:0.793254017829895
node12 epoch1:node_model train_loss:0.3908900661127908,train_acc:0.8707936406135559
node12 epoch2:node_model train_loss:0.24848955231053488,train_acc:0.9226190447807312
node12 epoch3:node_model train_loss:0.1845991084618228,train_acc:0.9524602890014648
node12 epoch4:node_model train_loss:0.12871222038354194,train_acc:0.9646031856536865
node12_model on test-dataset: loss:0.8476204453408718,acc:0.748699963092804
node12 weight score:1576.1771761684272
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6135370428860187,acc:0.8092999809980392
total cost energy:7.565457842230247 | all_enery_cp：6.0635 | all_enery_tp: 1.5019578422302464
ef: 25.156401187343995
reward: 17.59094334511375
step 442:loss:6.93949556350708|running q:33.13996887207031
episode7,iteration22 selected nodes:[17, 8, 4, 10, 19],center node:10
################################################## episode7,iteration22 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.565564688295126,train_acc:0.8117856383323669
node4 epoch1:node_model train_loss:0.3603020060275282,train_acc:0.8810715675354004
node4 epoch2:node_model train_loss:0.26354663478144047,train_acc:0.9142856597900391
node4 epoch3:node_model train_loss:0.17930010786013945,train_acc:0.9510714411735535
node4 epoch4:node_model train_loss:0.19039872022611753,train_acc:0.9410713911056519
node4_model on test-dataset: loss:0.9086348645016551,acc:0.7402998208999634
node4 weight score:2976.99340590851
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6965987665785683,train_acc:0.7802947759628296
node8 epoch1:node_model train_loss:0.4069574210378859,train_acc:0.854274332523346
node8 epoch2:node_model train_loss:0.26367514746056664,train_acc:0.9137527942657471
node8 epoch3:node_model train_loss:0.20627487781975004,train_acc:0.940510094165802
node8 epoch4:node_model train_loss:0.1432980423172315,train_acc:0.9666325449943542
node8_model on test-dataset: loss:0.7513085849583149,acc:0.7684999108314514
node8 weight score:2393.1577996007577
node10: train data size:1975
node10 epoch0:node_model train_loss:0.5212235450744629,train_acc:0.8156666159629822
node10 epoch1:node_model train_loss:0.30735143050551417,train_acc:0.8903331756591797
node10 epoch2:node_model train_loss:0.21083107739686965,train_acc:0.9344999194145203
node10 epoch3:node_model train_loss:0.16143423430621623,train_acc:0.9621664881706238
node10 epoch4:node_model train_loss:0.1260587316006422,train_acc:0.9646666646003723
node10_model on test-dataset: loss:0.7678777462244034,acc:0.7675999402999878
node10 weight score:2572.0240099559146
node17: train data size:442
node17 epoch0:node_model train_loss:0.8442352056503296,train_acc:0.7508571743965149
node17 epoch1:node_model train_loss:0.4194397866725922,train_acc:0.8721904754638672
node17 epoch2:node_model train_loss:0.2873209983110428,train_acc:0.9041904807090759
node17 epoch3:node_model train_loss:0.22741452157497405,train_acc:0.9194285273551941
node17 epoch4:node_model train_loss:0.1441085956990719,train_acc:0.9620000123977661
node17_model on test-dataset: loss:0.8856662578880787,acc:0.7440999150276184
node17 weight score:499.05931953868713
node19: train data size:4281
node19 epoch0:node_model train_loss:0.6002753411614618,train_acc:0.7960895299911499
node19 epoch1:node_model train_loss:0.4080383517714434,train_acc:0.8527794480323792
node19 epoch2:node_model train_loss:0.30185479272243587,train_acc:0.9004936814308167
node19 epoch3:node_model train_loss:0.23910883415577022,train_acc:0.9187425374984741
node19 epoch4:node_model train_loss:0.23407912982064624,train_acc:0.9279356002807617
node19_model on test-dataset: loss:0.7767504306137561,acc:0.7627000212669373
node19 weight score:5511.422757264944
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6122720922529697,acc:0.8059999805688858
total cost energy:7.025052312049013 | all_enery_cp：5.6005 | all_enery_tp: 1.4245523120490127
ef: 25.008904769771952
reward: 17.98385245772294
step 443:loss:8.677886009216309|running q:34.487548828125
episode7,iteration23 selected nodes:[8, 12, 11, 14, 4],center node:8
################################################## episode7,iteration23 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.41101813103471485,train_acc:0.8485714197158813
node4 epoch1:node_model train_loss:0.3056680423074535,train_acc:0.8953571319580078
node4 epoch2:node_model train_loss:0.18256669837449277,train_acc:0.9421428442001343
node4 epoch3:node_model train_loss:0.1463045755933438,train_acc:0.9574999809265137
node4 epoch4:node_model train_loss:0.16910778252141817,train_acc:0.9499999284744263
node4_model on test-dataset: loss:0.8493064995855093,acc:0.7561996579170227
node4 weight score:3184.9514884439627
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5341141207350625,train_acc:0.8119955062866211
node8 epoch1:node_model train_loss:0.336189188890987,train_acc:0.8943083882331848
node8 epoch2:node_model train_loss:0.2172535765502188,train_acc:0.934330940246582
node8 epoch3:node_model train_loss:0.1668956499132845,train_acc:0.9543877840042114
node8 epoch4:node_model train_loss:0.12445843012796508,train_acc:0.9710882902145386
node8_model on test-dataset: loss:0.7535712120682001,acc:0.7736002206802368
node8 weight score:2385.9722494776993
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5395613715929144,train_acc:0.8189095854759216
node11 epoch1:node_model train_loss:0.2823035129729439,train_acc:0.9030846357345581
node11 epoch2:node_model train_loss:0.189182748689371,train_acc:0.947460412979126
node11 epoch3:node_model train_loss:0.1350243223064086,train_acc:0.9620372653007507
node11 epoch4:node_model train_loss:0.09986655370277517,train_acc:0.9786943793296814
node11_model on test-dataset: loss:0.7911526855826377,acc:0.7663999199867249
node11 weight score:2126.011869328744
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6607832844768252,train_acc:0.7864286303520203
node12 epoch1:node_model train_loss:0.3631509712764195,train_acc:0.8736507892608643
node12 epoch2:node_model train_loss:0.2523787239832537,train_acc:0.9238889217376709
node12 epoch3:node_model train_loss:0.15722796214478357,train_acc:0.9640476107597351
node12 epoch4:node_model train_loss:0.11331618896552495,train_acc:0.9744444489479065
node12_model on test-dataset: loss:0.7579980316758156,acc:0.7702997922897339
node12 weight score:1762.5375583711111
node14: train data size:1172
node14 epoch0:node_model train_loss:0.6197010601560274,train_acc:0.8005092144012451
node14 epoch1:node_model train_loss:0.3959193912645181,train_acc:0.8630555868148804
node14 epoch2:node_model train_loss:0.2735258936882019,train_acc:0.918379545211792
node14 epoch3:node_model train_loss:0.16132733908792338,train_acc:0.9563888907432556
node14 epoch4:node_model train_loss:0.1288910067329804,train_acc:0.9687036275863647
node14_model on test-dataset: loss:0.8225365172326565,acc:0.7586999535560608
node14 weight score:1424.860751402356
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6102525082230568,acc:0.8117999839782715
total cost energy:5.724427174436485 | all_enery_cp：4.346500000000001 | all_enery_tp: 1.3779271744364845
ef: 25.029618454399888
reward: 19.305191279963402
step 444:loss:7.008491516113281|running q:35.84589767456055
episode7,iteration24 selected nodes:[16, 17, 7, 14, 19],center node:16
################################################## episode7,iteration24 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.561731393635273,train_acc:0.8217353224754333
node7 epoch1:node_model train_loss:0.30583771094679835,train_acc:0.8931176066398621
node7 epoch2:node_model train_loss:0.19800579845905303,train_acc:0.9430392384529114
node7 epoch3:node_model train_loss:0.15207163020968437,train_acc:0.9540587663650513
node7 epoch4:node_model train_loss:0.13096182160079478,train_acc:0.9620197415351868
node7_model on test-dataset: loss:0.8000079596042633,acc:0.7720999121665955
node7 weight score:2438.725735885295
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5247538139422735,train_acc:0.8206944465637207
node14 epoch1:node_model train_loss:0.3085516293843587,train_acc:0.9000462889671326
node14 epoch2:node_model train_loss:0.19963893728951612,train_acc:0.940740704536438
node14 epoch3:node_model train_loss:0.14524535338083902,train_acc:0.9590276479721069
node14 epoch4:node_model train_loss:0.11414207393924396,train_acc:0.9755091667175293
node14_model on test-dataset: loss:0.8336242543905974,acc:0.7572000622749329
node14 weight score:1405.9091896945401
node16: train data size:877
node16 epoch0:node_model train_loss:0.6965371502770318,train_acc:0.7829148769378662
node16 epoch1:node_model train_loss:0.4109886884689331,train_acc:0.8666811585426331
node16 epoch2:node_model train_loss:0.2809847957558102,train_acc:0.9011254906654358
node16 epoch3:node_model train_loss:0.1814758363697264,train_acc:0.9490042924880981
node16 epoch4:node_model train_loss:0.1361130252480507,train_acc:0.9630013704299927
node16_model on test-dataset: loss:0.8090069825947285,acc:0.7595000863075256
node16 weight score:1084.0450315857565
node17: train data size:442
node17 epoch0:node_model train_loss:0.8012753009796143,train_acc:0.7573333382606506
node17 epoch1:node_model train_loss:0.40512176752090456,train_acc:0.8554285168647766
node17 epoch2:node_model train_loss:0.2496714174747467,train_acc:0.9029523730278015
node17 epoch3:node_model train_loss:0.1648822009563446,train_acc:0.9449524283409119
node17 epoch4:node_model train_loss:0.07941077500581742,train_acc:0.983238160610199
node17_model on test-dataset: loss:1.0018659441173077,acc:0.7172999382019043
node17 weight score:441.17678876630885
node19: train data size:4281
node19 epoch0:node_model train_loss:0.47128921885823094,train_acc:0.8438328504562378
node19 epoch1:node_model train_loss:0.32465890049934387,train_acc:0.887292742729187
node19 epoch2:node_model train_loss:0.21178818961908652,train_acc:0.9341458082199097
node19 epoch3:node_model train_loss:0.177672999023005,train_acc:0.9461296200752258
node19 epoch4:node_model train_loss:0.13910598082597866,train_acc:0.9644184708595276
node19_model on test-dataset: loss:0.7548251695930958,acc:0.7719000577926636
node19 weight score:5671.511990396084
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6330359322577714,acc:0.8042999821901321
total cost energy:5.336430105346567 | all_enery_cp：4.3614999999999995 | all_enery_tp: 0.9749301053465671
ef: 25.073301024555278
reward: 19.736870919208712
step 445:loss:4.211138725280762|running q:37.157474517822266
episode7,iteration25 selected nodes:[19, 2, 5, 1, 12],center node:5
################################################## episode7,iteration25 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.5897192341439864,train_acc:0.8065441250801086
node1 epoch1:node_model train_loss:0.42043258381240506,train_acc:0.8547794818878174
node1 epoch2:node_model train_loss:0.3536262369769461,train_acc:0.8791911005973816
node1 epoch3:node_model train_loss:0.3018874980728416,train_acc:0.8961765170097351
node1 epoch4:node_model train_loss:0.24846505264148994,train_acc:0.9168382287025452
node1_model on test-dataset: loss:0.7607804596424103,acc:0.7744000554084778
node1 weight score:8817.26116250799
node2: train data size:4788
node2 epoch0:node_model train_loss:0.4006164288148284,train_acc:0.8592896461486816
node2 epoch1:node_model train_loss:0.21975353732705116,train_acc:0.9263161420822144
node2 epoch2:node_model train_loss:0.16307352824757496,train_acc:0.9471777677536011
node2 epoch3:node_model train_loss:0.1422093810979277,train_acc:0.9614298343658447
node2 epoch4:node_model train_loss:0.11731736672421296,train_acc:0.9716383218765259
node2_model on test-dataset: loss:0.8365817553550005,acc:0.7607001066207886
node2 weight score:5723.290006448
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5590871956787611,train_acc:0.8183833956718445
node5 epoch1:node_model train_loss:0.356647305582699,train_acc:0.8748873472213745
node5 epoch2:node_model train_loss:0.23244885708156385,train_acc:0.921729326248169
node5 epoch3:node_model train_loss:0.20270790081275136,train_acc:0.9394736886024475
node5 epoch4:node_model train_loss:0.17822038048976346,train_acc:0.9494736194610596
node5_model on test-dataset: loss:0.7815068197250367,acc:0.7717999219894409
node5 weight score:4779.228927668363
node12: train data size:1336
node12 epoch0:node_model train_loss:0.5618078772510801,train_acc:0.817380964756012
node12 epoch1:node_model train_loss:0.31764371054513113,train_acc:0.9003174901008606
node12 epoch2:node_model train_loss:0.2189911784870284,train_acc:0.9265873432159424
node12 epoch3:node_model train_loss:0.1495594536619527,train_acc:0.9596031308174133
node12 epoch4:node_model train_loss:0.10699180593448025,train_acc:0.9758729934692383
node12_model on test-dataset: loss:0.8043555657565594,acc:0.7659000754356384
node12 weight score:1660.95699075991
node19: train data size:4281
node19 epoch0:node_model train_loss:0.29253395594829734,train_acc:0.8974703550338745
node19 epoch1:node_model train_loss:0.21704633776531662,train_acc:0.9282228946685791
node19 epoch2:node_model train_loss:0.15874188982470092,train_acc:0.9557046890258789
node19 epoch3:node_model train_loss:0.1432884491460268,train_acc:0.9590151309967041
node19 epoch4:node_model train_loss:0.11810831231779831,train_acc:0.9675652980804443
node19_model on test-dataset: loss:0.8067823500931263,acc:0.765099823474884
node19 weight score:5306.263826304389
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.5915757231414318,acc:0.8174999803304672
total cost energy:12.475787018309122 | all_enery_cp：10.424 | all_enery_tp: 2.0517870183091227
ef: 25.15974376975027
reward: 12.68395675144115
step 446:loss:4.795375823974609|running q:38.496559143066406
episode7,iteration26 selected nodes:[1, 3, 2, 0, 18],center node:2
################################################## episode7,iteration26 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.469730994449212,train_acc:0.8402966856956482
node0 epoch1:node_model train_loss:0.29636356377830875,train_acc:0.8980675935745239
node0 epoch2:node_model train_loss:0.22191192410313165,train_acc:0.9234963059425354
node0 epoch3:node_model train_loss:0.17938666289242414,train_acc:0.9489549398422241
node0 epoch4:node_model train_loss:0.14783478227372354,train_acc:0.9558779001235962
node0_model on test-dataset: loss:0.7592432434856892,acc:0.7764002084732056
node0 weight score:6826.534242444916
node1: train data size:6708
node1 epoch0:node_model train_loss:0.4231259329792331,train_acc:0.8561766147613525
node1 epoch1:node_model train_loss:0.31483644904459224,train_acc:0.8868382573127747
node1 epoch2:node_model train_loss:0.3072303441517493,train_acc:0.8933823108673096
node1 epoch3:node_model train_loss:0.26983967949362364,train_acc:0.9055147767066956
node1 epoch4:node_model train_loss:0.20809214803225853,train_acc:0.9295587539672852
node1_model on test-dataset: loss:0.7808748152852059,acc:0.774699866771698
node1 weight score:8590.3654064576
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3067980833972494,train_acc:0.8906345367431641
node2 epoch1:node_model train_loss:0.19652767851948738,train_acc:0.9387595653533936
node2 epoch2:node_model train_loss:0.1485854109438757,train_acc:0.9527177810668945
node2 epoch3:node_model train_loss:0.115958778342853,train_acc:0.9680113196372986
node2 epoch4:node_model train_loss:0.11070973356254399,train_acc:0.971193253993988
node2_model on test-dataset: loss:0.7815112037211657,acc:0.7799001336097717
node2 weight score:6126.591630679045
node3: train data size:4247
node3 epoch0:node_model train_loss:0.6566915272973305,train_acc:0.7924938797950745
node3 epoch1:node_model train_loss:0.43439266536124915,train_acc:0.8522909283638
node3 epoch2:node_model train_loss:0.34905278266862383,train_acc:0.8758684396743774
node3 epoch3:node_model train_loss:0.2819726910702018,train_acc:0.9047054648399353
node3 epoch4:node_model train_loss:0.21497663093167682,train_acc:0.9307817220687866
node3_model on test-dataset: loss:0.7359544217586518,acc:0.777400016784668
node3 weight score:5770.737799021958
node18: train data size:472
node18 epoch0:node_model train_loss:0.6131504476070404,train_acc:0.8206666111946106
node18 epoch1:node_model train_loss:0.2742536425590515,train_acc:0.9045554995536804
node18 epoch2:node_model train_loss:0.25062950551509855,train_acc:0.9277777671813965
node18 epoch3:node_model train_loss:0.08568233400583267,train_acc:0.9772221446037292
node18 epoch4:node_model train_loss:0.059780404716730115,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.9355151943862439,acc:0.7382000088691711
node18 weight score:504.5348304681052
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6008506256341934,acc:0.8175999802350998
total cost energy:12.240421356237311 | all_enery_cp：10.699000000000002 | all_enery_tp: 1.5414213562373096
ef: 25.171363935218928
reward: 12.930942578981616
step 447:loss:5.158979415893555|running q:39.83333969116211
episode7,iteration27 selected nodes:[11, 1, 18, 17, 4],center node:11
################################################## episode7,iteration27 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.37022260532659645,train_acc:0.8719853758811951
node1 epoch1:node_model train_loss:0.27126394628601913,train_acc:0.9039704203605652
node1 epoch2:node_model train_loss:0.24013970375937574,train_acc:0.9163233041763306
node1 epoch3:node_model train_loss:0.20574840037700007,train_acc:0.9306617379188538
node1 epoch4:node_model train_loss:0.22095062868560061,train_acc:0.922646701335907
node1_model on test-dataset: loss:0.86073876157403,acc:0.7521998286247253
node1 weight score:7793.305355196394
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4491947575339249,train_acc:0.8435713648796082
node4 epoch1:node_model train_loss:0.26533485629728865,train_acc:0.9132143259048462
node4 epoch2:node_model train_loss:0.20979303121566772,train_acc:0.9274998903274536
node4 epoch3:node_model train_loss:0.15309261317764009,train_acc:0.9567857384681702
node4 epoch4:node_model train_loss:0.1324705146253109,train_acc:0.9571428298950195
node4_model on test-dataset: loss:0.7813337651640176,acc:0.7806999683380127
node4 weight score:3462.0288033145043
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5201594505239936,train_acc:0.8388379812240601
node11 epoch1:node_model train_loss:0.3069596141576767,train_acc:0.8908607959747314
node11 epoch2:node_model train_loss:0.1825019112404655,train_acc:0.9422955513000488
node11 epoch3:node_model train_loss:0.13501699838568182,train_acc:0.9615064263343811
node11 epoch4:node_model train_loss:0.08355348907849368,train_acc:0.9834002256393433
node11_model on test-dataset: loss:0.7584966872632504,acc:0.7811002731323242
node11 weight score:2217.544292868125
node17: train data size:442
node17 epoch0:node_model train_loss:0.6674690485000611,train_acc:0.793904721736908
node17 epoch1:node_model train_loss:0.3810106754302979,train_acc:0.8877142071723938
node17 epoch2:node_model train_loss:0.24956291615962983,train_acc:0.9104762077331543
node17 epoch3:node_model train_loss:0.14327457845211028,train_acc:0.9449524283409119
node17 epoch4:node_model train_loss:0.0882996916770935,train_acc:0.984000027179718
node17_model on test-dataset: loss:0.8690407828986645,acc:0.7563000321388245
node17 weight score:508.60674055562697
node18: train data size:472
node18 epoch0:node_model train_loss:0.5642946183681488,train_acc:0.8091110587120056
node18 epoch1:node_model train_loss:0.32086002826690674,train_acc:0.8953332901000977
node18 epoch2:node_model train_loss:0.1979103982448578,train_acc:0.9416667222976685
node18 epoch3:node_model train_loss:0.13993703722953796,train_acc:0.9616667032241821
node18 epoch4:node_model train_loss:0.07482012510299682,train_acc:0.9852222800254822
node18_model on test-dataset: loss:0.8776629193127156,acc:0.7558000087738037
node18 weight score:537.7918898175806
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6504807571321726,acc:0.8074999797344208
total cost energy:7.855004621207566 | all_enery_cp：6.0045 | all_enery_tp: 1.8505046212075658
ef: 24.611086709914602
reward: 16.756082088707036
step 448:loss:3.476378917694092|running q:41.173702239990234
episode7,iteration28 selected nodes:[5, 7, 14, 4, 16],center node:7
################################################## episode7,iteration28 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.38040305567639215,train_acc:0.8782142400741577
node4 epoch1:node_model train_loss:0.23754561319947243,train_acc:0.9210713505744934
node4 epoch2:node_model train_loss:0.2121361456811428,train_acc:0.9324999451637268
node4 epoch3:node_model train_loss:0.28648131074649946,train_acc:0.9046428203582764
node4 epoch4:node_model train_loss:0.20141856771494662,train_acc:0.92464280128479
node4_model on test-dataset: loss:0.8697804650664329,acc:0.7567999362945557
node4 weight score:3109.980171598123
node5: train data size:3735
node5 epoch0:node_model train_loss:0.5391327652492022,train_acc:0.8205638527870178
node5 epoch1:node_model train_loss:0.34189251143681376,train_acc:0.8778570890426636
node5 epoch2:node_model train_loss:0.24160810030604662,train_acc:0.9185713529586792
node5 epoch3:node_model train_loss:0.17132932202596413,train_acc:0.9497743844985962
node5 epoch4:node_model train_loss:0.13131567129963323,train_acc:0.9626690745353699
node5_model on test-dataset: loss:0.7528651658445596,acc:0.7755001187324524
node5 weight score:4961.047700766046
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5919644877314567,train_acc:0.8141570091247559
node7 epoch1:node_model train_loss:0.28778994344174863,train_acc:0.9030391573905945
node7 epoch2:node_model train_loss:0.18429648354649544,train_acc:0.9455588459968567
node7 epoch3:node_model train_loss:0.13099339082837105,train_acc:0.9655587077140808
node7 epoch4:node_model train_loss:0.10538844782859087,train_acc:0.9720391631126404
node7_model on test-dataset: loss:0.8179384599626064,acc:0.7683999538421631
node7 weight score:2385.2650235925985
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5496617232759794,train_acc:0.8124537467956543
node14 epoch1:node_model train_loss:0.32186763112743694,train_acc:0.8972684741020203
node14 epoch2:node_model train_loss:0.20034589494268099,train_acc:0.9340741038322449
node14 epoch3:node_model train_loss:0.11985102482140064,train_acc:0.9700461626052856
node14 epoch4:node_model train_loss:0.07586938484261434,train_acc:0.9874999523162842
node14_model on test-dataset: loss:0.8109822204709053,acc:0.7724000215530396
node14 weight score:1445.1611520157198
node16: train data size:877
node16 epoch0:node_model train_loss:0.6986243195003934,train_acc:0.7858008742332458
node16 epoch1:node_model train_loss:0.36807457771566177,train_acc:0.8725684881210327
node16 epoch2:node_model train_loss:0.21341634458965725,train_acc:0.9390042424201965
node16 epoch3:node_model train_loss:0.143033633629481,train_acc:0.9567820429801941
node16 epoch4:node_model train_loss:0.11739761879046758,train_acc:0.9686724543571472
node16_model on test-dataset: loss:0.8267250829935073,acc:0.7646000385284424
node16 weight score:1060.8121345785844
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.615050781071186,acc:0.8117999792098999
total cost energy:6.846129717376116 | all_enery_cp：5.22 | all_enery_tp: 1.6261297173761164
ef: 24.877867556703578
reward: 18.031737839327462
step 449:loss:6.440414905548096|running q:42.520050048828125
episode7,iteration29 selected nodes:[12, 7, 17, 0, 1],center node:7
################################################## episode7,iteration29 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.4569769318287189,train_acc:0.8453360199928284
node0 epoch1:node_model train_loss:0.2984114378117598,train_acc:0.8979935050010681
node0 epoch2:node_model train_loss:0.2113285748144755,train_acc:0.9306508898735046
node0 epoch3:node_model train_loss:0.16749905579938337,train_acc:0.9469925165176392
node0 epoch4:node_model train_loss:0.14892331668390676,train_acc:0.9553055763244629
node0_model on test-dataset: loss:0.7162958630919456,acc:0.789699912071228
node0 weight score:7235.836847677978
node1: train data size:6708
node1 epoch0:node_model train_loss:0.2794909929747091,train_acc:0.9032352566719055
node1 epoch1:node_model train_loss:0.19232614919104996,train_acc:0.9316177368164062
node1 epoch2:node_model train_loss:0.1798646665890427,train_acc:0.9400732517242432
node1 epoch3:node_model train_loss:0.2087594799916534,train_acc:0.9287497401237488
node1 epoch4:node_model train_loss:0.14974055263926,train_acc:0.9512499570846558
node1_model on test-dataset: loss:0.8043070074915886,acc:0.7797996997833252
node1 weight score:8340.098914369028
node7: train data size:1951
node7 epoch0:node_model train_loss:0.43132445886731147,train_acc:0.8555784225463867
node7 epoch1:node_model train_loss:0.24504790157079698,train_acc:0.9190587997436523
node7 epoch2:node_model train_loss:0.14758496396243573,train_acc:0.9580588340759277
node7 epoch3:node_model train_loss:0.09326574355363845,train_acc:0.9804999232292175
node7 epoch4:node_model train_loss:0.09081002222374082,train_acc:0.9770391583442688
node7_model on test-dataset: loss:0.8146153639256954,acc:0.7703996896743774
node7 weight score:2394.995339393033
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6556664109230042,train_acc:0.7934128046035767
node12 epoch1:node_model train_loss:0.31222380165542873,train_acc:0.8879365921020508
node12 epoch2:node_model train_loss:0.21998093277215958,train_acc:0.9337301850318909
node12 epoch3:node_model train_loss:0.15463636336582048,train_acc:0.953015923500061
node12 epoch4:node_model train_loss:0.10279141632573945,train_acc:0.9751587510108948
node12_model on test-dataset: loss:0.8264640237390996,acc:0.7703998684883118
node12 weight score:1616.5252952640951
node17: train data size:442
node17 epoch0:node_model train_loss:0.6993197679519654,train_acc:0.8066666722297668
node17 epoch1:node_model train_loss:0.33095709681510926,train_acc:0.888952374458313
node17 epoch2:node_model train_loss:0.2527108758687973,train_acc:0.9117142558097839
node17 epoch3:node_model train_loss:0.13843422681093215,train_acc:0.9644762277603149
node17 epoch4:node_model train_loss:0.097038022428751,train_acc:0.9760000109672546
node17_model on test-dataset: loss:0.9199190454185009,acc:0.7468000054359436
node17 weight score:480.47706176027685
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6231951633095741,acc:0.8159999841451645
total cost energy:9.694801684473257 | all_enery_cp：7.81 | all_enery_tp: 1.8848016844732576
ef: 24.87048687636159
reward: 15.175685191888332
step 450:loss:5.969822406768799|running q:43.74286651611328
episode7,iteration30 selected nodes:[9, 8, 5, 19, 13],center node:9
################################################## episode7,iteration30 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.4605303614547378,train_acc:0.8510150909423828
node5 epoch1:node_model train_loss:0.24403003426758865,train_acc:0.9189473986625671
node5 epoch2:node_model train_loss:0.18078760469430372,train_acc:0.9421427845954895
node5 epoch3:node_model train_loss:0.13264685810396545,train_acc:0.9640600085258484
node5 epoch4:node_model train_loss:0.09919487733982112,train_acc:0.9753382802009583
node5_model on test-dataset: loss:0.7717002879083157,acc:0.7796997427940369
node5 weight score:4839.961910761589
node8: train data size:1798
node8 epoch0:node_model train_loss:0.6916116525729498,train_acc:0.7903174757957458
node8 epoch1:node_model train_loss:0.3682496216562059,train_acc:0.876564621925354
node8 epoch2:node_model train_loss:0.21766655892133713,train_acc:0.927652895450592
node8 epoch3:node_model train_loss:0.16054080716437763,train_acc:0.9549660086631775
node8 epoch4:node_model train_loss:0.11311501864757803,train_acc:0.9704989194869995
node8_model on test-dataset: loss:0.748316899985075,acc:0.781200110912323
node8 weight score:2402.725369473629
node9: train data size:1857
node9 epoch0:node_model train_loss:0.7085483717290979,train_acc:0.7927330732345581
node9 epoch1:node_model train_loss:0.3878904290889439,train_acc:0.865253746509552
node9 epoch2:node_model train_loss:0.26277169153878566,train_acc:0.9130193591117859
node9 epoch3:node_model train_loss:0.18062102755433634,train_acc:0.9451246857643127
node9 epoch4:node_model train_loss:0.1251993293040677,train_acc:0.9624930024147034
node9_model on test-dataset: loss:0.8052903651446104,acc:0.7734999656677246
node9 weight score:2306.000519038283
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7342044363419215,train_acc:0.7818939089775085
node13 epoch1:node_model train_loss:0.44406701624393463,train_acc:0.8627273440361023
node13 epoch2:node_model train_loss:0.2838619301716487,train_acc:0.9070454835891724
node13 epoch3:node_model train_loss:0.19154351080457369,train_acc:0.9419697523117065
node13 epoch4:node_model train_loss:0.11583514356364806,train_acc:0.9718181490898132
node13_model on test-dataset: loss:0.808831803202629,acc:0.7692000269889832
node13 weight score:1427.9853925460059
node19: train data size:4281
node19 epoch0:node_model train_loss:0.46832997299904044,train_acc:0.8439821004867554
node19 epoch1:node_model train_loss:0.2710713442674903,train_acc:0.9054321646690369
node19 epoch2:node_model train_loss:0.18616265127825182,train_acc:0.9412316679954529
node19 epoch3:node_model train_loss:0.13352568421599476,train_acc:0.9629687070846558
node19 epoch4:node_model train_loss:0.12203772903181785,train_acc:0.9660608172416687
node19_model on test-dataset: loss:0.792486402541399,acc:0.7745999693870544
node19 weight score:5401.985429997789
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6187093356251716,acc:0.8156999808549881
total cost energy:8.605349571741527 | all_enery_cp：6.412999999999999 | all_enery_tp: 2.192349571741529
ef: 24.972965751615497
reward: 16.36761617987397
step 451:loss:4.399868011474609|running q:44.98065185546875
episode7,iteration31 selected nodes:[7, 19, 10, 18, 4],center node:10
################################################## episode7,iteration31 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.43059631952616784,train_acc:0.8517857193946838
node4 epoch1:node_model train_loss:0.2349416861044509,train_acc:0.9182143211364746
node4 epoch2:node_model train_loss:0.1908371653407812,train_acc:0.9357143044471741
node4 epoch3:node_model train_loss:0.19588654248842172,train_acc:0.9335713982582092
node4 epoch4:node_model train_loss:0.16917856968939304,train_acc:0.9446427822113037
node4_model on test-dataset: loss:0.8658995047211647,acc:0.7625002264976501
node4 weight score:3123.919098291965
node7: train data size:1951
node7 epoch0:node_model train_loss:0.5061226770281791,train_acc:0.8405783772468567
node7 epoch1:node_model train_loss:0.26364741548895837,train_acc:0.9145979881286621
node7 epoch2:node_model train_loss:0.15698656514286996,train_acc:0.9460393190383911
node7 epoch3:node_model train_loss:0.10310890674591064,train_acc:0.9749999046325684
node7 epoch4:node_model train_loss:0.07513820715248584,train_acc:0.9839999079704285
node7_model on test-dataset: loss:0.7731650903820991,acc:0.7832000851631165
node7 weight score:2523.3938058892613
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6342643916606903,train_acc:0.8023332953453064
node10 epoch1:node_model train_loss:0.36415096297860144,train_acc:0.8795000314712524
node10 epoch2:node_model train_loss:0.23407925590872763,train_acc:0.9263332486152649
node10 epoch3:node_model train_loss:0.16901686750352382,train_acc:0.9493333697319031
node10 epoch4:node_model train_loss:0.1144589640200138,train_acc:0.9678331613540649
node10_model on test-dataset: loss:0.775321327149868,acc:0.7786999344825745
node10 weight score:2547.33093343405
node18: train data size:472
node18 epoch0:node_model train_loss:0.6794256567955017,train_acc:0.792888879776001
node18 epoch1:node_model train_loss:0.3125384867191315,train_acc:0.903333306312561
node18 epoch2:node_model train_loss:0.17922832816839218,train_acc:0.9461111426353455
node18 epoch3:node_model train_loss:0.10657100155949592,train_acc:0.9756667017936707
node18 epoch4:node_model train_loss:0.04991767778992653,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.843558004423976,acc:0.7654998898506165
node18 weight score:559.5347297099095
node19: train data size:4281
node19 epoch0:node_model train_loss:0.314324576147767,train_acc:0.8882916569709778
node19 epoch1:node_model train_loss:0.18760112551755684,train_acc:0.9396035671234131
node19 epoch2:node_model train_loss:0.13367558010788852,train_acc:0.9598361849784851
node19 epoch3:node_model train_loss:0.10224855647877205,train_acc:0.9753631949424744
node19 epoch4:node_model train_loss:0.0840937159089155,train_acc:0.9805884957313538
node19_model on test-dataset: loss:0.8026656851172447,acc:0.7795999646186829
node19 weight score:5333.478282897665
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6290658648312092,acc:0.8153999811410904
total cost energy:7.739280685971037 | all_enery_cp：5.691999999999999 | all_enery_tp: 2.047280685971038
ef: 24.84254047024301
reward: 17.10325978427197
step 452:loss:5.158191204071045|running q:46.20383071899414
episode7,iteration32 selected nodes:[1, 14, 2, 9, 11],center node:9
################################################## episode7,iteration32 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.2868894244653775,train_acc:0.8954411149024963
node1 epoch1:node_model train_loss:0.17410917525344036,train_acc:0.9391908645629883
node1 epoch2:node_model train_loss:0.16318226353648832,train_acc:0.9444118142127991
node1 epoch3:node_model train_loss:0.21728212123408036,train_acc:0.9223530292510986
node1 epoch4:node_model train_loss:0.15481148547876408,train_acc:0.9504410028457642
node1_model on test-dataset: loss:0.8386218084394932,acc:0.7739999294281006
node1 weight score:7998.838013147119
node2: train data size:4788
node2 epoch0:node_model train_loss:0.40763382116953534,train_acc:0.8599528074264526
node2 epoch1:node_model train_loss:0.2399470031571885,train_acc:0.9160510897636414
node2 epoch2:node_model train_loss:0.15822014398872852,train_acc:0.9498860836029053
node2 epoch3:node_model train_loss:0.1252022827975452,train_acc:0.9635133147239685
node2 epoch4:node_model train_loss:0.10216504102572799,train_acc:0.9718466997146606
node2_model on test-dataset: loss:0.82459461286664,acc:0.7685999274253845
node2 weight score:5806.489546851252
node9: train data size:1857
node9 epoch0:node_model train_loss:0.6571941014967466,train_acc:0.804182767868042
node9 epoch1:node_model train_loss:0.40877523783006164,train_acc:0.8674699664115906
node9 epoch2:node_model train_loss:0.24679516491137052,train_acc:0.9184117913246155
node9 epoch3:node_model train_loss:0.16360321523327576,train_acc:0.9556509256362915
node9 epoch4:node_model train_loss:0.10080230706616451,train_acc:0.9759186506271362
node9_model on test-dataset: loss:0.7565383008122444,acc:0.7806998491287231
node9 weight score:2454.6014365779815
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5596287969280692,train_acc:0.8358392715454102
node11 epoch1:node_model train_loss:0.2743346656070036,train_acc:0.906413197517395
node11 epoch2:node_model train_loss:0.18084384369499543,train_acc:0.9447202086448669
node11 epoch3:node_model train_loss:0.11482837270287906,train_acc:0.9726827144622803
node11 epoch4:node_model train_loss:0.08210351997438599,train_acc:0.9803299903869629
node11_model on test-dataset: loss:0.7747838449478149,acc:0.7844001054763794
node11 weight score:2170.9280736400615
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5306488970915476,train_acc:0.8341204524040222
node14 epoch1:node_model train_loss:0.3281932796041171,train_acc:0.8860647678375244
node14 epoch2:node_model train_loss:0.21726995582381883,train_acc:0.9331943392753601
node14 epoch3:node_model train_loss:0.11712244028846423,train_acc:0.9701851606369019
node14 epoch4:node_model train_loss:0.08189419625947873,train_acc:0.9823610186576843
node14_model on test-dataset: loss:0.7999891452491283,acc:0.7798001170158386
node14 weight score:1465.0198780322476
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6372472766041756,acc:0.8186999797821045
total cost energy:9.286732403787834 | all_enery_cp：8.103499999999999 | all_enery_tp: 1.183232403787835
ef: 25.260666228117806
reward: 15.973933824329972
step 453:loss:4.0474982261657715|running q:47.40962219238281
episode7,iteration33 selected nodes:[15, 17, 7, 9, 8],center node:7
################################################## episode7,iteration33 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.47004382461309435,train_acc:0.8485979437828064
node7 epoch1:node_model train_loss:0.21154087632894517,train_acc:0.9275391697883606
node7 epoch2:node_model train_loss:0.13157973289489747,train_acc:0.9585391879081726
node7 epoch3:node_model train_loss:0.09626867845654488,train_acc:0.9740391969680786
node7 epoch4:node_model train_loss:0.06395047018304467,train_acc:0.9894999861717224
node7_model on test-dataset: loss:0.7907376536726951,acc:0.7808998823165894
node7 weight score:2467.316424022935
node8: train data size:1798
node8 epoch0:node_model train_loss:0.5924041155311797,train_acc:0.8215306401252747
node8 epoch1:node_model train_loss:0.29554877761337495,train_acc:0.8999319076538086
node8 epoch2:node_model train_loss:0.19376578430334726,train_acc:0.9443424344062805
node8 epoch3:node_model train_loss:0.13559956269131768,train_acc:0.9560545086860657
node8 epoch4:node_model train_loss:0.08820273437433773,train_acc:0.9822109341621399
node8_model on test-dataset: loss:0.758812530040741,acc:0.7803000211715698
node8 weight score:2369.4917108227833
node9: train data size:1857
node9 epoch0:node_model train_loss:0.5039622720919157,train_acc:0.8335364460945129
node9 epoch1:node_model train_loss:0.2948160602858192,train_acc:0.9059094786643982
node9 epoch2:node_model train_loss:0.1899639273944654,train_acc:0.9322252869606018
node9 epoch3:node_model train_loss:0.12443533735839944,train_acc:0.9613111615180969
node9 epoch4:node_model train_loss:0.08490431818522905,train_acc:0.9765742421150208
node9_model on test-dataset: loss:0.8007572443783283,acc:0.7831999659538269
node9 weight score:2319.0548859057662
node15: train data size:629
node15 epoch0:node_model train_loss:0.8215921095439366,train_acc:0.7683743834495544
node15 epoch1:node_model train_loss:0.4163234063557216,train_acc:0.8690147995948792
node15 epoch2:node_model train_loss:0.23702438388551986,train_acc:0.9301478266716003
node15 epoch3:node_model train_loss:0.17388757850442613,train_acc:0.9479309916496277
node15 epoch4:node_model train_loss:0.11244194741759982,train_acc:0.9722167253494263
node15_model on test-dataset: loss:0.9022285602986813,acc:0.7486000061035156
node15 weight score:697.1625901443097
node17: train data size:442
node17 epoch0:node_model train_loss:0.717653465270996,train_acc:0.7696190476417542
node17 epoch1:node_model train_loss:0.344999235868454,train_acc:0.8801904916763306
node17 epoch2:node_model train_loss:0.2594823747873306,train_acc:0.9217142462730408
node17 epoch3:node_model train_loss:0.18469664752483367,train_acc:0.9372380375862122
node17 epoch4:node_model train_loss:0.09009326323866844,train_acc:0.9832380414009094
node17_model on test-dataset: loss:0.8943125009536743,acc:0.7592999339103699
node17 weight score:494.2343974043317
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6230822141468525,acc:0.8186999768018722
total cost energy:5.140935702206715 | all_enery_cp：3.3385000000000002 | all_enery_tp: 1.8024357022067143
ef: 24.485789176096798
reward: 19.344853473890083
step 454:loss:4.102439880371094|running q:48.600685119628906
episode7,iteration34 selected nodes:[16, 11, 14, 8, 17],center node:14
################################################## episode7,iteration34 ##################################################
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3965795694126023,train_acc:0.848163366317749
node8 epoch1:node_model train_loss:0.2145006044043435,train_acc:0.9204308390617371
node8 epoch2:node_model train_loss:0.1527747619483206,train_acc:0.953276515007019
node8 epoch3:node_model train_loss:0.10662176377243465,train_acc:0.9705101847648621
node8 epoch4:node_model train_loss:0.07249491579002804,train_acc:0.9838547110557556
node8_model on test-dataset: loss:0.767884337157011,acc:0.7866001129150391
node8 weight score:2341.498469231518
node11: train data size:1682
node11 epoch0:node_model train_loss:0.5261304220732521,train_acc:0.8185222148895264
node11 epoch1:node_model train_loss:0.24887621402740479,train_acc:0.913543701171875
node11 epoch2:node_model train_loss:0.1427087227211279,train_acc:0.9596126675605774
node11 epoch3:node_model train_loss:0.11163514677216024,train_acc:0.9694833159446716
node11 epoch4:node_model train_loss:0.07876284727278877,train_acc:0.9805880188941956
node11_model on test-dataset: loss:0.7817871762812137,acc:0.7784001231193542
node11 weight score:2151.4806727847554
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5206403012077013,train_acc:0.8309258818626404
node14 epoch1:node_model train_loss:0.29558390751481056,train_acc:0.8972684741020203
node14 epoch2:node_model train_loss:0.2091554800669352,train_acc:0.933055579662323
node14 epoch3:node_model train_loss:0.12421080563217402,train_acc:0.9631944894790649
node14 epoch4:node_model train_loss:0.08470557412753503,train_acc:0.9778703451156616
node14_model on test-dataset: loss:0.8421805112063885,acc:0.7718997597694397
node14 weight score:1391.6256484267951
node16: train data size:877
node16 epoch0:node_model train_loss:0.7057604723506503,train_acc:0.8021356463432312
node16 epoch1:node_model train_loss:0.4059719658560223,train_acc:0.8694660663604736
node16 epoch2:node_model train_loss:0.20723310775227016,train_acc:0.9342279434204102
node16 epoch3:node_model train_loss:0.1390303149819374,train_acc:0.9577777981758118
node16 epoch4:node_model train_loss:0.07727752543158001,train_acc:0.985223650932312
node16_model on test-dataset: loss:0.7760079228132963,acc:0.7778998613357544
node16 weight score:1130.1431006278551
node17: train data size:442
node17 epoch0:node_model train_loss:0.6198756456375122,train_acc:0.8048571944236755
node17 epoch1:node_model train_loss:0.3380442768335342,train_acc:0.898190438747406
node17 epoch2:node_model train_loss:0.19359463155269624,train_acc:0.9392380714416504
node17 epoch3:node_model train_loss:0.1696104511618614,train_acc:0.9577142596244812
node17 epoch4:node_model train_loss:0.06470739915966987,train_acc:0.9852380752563477
node17_model on test-dataset: loss:0.9778051482141018,acc:0.7458000779151917
node17 weight score:452.03280102102605
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6356605197489261,acc:0.816499981880188
total cost energy:4.116213578936527 | all_enery_cp：2.9855 | all_enery_tp: 1.1307135789365268
ef: 24.85716183649649
reward: 20.740948257559964
step 455:loss:4.07243013381958|running q:49.738380432128906
episode7,iteration35 selected nodes:[2, 8, 9, 16, 6],center node:9
################################################## episode7,iteration35 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.3312917829801639,train_acc:0.887121319770813
node2 epoch1:node_model train_loss:0.18841977572689453,train_acc:0.9363826513290405
node2 epoch2:node_model train_loss:0.14501316519454122,train_acc:0.9532196521759033
node2 epoch3:node_model train_loss:0.10856867046095431,train_acc:0.9674432277679443
node2 epoch4:node_model train_loss:0.08456589297081034,train_acc:0.979914665222168
node2_model on test-dataset: loss:0.8194698227196932,acc:0.7879999876022339
node2 weight score:5842.80209869031
node6: train data size:3007
node6 epoch0:node_model train_loss:0.6229796688402852,train_acc:0.807880163192749
node6 epoch1:node_model train_loss:0.4205556011969043,train_acc:0.8629031181335449
node6 epoch2:node_model train_loss:0.2762579040661935,train_acc:0.9103225469589233
node6 epoch3:node_model train_loss:0.1918465569615364,train_acc:0.9402304291725159
node6 epoch4:node_model train_loss:0.2121716756974497,train_acc:0.9308755993843079
node6_model on test-dataset: loss:0.8739527908712625,acc:0.7563998103141785
node6 weight score:3440.689281399578
node8: train data size:1798
node8 epoch0:node_model train_loss:0.281215680970086,train_acc:0.8998866677284241
node8 epoch1:node_model train_loss:0.1769571647875839,train_acc:0.9404988884925842
node8 epoch2:node_model train_loss:0.09056572429835796,train_acc:0.9760430455207825
node8 epoch3:node_model train_loss:0.07458457909524441,train_acc:0.9827436804771423
node8 epoch4:node_model train_loss:0.04993950554894076,train_acc:0.9922221302986145
node8_model on test-dataset: loss:0.7930502957105636,acc:0.7818000912666321
node8 weight score:2267.1954221882147
node9: train data size:1857
node9 epoch0:node_model train_loss:0.441894099900597,train_acc:0.8615604043006897
node9 epoch1:node_model train_loss:0.23393654627235314,train_acc:0.9207848310470581
node9 epoch2:node_model train_loss:0.13762954249978065,train_acc:0.9616988301277161
node9 epoch3:node_model train_loss:0.09980360261703793,train_acc:0.9742012023925781
node9 epoch4:node_model train_loss:0.06666627330215354,train_acc:0.9853923320770264
node9_model on test-dataset: loss:0.7999064563214779,acc:0.7755001187324524
node9 weight score:2321.5214545707854
node16: train data size:877
node16 epoch0:node_model train_loss:0.5446539686785804,train_acc:0.8449061512947083
node16 epoch1:node_model train_loss:0.2918010536167357,train_acc:0.9029005169868469
node16 epoch2:node_model train_loss:0.2069982530342208,train_acc:0.9298990368843079
node16 epoch3:node_model train_loss:0.11290757440858418,train_acc:0.9718902707099915
node16 epoch4:node_model train_loss:0.06515124481585291,train_acc:0.9811111092567444
node16_model on test-dataset: loss:0.8134031769633293,acc:0.7754000425338745
node16 weight score:1078.1861011092876
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6261506481468677,acc:0.8185999822616578
total cost energy:7.654618965504441 | all_enery_cp：6.1635 | all_enery_tp: 1.491118965504441
ef: 24.985548365082547
reward: 17.330929399578107
step 456:loss:6.5976057052612305|running q:50.86700439453125
episode7,iteration36 selected nodes:[7, 6, 15, 9, 8],center node:9
################################################## episode7,iteration36 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4701368096134355,train_acc:0.8493546843528748
node6 epoch1:node_model train_loss:0.30809820611630717,train_acc:0.9019353985786438
node6 epoch2:node_model train_loss:0.24530155235721218,train_acc:0.9150691032409668
node6 epoch3:node_model train_loss:0.18269632372163958,train_acc:0.9399077296257019
node6 epoch4:node_model train_loss:0.1430047322064638,train_acc:0.9593548774719238
node6_model on test-dataset: loss:0.8292699451744556,acc:0.7704001665115356
node6 weight score:3626.0810095648767
node7: train data size:1951
node7 epoch0:node_model train_loss:0.40725480690598487,train_acc:0.8616374135017395
node7 epoch1:node_model train_loss:0.21368628665804862,train_acc:0.9290784001350403
node7 epoch2:node_model train_loss:0.12379717025905848,train_acc:0.9624997973442078
node7 epoch3:node_model train_loss:0.09163899626582861,train_acc:0.9735391736030579
node7 epoch4:node_model train_loss:0.053961674124002455,train_acc:0.9899999499320984
node7_model on test-dataset: loss:0.7687619553506374,acc:0.7909998297691345
node7 weight score:2537.846711092949
node8: train data size:1798
node8 epoch0:node_model train_loss:0.289284841881858,train_acc:0.8920634388923645
node8 epoch1:node_model train_loss:0.14825915752185714,train_acc:0.9494103193283081
node8 epoch2:node_model train_loss:0.08926189090642664,train_acc:0.9794102907180786
node8 epoch3:node_model train_loss:0.063958874075777,train_acc:0.9861109852790833
node8 epoch4:node_model train_loss:0.05022138450294733,train_acc:0.9899771809577942
node8_model on test-dataset: loss:0.8111894161626697,acc:0.7791001200675964
node8 weight score:2216.498346964925
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4170799749462228,train_acc:0.858264148235321
node9 epoch1:node_model train_loss:0.2170321012013837,train_acc:0.9224929809570312
node9 epoch2:node_model train_loss:0.13010875233694127,train_acc:0.9578854441642761
node9 epoch3:node_model train_loss:0.0943269864901116,train_acc:0.9732869863510132
node9 epoch4:node_model train_loss:0.06878316578896422,train_acc:0.9852630496025085
node9_model on test-dataset: loss:0.7579347752034664,acc:0.7824001908302307
node9 weight score:2450.0788996012107
node15: train data size:629
node15 epoch0:node_model train_loss:0.7101399217333112,train_acc:0.8004434108734131
node15 epoch1:node_model train_loss:0.3270769225699561,train_acc:0.8795073628425598
node15 epoch2:node_model train_loss:0.23122059128114156,train_acc:0.9279310703277588
node15 epoch3:node_model train_loss:0.13996081400130475,train_acc:0.9628571271896362
node15 epoch4:node_model train_loss:0.10060067900589534,train_acc:0.9736452698707581
node15_model on test-dataset: loss:0.8486548198759556,acc:0.7674000859260559
node15 weight score:741.1729542665395
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6258888702094555,acc:0.8208999800682067
total cost energy:6.00009664627976 | all_enery_cp：4.621 | all_enery_tp: 1.3790966462797591
ef: 24.979839886960804
reward: 18.979743240681046
step 457:loss:4.850391387939453|running q:52.02521514892578
episode7,iteration37 selected nodes:[19, 9, 14, 0, 18],center node:14
################################################## episode7,iteration37 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.46878642216324806,train_acc:0.8434476256370544
node0 epoch1:node_model train_loss:0.27447433311205643,train_acc:0.9057205319404602
node0 epoch2:node_model train_loss:0.1933460788658032,train_acc:0.9345364570617676
node0 epoch3:node_model train_loss:0.14455589107595956,train_acc:0.9573028683662415
node0 epoch4:node_model train_loss:0.12319152537160195,train_acc:0.9658039212226868
node0_model on test-dataset: loss:0.7905070611461997,acc:0.7850998640060425
node0 weight score:6556.551174236043
node9: train data size:1857
node9 epoch0:node_model train_loss:0.33526635248410075,train_acc:0.8855124115943909
node9 epoch1:node_model train_loss:0.1523786261677742,train_acc:0.9540720582008362
node9 epoch2:node_model train_loss:0.11822318226883285,train_acc:0.9607754945755005
node9 epoch3:node_model train_loss:0.07217369365848993,train_acc:0.9815788269042969
node9 epoch4:node_model train_loss:0.04803266395863734,train_acc:0.9926314949989319
node9_model on test-dataset: loss:0.83208649918437,acc:0.7698999643325806
node9 weight score:2231.7391302710394
node14: train data size:1172
node14 epoch0:node_model train_loss:0.45300424595673877,train_acc:0.8559259176254272
node14 epoch1:node_model train_loss:0.25268427034219104,train_acc:0.917546272277832
node14 epoch2:node_model train_loss:0.1610078023125728,train_acc:0.9431945085525513
node14 epoch3:node_model train_loss:0.1030275324980418,train_acc:0.9740276336669922
node14 epoch4:node_model train_loss:0.08334653141597907,train_acc:0.9763425588607788
node14_model on test-dataset: loss:0.8100932942330837,acc:0.7761000394821167
node14 weight score:1446.7469467322696
node18: train data size:472
node18 epoch0:node_model train_loss:0.751231062412262,train_acc:0.7811111211776733
node18 epoch1:node_model train_loss:0.2655385464429855,train_acc:0.9181110262870789
node18 epoch2:node_model train_loss:0.16124609857797623,train_acc:0.9396666884422302
node18 epoch3:node_model train_loss:0.10842790007591248,train_acc:0.9739999771118164
node18 epoch4:node_model train_loss:0.08491088971495628,train_acc:0.9799999594688416
node18_model on test-dataset: loss:0.9210870704054832,acc:0.7465999722480774
node18 weight score:512.4379824289739
node19: train data size:4281
node19 epoch0:node_model train_loss:0.38272905453693035,train_acc:0.8694546818733215
node19 epoch1:node_model train_loss:0.2127726651554884,train_acc:0.9288113713264465
node19 epoch2:node_model train_loss:0.13085311581922132,train_acc:0.9603157043457031
node19 epoch3:node_model train_loss:0.08781464016714761,train_acc:0.976813018321991
node19 epoch4:node_model train_loss:0.0706219226988249,train_acc:0.9857595562934875
node19_model on test-dataset: loss:0.8200923875719309,acc:0.7814000248908997
node19 weight score:5220.143565379102
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6301238924264908,acc:0.820899983048439
total cost energy:8.311378978139153 | all_enery_cp：6.482499999999999 | all_enery_tp: 1.8288789781391548
ef: 24.87443298471001
reward: 16.563054006570855
step 458:loss:2.7744925022125244|running q:53.17461395263672
episode7,iteration38 selected nodes:[6, 16, 2, 8, 14],center node:14
################################################## episode7,iteration38 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2790149807309111,train_acc:0.9005207419395447
node2 epoch1:node_model train_loss:0.15484722416537502,train_acc:0.9475380778312683
node2 epoch2:node_model train_loss:0.11042368109337986,train_acc:0.9676799774169922
node2 epoch3:node_model train_loss:0.09005959223334987,train_acc:0.9723862409591675
node2 epoch4:node_model train_loss:0.06972903641872108,train_acc:0.9841667413711548
node2_model on test-dataset: loss:0.7652485311776399,acc:0.788800060749054
node2 weight score:6256.790839744252
node6: train data size:3007
node6 epoch0:node_model train_loss:0.43262105242859933,train_acc:0.8508756160736084
node6 epoch1:node_model train_loss:0.2671054188282259,train_acc:0.9112902283668518
node6 epoch2:node_model train_loss:0.17743209893665007,train_acc:0.9467741847038269
node6 epoch3:node_model train_loss:0.15862949096387433,train_acc:0.9514284133911133
node6 epoch4:node_model train_loss:0.19223645834192152,train_acc:0.9341012835502625
node6_model on test-dataset: loss:0.8326551389694213,acc:0.7638000845909119
node6 weight score:3611.339027729738
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3220506128337648,train_acc:0.8876756429672241
node8 epoch1:node_model train_loss:0.1791775677767065,train_acc:0.9349659085273743
node8 epoch2:node_model train_loss:0.10186120784944958,train_acc:0.9666325449943542
node8 epoch3:node_model train_loss:0.07024526513285106,train_acc:0.9822108149528503
node8 epoch4:node_model train_loss:0.0646156994625926,train_acc:0.9844330549240112
node8_model on test-dataset: loss:0.799859278947115,acc:0.7815001010894775
node8 weight score:2247.8954077606945
node14: train data size:1172
node14 epoch0:node_model train_loss:0.43380561222632724,train_acc:0.8561111688613892
node14 epoch1:node_model train_loss:0.2210691695412,train_acc:0.9181944131851196
node14 epoch2:node_model train_loss:0.1115196452786525,train_acc:0.9646758437156677
node14 epoch3:node_model train_loss:0.09762055519968271,train_acc:0.9685184359550476
node14 epoch4:node_model train_loss:0.053536812930057444,train_acc:0.9855091571807861
node14_model on test-dataset: loss:0.7892981430888176,acc:0.7857001423835754
node14 weight score:1484.8634958312805
node16: train data size:877
node16 epoch0:node_model train_loss:0.5860944986343384,train_acc:0.8332467079162598
node16 epoch1:node_model train_loss:0.31753097309006584,train_acc:0.8932322263717651
node16 epoch2:node_model train_loss:0.1769852423005634,train_acc:0.951226532459259
node16 epoch3:node_model train_loss:0.09902875042623943,train_acc:0.973780632019043
node16 epoch4:node_model train_loss:0.06947084723247422,train_acc:0.9799999594688416
node16_model on test-dataset: loss:0.758121085613966,acc:0.7889999747276306
node16 weight score:1156.8072919245608
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6225498059391975,acc:0.8207999789714813
total cost energy:7.6156328004297915 | all_enery_cp：5.821000000000001 | all_enery_tp: 1.7946328004297907
ef: 25.11300438864978
reward: 17.49737158821999
step 459:loss:4.657522678375244|running q:54.33218765258789
episode7,iteration39 selected nodes:[19, 11, 2, 12, 8],center node:11
################################################## episode7,iteration39 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.16172205711094043,train_acc:0.9437025785446167
node2 epoch1:node_model train_loss:0.11229167428488533,train_acc:0.9659563899040222
node2 epoch2:node_model train_loss:0.08737413546380897,train_acc:0.9719979763031006
node2 epoch3:node_model train_loss:0.06300651658481608,train_acc:0.9857766628265381
node2 epoch4:node_model train_loss:0.05802855636769285,train_acc:0.986609697341919
node2_model on test-dataset: loss:0.8434449626505375,acc:0.7821999788284302
node2 weight score:5676.718946727294
node8: train data size:1798
node8 epoch0:node_model train_loss:0.28943933215406203,train_acc:0.8937301635742188
node8 epoch1:node_model train_loss:0.16702701523900032,train_acc:0.9432652592658997
node8 epoch2:node_model train_loss:0.09183968210385905,train_acc:0.9744216203689575
node8 epoch3:node_model train_loss:0.05941603612154722,train_acc:0.9877662658691406
node8 epoch4:node_model train_loss:0.04291249454642335,train_acc:0.9916666150093079
node8_model on test-dataset: loss:0.7865738797187806,acc:0.7829998731613159
node8 weight score:2285.8628367405604
node11: train data size:1682
node11 epoch0:node_model train_loss:0.4396046180935467,train_acc:0.8671448826789856
node11 epoch1:node_model train_loss:0.23095702073153326,train_acc:0.9267430901527405
node11 epoch2:node_model train_loss:0.1641384032281006,train_acc:0.9475179314613342
node11 epoch3:node_model train_loss:0.10773019470712718,train_acc:0.9678479433059692
node11 epoch4:node_model train_loss:0.06629927237244214,train_acc:0.9834002256393433
node11_model on test-dataset: loss:0.8024929643422365,acc:0.7854999303817749
node11 weight score:2095.9685314856956
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6776038961751121,train_acc:0.7985714673995972
node12 epoch1:node_model train_loss:0.3130510374903679,train_acc:0.8951587677001953
node12 epoch2:node_model train_loss:0.2230712010392121,train_acc:0.9269048571586609
node12 epoch3:node_model train_loss:0.13546321168541908,train_acc:0.9580157995223999
node12 epoch4:node_model train_loss:0.11117632394390446,train_acc:0.9703174233436584
node12_model on test-dataset: loss:0.8151066549122333,acc:0.7786998748779297
node12 weight score:1639.0493096193086
node19: train data size:4281
node19 epoch0:node_model train_loss:0.3061351446911346,train_acc:0.8925723433494568
node19 epoch1:node_model train_loss:0.16892495820688647,train_acc:0.9409445524215698
node19 epoch2:node_model train_loss:0.10826834031315737,train_acc:0.9664570689201355
node19 epoch3:node_model train_loss:0.07787436038948768,train_acc:0.9825580716133118
node19 epoch4:node_model train_loss:0.05719638841096745,train_acc:0.988604724407196
node19_model on test-dataset: loss:0.7616533693671227,acc:0.7929999232292175
node19 weight score:5620.667054302133
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6411522462219,acc:0.8196999770402909
total cost energy:8.889713595499957 | all_enery_cp：6.9425 | all_enery_tp: 1.9472135954999579
ef: 25.03826151021738
reward: 16.148547914717426
step 460:loss:5.528950214385986|running q:55.43354415893555
episode7,iteration40 selected nodes:[11, 13, 3, 19, 9],center node:11
################################################## episode7,iteration40 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.6655649833900984,train_acc:0.7981939911842346
node3 epoch1:node_model train_loss:0.4066053576940714,train_acc:0.8634240627288818
node3 epoch2:node_model train_loss:0.29493571679259456,train_acc:0.9003166556358337
node3 epoch3:node_model train_loss:0.2034056330489558,train_acc:0.9327857494354248
node3 epoch4:node_model train_loss:0.18175984641840293,train_acc:0.9433695077896118
node3_model on test-dataset: loss:0.7887086164206266,acc:0.7800999283790588
node3 weight score:5384.751620026717
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4235650206867017,train_acc:0.8556417226791382
node9 epoch1:node_model train_loss:0.23404818616415324,train_acc:0.9185410737991333
node9 epoch2:node_model train_loss:0.12514676996751836,train_acc:0.9640719890594482
node9 epoch3:node_model train_loss:0.06880271415177144,train_acc:0.9855217337608337
node9 epoch4:node_model train_loss:0.05642154371660007,train_acc:0.9877561330795288
node9_model on test-dataset: loss:0.769757356569171,acc:0.7905999422073364
node9 weight score:2412.4485256973685
node11: train data size:1682
node11 epoch0:node_model train_loss:0.3510717460337807,train_acc:0.8720372915267944
node11 epoch1:node_model train_loss:0.17622768221532598,train_acc:0.9385077953338623
node11 epoch2:node_model train_loss:0.13122385762193622,train_acc:0.9556956887245178
node11 epoch3:node_model train_loss:0.08353418265195454,train_acc:0.9786942601203918
node11 epoch4:node_model train_loss:0.06118208936908666,train_acc:0.9863413572311401
node11_model on test-dataset: loss:0.7938900591433048,acc:0.7900998592376709
node11 weight score:2118.6812715794226
node13: train data size:1155
node13 epoch0:node_model train_loss:0.7331730127334595,train_acc:0.8071970343589783
node13 epoch1:node_model train_loss:0.3963617868721485,train_acc:0.8812121748924255
node13 epoch2:node_model train_loss:0.20671274016300836,train_acc:0.9379545450210571
node13 epoch3:node_model train_loss:0.1591178346425295,train_acc:0.9507575035095215
node13 epoch4:node_model train_loss:0.11874913362165292,train_acc:0.9644696712493896
node13_model on test-dataset: loss:0.8439329627156258,acc:0.7714998722076416
node13 weight score:1368.592116941867
node19: train data size:4281
node19 epoch0:node_model train_loss:0.1917459212763365,train_acc:0.9335572123527527
node19 epoch1:node_model train_loss:0.11823073046845059,train_acc:0.9609443545341492
node19 epoch2:node_model train_loss:0.0899259585800559,train_acc:0.975472092628479
node19 epoch3:node_model train_loss:0.06773116664831029,train_acc:0.9850618839263916
node19 epoch4:node_model train_loss:0.06132515875059505,train_acc:0.9880850911140442
node19_model on test-dataset: loss:0.7929401908814907,acc:0.7892999649047852
node19 weight score:5398.8939509308075
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6336071504652501,acc:0.818199982047081
total cost energy:8.205063876655656 | all_enery_cp：6.610999999999999 | all_enery_tp: 1.5940638766556563
ef: 25.139922061173888
reward: 16.934858184518234
step 461:loss:4.219919204711914|running q:56.52347183227539
episode7,iteration41 selected nodes:[6, 3, 17, 18, 19],center node:17
################################################## episode7,iteration41 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.40712483954984086,train_acc:0.8614199161529541
node3 epoch1:node_model train_loss:0.2720619802211606,train_acc:0.906654953956604
node3 epoch2:node_model train_loss:0.2085332737066025,train_acc:0.9348193407058716
node3 epoch3:node_model train_loss:0.16573518982460334,train_acc:0.9436318278312683
node3 epoch4:node_model train_loss:0.11258410913653152,train_acc:0.9708707928657532
node3_model on test-dataset: loss:0.7817984935641289,acc:0.7861999869346619
node3 weight score:5432.346103199072
node6: train data size:3007
node6 epoch0:node_model train_loss:0.44576492521070665,train_acc:0.8518433570861816
node6 epoch1:node_model train_loss:0.27326748928716105,train_acc:0.9108756184577942
node6 epoch2:node_model train_loss:0.2766812753773505,train_acc:0.9020736813545227
node6 epoch3:node_model train_loss:0.19315631819828863,train_acc:0.9380645751953125
node6 epoch4:node_model train_loss:0.11646244386511465,train_acc:0.9648386240005493
node6_model on test-dataset: loss:0.8001175152510405,acc:0.7787001729011536
node6 weight score:3758.197943031581
node17: train data size:442
node17 epoch0:node_model train_loss:0.714714127779007,train_acc:0.8049523234367371
node17 epoch1:node_model train_loss:0.48057869672775266,train_acc:0.8599047660827637
node17 epoch2:node_model train_loss:0.24542315155267716,train_acc:0.9314285516738892
node17 epoch3:node_model train_loss:0.13142487555742263,train_acc:0.9572381377220154
node17 epoch4:node_model train_loss:0.06614450141787528,train_acc:0.984000027179718
node17_model on test-dataset: loss:0.8877099820971489,acc:0.7661999464035034
node17 weight score:497.91036364805524
node18: train data size:472
node18 epoch0:node_model train_loss:0.6575067400932312,train_acc:0.8026666641235352
node18 epoch1:node_model train_loss:0.29699283838272095,train_acc:0.8918889164924622
node18 epoch2:node_model train_loss:0.1692095458507538,train_acc:0.9457778334617615
node18 epoch3:node_model train_loss:0.07961599081754685,train_acc:0.9772221446037292
node18 epoch4:node_model train_loss:0.06636499725282193,train_acc:0.9832221865653992
node18_model on test-dataset: loss:0.8664335188269615,acc:0.766499936580658
node18 weight score:544.7619346941087
node19: train data size:4281
node19 epoch0:node_model train_loss:0.13655904593855836,train_acc:0.9545965790748596
node19 epoch1:node_model train_loss:0.09755034484835558,train_acc:0.9722711443901062
node19 epoch2:node_model train_loss:0.07336344245041526,train_acc:0.9800688028335571
node19 epoch3:node_model train_loss:0.05709971361901871,train_acc:0.9872093200683594
node19 epoch4:node_model train_loss:0.04328320184072783,train_acc:0.9911627769470215
node19_model on test-dataset: loss:0.8346532747149468,acc:0.7852997183799744
node19 weight score:5129.075904556967
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6496434918045998,acc:0.8177999818325042
total cost energy:8.355931389247726 | all_enery_cp：6.224499999999999 | all_enery_tp: 2.131431389247726
ef: 24.789194694071874
reward: 16.43326330482415
step 462:loss:3.821460485458374|running q:57.62171173095703
episode7,iteration42 selected nodes:[16, 13, 2, 19, 10],center node:16
################################################## episode7,iteration42 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.22325951528425017,train_acc:0.9239108562469482
node2 epoch1:node_model train_loss:0.10763967673604687,train_acc:0.9653030633926392
node2 epoch2:node_model train_loss:0.06600414488154153,train_acc:0.9839016795158386
node2 epoch3:node_model train_loss:0.05431274080183357,train_acc:0.9886648654937744
node2 epoch4:node_model train_loss:0.04200417086637268,train_acc:0.9918751120567322
node2_model on test-dataset: loss:0.7846657295525074,acc:0.7920999526977539
node2 weight score:6101.961408115253
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6207631409168244,train_acc:0.8140000700950623
node10 epoch1:node_model train_loss:0.32728457525372506,train_acc:0.9003333449363708
node10 epoch2:node_model train_loss:0.1754184290766716,train_acc:0.9471666216850281
node10 epoch3:node_model train_loss:0.1279803618788719,train_acc:0.9656664729118347
node10 epoch4:node_model train_loss:0.09313021991401911,train_acc:0.9719999432563782
node10_model on test-dataset: loss:0.7611571273207665,acc:0.7885000705718994
node10 weight score:2594.733635290124
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6504271477460861,train_acc:0.8050000071525574
node13 epoch1:node_model train_loss:0.42618488768736523,train_acc:0.8585606813430786
node13 epoch2:node_model train_loss:0.19609157368540764,train_acc:0.9372726678848267
node13 epoch3:node_model train_loss:0.10884000671406587,train_acc:0.9676513671875
node13 epoch4:node_model train_loss:0.07926756935194135,train_acc:0.9799999594688416
node13_model on test-dataset: loss:0.8125500294566155,acc:0.7791001200675964
node13 weight score:1421.4509361009984
node16: train data size:877
node16 epoch0:node_model train_loss:0.6653501126501296,train_acc:0.8170273900032043
node16 epoch1:node_model train_loss:0.3833063907093472,train_acc:0.8921211957931519
node16 epoch2:node_model train_loss:0.20068429327673382,train_acc:0.9393362402915955
node16 epoch3:node_model train_loss:0.12888698942131466,train_acc:0.959004282951355
node16 epoch4:node_model train_loss:0.07221211658583747,train_acc:0.9820056557655334
node16_model on test-dataset: loss:0.8306487838923932,acc:0.7732999324798584
node16 weight score:1055.8012206921035
node19: train data size:4281
node19 epoch0:node_model train_loss:0.1320736359718234,train_acc:0.958495557308197
node19 epoch1:node_model train_loss:0.07079973262409832,train_acc:0.9817512631416321
node19 epoch2:node_model train_loss:0.04477947998012221,train_acc:0.9906976222991943
node19 epoch3:node_model train_loss:0.04308263996486054,train_acc:0.9922165274620056
node19 epoch4:node_model train_loss:0.039285256202484284,train_acc:0.9898363947868347
node19_model on test-dataset: loss:0.8714691586792469,acc:0.7801999449729919
node19 weight score:4912.394153440909
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6505642648041249,acc:0.8184999787807464
total cost energy:8.43705895634056 | all_enery_cp：6.537999999999999 | all_enery_tp: 1.8990589563405604
ef: 25.01737539767505
reward: 16.58031644133449
step 463:loss:4.28084135055542|running q:58.71428680419922
episode7,iteration43 selected nodes:[13, 15, 4, 18, 6],center node:6
################################################## episode7,iteration43 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4935836760061128,train_acc:0.8464285731315613
node4 epoch1:node_model train_loss:0.29333159806472914,train_acc:0.9078570604324341
node4 epoch2:node_model train_loss:0.22355761724923337,train_acc:0.9267855882644653
node4 epoch3:node_model train_loss:0.14416623062321118,train_acc:0.958214282989502
node4 epoch4:node_model train_loss:0.09945469855197839,train_acc:0.9682140350341797
node4_model on test-dataset: loss:0.8291246186196805,acc:0.7773001194000244
node4 weight score:3262.47700195329
node6: train data size:3007
node6 epoch0:node_model train_loss:0.4064138339411828,train_acc:0.8561751842498779
node6 epoch1:node_model train_loss:0.2832502544887604,train_acc:0.9044239521026611
node6 epoch2:node_model train_loss:0.18739062355410668,train_acc:0.9396774172782898
node6 epoch3:node_model train_loss:0.1222189492095382,train_acc:0.9635481834411621
node6 epoch4:node_model train_loss:0.07045816551263054,train_acc:0.9822578430175781
node6_model on test-dataset: loss:0.8074734461307526,acc:0.7826998829841614
node6 weight score:3723.9614682178444
node13: train data size:1155
node13 epoch0:node_model train_loss:0.6016154512763023,train_acc:0.8198484778404236
node13 epoch1:node_model train_loss:0.28369777277112007,train_acc:0.9015909433364868
node13 epoch2:node_model train_loss:0.2008147177596887,train_acc:0.9317423701286316
node13 epoch3:node_model train_loss:0.11789251336206992,train_acc:0.9644696116447449
node13 epoch4:node_model train_loss:0.07213770660261314,train_acc:0.9808332920074463
node13_model on test-dataset: loss:0.8390361617505551,acc:0.7772000432014465
node13 weight score:1376.5795238076769
node15: train data size:629
node15 epoch0:node_model train_loss:0.7163986223084586,train_acc:0.8069458603858948
node15 epoch1:node_model train_loss:0.4191975082669939,train_acc:0.8585221767425537
node15 epoch2:node_model train_loss:0.2190920731851033,train_acc:0.9214285612106323
node15 epoch3:node_model train_loss:0.16414762607642583,train_acc:0.9515763521194458
node15 epoch4:node_model train_loss:0.11103390263659614,train_acc:0.9665024280548096
node15_model on test-dataset: loss:0.8682236741483211,acc:0.7755998969078064
node15 weight score:724.4676904451078
node18: train data size:472
node18 epoch0:node_model train_loss:0.6678950071334839,train_acc:0.8186666369438171
node18 epoch1:node_model train_loss:0.24788233041763305,train_acc:0.9221111536026001
node18 epoch2:node_model train_loss:0.20639138221740722,train_acc:0.9361110925674438
node18 epoch3:node_model train_loss:0.10773774683475494,train_acc:0.9624444246292114
node18 epoch4:node_model train_loss:0.07795797064900398,train_acc:0.9812221527099609
node18_model on test-dataset: loss:0.9443786895275116,acc:0.7582000494003296
node18 weight score:499.7995033498156
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6546206061914563,acc:0.8178999811410904
total cost energy:6.426451274692796 | all_enery_cp：3.984 | all_enery_tp: 2.4424512746927958
ef: 24.126089981561197
reward: 17.699638706868402
step 464:loss:3.337709426879883|running q:59.78267288208008
episode7,iteration44 selected nodes:[13, 3, 18, 6, 7],center node:7
################################################## episode7,iteration44 ##################################################
node3: train data size:4247
node3 epoch0:node_model train_loss:0.447924627814182,train_acc:0.8506085276603699
node3 epoch1:node_model train_loss:0.24093162840188936,train_acc:0.917758584022522
node3 epoch2:node_model train_loss:0.15966139023387155,train_acc:0.9483423233032227
node3 epoch3:node_model train_loss:0.12184137443816939,train_acc:0.9620630741119385
node3 epoch4:node_model train_loss:0.09678059703735419,train_acc:0.9723254442214966
node3_model on test-dataset: loss:0.7891468246281147,acc:0.7872999906539917
node3 weight score:5381.761501734989
node6: train data size:3007
node6 epoch0:node_model train_loss:0.2437606682219813,train_acc:0.9167740941047668
node6 epoch1:node_model train_loss:0.1440259144911843,train_acc:0.9538708329200745
node6 epoch2:node_model train_loss:0.1233027073885164,train_acc:0.9624883532524109
node6 epoch3:node_model train_loss:0.18265199493015966,train_acc:0.9389398694038391
node6 epoch4:node_model train_loss:0.11676519772698803,train_acc:0.9645159840583801
node6_model on test-dataset: loss:0.8589874908328057,acc:0.7723999619483948
node6 weight score:3500.6330500629906
node7: train data size:1951
node7 epoch0:node_model train_loss:0.4847389988601208,train_acc:0.8465979695320129
node7 epoch1:node_model train_loss:0.2519056640565395,train_acc:0.9120783805847168
node7 epoch2:node_model train_loss:0.14146138466894626,train_acc:0.9500194787979126
node7 epoch3:node_model train_loss:0.09017291478812695,train_acc:0.974578320980072
node7 epoch4:node_model train_loss:0.059220369625836614,train_acc:0.9870195388793945
node7_model on test-dataset: loss:0.7771974755078555,acc:0.7914998531341553
node7 weight score:2510.301514714944
node13: train data size:1155
node13 epoch0:node_model train_loss:0.4597026581565539,train_acc:0.8465909957885742
node13 epoch1:node_model train_loss:0.23861903076370558,train_acc:0.9154545068740845
node13 epoch2:node_model train_loss:0.16529574804008007,train_acc:0.9506060481071472
node13 epoch3:node_model train_loss:0.10201082502802213,train_acc:0.9643181562423706
node13 epoch4:node_model train_loss:0.05304059820870558,train_acc:0.9884847402572632
node13_model on test-dataset: loss:0.8319336465001106,acc:0.7759000062942505
node13 weight score:1388.3318758160678
node18: train data size:472
node18 epoch0:node_model train_loss:0.5569074034690857,train_acc:0.8445555567741394
node18 epoch1:node_model train_loss:0.3261033147573471,train_acc:0.9028888940811157
node18 epoch2:node_model train_loss:0.1342415377497673,train_acc:0.9588889479637146
node18 epoch3:node_model train_loss:0.10050123184919357,train_acc:0.9716666340827942
node18 epoch4:node_model train_loss:0.06447206288576127,train_acc:0.9819999933242798
node18_model on test-dataset: loss:0.9193496803939343,acc:0.7620999813079834
node18 weight score:513.4063893922839
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6290690527111292,acc:0.8225999820232391
total cost energy:7.143812242327307 | all_enery_cp：5.416 | all_enery_tp: 1.727812242327307
ef: 24.758316565656173
reward: 17.614504323328866
step 465:loss:4.937469005584717|running q:60.80521011352539
episode7,iteration45 selected nodes:[12, 1, 5, 7, 9],center node:9
################################################## episode7,iteration45 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.38025628079605456,train_acc:0.8733821511268616
node1 epoch1:node_model train_loss:0.2118768697255291,train_acc:0.9247056841850281
node1 epoch2:node_model train_loss:0.14616817679694472,train_acc:0.9555880427360535
node1 epoch3:node_model train_loss:0.1901110627085847,train_acc:0.9371323585510254
node1 epoch4:node_model train_loss:0.13044011203900857,train_acc:0.9568381905555725
node1_model on test-dataset: loss:0.8279693292081356,acc:0.784200131893158
node1 weight score:8101.749380518101
node5: train data size:3735
node5 epoch0:node_model train_loss:0.501483144728761,train_acc:0.8432331085205078
node5 epoch1:node_model train_loss:0.28383846973118027,train_acc:0.9014285206794739
node5 epoch2:node_model train_loss:0.1949895183114629,train_acc:0.9307141900062561
node5 epoch3:node_model train_loss:0.13787076053650757,train_acc:0.9584585428237915
node5 epoch4:node_model train_loss:0.10243297985901959,train_acc:0.9742481112480164
node5_model on test-dataset: loss:0.7692267123609782,acc:0.7913998365402222
node5 weight score:4855.525607705705
node7: train data size:1951
node7 epoch0:node_model train_loss:0.2964817702770233,train_acc:0.8980979919433594
node7 epoch1:node_model train_loss:0.16741994805634022,train_acc:0.943078339099884
node7 epoch2:node_model train_loss:0.11178362146019935,train_acc:0.965539276599884
node7 epoch3:node_model train_loss:0.0870646744966507,train_acc:0.9725587964057922
node7 epoch4:node_model train_loss:0.0645672750659287,train_acc:0.9820391535758972
node7_model on test-dataset: loss:0.8539282436668872,acc:0.7826001644134521
node7 weight score:2284.735297689808
node9: train data size:1857
node9 epoch0:node_model train_loss:0.48130581802443456,train_acc:0.8409048318862915
node9 epoch1:node_model train_loss:0.2082449068364344,train_acc:0.930120050907135
node9 epoch2:node_model train_loss:0.16738187051133105,train_acc:0.9490673542022705
node9 epoch3:node_model train_loss:0.09470506737890996,train_acc:0.9736840128898621
node9 epoch4:node_model train_loss:0.06213970972519172,train_acc:0.9848660230636597
node9_model on test-dataset: loss:0.7768002611398697,acc:0.7852001190185547
node9 weight score:2390.575921376565
node12: train data size:1336
node12 epoch0:node_model train_loss:0.6188702923910958,train_acc:0.821587324142456
node12 epoch1:node_model train_loss:0.34694129547902514,train_acc:0.8811111450195312
node12 epoch2:node_model train_loss:0.20558340208871023,train_acc:0.9270634651184082
node12 epoch3:node_model train_loss:0.16192225313612393,train_acc:0.9487301707267761
node12 epoch4:node_model train_loss:0.10054304424141135,train_acc:0.9750000238418579
node12_model on test-dataset: loss:0.8407881104946137,acc:0.7789002656936646
node12 weight score:1588.9853618577768
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6472576433420181,acc:0.8208999794721603
total cost energy:9.264048142703343 | all_enery_cp：7.7935 | all_enery_tp: 1.4705481427033433
ef: 25.16000023936714
reward: 15.895952096663796
step 466:loss:4.106004238128662|running q:61.90576934814453
episode7,iteration46 selected nodes:[4, 16, 9, 19, 5],center node:9
################################################## episode7,iteration46 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4291793688067368,train_acc:0.8657143115997314
node4 epoch1:node_model train_loss:0.24693871289491653,train_acc:0.9178571701049805
node4 epoch2:node_model train_loss:0.14567537951682294,train_acc:0.9524999856948853
node4 epoch3:node_model train_loss:0.0892740533288036,train_acc:0.9789284467697144
node4 epoch4:node_model train_loss:0.07178364361503295,train_acc:0.9821426868438721
node4_model on test-dataset: loss:0.7757787653803825,acc:0.7933001518249512
node4 weight score:3486.8188209220643
node5: train data size:3735
node5 epoch0:node_model train_loss:0.3814151055718723,train_acc:0.8669549226760864
node5 epoch1:node_model train_loss:0.23101217378126948,train_acc:0.9221804738044739
node5 epoch2:node_model train_loss:0.15515710472276337,train_acc:0.9498118162155151
node5 epoch3:node_model train_loss:0.10559957307812415,train_acc:0.9687216877937317
node5 epoch4:node_model train_loss:0.07664907292315834,train_acc:0.9813532829284668
node5_model on test-dataset: loss:0.7369734950363636,acc:0.7971000671386719
node5 weight score:5068.024868134109
node9: train data size:1857
node9 epoch0:node_model train_loss:0.4184317510378988,train_acc:0.8649861216545105
node9 epoch1:node_model train_loss:0.17023767844626778,train_acc:0.9357709288597107
node9 epoch2:node_model train_loss:0.11025342266810567,train_acc:0.964727520942688
node9 epoch3:node_model train_loss:0.07921615086103741,train_acc:0.9827607870101929
node9 epoch4:node_model train_loss:0.0397642541088556,train_acc:0.9938133955001831
node9_model on test-dataset: loss:0.7727185444533825,acc:0.7952998876571655
node9 weight score:2403.203615766247
node16: train data size:877
node16 epoch0:node_model train_loss:0.5324636664655473,train_acc:0.840461790561676
node16 epoch1:node_model train_loss:0.33085403839747113,train_acc:0.8980086445808411
node16 epoch2:node_model train_loss:0.17106950283050537,train_acc:0.9373448491096497
node16 epoch3:node_model train_loss:0.11196259864502484,train_acc:0.9634487628936768
node16 epoch4:node_model train_loss:0.07772006880905893,train_acc:0.9734487533569336
node16_model on test-dataset: loss:0.836800456494093,acc:0.7742999792098999
node16 weight score:1048.0395812334154
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2136556991311007,train_acc:0.9229140281677246
node19 epoch1:node_model train_loss:0.09382626474943272,train_acc:0.971518874168396
node19 epoch2:node_model train_loss:0.05093279117068579,train_acc:0.9881395697593689
node19 epoch3:node_model train_loss:0.03768968326581079,train_acc:0.9938443303108215
node19 epoch4:node_model train_loss:0.03231428284198046,train_acc:0.9946513175964355
node19_model on test-dataset: loss:0.7854274792969227,acc:0.7952001690864563
node19 weight score:5450.535043454486
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6305611746013164,acc:0.8237999832630157
total cost energy:8.622078810497104 | all_enery_cp：6.727499999999999 | all_enery_tp: 1.8945788104971053
ef: 25.103267708085674
reward: 16.48118889758857
step 467:loss:4.619659900665283|running q:62.9837646484375
episode7,iteration47 selected nodes:[18, 13, 11, 17, 9],center node:11
################################################## episode7,iteration47 ##################################################
node9: train data size:1857
node9 epoch0:node_model train_loss:0.3600902588743913,train_acc:0.8712928295135498
node9 epoch1:node_model train_loss:0.1937769259277143,train_acc:0.9291965961456299
node9 epoch2:node_model train_loss:0.12551329637828626,train_acc:0.9596028923988342
node9 epoch3:node_model train_loss:0.06474309061702929,train_acc:0.9852631092071533
node9 epoch4:node_model train_loss:0.05499846350989843,train_acc:0.9856508374214172
node9_model on test-dataset: loss:0.848566897213459,acc:0.7797999978065491
node9 weight score:2188.395524381229
node11: train data size:1682
node11 epoch0:node_model train_loss:0.47218669162077065,train_acc:0.8526973128318787
node11 epoch1:node_model train_loss:0.23245766013860703,train_acc:0.9170013070106506
node11 epoch2:node_model train_loss:0.14780386099044016,train_acc:0.9517073035240173
node11 epoch3:node_model train_loss:0.08645816103500478,train_acc:0.9811763763427734
node11 epoch4:node_model train_loss:0.0612287048031302,train_acc:0.9826828241348267
node11_model on test-dataset: loss:0.8074938267469406,acc:0.7848999500274658
node11 weight score:2082.9880604488135
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5544015268484751,train_acc:0.8135606050491333
node13 epoch1:node_model train_loss:0.24222625667850176,train_acc:0.918181836605072
node13 epoch2:node_model train_loss:0.1796480348954598,train_acc:0.9429545402526855
node13 epoch3:node_model train_loss:0.11506593568871419,train_acc:0.9586362838745117
node13 epoch4:node_model train_loss:0.07535206340253353,train_acc:0.9801514148712158
node13_model on test-dataset: loss:0.8116574670374394,acc:0.7817999124526978
node13 weight score:1423.0140754027257
node17: train data size:442
node17 epoch0:node_model train_loss:0.631350827217102,train_acc:0.8277142643928528
node17 epoch1:node_model train_loss:0.284173509478569,train_acc:0.9081904292106628
node17 epoch2:node_model train_loss:0.21220761239528657,train_acc:0.9432380795478821
node17 epoch3:node_model train_loss:0.07097959518432617,train_acc:0.9880000352859497
node17 epoch4:node_model train_loss:0.08420956134796143,train_acc:0.9684762358665466
node17_model on test-dataset: loss:0.8607784587144852,acc:0.7694000005722046
node17 weight score:513.4886863457264
node18: train data size:472
node18 epoch0:node_model train_loss:0.6367101073265076,train_acc:0.8171110153198242
node18 epoch1:node_model train_loss:0.2755578100681305,train_acc:0.9076666831970215
node18 epoch2:node_model train_loss:0.18903390020132066,train_acc:0.9352222681045532
node18 epoch3:node_model train_loss:0.10966654121875763,train_acc:0.9592222571372986
node18 epoch4:node_model train_loss:0.051807350292801854,train_acc:0.9940000772476196
node18_model on test-dataset: loss:0.9174082245677709,acc:0.7661997675895691
node18 weight score:514.4928804430315
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.632328863069415,acc:0.8181999772787094
total cost energy:4.463749301604895 | all_enery_cp：2.8040000000000003 | all_enery_tp: 1.6597493016048945
ef: 24.310355004229073
reward: 19.846605702624178
step 468:loss:5.531700134277344|running q:64.03446197509766
episode7,iteration48 selected nodes:[4, 17, 3, 10, 0],center node:3
################################################## episode7,iteration48 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.4561304882741891,train_acc:0.8540987372398376
node0 epoch1:node_model train_loss:0.24406105523499158,train_acc:0.913919985294342
node0 epoch2:node_model train_loss:0.179730543293632,train_acc:0.9398029446601868
node0 epoch3:node_model train_loss:0.11632172118585843,train_acc:0.9664202928543091
node0 epoch4:node_model train_loss:0.09967792664582913,train_acc:0.9711143374443054
node0_model on test-dataset: loss:0.7468663288652897,acc:0.8016000390052795
node0 weight score:6939.662158654958
node3: train data size:4247
node3 epoch0:node_model train_loss:0.37592029918071834,train_acc:0.865635871887207
node3 epoch1:node_model train_loss:0.21391658606224281,train_acc:0.9242107272148132
node3 epoch2:node_model train_loss:0.14151028431085652,train_acc:0.9536616206169128
node3 epoch3:node_model train_loss:0.09385767284520836,train_acc:0.9748243093490601
node3 epoch4:node_model train_loss:0.08069849295844865,train_acc:0.9769468903541565
node3_model on test-dataset: loss:0.7748303811252117,acc:0.7925999164581299
node3 weight score:5481.199632147219
node4: train data size:2705
node4 epoch0:node_model train_loss:0.34740167084549156,train_acc:0.8789287209510803
node4 epoch1:node_model train_loss:0.14923891397691996,train_acc:0.9489284753799438
node4 epoch2:node_model train_loss:0.13395770133606025,train_acc:0.9574999809265137
node4 epoch3:node_model train_loss:0.14487624979977096,train_acc:0.9503569602966309
node4 epoch4:node_model train_loss:0.10286094407950129,train_acc:0.9674997329711914
node4_model on test-dataset: loss:0.8055426722764969,acc:0.7893998026847839
node4 weight score:3357.984738853819
node10: train data size:1975
node10 epoch0:node_model train_loss:0.6011166453361512,train_acc:0.8166666030883789
node10 epoch1:node_model train_loss:0.31294315680861473,train_acc:0.8984999656677246
node10 epoch2:node_model train_loss:0.18985856994986533,train_acc:0.9403333067893982
node10 epoch3:node_model train_loss:0.14328707195818424,train_acc:0.9551666378974915
node10 epoch4:node_model train_loss:0.09016239531338215,train_acc:0.9731665849685669
node10_model on test-dataset: loss:0.7428831167519092,acc:0.7922999262809753
node10 weight score:2658.5608899489425
node17: train data size:442
node17 epoch0:node_model train_loss:0.5433760344982147,train_acc:0.8409523963928223
node17 epoch1:node_model train_loss:0.27716217935085297,train_acc:0.9229523539543152
node17 epoch2:node_model train_loss:0.11015433520078659,train_acc:0.9760000109672546
node17 epoch3:node_model train_loss:0.0968155637383461,train_acc:0.9760000109672546
node17 epoch4:node_model train_loss:0.0626691497862339,train_acc:0.9872381091117859
node17_model on test-dataset: loss:0.925013198107481,acc:0.761900007724762
node17 weight score:477.83102003766464
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6239352806657553,acc:0.8212999808788299
total cost energy:8.8078830507798 | all_enery_cp：7.276 | all_enery_tp: 1.5318830507798011
ef: 25.07813522052085
reward: 16.27025216974105
step 469:loss:13.441603660583496|running q:65.17581939697266
episode7,iteration49 selected nodes:[0, 2, 9, 10, 7],center node:9
################################################## episode7,iteration49 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.26843681344046044,train_acc:0.9022984504699707
node0 epoch1:node_model train_loss:0.17724075383291796,train_acc:0.938417375087738
node0 epoch2:node_model train_loss:0.13058692453285822,train_acc:0.958728015422821
node0 epoch3:node_model train_loss:0.08494117626777062,train_acc:0.977268397808075
node0 epoch4:node_model train_loss:0.07312299909356695,train_acc:0.9826530814170837
node0_model on test-dataset: loss:0.7664314392209053,acc:0.8001999258995056
node0 weight score:6762.509645048794
node2: train data size:4788
node2 epoch0:node_model train_loss:0.24852795572951436,train_acc:0.9113826155662537
node2 epoch1:node_model train_loss:0.13833984554124376,train_acc:0.9568465948104858
node2 epoch2:node_model train_loss:0.0849678337884446,train_acc:0.9761362075805664
node2 epoch3:node_model train_loss:0.05939318876092633,train_acc:0.9841382503509521
node2 epoch4:node_model train_loss:0.04385177721269429,train_acc:0.9904168844223022
node2_model on test-dataset: loss:0.7550934721529484,acc:0.7994999289512634
node2 weight score:6340.936819846012
node7: train data size:1951
node7 epoch0:node_model train_loss:0.41618527844548225,train_acc:0.860696017742157
node7 epoch1:node_model train_loss:0.2026055660098791,train_acc:0.9295979738235474
node7 epoch2:node_model train_loss:0.12026359606534243,train_acc:0.9605194926261902
node7 epoch3:node_model train_loss:0.07607748415321111,train_acc:0.9794998168945312
node7 epoch4:node_model train_loss:0.04699630392715335,train_acc:0.9930195808410645
node7_model on test-dataset: loss:0.747293490767479,acc:0.797700047492981
node7 weight score:2610.7547089648815
node9: train data size:1857
node9 epoch0:node_model train_loss:0.2937603804625963,train_acc:0.8965650796890259
node9 epoch1:node_model train_loss:0.1185020214240802,train_acc:0.9611818194389343
node9 epoch2:node_model train_loss:0.08556974522377316,train_acc:0.9801292419433594
node9 epoch3:node_model train_loss:0.05004185016610121,train_acc:0.9880239367485046
node9 epoch4:node_model train_loss:0.03463037606132658,train_acc:0.9963157176971436
node9_model on test-dataset: loss:0.7851193031668663,acc:0.7900999784469604
node9 weight score:2365.245628924908
node10: train data size:1975
node10 epoch0:node_model train_loss:0.47232804000377654,train_acc:0.8478332757949829
node10 epoch1:node_model train_loss:0.2708111695945263,train_acc:0.9135000109672546
node10 epoch2:node_model train_loss:0.13578925281763077,train_acc:0.9569999575614929
node10 epoch3:node_model train_loss:0.10485891737043858,train_acc:0.9741665720939636
node10 epoch4:node_model train_loss:0.06271070586517453,train_acc:0.9898332953453064
node10_model on test-dataset: loss:0.7391267783939839,acc:0.7914997935295105
node10 weight score:2672.072041945755
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.612589187398553,acc:0.8284999811649323
total cost energy:9.244004637770997 | all_enery_cp：7.877 | all_enery_tp: 1.367004637770997
ef: 25.400742890185928
reward: 16.156738252414932
step 470:loss:7.4058308601379395|running q:66.26654052734375
episode7,iteration50 selected nodes:[1, 0, 14, 6, 8],center node:6
################################################## episode7,iteration50 ##################################################
node0: train data size:5183
node0 epoch0:node_model train_loss:0.20195909474904722,train_acc:0.9273378252983093
node0 epoch1:node_model train_loss:0.1336914233576793,train_acc:0.9544578194618225
node0 epoch2:node_model train_loss:0.10317118014567174,train_acc:0.9684221148490906
node0 epoch3:node_model train_loss:0.08603000963250032,train_acc:0.9735357165336609
node0 epoch4:node_model train_loss:0.06439354725611898,train_acc:0.982076108455658
node0_model on test-dataset: loss:0.7883080545067788,acc:0.7923999428749084
node0 weight score:6574.840851071667
node1: train data size:6708
node1 epoch0:node_model train_loss:0.31795660263913517,train_acc:0.8934556841850281
node1 epoch1:node_model train_loss:0.1863640208783395,train_acc:0.9336028099060059
node1 epoch2:node_model train_loss:0.13758795648155844,train_acc:0.9549265503883362
node1 epoch3:node_model train_loss:0.13852599449455738,train_acc:0.954558789730072
node1 epoch4:node_model train_loss:0.15736515250276117,train_acc:0.9473527073860168
node1_model on test-dataset: loss:0.8337175245583057,acc:0.7759998440742493
node1 weight score:8045.890607317897
node6: train data size:3007
node6 epoch0:node_model train_loss:0.3569091695450967,train_acc:0.8835482597351074
node6 epoch1:node_model train_loss:0.2044617560121321,train_acc:0.9223962426185608
node6 epoch2:node_model train_loss:0.13945275623231165,train_acc:0.9570966958999634
node6 epoch3:node_model train_loss:0.09144089647358464,train_acc:0.976128876209259
node6 epoch4:node_model train_loss:0.06375346236651944,train_acc:0.9850689172744751
node6_model on test-dataset: loss:0.8127421207726002,acc:0.7809000015258789
node6 weight score:3699.8205496492765
node8: train data size:1798
node8 epoch0:node_model train_loss:0.47213491217957604,train_acc:0.845963716506958
node8 epoch1:node_model train_loss:0.2295446222027143,train_acc:0.919387698173523
node8 epoch2:node_model train_loss:0.13749794993135664,train_acc:0.9565873742103577
node8 epoch3:node_model train_loss:0.08489204591347112,train_acc:0.9771540760993958
node8 epoch4:node_model train_loss:0.05903734639286995,train_acc:0.9855440855026245
node8_model on test-dataset: loss:0.7764862768352032,acc:0.7913000583648682
node8 weight score:2315.5592746961024
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4938052420814832,train_acc:0.8444445133209229
node14 epoch1:node_model train_loss:0.22704181571801504,train_acc:0.9203704595565796
node14 epoch2:node_model train_loss:0.14416661051412424,train_acc:0.9592128992080688
node14 epoch3:node_model train_loss:0.10032850938538711,train_acc:0.9698610305786133
node14 epoch4:node_model train_loss:0.06851130863651633,train_acc:0.9880092144012451
node14_model on test-dataset: loss:0.7969956684112549,acc:0.7950001955032349
node14 weight score:1470.5224212024707
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6341316471993923,acc:0.8213999837636947
total cost energy:10.487663105724558 | all_enery_cp：8.934000000000001 | all_enery_tp: 1.553663105724556
ef: 25.110103008303664
reward: 14.622439902579107
step 471:loss:6.910750865936279|running q:67.33706665039062
episode7,iteration51 selected nodes:[18, 13, 6, 9, 11],center node:11
################################################## episode7,iteration51 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.32935154101540964,train_acc:0.8912903070449829
node6 epoch1:node_model train_loss:0.1897948958700703,train_acc:0.9351612329483032
node6 epoch2:node_model train_loss:0.12850538448941323,train_acc:0.9638707041740417
node6 epoch3:node_model train_loss:0.09319290783136122,train_acc:0.9699075818061829
node6 epoch4:node_model train_loss:0.09686146700574506,train_acc:0.9758062958717346
node6_model on test-dataset: loss:0.9015953612700105,acc:0.77510005235672
node6 weight score:3335.1990584382133
node9: train data size:1857
node9 epoch0:node_model train_loss:0.37294498791820124,train_acc:0.8743212819099426
node9 epoch1:node_model train_loss:0.17895690038015968,train_acc:0.9393351674079895
node9 epoch2:node_model train_loss:0.09996230645399344,train_acc:0.9686795473098755
node9 epoch3:node_model train_loss:0.06223169273059619,train_acc:0.9874975085258484
node9 epoch4:node_model train_loss:0.04732170220660536,train_acc:0.9926316142082214
node9_model on test-dataset: loss:0.7926941540837288,acc:0.7923001646995544
node9 weight score:2342.6437427768055
node11: train data size:1682
node11 epoch0:node_model train_loss:0.43242496515021606,train_acc:0.8582496643066406
node11 epoch1:node_model train_loss:0.2427869456655839,train_acc:0.9188952445983887
node11 epoch2:node_model train_loss:0.11410224766415708,train_acc:0.9703299403190613
node11 epoch3:node_model train_loss:0.0835313420085346,train_acc:0.976212203502655
node11 epoch4:node_model train_loss:0.056336804457447105,train_acc:0.9870587587356567
node11_model on test-dataset: loss:0.8284879362583161,acc:0.7868998646736145
node11 weight score:2030.2045767816292
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5651159087816874,train_acc:0.8271970152854919
node13 epoch1:node_model train_loss:0.31993232915798825,train_acc:0.8989393711090088
node13 epoch2:node_model train_loss:0.1631411366785566,train_acc:0.9496212005615234
node13 epoch3:node_model train_loss:0.11658025284608205,train_acc:0.9651514887809753
node13 epoch4:node_model train_loss:0.05863864937176307,train_acc:0.9893181324005127
node13_model on test-dataset: loss:0.8456312549114228,acc:0.7823998332023621
node13 weight score:1365.8435556772115
node18: train data size:472
node18 epoch0:node_model train_loss:0.6053035676479339,train_acc:0.8275555968284607
node18 epoch1:node_model train_loss:0.3344164162874222,train_acc:0.918666660785675
node18 epoch2:node_model train_loss:0.18195801973342896,train_acc:0.9421111345291138
node18 epoch3:node_model train_loss:0.11293660625815391,train_acc:0.9676666259765625
node18 epoch4:node_model train_loss:0.07391670756042004,train_acc:0.9872221946716309
node18_model on test-dataset: loss:0.9118067537248135,acc:0.7631998062133789
node18 weight score:517.6535467321745
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6598306349664926,acc:0.821999979019165
total cost energy:5.668536886533115 | all_enery_cp：4.0865 | all_enery_tp: 1.582036886533115
ef: 24.630156994316152
reward: 18.961620107783038
step 472:loss:8.3399019241333|running q:68.39698791503906
episode7,iteration52 selected nodes:[8, 5, 11, 16, 15],center node:11
################################################## episode7,iteration52 ##################################################
node5: train data size:3735
node5 epoch0:node_model train_loss:0.40744905644341517,train_acc:0.8654512166976929
node5 epoch1:node_model train_loss:0.21427701099922783,train_acc:0.9295864105224609
node5 epoch2:node_model train_loss:0.16772747382913766,train_acc:0.9437594413757324
node5 epoch3:node_model train_loss:0.10943428377963994,train_acc:0.9637593030929565
node5 epoch4:node_model train_loss:0.07082155444904377,train_acc:0.9813156127929688
node5_model on test-dataset: loss:0.7538240379840135,acc:0.803800106048584
node5 weight score:4954.737195683866
node8: train data size:1798
node8 epoch0:node_model train_loss:0.4000413740674655,train_acc:0.8665646910667419
node8 epoch1:node_model train_loss:0.21345373118917146,train_acc:0.9260204434394836
node8 epoch2:node_model train_loss:0.1304302319056458,train_acc:0.9549205899238586
node8 epoch3:node_model train_loss:0.07058872158328693,train_acc:0.9833219647407532
node8 epoch4:node_model train_loss:0.05328780650678608,train_acc:0.9883332848548889
node8_model on test-dataset: loss:0.8068512651324272,acc:0.7874999642372131
node8 weight score:2228.4156668018572
node11: train data size:1682
node11 epoch0:node_model train_loss:0.3065124925445108,train_acc:0.8872022032737732
node11 epoch1:node_model train_loss:0.1667433113736265,train_acc:0.9472596645355225
node11 epoch2:node_model train_loss:0.10144997629172661,train_acc:0.9739883542060852
node11 epoch3:node_model train_loss:0.05416400873047464,train_acc:0.9875178933143616
node11 epoch4:node_model train_loss:0.03134987821035525,train_acc:0.9938593506813049
node11_model on test-dataset: loss:0.8184657728672028,acc:0.7909999489784241
node11 weight score:2055.064555855174
node15: train data size:629
node15 epoch0:node_model train_loss:0.6737134541784014,train_acc:0.7990148067474365
node15 epoch1:node_model train_loss:0.3664214142731258,train_acc:0.8874385356903076
node15 epoch2:node_model train_loss:0.2010294156415122,train_acc:0.9444335103034973
node15 epoch3:node_model train_loss:0.11196164307849747,train_acc:0.9657142758369446
node15 epoch4:node_model train_loss:0.10164489384208407,train_acc:0.9757143259048462
node15_model on test-dataset: loss:0.8405170233547687,acc:0.7763999104499817
node15 weight score:748.3489120654124
node16: train data size:877
node16 epoch0:node_model train_loss:0.6037627756595612,train_acc:0.8323521018028259
node16 epoch1:node_model train_loss:0.33741704126199085,train_acc:0.881240963935852
node16 epoch2:node_model train_loss:0.16929108732276493,train_acc:0.9501153230667114
node16 epoch3:node_model train_loss:0.13055336020059055,train_acc:0.9638960361480713
node16 epoch4:node_model train_loss:0.06950614663461845,train_acc:0.9822221398353577
node16_model on test-dataset: loss:0.8524681244790554,acc:0.7802000045776367
node16 weight score:1028.7774695809724
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.650420509018004,acc:0.8215999811887741
total cost energy:6.071154957016754 | all_enery_cp：4.3605 | all_enery_tp: 1.710654957016754
ef: 24.720135511963065
reward: 18.648980554946313
step 473:loss:6.87241268157959|running q:69.41266632080078
episode7,iteration53 selected nodes:[7, 6, 12, 8, 11],center node:7
################################################## episode7,iteration53 ##################################################
node6: train data size:3007
node6 epoch0:node_model train_loss:0.2337279013928867,train_acc:0.9145159721374512
node6 epoch1:node_model train_loss:0.11285566003812898,train_acc:0.9638708829879761
node6 epoch2:node_model train_loss:0.08510095566030472,train_acc:0.9757140278816223
node6 epoch3:node_model train_loss:0.12636604808991955,train_acc:0.9577418565750122
node6 epoch4:node_model train_loss:0.07631322969832728,train_acc:0.9758061766624451
node6_model on test-dataset: loss:0.8503186623752117,acc:0.7839999794960022
node6 weight score:3536.3213028871883
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3827019535005093,train_acc:0.8676372766494751
node7 epoch1:node_model train_loss:0.19639517478644847,train_acc:0.932539165019989
node7 epoch2:node_model train_loss:0.11874031908810138,train_acc:0.9650195240974426
node7 epoch3:node_model train_loss:0.07951722554862499,train_acc:0.9810194969177246
node7 epoch4:node_model train_loss:0.0388801695778966,train_acc:0.9939999580383301
node7_model on test-dataset: loss:0.7988440951704979,acc:0.7914000749588013
node7 weight score:2442.2788023282524
node8: train data size:1798
node8 epoch0:node_model train_loss:0.2903801318671968,train_acc:0.8959863781929016
node8 epoch1:node_model train_loss:0.16926376438803142,train_acc:0.9416325688362122
node8 epoch2:node_model train_loss:0.09572555269632074,train_acc:0.9705327153205872
node8 epoch3:node_model train_loss:0.05919620260182354,train_acc:0.9822221398353577
node8 epoch4:node_model train_loss:0.042846609890047044,train_acc:0.9922221302986145
node8_model on test-dataset: loss:0.7655666235089302,acc:0.793199896812439
node8 weight score:2348.5872356333552
node11: train data size:1682
node11 epoch0:node_model train_loss:0.2109474127783495,train_acc:0.9164849519729614
node11 epoch1:node_model train_loss:0.13887099132818334,train_acc:0.9513772130012512
node11 epoch2:node_model train_loss:0.07531851793036741,train_acc:0.9805881381034851
node11 epoch3:node_model train_loss:0.04742880461408811,train_acc:0.9904590249061584
node11 epoch4:node_model train_loss:0.03685697255765691,train_acc:0.992223858833313
node11_model on test-dataset: loss:0.8129140143096447,acc:0.7925999164581299
node11 weight score:2069.099523924943
node12: train data size:1336
node12 epoch0:node_model train_loss:0.632064551115036,train_acc:0.8295238018035889
node12 epoch1:node_model train_loss:0.3237685976283891,train_acc:0.8938889503479004
node12 epoch2:node_model train_loss:0.19261152669787407,train_acc:0.9299206733703613
node12 epoch3:node_model train_loss:0.1242758711533887,train_acc:0.9637300968170166
node12 epoch4:node_model train_loss:0.07586086089057582,train_acc:0.9814286231994629
node12_model on test-dataset: loss:0.8031833925843239,acc:0.79069983959198
node12 weight score:1663.38100654856
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6461041305959224,acc:0.8255999839305878
total cost energy:6.225516480713451 | all_enery_cp：4.8870000000000005 | all_enery_tp: 1.3385164807134504
ef: 25.056325785780903
reward: 18.83080930506745
step 474:loss:2.940617561340332|running q:70.44059753417969
episode7,iteration54 selected nodes:[2, 11, 7, 6, 12],center node:7
################################################## episode7,iteration54 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2545372002447645,train_acc:0.9121875166893005
node2 epoch1:node_model train_loss:0.12392564335217078,train_acc:0.9578880667686462
node2 epoch2:node_model train_loss:0.06950335033858816,train_acc:0.9803597331047058
node2 epoch3:node_model train_loss:0.04297688329825178,train_acc:0.991165041923523
node2 epoch4:node_model train_loss:0.03724545116225878,train_acc:0.9928884506225586
node2_model on test-dataset: loss:0.7673943039774894,acc:0.8017998933792114
node2 weight score:6239.295724744459
node6: train data size:3007
node6 epoch0:node_model train_loss:0.13517770471592103,train_acc:0.9538710117340088
node6 epoch1:node_model train_loss:0.09161967015074145,train_acc:0.9725803732872009
node6 epoch2:node_model train_loss:0.07198714278638363,train_acc:0.9802302122116089
node6 epoch3:node_model train_loss:0.10789106497841497,train_acc:0.965806245803833
node6 epoch4:node_model train_loss:0.06632113414666345,train_acc:0.98096764087677
node6_model on test-dataset: loss:0.964239316135645,acc:0.765500009059906
node6 weight score:3118.520422970378
node7: train data size:1951
node7 epoch0:node_model train_loss:0.28383602276444436,train_acc:0.9025980234146118
node7 epoch1:node_model train_loss:0.13793226927518845,train_acc:0.9555391669273376
node7 epoch2:node_model train_loss:0.0971008287742734,train_acc:0.970019519329071
node7 epoch3:node_model train_loss:0.061686781514436004,train_acc:0.9835195541381836
node7 epoch4:node_model train_loss:0.03972497032955289,train_acc:0.9895391464233398
node7_model on test-dataset: loss:0.7944525422155857,acc:0.7962998747825623
node7 weight score:2455.779164050518
node11: train data size:1682
node11 epoch0:node_model train_loss:0.19507846543017557,train_acc:0.9275895953178406
node11 epoch1:node_model train_loss:0.09525933007107061,train_acc:0.9679768681526184
node11 epoch2:node_model train_loss:0.07054582194370382,train_acc:0.9809898138046265
node11 epoch3:node_model train_loss:0.0445341962553999,train_acc:0.9911763668060303
node11 epoch4:node_model train_loss:0.026315662955098292,train_acc:0.9958822727203369
node11_model on test-dataset: loss:0.7865301392972469,acc:0.7988999485969543
node11 weight score:2138.5067347868476
node12: train data size:1336
node12 epoch0:node_model train_loss:0.47582692333630155,train_acc:0.8457936644554138
node12 epoch1:node_model train_loss:0.21391755289265088,train_acc:0.921746015548706
node12 epoch2:node_model train_loss:0.14587853050657681,train_acc:0.9501587152481079
node12 epoch3:node_model train_loss:0.08671279570886067,train_acc:0.9765872955322266
node12 epoch4:node_model train_loss:0.05761472960667951,train_acc:0.9857141971588135
node12_model on test-dataset: loss:0.8539252182841301,acc:0.7790999412536621
node12 weight score:1564.5398114422087
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6539994910359382,acc:0.8250999802350998
total cost energy:7.732827043275217 | all_enery_cp：6.382000000000001 | all_enery_tp: 1.3508270432752165
ef: 25.09931461741448
reward: 17.366487574139263
step 475:loss:7.57485294342041|running q:71.43844604492188
episode7,iteration55 selected nodes:[3, 8, 14, 12, 1],center node:3
################################################## episode7,iteration55 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.27519430329694466,train_acc:0.905147135257721
node1 epoch1:node_model train_loss:0.14424750329378774,train_acc:0.951029360294342
node1 epoch2:node_model train_loss:0.10794828843106241,train_acc:0.9639706611633301
node1 epoch3:node_model train_loss:0.09777476786471465,train_acc:0.9726471900939941
node1 epoch4:node_model train_loss:0.06766987298889195,train_acc:0.9802941679954529
node1_model on test-dataset: loss:0.7991589890420436,acc:0.7950000762939453
node1 weight score:8393.824122582813
node3: train data size:4247
node3 epoch0:node_model train_loss:0.4457321506600047,train_acc:0.8543839454650879
node3 epoch1:node_model train_loss:0.23869799926530483,train_acc:0.9156654477119446
node3 epoch2:node_model train_loss:0.14148992429985557,train_acc:0.954156219959259
node3 epoch3:node_model train_loss:0.1060164058364408,train_acc:0.9702028036117554
node3 epoch4:node_model train_loss:0.08827273547649384,train_acc:0.9737207889556885
node3_model on test-dataset: loss:0.7789639058709145,acc:0.7968997955322266
node3 weight score:5452.113978569617
node8: train data size:1798
node8 epoch0:node_model train_loss:0.29909462481737137,train_acc:0.9010429382324219
node8 epoch1:node_model train_loss:0.15097366852892768,train_acc:0.9504988193511963
node8 epoch2:node_model train_loss:0.08572621374494499,train_acc:0.9699772596359253
node8 epoch3:node_model train_loss:0.04756996108012067,train_acc:0.991088330745697
node8 epoch4:node_model train_loss:0.0334009971573121,train_acc:0.9922108054161072
node8_model on test-dataset: loss:0.770514842569828,acc:0.7988001108169556
node8 weight score:2333.5046914908144
node12: train data size:1336
node12 epoch0:node_model train_loss:0.43603766177381786,train_acc:0.8578572273254395
node12 epoch1:node_model train_loss:0.21905567071267537,train_acc:0.9179364442825317
node12 epoch2:node_model train_loss:0.14470280121479714,train_acc:0.9507935643196106
node12 epoch3:node_model train_loss:0.07854722892599446,train_acc:0.9787300825119019
node12 epoch4:node_model train_loss:0.04968844713377101,train_acc:0.9921428561210632
node12_model on test-dataset: loss:0.7981738978624344,acc:0.7949999570846558
node12 weight score:1673.8207094693294
node14: train data size:1172
node14 epoch0:node_model train_loss:0.4707467357317607,train_acc:0.8495833277702332
node14 epoch1:node_model train_loss:0.27059144402543706,train_acc:0.9135648608207703
node14 epoch2:node_model train_loss:0.14679785259068012,train_acc:0.9517128467559814
node14 epoch3:node_model train_loss:0.08576201212902863,train_acc:0.9785184860229492
node14 epoch4:node_model train_loss:0.06856152849892776,train_acc:0.9821758270263672
node14_model on test-dataset: loss:0.8160577176511288,acc:0.7894002199172974
node14 weight score:1436.17292582366
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6443505965173244,acc:0.8258999806642532
total cost energy:9.326789298534637 | all_enery_cp：7.6305 | all_enery_tp: 1.6962892985346365
ef: 25.02384876991102
reward: 15.697059471376383
step 476:loss:4.358888626098633|running q:72.4443130493164
episode7,iteration56 selected nodes:[8, 13, 1, 19, 17],center node:17
################################################## episode7,iteration56 ##################################################
node1: train data size:6708
node1 epoch0:node_model train_loss:0.15780987557681167,train_acc:0.9441176056861877
node1 epoch1:node_model train_loss:0.09257403843323975,train_acc:0.9705146551132202
node1 epoch2:node_model train_loss:0.142507203416351,train_acc:0.9518381953239441
node1 epoch3:node_model train_loss:0.17986810749725385,train_acc:0.9388970136642456
node1 epoch4:node_model train_loss:0.10585376488811829,train_acc:0.9689705967903137
node1_model on test-dataset: loss:0.8235578119754792,acc:0.7880998253822327
node1 weight score:8145.147678108267
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3023543018433783,train_acc:0.8882312774658203
node8 epoch1:node_model train_loss:0.11961308659778701,train_acc:0.9627211093902588
node8 epoch2:node_model train_loss:0.08861784761150678,train_acc:0.9721994400024414
node8 epoch3:node_model train_loss:0.05911844875663519,train_acc:0.982199490070343
node8 epoch4:node_model train_loss:0.03461335847775141,train_acc:0.9955554604530334
node8_model on test-dataset: loss:0.7783131566643715,acc:0.8008999228477478
node8 weight score:2310.124124980382
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5517204081018766,train_acc:0.8415151834487915
node13 epoch1:node_model train_loss:0.2382841557264328,train_acc:0.9196211695671082
node13 epoch2:node_model train_loss:0.16431298976143202,train_acc:0.9537879228591919
node13 epoch3:node_model train_loss:0.1452387614796559,train_acc:0.9594696760177612
node13 epoch4:node_model train_loss:0.06366399209946394,train_acc:0.9811362624168396
node13_model on test-dataset: loss:0.8098875623941422,acc:0.7859999537467957
node13 weight score:1426.1238888342186
node17: train data size:442
node17 epoch0:node_model train_loss:0.6774269700050354,train_acc:0.8314285278320312
node17 epoch1:node_model train_loss:0.36305457949638364,train_acc:0.8934286236763
node17 epoch2:node_model train_loss:0.14521053582429885,train_acc:0.9477142691612244
node17 epoch3:node_model train_loss:0.11118867620825768,train_acc:0.9624761939048767
node17 epoch4:node_model train_loss:0.04623778834939003,train_acc:0.9892380833625793
node17_model on test-dataset: loss:0.8244423353672028,acc:0.7821999192237854
node17 weight score:536.1199698741032
node19: train data size:4281
node19 epoch0:node_model train_loss:0.2706784460433694,train_acc:0.9009733200073242
node19 epoch1:node_model train_loss:0.1182027397633985,train_acc:0.9615188241004944
node19 epoch2:node_model train_loss:0.07386867372795593,train_acc:0.9799597859382629
node19 epoch3:node_model train_loss:0.05236851047118043,train_acc:0.9863479137420654
node19 epoch4:node_model train_loss:0.047527232488920525,train_acc:0.9909301400184631
node19_model on test-dataset: loss:0.7934984700381755,acc:0.7993000149726868
node19 weight score:5395.095468544558
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6632660113275051,acc:0.8255999797582626
total cost energy:9.234146847438732 | all_enery_cp：7.192 | all_enery_tp: 2.042146847438731
ef: 25.0456390876714
reward: 15.811492240232669
step 477:loss:4.991923809051514|running q:73.42744445800781
episode7,iteration57 selected nodes:[16, 11, 6, 4, 2],center node:11
################################################## episode7,iteration57 ##################################################
node2: train data size:4788
node2 epoch0:node_model train_loss:0.2071820960069696,train_acc:0.925719678401947
node2 epoch1:node_model train_loss:0.11617178431091209,train_acc:0.9592044353485107
node2 epoch2:node_model train_loss:0.07239228681040306,train_acc:0.978248119354248
node2 epoch3:node_model train_loss:0.0546526440884918,train_acc:0.9845549464225769
node2 epoch4:node_model train_loss:0.0378649509123837,train_acc:0.990776538848877
node2_model on test-dataset: loss:0.7702758483588695,acc:0.8052000403404236
node2 weight score:6215.954985738153
node4: train data size:2705
node4 epoch0:node_model train_loss:0.4605350222970758,train_acc:0.8589285612106323
node4 epoch1:node_model train_loss:0.29069460768784794,train_acc:0.9039285778999329
node4 epoch2:node_model train_loss:0.1991972949888025,train_acc:0.934999942779541
node4 epoch3:node_model train_loss:0.13705339176314218,train_acc:0.9535712003707886
node4 epoch4:node_model train_loss:0.10107576860381025,train_acc:0.9710713624954224
node4_model on test-dataset: loss:0.8383281622827053,acc:0.7879998683929443
node4 weight score:3226.660061895673
node6: train data size:3007
node6 epoch0:node_model train_loss:0.2802196559946864,train_acc:0.9022579789161682
node6 epoch1:node_model train_loss:0.14635724526259206,train_acc:0.949492871761322
node6 epoch2:node_model train_loss:0.09991572788285633,train_acc:0.9696772694587708
node6 epoch3:node_model train_loss:0.06575576131862979,train_acc:0.9851611256599426
node6 epoch4:node_model train_loss:0.053001712151472605,train_acc:0.9858062863349915
node6_model on test-dataset: loss:0.8111605358868837,acc:0.7908002138137817
node6 weight score:3707.0343871109208
node11: train data size:1682
node11 epoch0:node_model train_loss:0.32466822599663453,train_acc:0.8933427929878235
node11 epoch1:node_model train_loss:0.16783968972809174,train_acc:0.9451075196266174
node11 epoch2:node_model train_loss:0.07200616247513715,train_acc:0.9781060218811035
node11 epoch3:node_model train_loss:0.04965525887468282,train_acc:0.9899998903274536
node11 epoch4:node_model train_loss:0.03144391198806903,train_acc:0.9947057366371155
node11_model on test-dataset: loss:0.8131684719026089,acc:0.7975000739097595
node11 weight score:2068.452058974378
node16: train data size:877
node16 epoch0:node_model train_loss:0.5998636219236586,train_acc:0.8407936692237854
node16 epoch1:node_model train_loss:0.2753421150975757,train_acc:0.9142279028892517
node16 epoch2:node_model train_loss:0.16604676677121055,train_acc:0.9438961148262024
node16 epoch3:node_model train_loss:0.10289861096276177,train_acc:0.9675613045692444
node16 epoch4:node_model train_loss:0.06121565029025078,train_acc:0.9826695322990417
node16_model on test-dataset: loss:0.815077873468399,acc:0.7875001430511475
node16 weight score:1075.9708103326423
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6586223564296961,acc:0.8266999787092209
total cost energy:8.052834547203386 | all_enery_cp：6.5295000000000005 | all_enery_tp: 1.5233345472033857
ef: 25.0997774899888
reward: 17.046942942785414
step 478:loss:4.846377372741699|running q:74.4195785522461
episode7,iteration58 selected nodes:[19, 13, 17, 7, 14],center node:17
################################################## episode7,iteration58 ##################################################
node7: train data size:1951
node7 epoch0:node_model train_loss:0.3822892915457487,train_acc:0.8771175742149353
node7 epoch1:node_model train_loss:0.1902276910841465,train_acc:0.939058780670166
node7 epoch2:node_model train_loss:0.09202533904463053,train_acc:0.9704999923706055
node7 epoch3:node_model train_loss:0.0643884927034378,train_acc:0.9829999208450317
node7 epoch4:node_model train_loss:0.040731845423579216,train_acc:0.9915000200271606
node7_model on test-dataset: loss:0.7839979074895382,acc:0.7993000149726868
node7 weight score:2488.527050087866
node13: train data size:1155
node13 epoch0:node_model train_loss:0.5446051905552546,train_acc:0.8328030109405518
node13 epoch1:node_model train_loss:0.26178682533403236,train_acc:0.9156060218811035
node13 epoch2:node_model train_loss:0.18172609681884447,train_acc:0.948939323425293
node13 epoch3:node_model train_loss:0.10324759470919768,train_acc:0.9759847521781921
node13 epoch4:node_model train_loss:0.05294375059505304,train_acc:0.9891666769981384
node13_model on test-dataset: loss:0.8321245740354061,acc:0.7877999544143677
node13 weight score:1388.01332882023
node14: train data size:1172
node14 epoch0:node_model train_loss:0.5066485777497292,train_acc:0.8504166603088379
node14 epoch1:node_model train_loss:0.22667508323987326,train_acc:0.9298610687255859
node14 epoch2:node_model train_loss:0.1452360867212216,train_acc:0.9540277719497681
node14 epoch3:node_model train_loss:0.09739148120085399,train_acc:0.9710184931755066
node14 epoch4:node_model train_loss:0.05415568687021732,train_acc:0.9880091547966003
node14_model on test-dataset: loss:0.8251773785054684,acc:0.788300096988678
node14 weight score:1420.300689922795
node17: train data size:442
node17 epoch0:node_model train_loss:0.6621293008327485,train_acc:0.815142810344696
node17 epoch1:node_model train_loss:0.33014014959335325,train_acc:0.9061905145645142
node17 epoch2:node_model train_loss:0.14505531191825866,train_acc:0.9564762115478516
node17 epoch3:node_model train_loss:0.14216511249542235,train_acc:0.974476158618927
node17 epoch4:node_model train_loss:0.04902148488909006,train_acc:0.9939999580383301
node17_model on test-dataset: loss:0.9106630688160657,acc:0.7664002180099487
node17 weight score:485.3606291233871
node19: train data size:4281
node19 epoch0:node_model train_loss:0.22002698429102122,train_acc:0.9234481453895569
node19 epoch1:node_model train_loss:0.1090000174766363,train_acc:0.9657736420631409
node19 epoch2:node_model train_loss:0.0697767605008774,train_acc:0.9805339574813843
node19 epoch3:node_model train_loss:0.05191641335570535,train_acc:0.9862245917320251
node19 epoch4:node_model train_loss:0.03401706304920967,train_acc:0.9929689168930054
node19_model on test-dataset: loss:0.8022589753568172,acc:0.7972998023033142
node19 weight score:5336.182120113967
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6761257989704609,acc:0.8210999816656113
total cost energy:5.906021118484448 | all_enery_cp：4.500500000000001 | all_enery_tp: 1.4055211184844474
ef: 24.93074732685517
reward: 19.024726208370723
step 479:loss:4.318576335906982|running q:75.3704833984375
episode7,iteration59 selected nodes:[4, 8, 6, 7, 17],center node:7
################################################## episode7,iteration59 ##################################################
node4: train data size:2705
node4 epoch0:node_model train_loss:0.37604606045143946,train_acc:0.8742857575416565
node4 epoch1:node_model train_loss:0.18577225942031614,train_acc:0.9353572130203247
node4 epoch2:node_model train_loss:0.114717500018222,train_acc:0.9682142734527588
node4 epoch3:node_model train_loss:0.0711104823941631,train_acc:0.9807140827178955
node4 epoch4:node_model train_loss:0.05498603265732527,train_acc:0.9817855954170227
node4_model on test-dataset: loss:0.8188628596067429,acc:0.7974996566772461
node4 weight score:3303.3614460168224
node6: train data size:3007
node6 epoch0:node_model train_loss:0.2274575642039699,train_acc:0.9216129779815674
node6 epoch1:node_model train_loss:0.1216063809490973,train_acc:0.9612900614738464
node6 epoch2:node_model train_loss:0.07925705563637518,train_acc:0.9777416586875916
node6 epoch3:node_model train_loss:0.05876787655776547,train_acc:0.9854836463928223
node6 epoch4:node_model train_loss:0.07192084227778739,train_acc:0.9799998998641968
node6_model on test-dataset: loss:0.989635250940919,acc:0.7673000693321228
node6 weight score:3038.4932197403277
node7: train data size:1951
node7 epoch0:node_model train_loss:0.21674152798950672,train_acc:0.9225000739097595
node7 epoch1:node_model train_loss:0.12549175843596458,train_acc:0.9545391201972961
node7 epoch2:node_model train_loss:0.09030961953103542,train_acc:0.9715391397476196
node7 epoch3:node_model train_loss:0.05469743544235826,train_acc:0.9834998250007629
node7 epoch4:node_model train_loss:0.03312588892877102,train_acc:0.9910195469856262
node7_model on test-dataset: loss:0.8522731675207615,acc:0.7886999845504761
node7 weight score:2289.172150843847
node8: train data size:1798
node8 epoch0:node_model train_loss:0.3608666939867867,train_acc:0.8792970180511475
node8 epoch1:node_model train_loss:0.18804718843764728,train_acc:0.9405214786529541
node8 epoch2:node_model train_loss:0.10664632109304269,train_acc:0.9632539749145508
node8 epoch3:node_model train_loss:0.07254978052030008,train_acc:0.9783104658126831
node8 epoch4:node_model train_loss:0.04354767014996873,train_acc:0.9911110401153564
node8_model on test-dataset: loss:0.8361894771456718,acc:0.7899999022483826
node8 weight score:2150.2303594365517
node17: train data size:442
node17 epoch0:node_model train_loss:0.5801136046648026,train_acc:0.8344761729240417
node17 epoch1:node_model train_loss:0.3025361210107803,train_acc:0.9137142300605774
node17 epoch2:node_model train_loss:0.1319014310836792,train_acc:0.9604762196540833
node17 epoch3:node_model train_loss:0.08908619731664658,train_acc:0.9712380766868591
node17 epoch4:node_model train_loss:0.06871766597032547,train_acc:0.9804762005805969
node17_model on test-dataset: loss:0.9027404484152793,acc:0.7738998532295227
node17 weight score:489.6202455267307
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.6788647259026765,acc:0.8230999827384948
total cost energy:6.406244246730289 | all_enery_cp：4.9515 | all_enery_tp: 1.4547442467302885
ef: 24.6875987576065
reward: 18.281354510876213
step 480:loss:5.929722309112549|running q:76.33724975585938
episode7_cost time: 27365.3075633049

Process finished with exit code 0

