D:\python\python.exe "D:/pengyubo/pythonProj/SBM-master/experiment_2 - 2/centralization_train.py"
train data size:50000
test data size:10000
epoch0:model train_loss:1.279819669365883,train_acc:0.5471999645233154
epoch1:model train_loss:0.8965543287992478,train_acc:0.6907205581665039
epoch2:model train_loss:0.7805304349660873,train_acc:0.7314799427986145
epoch3:model train_loss:0.6963445411324501,train_acc:0.759579598903656
epoch4:model train_loss:0.6325325666666031,train_acc:0.77979975938797
model on test-dataset: loss:0.7052479320764542,acc:0.7548999840021133
epoch5:model train_loss:0.5800275185108185,train_acc:0.7989991903305054
epoch6:model train_loss:0.5456277032494545,train_acc:0.8122798204421997
epoch7:model train_loss:0.5112275736927986,train_acc:0.8227990865707397
epoch8:model train_loss:0.4829456174373627,train_acc:0.8331198692321777
epoch9:model train_loss:0.45347334146499635,train_acc:0.8438000082969666
model on test-dataset: loss:0.6118918503820896,acc:0.7969999814033508
epoch10:model train_loss:0.4275987165272236,train_acc:0.8516995906829834
epoch11:model train_loss:0.40359562104940416,train_acc:0.8609198927879333
epoch12:model train_loss:0.38717855349183083,train_acc:0.8655200004577637
epoch13:model train_loss:0.36222966355085373,train_acc:0.8747397661209106
epoch14:model train_loss:0.3435561832636595,train_acc:0.8801994919776917
model on test-dataset: loss:0.6338530526682734,acc:0.7976999825239182
epoch15:model train_loss:0.3268055357336998,train_acc:0.886880099773407
epoch16:model train_loss:0.307161130875349,train_acc:0.8935202956199646
epoch17:model train_loss:0.2905485547482967,train_acc:0.8983009457588196
epoch18:model train_loss:0.27611280995607373,train_acc:0.9044809341430664
epoch19:model train_loss:0.2675489364117384,train_acc:0.9070006608963013
model on test-dataset: loss:0.5976968511939049,acc:0.8142999762296677
epoch20:model train_loss:0.25325483520329,train_acc:0.91222083568573
epoch21:model train_loss:0.23589509423077107,train_acc:0.9182809591293335
epoch22:model train_loss:0.23003886142373084,train_acc:0.9189810752868652
epoch23:model train_loss:0.21726632250845432,train_acc:0.9239410161972046
epoch24:model train_loss:0.20540629683434963,train_acc:0.9279807806015015
model on test-dataset: loss:0.6048545286059379,acc:0.8245999795198441
epoch25:model train_loss:0.19418482440710066,train_acc:0.9326610565185547
epoch26:model train_loss:0.1890060315653682,train_acc:0.9353213906288147
epoch27:model train_loss:0.1788283474817872,train_acc:0.937921404838562
epoch28:model train_loss:0.16991680812090634,train_acc:0.9406211972236633
epoch29:model train_loss:0.16469183444976807,train_acc:0.9423609375953674
model on test-dataset: loss:0.6379082036018372,acc:0.8234999829530716
epoch30:model train_loss:0.15498934149742127,train_acc:0.9462013840675354
epoch31:model train_loss:0.14855702067911625,train_acc:0.9491415619850159
epoch32:model train_loss:0.14549148152023553,train_acc:0.9498012065887451
epoch33:model train_loss:0.13661124431341887,train_acc:0.9518810510635376
epoch34:model train_loss:0.13587882172316312,train_acc:0.9526211619377136
model on test-dataset: loss:0.6876805327087641,acc:0.8160999751091004
epoch35:model train_loss:0.1274872333481908,train_acc:0.9559614658355713
epoch36:model train_loss:0.12110251914709806,train_acc:0.958101212978363
epoch37:model train_loss:0.11930352048575878,train_acc:0.9577005505561829
epoch38:model train_loss:0.11185669567063451,train_acc:0.9607412219047546
epoch39:model train_loss:0.11350079383701087,train_acc:0.9597814679145813
model on test-dataset: loss:0.70345220297575,acc:0.830299978852272
epoch40:model train_loss:0.10892071742191911,train_acc:0.9614807367324829
epoch41:model train_loss:0.10059623881801963,train_acc:0.964881181716919
epoch42:model train_loss:0.09996681584790348,train_acc:0.9654213190078735
epoch43:model train_loss:0.09696130244806409,train_acc:0.9652809500694275
epoch44:model train_loss:0.09770773215964436,train_acc:0.9655412435531616
model on test-dataset: loss:0.7101725336909294,acc:0.8233999818563461
epoch45:model train_loss:0.0886716221421957,train_acc:0.9685609936714172
epoch46:model train_loss:0.08855478094145655,train_acc:0.9694606065750122
epoch47:model train_loss:0.08642723854072391,train_acc:0.9701206088066101
epoch48:model train_loss:0.08809017625823617,train_acc:0.9692409634590149
epoch49:model train_loss:0.08196544850990176,train_acc:0.9717410802841187
model on test-dataset: loss:0.8084517835080623,acc:0.8171999806165695
epoch50:model train_loss:0.07587258407101036,train_acc:0.973840594291687
epoch51:model train_loss:0.08225406866334378,train_acc:0.9709213376045227
epoch52:model train_loss:0.07556421122513711,train_acc:0.9739012122154236
epoch53:model train_loss:0.07523600638844073,train_acc:0.9737212061882019
epoch54:model train_loss:0.07052459428459405,train_acc:0.9756206274032593
D:/pengyubo/pythonProj/SBM-master/experiment_2 - 2/centralization_train.py:140: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
model on test-dataset: loss:0.8248635382205248,acc:0.8213999807834625
epoch55:model train_loss:0.06853035009279847,train_acc:0.9760605096817017
epoch56:model train_loss:0.07267976354248822,train_acc:0.9744409322738647
epoch57:model train_loss:0.07161332346871495,train_acc:0.9747805595397949
epoch58:model train_loss:0.06335266791377217,train_acc:0.9782007336616516
epoch59:model train_loss:0.07486158647947014,train_acc:0.9738205671310425
model on test-dataset: loss:0.8794042340666056,acc:0.8270999789237976
epoch60:model train_loss:0.06256346868164837,train_acc:0.9780203700065613
epoch61:model train_loss:0.060303110490553084,train_acc:0.9790409803390503
epoch62:model train_loss:0.05885408047400415,train_acc:0.9795409440994263
epoch63:model train_loss:0.05877168712206185,train_acc:0.9794010519981384
epoch64:model train_loss:0.06543455339036881,train_acc:0.9775007963180542
model on test-dataset: loss:0.9210654997080564,acc:0.82299997985363
epoch65:model train_loss:0.06580882751010358,train_acc:0.9770002961158752
epoch66:model train_loss:0.05440330586675555,train_acc:0.9807803630828857
epoch67:model train_loss:0.05446749604865909,train_acc:0.9807608723640442
epoch68:model train_loss:0.06006515572965145,train_acc:0.9787609577178955
epoch69:model train_loss:0.053902532208710906,train_acc:0.9815609455108643
model on test-dataset: loss:0.9489747481048108,acc:0.8170999813079834
epoch70:model train_loss:0.052925079062581065,train_acc:0.9809000492095947
epoch71:model train_loss:0.05184304179716855,train_acc:0.9819003343582153
epoch72:model train_loss:0.05759439404401928,train_acc:0.9798005223274231
epoch73:model train_loss:0.0526363025996834,train_acc:0.981060802936554
epoch74:model train_loss:0.04937159307766706,train_acc:0.9825005531311035
model on test-dataset: loss:0.9629203340411187,acc:0.8230999791622162
epoch75:model train_loss:0.055573651316575705,train_acc:0.9799404144287109
epoch76:model train_loss:0.04950689286272973,train_acc:0.9825803637504578
epoch77:model train_loss:0.04833703178353608,train_acc:0.9832204580307007
epoch78:model train_loss:0.04568834317848086,train_acc:0.9838201999664307
epoch79:model train_loss:0.050412352584302426,train_acc:0.983100414276123
model on test-dataset: loss:1.0163597632944583,acc:0.8205999821424484
epoch80:model train_loss:0.04639004073292017,train_acc:0.9836804866790771
epoch81:model train_loss:0.04594553411751986,train_acc:0.9837406277656555
epoch82:model train_loss:0.048965286895632745,train_acc:0.9827201962471008
epoch83:model train_loss:0.046631333847297354,train_acc:0.9832403063774109
epoch84:model train_loss:0.04179747866140678,train_acc:0.9851396083831787
model on test-dataset: loss:1.0108531445264817,acc:0.8204999828338623
epoch85:model train_loss:0.038806764985434714,train_acc:0.9868201613426208
epoch86:model train_loss:0.048007248932495715,train_acc:0.9835602045059204
epoch87:model train_loss:0.0471789866797626,train_acc:0.9833407998085022
epoch88:model train_loss:0.04502185516059399,train_acc:0.983880341053009
epoch89:model train_loss:0.03974812561320141,train_acc:0.9862603545188904
model on test-dataset: loss:1.0180096977949142,acc:0.8207999777793884
epoch90:model train_loss:0.04280069719674066,train_acc:0.9851599335670471
epoch91:model train_loss:0.0443651339286007,train_acc:0.9848398566246033
epoch92:model train_loss:0.040729400431737305,train_acc:0.985420286655426
epoch93:model train_loss:0.035860253828577694,train_acc:0.9876800775527954
epoch94:model train_loss:0.03800994280399755,train_acc:0.9864395260810852
model on test-dataset: loss:1.0373881912231446,acc:0.8218999797105789
epoch95:model train_loss:0.036996078970609234,train_acc:0.9869199395179749
epoch96:model train_loss:0.047208587989211084,train_acc:0.983160674571991
epoch97:model train_loss:0.04034652822930366,train_acc:0.9863002896308899
epoch98:model train_loss:0.04047889527864754,train_acc:0.9863398671150208
epoch99:model train_loss:0.037785088269039986,train_acc:0.9869400858879089
model on test-dataset: loss:1.0627889882028103,acc:0.8189999806880951
epoch100:model train_loss:0.03704236620990559,train_acc:0.9871400594711304
epoch101:model train_loss:0.03500788079341874,train_acc:0.9877197742462158
epoch102:model train_loss:0.035518306432757524,train_acc:0.9877601265907288
epoch103:model train_loss:0.03964674668433145,train_acc:0.9864199161529541
epoch104:model train_loss:0.03850363117875531,train_acc:0.986740231513977
model on test-dataset: loss:1.17328759688884,acc:0.819899982213974
epoch105:model train_loss:0.03827367372345179,train_acc:0.986880362033844
epoch106:model train_loss:0.03606040590535849,train_acc:0.9871395230293274
epoch107:model train_loss:0.0344209423805587,train_acc:0.9879403114318848
epoch108:model train_loss:0.035765426194760946,train_acc:0.9879399538040161
epoch109:model train_loss:0.03633254705928266,train_acc:0.987220048904419
model on test-dataset: loss:1.0933024773001672,acc:0.8313999807834626
epoch110:model train_loss:0.03257857014471665,train_acc:0.98826003074646
epoch111:model train_loss:0.03479283701628447,train_acc:0.9877400398254395
epoch112:model train_loss:0.03872177260415628,train_acc:0.9869998097419739
epoch113:model train_loss:0.03064493760303594,train_acc:0.9894194602966309
epoch114:model train_loss:0.02726990280346945,train_acc:0.9906795620918274
model on test-dataset: loss:1.177737448513508,acc:0.8169999814033508
epoch115:model train_loss:0.04099678469914943,train_acc:0.9862205982208252
epoch116:model train_loss:0.03212160889292136,train_acc:0.9890796542167664
epoch117:model train_loss:0.03060348348785192,train_acc:0.9898399114608765
epoch118:model train_loss:0.03835630607744679,train_acc:0.9866601824760437
epoch119:model train_loss:0.036929500620346514,train_acc:0.9865996241569519
model on test-dataset: loss:1.12173588514328,acc:0.8262999802827835
epoch120:model train_loss:0.026679114167578517,train_acc:0.9911395311355591
epoch121:model train_loss:0.030557065036147833,train_acc:0.989020049571991
epoch122:model train_loss:0.032082898537861185,train_acc:0.9889395236968994
epoch123:model train_loss:0.03402476434339769,train_acc:0.9882596731185913
epoch124:model train_loss:0.02957178271666635,train_acc:0.990159809589386
model on test-dataset: loss:1.1376947937905788,acc:0.8214999800920486
epoch125:model train_loss:0.0323357977650594,train_acc:0.9889997243881226
epoch126:model train_loss:0.03158251556917094,train_acc:0.9891200065612793
epoch127:model train_loss:0.02958768321480602,train_acc:0.990039587020874
epoch128:model train_loss:0.028034977820701898,train_acc:0.9907394051551819
epoch129:model train_loss:0.029475944318575784,train_acc:0.9894999265670776
model on test-dataset: loss:1.1469228295981884,acc:0.8310999810695648
epoch130:model train_loss:0.028302760033402593,train_acc:0.9900798797607422
epoch131:model train_loss:0.03275845750258304,train_acc:0.9886998534202576
epoch132:model train_loss:0.026629613448865712,train_acc:0.9905598759651184
epoch133:model train_loss:0.03029435780539643,train_acc:0.9897996783256531
epoch134:model train_loss:0.035309294495382344,train_acc:0.9879199266433716
model on test-dataset: loss:1.1588038766384126,acc:0.824699980020523
epoch135:model train_loss:0.03043585880543105,train_acc:0.9893798828125
epoch136:model train_loss:0.028193454551510514,train_acc:0.9905794262886047
epoch137:model train_loss:0.02557909441634547,train_acc:0.9910995960235596
epoch138:model train_loss:0.029387690914561972,train_acc:0.9898397326469421
epoch139:model train_loss:0.03036463888688013,train_acc:0.9895991683006287
model on test-dataset: loss:1.157741441167891,acc:0.8279999822378159
epoch140:model train_loss:0.03257021790044382,train_acc:0.988580048084259
epoch141:model train_loss:0.02458826130500529,train_acc:0.9913997054100037
epoch142:model train_loss:0.0260311377402395,train_acc:0.9906997084617615
epoch143:model train_loss:0.02735386891744565,train_acc:0.9906994104385376
epoch144:model train_loss:0.03040929279779084,train_acc:0.9899795651435852
model on test-dataset: loss:1.2817740939557551,acc:0.8229999822378159
epoch145:model train_loss:0.025530558688682505,train_acc:0.9909196496009827
epoch146:model train_loss:0.02696755136270076,train_acc:0.9908798933029175
epoch147:model train_loss:0.028091852923622356,train_acc:0.9899795651435852
epoch148:model train_loss:0.024311717838281767,train_acc:0.9914795160293579
epoch149:model train_loss:0.02406027724966407,train_acc:0.9917802810668945
model on test-dataset: loss:1.2211328260600567,acc:0.8263999801874161
epoch150:model train_loss:0.028911829779390247,train_acc:0.9896994829177856
epoch151:model train_loss:0.024211127380956897,train_acc:0.9917000532150269
epoch152:model train_loss:0.030846502117812633,train_acc:0.9892400503158569
epoch153:model train_loss:0.027533759872894734,train_acc:0.9906194806098938
epoch154:model train_loss:0.023968383704079315,train_acc:0.9917792081832886
model on test-dataset: loss:1.1808440771698951,acc:0.8309999823570251
epoch155:model train_loss:0.026505242678220384,train_acc:0.9908998012542725
epoch156:model train_loss:0.025516949812532403,train_acc:0.991459846496582
epoch157:model train_loss:0.02736734921345487,train_acc:0.99057936668396
epoch158:model train_loss:0.024121022512670606,train_acc:0.9914794564247131
epoch159:model train_loss:0.02528756016760599,train_acc:0.9913792610168457
model on test-dataset: loss:1.2322540673986078,acc:0.8247999823093415
epoch160:model train_loss:0.02218034423806239,train_acc:0.9920396208763123
epoch161:model train_loss:0.021445016657584345,train_acc:0.9920795559883118
epoch162:model train_loss:0.026407753292151028,train_acc:0.9910596609115601
epoch163:model train_loss:0.02681464408163447,train_acc:0.9908397197723389
epoch164:model train_loss:0.02327867715994944,train_acc:0.991899847984314
model on test-dataset: loss:1.3498809617757797,acc:0.8218999797105789
epoch165:model train_loss:0.022946300977375357,train_acc:0.9921994209289551
epoch166:model train_loss:0.02579857397073647,train_acc:0.9909396171569824
epoch167:model train_loss:0.025793726220377722,train_acc:0.9913197159767151
epoch168:model train_loss:0.02449175031669438,train_acc:0.9916996359825134
epoch169:model train_loss:0.024154321873968,train_acc:0.991860032081604
model on test-dataset: loss:1.2409289510548114,acc:0.8234999817609787
epoch170:model train_loss:0.024034273306489922,train_acc:0.992039680480957
epoch171:model train_loss:0.02255462793807965,train_acc:0.9918199777603149
epoch172:model train_loss:0.017291526359156706,train_acc:0.9942796230316162
epoch173:model train_loss:0.026033986176364123,train_acc:0.9906994104385376
epoch174:model train_loss:0.02178099996771198,train_acc:0.992259681224823
model on test-dataset: loss:1.2899566021002828,acc:0.8250999832153321
epoch175:model train_loss:0.027457818563794718,train_acc:0.9902597665786743
epoch176:model train_loss:0.024523860938264988,train_acc:0.9914196133613586
epoch177:model train_loss:0.018447037699632348,train_acc:0.9937797784805298
epoch178:model train_loss:0.019438548893667758,train_acc:0.993479311466217
epoch179:model train_loss:0.02531852983147837,train_acc:0.9914991855621338
model on test-dataset: loss:1.3129201853275299,acc:0.8276999807357788
epoch180:model train_loss:0.025494434735679534,train_acc:0.9908594489097595
epoch181:model train_loss:0.02317540701816324,train_acc:0.9921196699142456
epoch182:model train_loss:0.025725685619283466,train_acc:0.9907798767089844
epoch183:model train_loss:0.021792504683078733,train_acc:0.9923993945121765
epoch184:model train_loss:0.01857763442059513,train_acc:0.9934595227241516
model on test-dataset: loss:1.2800109373033046,acc:0.8291999816894531
epoch185:model train_loss:0.023604248681745956,train_acc:0.9916196465492249
epoch186:model train_loss:0.027394314611912704,train_acc:0.9907795786857605
epoch187:model train_loss:0.01922837477788562,train_acc:0.993419349193573
epoch188:model train_loss:0.019150557963352185,train_acc:0.9928396344184875
epoch189:model train_loss:0.025640112397493794,train_acc:0.9913992285728455
model on test-dataset: loss:1.3943773908913135,acc:0.8194999808073044
epoch190:model train_loss:0.01885130531026516,train_acc:0.9930592775344849
epoch191:model train_loss:0.022346139620407483,train_acc:0.9925796389579773
epoch192:model train_loss:0.02320083721014089,train_acc:0.9920800924301147
epoch193:model train_loss:0.01833199751836946,train_acc:0.9937597513198853
epoch194:model train_loss:0.020998603312531484,train_acc:0.9926196932792664
model on test-dataset: loss:1.2919757804274559,acc:0.8308999800682068
epoch195:model train_loss:0.017468090313253925,train_acc:0.9938197731971741
epoch196:model train_loss:0.027924707255791874,train_acc:0.990159809589386
epoch197:model train_loss:0.020360069962451235,train_acc:0.9926596283912659
epoch198:model train_loss:0.020043190207041334,train_acc:0.9931795001029968
epoch199:model train_loss:0.019795115012908353,train_acc:0.993239164352417
model on test-dataset: loss:1.3727011889219285,acc:0.8213999819755554
epoch200:model train_loss:0.023048592520761305,train_acc:0.9917594790458679
epoch201:model train_loss:0.022763712901738474,train_acc:0.9923391938209534
epoch202:model train_loss:0.01794290372735122,train_acc:0.9936597347259521
epoch203:model train_loss:0.02266391824343009,train_acc:0.9920796155929565
epoch204:model train_loss:0.018565710276801838,train_acc:0.9939990639686584
model on test-dataset: loss:1.3246302664279939,acc:0.8275999784469604
epoch205:model train_loss:0.02013094824977452,train_acc:0.993239164352417
epoch206:model train_loss:0.022613249766931402,train_acc:0.9925194978713989
epoch207:model train_loss:0.019967247383698122,train_acc:0.9935193657875061
epoch208:model train_loss:0.020463980358152185,train_acc:0.9930593371391296
epoch209:model train_loss:0.018984875053283758,train_acc:0.9934595227241516
model on test-dataset: loss:1.2825672340393066,acc:0.8293999814987183
epoch210:model train_loss:0.022700605357356834,train_acc:0.9923396706581116
epoch211:model train_loss:0.022805818019260186,train_acc:0.9918797612190247
epoch212:model train_loss:0.01718150592292659,train_acc:0.9940391182899475
epoch213:model train_loss:0.01920444016397232,train_acc:0.9932996034622192
epoch214:model train_loss:0.020160393852042033,train_acc:0.9926993250846863
model on test-dataset: loss:1.3309685325622558,acc:0.8288999795913696
epoch215:model train_loss:0.020705890699930025,train_acc:0.9929394125938416
epoch216:model train_loss:0.022288818230787003,train_acc:0.9925394654273987
epoch217:model train_loss:0.02175551878969418,train_acc:0.9926591515541077
epoch218:model train_loss:0.016082565933756995,train_acc:0.9946793913841248
epoch219:model train_loss:0.018936494074238,train_acc:0.9936594367027283
model on test-dataset: loss:1.4385539072751998,acc:0.8200999826192856
epoch220:model train_loss:0.0226425371915102,train_acc:0.9923992156982422
epoch221:model train_loss:0.016827994055114686,train_acc:0.9942394495010376
epoch222:model train_loss:0.02098363670040271,train_acc:0.9924991726875305
epoch223:model train_loss:0.01756460366444662,train_acc:0.9942194819450378
epoch224:model train_loss:0.018922897635842675,train_acc:0.9936191439628601
model on test-dataset: loss:1.3776816534996033,acc:0.8299999791383743
epoch225:model train_loss:0.019408669918382658,train_acc:0.9935396313667297
epoch226:model train_loss:0.017397943332907742,train_acc:0.9939994215965271
epoch227:model train_loss:0.022193226392322685,train_acc:0.9927794933319092
epoch228:model train_loss:0.015205729335953946,train_acc:0.9950993657112122
epoch229:model train_loss:0.01749265978022595,train_acc:0.9938193559646606
model on test-dataset: loss:1.399854263290763,acc:0.8253999811410904
epoch230:model train_loss:0.01956699274876155,train_acc:0.9933996200561523
epoch231:model train_loss:0.020349861437862275,train_acc:0.9927995800971985
epoch232:model train_loss:0.01690139901812654,train_acc:0.9940192103385925
epoch233:model train_loss:0.017213567397004227,train_acc:0.9941396117210388
epoch234:model train_loss:0.024245288033474935,train_acc:0.9919193983078003
model on test-dataset: loss:1.4430105601251126,acc:0.8258999806642532
epoch235:model train_loss:0.017146723953075708,train_acc:0.9940794110298157
epoch236:model train_loss:0.016598514938581502,train_acc:0.9940991401672363
epoch237:model train_loss:0.016034952526402777,train_acc:0.9945593476295471
epoch238:model train_loss:0.015925326670709183,train_acc:0.9948192834854126
epoch239:model train_loss:0.019282938861113506,train_acc:0.9932593703269958
model on test-dataset: loss:1.4484517878293992,acc:0.8226999819278717
epoch240:model train_loss:0.02116909398676944,train_acc:0.9921591281890869
epoch241:model train_loss:0.01776502498681657,train_acc:0.9940594434738159
epoch242:model train_loss:0.018073493895644787,train_acc:0.993779182434082
epoch243:model train_loss:0.016942022720962995,train_acc:0.9939199090003967
epoch244:model train_loss:0.01699161125026876,train_acc:0.9943992495536804
model on test-dataset: loss:1.4849502381682396,acc:0.8208999806642532
epoch245:model train_loss:0.014526436019179528,train_acc:0.9947994351387024
epoch246:model train_loss:0.017865493009041528,train_acc:0.9939992427825928
epoch247:model train_loss:0.020945837220584507,train_acc:0.9928591847419739
epoch248:model train_loss:0.023705450654873856,train_acc:0.9918597936630249
epoch249:model train_loss:0.016698653329513036,train_acc:0.9944391250610352
model on test-dataset: loss:1.4120894990861417,acc:0.8308999782800675
epoch250:model train_loss:0.012997272104898002,train_acc:0.9953590631484985
epoch251:model train_loss:0.020991923472640336,train_acc:0.9926193952560425
epoch252:model train_loss:0.020352057683514432,train_acc:0.9930198192596436
epoch253:model train_loss:0.01408637605709373,train_acc:0.9951793551445007
epoch254:model train_loss:0.021855537248193287,train_acc:0.9924390912055969
model on test-dataset: loss:1.513995802104473,acc:0.8220999819040299
epoch255:model train_loss:0.014432704326958628,train_acc:0.995219349861145
epoch256:model train_loss:0.011452634112676606,train_acc:0.996259331703186
epoch257:model train_loss:0.015340150723292027,train_acc:0.9945794939994812
epoch258:model train_loss:0.021766969394433545,train_acc:0.9920994639396667
epoch259:model train_loss:0.017124021873343737,train_acc:0.994099497795105
model on test-dataset: loss:1.414931313842535,acc:0.8252999812364579
epoch260:model train_loss:0.015395980788598536,train_acc:0.9946591854095459
epoch261:model train_loss:0.020135346081951867,train_acc:0.9932394623756409
epoch262:model train_loss:0.01773888019018341,train_acc:0.9941192269325256
epoch263:model train_loss:0.01428699304172187,train_acc:0.9954391121864319
epoch264:model train_loss:0.015560829127847682,train_acc:0.9950191378593445
model on test-dataset: loss:1.42448566198349,acc:0.830299978852272
epoch265:model train_loss:0.016164884926256493,train_acc:0.9944993257522583
epoch266:model train_loss:0.016819787342567,train_acc:0.9944992065429688
epoch267:model train_loss:0.020448769947281106,train_acc:0.992919385433197
epoch268:model train_loss:0.017999102072208187,train_acc:0.9939590692520142
epoch269:model train_loss:0.018758675487726577,train_acc:0.9935596585273743
model on test-dataset: loss:1.4618185256421565,acc:0.8254999798536301
epoch270:model train_loss:0.014647731487872078,train_acc:0.9948795437812805
epoch271:model train_loss:0.014059529414400458,train_acc:0.9950990676879883
epoch272:model train_loss:0.018382759053245535,train_acc:0.9935795068740845
epoch273:model train_loss:0.01849695087040891,train_acc:0.9936392903327942
epoch274:model train_loss:0.015215377807711775,train_acc:0.9946794509887695
model on test-dataset: loss:1.4649943509697914,acc:0.8302999806404113
epoch275:model train_loss:0.010799378718482331,train_acc:0.996059000492096
epoch276:model train_loss:0.017676465846743667,train_acc:0.9934991598129272
epoch277:model train_loss:0.02087664721926558,train_acc:0.9929797053337097
epoch278:model train_loss:0.014348225409223233,train_acc:0.9950191974639893
epoch279:model train_loss:0.012399821188730129,train_acc:0.9957996606826782
model on test-dataset: loss:1.4902562905848027,acc:0.8273999792337418
epoch280:model train_loss:0.013194279912451748,train_acc:0.9955993294715881
epoch281:model train_loss:0.017858378620774603,train_acc:0.9939194321632385
epoch282:model train_loss:0.019430962021637244,train_acc:0.9933594465255737
epoch283:model train_loss:0.017330351482174593,train_acc:0.9940592050552368
epoch284:model train_loss:0.014097295569430571,train_acc:0.9952593445777893
model on test-dataset: loss:1.4802254217863082,acc:0.8252999824285507
epoch285:model train_loss:0.01349444981401757,train_acc:0.9954994320869446
epoch286:model train_loss:0.01670157161446696,train_acc:0.9941995739936829
epoch287:model train_loss:0.015994385965837864,train_acc:0.9939992427825928
epoch288:model train_loss:0.014043953933040029,train_acc:0.9951590299606323
epoch289:model train_loss:0.014631725135317538,train_acc:0.9949593544006348
model on test-dataset: loss:1.54390958070755,acc:0.8233999800682068
epoch290:model train_loss:0.015063371868032846,train_acc:0.9948595762252808
epoch291:model train_loss:0.018154658253712114,train_acc:0.9936991930007935
epoch292:model train_loss:0.01613607428863179,train_acc:0.9945592284202576
epoch293:model train_loss:0.011793462423942401,train_acc:0.9956191182136536
epoch294:model train_loss:0.016599814628869353,train_acc:0.9941592812538147
model on test-dataset: loss:1.5313292668759824,acc:0.8226999831199646
epoch295:model train_loss:0.016446182288302226,train_acc:0.9944193959236145
epoch296:model train_loss:0.014947968038963155,train_acc:0.9947795867919922
epoch297:model train_loss:0.014311082010899555,train_acc:0.9949796199798584
epoch298:model train_loss:0.018049776861415012,train_acc:0.9937792420387268
epoch299:model train_loss:0.012609075168613345,train_acc:0.9959192276000977
model on test-dataset: loss:1.5429762092232704,acc:0.8288999831676483

