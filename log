D:\python\python.exe D:/pengyubo/pythonProj/SBM-master/experiment_1/RL_train.py
Namespace(avg_alloc=False, b_ratio=0.9, batch_size=100, cp_ratio=0.0005, device='cuda', e_greedy=0.95, e_greedy_increment=0.003, episodes=8, iterations=60, kerset=[18, 30, 60, 45, 65, 50], log_dir='logs', lr=0.005, memory_size=60, node_epochs=2, node_lr=0.001, node_num=20, node_num_workers=4, num_classes=10, pretrained_weight='', random_imgs=True, replace_target_iter=60, reward_decay=0.9, rl_batch_size=60, rl_checkpoints='./checkpoints', rl_weights='checkpoints/rl_episode3.pth', save_dir='checkpoints', select_method='ea', select_node_num=5, test_data_pth='MNIST/test', tp_ratio=0.1, train_data_pth='MNIST/train')
7165
6657
4610
3762
4298
4837
3529
3637
2290
2125
1915
1575
1406
1056
1540
1376
920
719
801
5781
node_position:
 [[0 0 0 1 0 0 1 0 0 0]
 [0 0 0 0 0 0 1 0 0 0]
 [1 0 0 0 0 0 0 0 0 0]
 [1 0 0 0 1 0 1 1 0 0]
 [0 0 0 1 0 0 0 0 0 0]
 [1 0 0 1 0 1 1 0 0 1]
 [0 1 0 0 0 0 0 1 1 0]
 [0 0 0 0 0 1 0 0 1 0]
 [0 0 0 0 0 0 1 0 0 0]
 [0 0 0 0 0 0 0 0 0 0]]
tp_energy matrix:
 [[0.         0.3        0.31622777 0.36055513 0.42426407 0.31622777
  0.42426407 0.5        0.4        0.58309519 0.5        0.53851648
  0.58309519 0.78102497 0.63245553 0.72111026 0.78102497 0.72801099
  0.86023253 0.85440037]
 [0.3        0.         0.1        0.63245553 0.67082039 0.36055513
  0.3        0.31622777 0.5        0.78102497 0.58309519 0.50990195
  0.5        0.58309519 0.78102497 0.60827625 0.63245553 0.70710678
  0.72801099 0.8       ]
 [0.31622777 0.1        0.         0.60827625 0.63245553 0.28284271
  0.2        0.2236068  0.42426407 0.72111026 0.5        0.41231056
  0.4        0.5        0.70710678 0.50990195 0.53851648 0.60827625
  0.63245553 0.7       ]
 [0.36055513 0.63245553 0.60827625 0.         0.1        0.41231056
  0.60827625 0.70710678 0.36055513 0.3        0.42426407 0.58309519
  0.67082039 0.9486833  0.41231056 0.80622577 0.89442719 0.70710678
  0.94339811 0.84852814]
 [0.42426407 0.67082039 0.63245553 0.1        0.         0.4
  0.6        0.7        0.31622777 0.2        0.36055513 0.53851648
  0.63245553 0.92195445 0.31622777 0.76157731 0.85440037 0.64031242
  0.89442719 0.78102497]
 [0.31622777 0.36055513 0.28284271 0.41231056 0.4        0.
  0.2        0.3        0.14142136 0.4472136  0.2236068  0.2236068
  0.28284271 0.53851648 0.42426407 0.42426407 0.5        0.41231056
  0.56568542 0.53851648]
 [0.42426407 0.3        0.2        0.60827625 0.6        0.2
  0.         0.1        0.31622777 0.63245553 0.36055513 0.2236068
  0.2        0.36055513 0.58309519 0.31622777 0.36055513 0.41231056
  0.4472136  0.5       ]
 [0.5        0.31622777 0.2236068  0.70710678 0.7        0.3
  0.1        0.         0.41231056 0.72801099 0.4472136  0.28284271
  0.2236068  0.28284271 0.67082039 0.3        0.31622777 0.4472136
  0.41231056 0.50990195]
 [0.4        0.5        0.42426407 0.36055513 0.31622777 0.14142136
  0.31622777 0.41231056 0.         0.31622777 0.1        0.2236068
  0.31622777 0.60827625 0.28284271 0.4472136  0.53851648 0.36055513
  0.58309519 0.5       ]
 [0.58309519 0.78102497 0.72111026 0.3        0.2        0.4472136
  0.63245553 0.72801099 0.31622777 0.         0.3        0.5
  0.6        0.9        0.14142136 0.70710678 0.80622577 0.53851648
  0.82462113 0.67082039]
 [0.5        0.58309519 0.5        0.42426407 0.36055513 0.2236068
  0.36055513 0.4472136  0.1        0.3        0.         0.2
  0.3        0.6        0.2236068  0.41231056 0.50990195 0.28284271
  0.53851648 0.42426407]
 [0.53851648 0.50990195 0.41231056 0.58309519 0.53851648 0.2236068
  0.2236068  0.28284271 0.2236068  0.5        0.2        0.
  0.1        0.4        0.41231056 0.2236068  0.31622777 0.2
  0.36055513 0.31622777]
 [0.58309519 0.5        0.4        0.67082039 0.63245553 0.28284271
  0.2        0.2236068  0.31622777 0.6        0.3        0.1
  0.         0.3        0.50990195 0.14142136 0.2236068  0.2236068
  0.28284271 0.3       ]
 [0.78102497 0.58309519 0.5        0.9486833  0.92195445 0.53851648
  0.36055513 0.28284271 0.60827625 0.9        0.6        0.4
  0.3        0.         0.80622577 0.2236068  0.14142136 0.4472136
  0.2236068  0.42426407]
 [0.63245553 0.78102497 0.70710678 0.41231056 0.31622777 0.42426407
  0.58309519 0.67082039 0.28284271 0.14142136 0.2236068  0.41231056
  0.50990195 0.80622577 0.         0.6        0.7        0.41231056
  0.70710678 0.53851648]
 [0.72111026 0.60827625 0.50990195 0.80622577 0.76157731 0.42426407
  0.31622777 0.3        0.4472136  0.70710678 0.41231056 0.2236068
  0.14142136 0.2236068  0.6        0.         0.1        0.2236068
  0.14142136 0.2236068 ]
 [0.78102497 0.63245553 0.53851648 0.89442719 0.85440037 0.5
  0.36055513 0.31622777 0.53851648 0.80622577 0.50990195 0.31622777
  0.2236068  0.14142136 0.7        0.1        0.         0.31622777
  0.1        0.28284271]
 [0.72801099 0.70710678 0.60827625 0.70710678 0.64031242 0.41231056
  0.41231056 0.4472136  0.36055513 0.53851648 0.28284271 0.2
  0.2236068  0.4472136  0.41231056 0.2236068  0.31622777 0.
  0.3        0.14142136]
 [0.86023253 0.72801099 0.63245553 0.94339811 0.89442719 0.56568542
  0.4472136  0.41231056 0.58309519 0.82462113 0.53851648 0.36055513
  0.28284271 0.2236068  0.70710678 0.14142136 0.1        0.3
  0.         0.2236068 ]
 [0.85440037 0.8        0.7        0.84852814 0.78102497 0.53851648
  0.5        0.50990195 0.5        0.67082039 0.42426407 0.31622777
  0.3        0.42426407 0.53851648 0.2236068  0.28284271 0.14142136
  0.2236068  0.        ]]
episode0,iteration0 selected nodes:[17, 1, 15, 11, 12],center node:12
################################################## episode0,iteration0 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.6863815264025731,train_acc:0.7933542132377625
node1 epoch1:node_model train_loss:0.2384045588881222,train_acc:0.9246214032173157
node1_model on test-dataset: loss:0.7108423430565745,acc:0.7954766154289246
node1 weight score:9364.945778800045
node11: train data size:1575
node11 epoch0:node_model train_loss:1.526543501764536,train_acc:0.544166624546051
node11 epoch1:node_model train_loss:0.5369852837175131,train_acc:0.8379166722297668
node11_model on test-dataset: loss:0.5272987718880177,acc:0.8235869407653809
node11 weight score:2986.921426652749
node12: train data size:1406
node12 epoch0:node_model train_loss:1.5000240802764893,train_acc:0.5355555415153503
node12 epoch1:node_model train_loss:0.5515629867712657,train_acc:0.8295556306838989
node12_model on test-dataset: loss:1.137191028795205,acc:0.6099342107772827
node12 weight score:1236.3797852763437
node15: train data size:1376
node15 epoch0:node_model train_loss:1.742349054132189,train_acc:0.45879700779914856
node15 epoch1:node_model train_loss:0.6830914318561554,train_acc:0.7921804785728455
node15_model on test-dataset: loss:0.7345977857150138,acc:0.751584529876709
node15 weight score:1873.1338791889816
node17: train data size:719
node17 epoch0:node_model train_loss:2.0950012803077698,train_acc:0.3314473628997803
node17 epoch1:node_model train_loss:1.090608887374401,train_acc:0.6674342155456543
node17_model on test-dataset: loss:1.216699969060719,acc:0.5929535627365112
node17 weight score:590.942728925243
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.45548022373579444,acc:0.8692888674139977
total cost energy:6.831528153987289 | all_enery_cp：5.8665 | all_enery_tp: 0.9650281539872885
ef: 25.322796007984692
reward: 18.491267853997403
episode0,iteration1 selected nodes:[4, 14, 7, 6, 16],center node:6
################################################## episode0,iteration1 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.33396581854931146,train_acc:0.8918318748474121
node4 epoch1:node_model train_loss:0.1767934080819751,train_acc:0.9453343152999878
node4_model on test-dataset: loss:0.20068407541606575,acc:0.932898998260498
node4 weight score:21416.746650621008
node6: train data size:3529
node6 epoch0:node_model train_loss:0.30300915572378373,train_acc:0.9097509384155273
node6 epoch1:node_model train_loss:0.18733804745392668,train_acc:0.9372797012329102
node6_model on test-dataset: loss:0.17112684930674732,acc:0.9436970949172974
node6 weight score:20622.129223417287
node7: train data size:3637
node7 epoch0:node_model train_loss:0.3353077157123669,train_acc:0.8935647010803223
node7 epoch1:node_model train_loss:0.19263000282886866,train_acc:0.9390503168106079
node7_model on test-dataset: loss:0.15352307038847357,acc:0.9527959823608398
node7 weight score:23690.250532359496
node14: train data size:1540
node14 epoch0:node_model train_loss:0.46361428778618574,train_acc:0.8475000858306885
node14 epoch1:node_model train_loss:0.2792111523449421,train_acc:0.9074999094009399
node14_model on test-dataset: loss:0.39162435311242005,acc:0.8693981170654297
node14 weight score:3932.3397223918964
node16: train data size:920
node16 epoch0:node_model train_loss:0.5019078001379966,train_acc:0.8320000767707825
node16 epoch1:node_model train_loss:0.28369498550891875,train_acc:0.9169999957084656
node16_model on test-dataset: loss:0.2854004259593785,acc:0.905599057674408
node16 weight score:3223.541089356836
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.14187855970114469,acc:0.9577979558706283
total cost energy:8.605650317030928 | all_enery_cp：6.962 | all_enery_tp: 1.6436503170309291
ef: 28.75592023844726
reward: 20.15026992141633
episode0,iteration2 selected nodes:[5, 17, 19, 15, 12],center node:12
################################################## episode0,iteration2 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.24064842535524952,train_acc:0.9233866333961487
node5 epoch1:node_model train_loss:0.14235280940727313,train_acc:0.9542854428291321
node5_model on test-dataset: loss:0.13791266316664405,acc:0.9555928707122803
node5 weight score:35072.92143401876
node12: train data size:1406
node12 epoch0:node_model train_loss:0.26109866549571353,train_acc:0.9099999666213989
node12 epoch1:node_model train_loss:0.18801580518484115,train_acc:0.9335556030273438
node12_model on test-dataset: loss:0.15390310418326408,acc:0.9496901631355286
node12 weight score:9135.618202513766
node15: train data size:1376
node15 epoch0:node_model train_loss:0.22587011488420622,train_acc:0.9355639219284058
node15 epoch1:node_model train_loss:0.13150173212800706,train_acc:0.962180495262146
node15_model on test-dataset: loss:0.17231425320729613,acc:0.9454969763755798
node15 weight score:7985.4102280480265
node17: train data size:719
node17 epoch0:node_model train_loss:0.343939945101738,train_acc:0.8930920362472534
node17 epoch1:node_model train_loss:0.21339154057204723,train_acc:0.9350000023841858
node17_model on test-dataset: loss:0.2834879169985652,acc:0.9089936017990112
node17 weight score:2536.263300434209
node19: train data size:5781
node19 epoch0:node_model train_loss:0.22081501948936233,train_acc:0.9303959012031555
node19 epoch1:node_model train_loss:0.14976234618445922,train_acc:0.9534992575645447
node19_model on test-dataset: loss:0.15038602281361818,acc:0.9549917578697205
node19 weight score:38441.07246033574
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.11851421530824155,acc:0.9612969446182251
total cost energy:8.007370866461908 | all_enery_cp：7.0595 | all_enery_tp: 0.9478708664619075
ef: 29.62130221780199
reward: 21.61393135134008
episode0,iteration3 selected nodes:[11, 6, 15, 14, 3],center node:11
################################################## episode0,iteration3 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.21347962202210174,train_acc:0.9291340708732605
node3 epoch1:node_model train_loss:0.12800933038325688,train_acc:0.959575355052948
node3_model on test-dataset: loss:0.1523335124924779,acc:0.9505950212478638
node3 weight score:24695.813406034107
node6: train data size:3529
node6 epoch0:node_model train_loss:0.17695424064166015,train_acc:0.9451532363891602
node6 epoch1:node_model train_loss:0.10607025534328487,train_acc:0.9644442796707153
node6_model on test-dataset: loss:0.14679228079970927,acc:0.9512948393821716
node6 weight score:24040.773675389268
node11: train data size:1575
node11 epoch0:node_model train_loss:0.22311122016981244,train_acc:0.92208331823349
node11 epoch1:node_model train_loss:0.14028432313352823,train_acc:0.9574998617172241
node11_model on test-dataset: loss:0.1800500372465467,acc:0.939583957195282
node11 weight score:8747.568309821096
node14: train data size:1540
node14 epoch0:node_model train_loss:0.23742024088278413,train_acc:0.9212499856948853
node14 epoch1:node_model train_loss:0.1272085513919592,train_acc:0.9587498903274536
node14_model on test-dataset: loss:0.15225314225186592,acc:0.9497941136360168
node14 weight score:10114.733773129248
node15: train data size:1376
node15 epoch0:node_model train_loss:0.23629577245031083,train_acc:0.9222556352615356
node15 epoch1:node_model train_loss:0.1346403732895851,train_acc:0.9583834409713745
node15_model on test-dataset: loss:0.2770845128386281,acc:0.9118848443031311
node15 weight score:4965.99389804717
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.10223708518315107,acc:0.965297954082489
total cost energy:7.333619347546254 | all_enery_cp：5.891 | all_enery_tp: 1.442619347546254
ef: 29.337110766017748
reward: 22.003491418471494
episode0,iteration4 selected nodes:[5, 13, 9, 6, 11],center node:11
################################################## episode0,iteration4 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.1609489468439501,train_acc:0.9463872909545898
node5 epoch1:node_model train_loss:0.1124375861290158,train_acc:0.9641421437263489
node5_model on test-dataset: loss:0.1384662671503611,acc:0.9562970995903015
node5 weight score:34932.69587998268
node6: train data size:3529
node6 epoch0:node_model train_loss:0.14815521043621832,train_acc:0.9569443464279175
node6 epoch1:node_model train_loss:0.09880266787432548,train_acc:0.9691665768623352
node6_model on test-dataset: loss:0.471608140542412,acc:0.8618665337562561
node6 weight score:7482.907305079979
node9: train data size:2125
node9 epoch0:node_model train_loss:0.20121971907263453,train_acc:0.9322726130485535
node9 epoch1:node_model train_loss:0.11275746681812135,train_acc:0.964090883731842
node9_model on test-dataset: loss:0.12071629100479185,acc:0.9615969657897949
node9 weight score:17603.257872755945
node11: train data size:1575
node11 epoch0:node_model train_loss:0.19716774532571435,train_acc:0.9335415959358215
node11 epoch1:node_model train_loss:0.11738602234981954,train_acc:0.9622916579246521
node11_model on test-dataset: loss:0.1806397349340841,acc:0.9435948729515076
node11 weight score:8719.01190828652
node13: train data size:1056
node13 epoch0:node_model train_loss:0.20689248767766086,train_acc:0.9312987327575684
node13 epoch1:node_model train_loss:0.11426835490221326,train_acc:0.9590909481048584
node13_model on test-dataset: loss:0.20136411492392653,acc:0.9378927946090698
node13 weight score:5244.231328898632
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0861871650465764,acc:0.9722979527711868
total cost energy:7.908213595499958 | all_enery_cp：6.561 | all_enery_tp: 1.347213595499958
ef: 29.194192292826877
reward: 21.285978697326918
episode0,iteration5 selected nodes:[4, 9, 5, 18, 10],center node:10
################################################## episode0,iteration5 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.14937896931240724,train_acc:0.9557901620864868
node4 epoch1:node_model train_loss:0.09626621614361919,train_acc:0.9718508720397949
node4_model on test-dataset: loss:0.13351793620502575,acc:0.9573959708213806
node4 weight score:32190.431654067306
node5: train data size:4837
node5 epoch0:node_model train_loss:0.1386300717689553,train_acc:0.9541423320770264
node5 epoch1:node_model train_loss:0.08892480494948674,train_acc:0.9700605273246765
node5_model on test-dataset: loss:0.157284255164559,acc:0.9517909288406372
node5 weight score:30753.237156123978
node9: train data size:2125
node9 epoch0:node_model train_loss:0.14494664277034727,train_acc:0.9554545283317566
node9 epoch1:node_model train_loss:0.07723391699520024,train_acc:0.9754544496536255
node9_model on test-dataset: loss:0.12690977158606984,acc:0.9575961232185364
node9 weight score:16744.179533558072
node10: train data size:1915
node10 epoch0:node_model train_loss:0.14010978976730257,train_acc:0.9599999785423279
node10 epoch1:node_model train_loss:0.08640985675156117,train_acc:0.9734998941421509
node10_model on test-dataset: loss:0.1822823716473067,acc:0.9405929446220398
node10 weight score:10505.678539805716
node18: train data size:801
node18 epoch0:node_model train_loss:0.1178117459559063,train_acc:0.9633333683013916
node18 epoch1:node_model train_loss:0.21260934654209349,train_acc:0.8555554747581482
node18_model on test-dataset: loss:0.14473358906139766,acc:0.9540939927101135
node18 weight score:5534.306204900416
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07879205195815303,acc:0.9740969473123551
total cost energy:8.410678406009827 | all_enery_cp：6.9879999999999995 | all_enery_tp: 1.4226784060098283
ef: 29.712155863626016
reward: 21.301477457616187
episode0,iteration6 selected nodes:[1, 4, 2, 6, 7],center node:2
################################################## episode0,iteration6 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.13368606214314255,train_acc:0.9594002962112427
node1 epoch1:node_model train_loss:0.10952234624037102,train_acc:0.9642155170440674
node1_model on test-dataset: loss:0.10822292584140086,acc:0.9648970365524292
node1 weight score:61511.92040174314
node2: train data size:4610
node2 epoch0:node_model train_loss:0.13468851933770992,train_acc:0.9559575319290161
node2 epoch1:node_model train_loss:0.1009701352320027,train_acc:0.9665957093238831
node2_model on test-dataset: loss:0.09710525881499052,acc:0.9679948687553406
node2 weight score:47474.25686577065
node4: train data size:4298
node4 epoch0:node_model train_loss:0.12999172610512308,train_acc:0.9567345976829529
node4 epoch1:node_model train_loss:0.09505803475892821,train_acc:0.9704508185386658
node4_model on test-dataset: loss:0.30550672128483713,acc:0.9086928963661194
node4 weight score:14068.430252284987
node6: train data size:3529
node6 epoch0:node_model train_loss:0.11929953059492011,train_acc:0.9624999165534973
node6 epoch1:node_model train_loss:0.08280477947038081,train_acc:0.9749040603637695
node6_model on test-dataset: loss:0.12178904732689261,acc:0.9627963900566101
node6 weight score:28976.33307310345
node7: train data size:3637
node7 epoch0:node_model train_loss:0.14819618697102005,train_acc:0.9524322748184204
node7 epoch1:node_model train_loss:0.09388239258849942,train_acc:0.9708908796310425
node7_model on test-dataset: loss:0.16975492639932782,acc:0.946995735168457
node7 weight score:21425.004134751296
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07240319913078565,acc:0.9774969464540482
total cost energy:12.521562329783656 | all_enery_cp：11.3655 | all_enery_tp: 1.156062329783655
ef: 30.463672065918146
reward: 17.94210973613449
episode0,iteration7 selected nodes:[8, 0, 3, 1, 17],center node:8
################################################## episode0,iteration7 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.13626168983884984,train_acc:0.9591453671455383
node0 epoch1:node_model train_loss:0.09165110136382282,train_acc:0.9702671766281128
node0_model on test-dataset: loss:0.09863053639128339,acc:0.9684970378875732
node0 weight score:72644.8447119387
node1: train data size:6657
node1 epoch0:node_model train_loss:0.1181507169255125,train_acc:0.9644777178764343
node1 epoch1:node_model train_loss:0.08639988542270305,train_acc:0.9717910289764404
node1_model on test-dataset: loss:0.21636510860611452,acc:0.9284000992774963
node1 weight score:30767.437702346208
node3: train data size:3762
node3 epoch0:node_model train_loss:0.13154371014158978,train_acc:0.9552630186080933
node3 epoch1:node_model train_loss:0.08347473215115697,train_acc:0.9707299470901489
node3_model on test-dataset: loss:0.09963999860076,acc:0.9684961438179016
node3 weight score:37755.92184694496
node8: train data size:2290
node8 epoch0:node_model train_loss:0.15424615630637045,train_acc:0.9545893669128418
node8 epoch1:node_model train_loss:0.11035761382916699,train_acc:0.9672462940216064
node8_model on test-dataset: loss:0.11998837752500549,acc:0.9646979570388794
node8 weight score:19085.181808736146
node17: train data size:719
node17 epoch0:node_model train_loss:0.17955456778872758,train_acc:0.9596710801124573
node17 epoch1:node_model train_loss:0.1073622356634587,train_acc:0.966249942779541
node17_model on test-dataset: loss:0.1253894064530323,acc:0.9623969793319702
node17 weight score:5734.1367212653595
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06662868326762691,acc:0.9785969454050064
total cost energy:11.9176102550928 | all_enery_cp：10.296500000000002 | all_enery_tp: 1.6211102550927978
ef: 30.41534412628813
reward: 18.49773387119533
episode0,iteration8 selected nodes:[16, 1, 0, 18, 10],center node:16
################################################## episode0,iteration8 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.11192533580793275,train_acc:0.9634615778923035
node0 epoch1:node_model train_loss:0.08016145361600341,train_acc:0.9743058085441589
node0_model on test-dataset: loss:0.08846146395953838,acc:0.9716991782188416
node0 weight score:80995.7203882271
node1: train data size:6657
node1 epoch0:node_model train_loss:0.10633980113067734,train_acc:0.9657081365585327
node1 epoch1:node_model train_loss:0.07822755977177798,train_acc:0.9744018912315369
node1_model on test-dataset: loss:0.11035028871614486,acc:0.966495931148529
node1 weight score:60326.07687256594
node10: train data size:1915
node10 epoch0:node_model train_loss:0.10329698710702359,train_acc:0.9660000205039978
node10 epoch1:node_model train_loss:0.09069474714342504,train_acc:0.9745000004768372
node10_model on test-dataset: loss:0.08035165374079951,acc:0.9731941223144531
node10 weight score:23832.73910176706
node16: train data size:920
node16 epoch0:node_model train_loss:0.09910282529890538,train_acc:0.9649998545646667
node16 epoch1:node_model train_loss:0.09092290177941323,train_acc:0.9770000576972961
node16_model on test-dataset: loss:0.1388145470293239,acc:0.9572865962982178
node16 weight score:6627.547470263722
node18: train data size:801
node18 epoch0:node_model train_loss:0.43257347328795326,train_acc:0.861111044883728
node18 epoch1:node_model train_loss:0.17209275708430344,train_acc:0.9522221684455872
node18_model on test-dataset: loss:0.2205462992428511,acc:0.9354850053787231
node18 weight score:3631.890459055001
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06423319941444788,acc:0.9791979551315307
total cost energy:10.752382450983621 | all_enery_cp：8.729000000000001 | all_enery_tp: 2.02338245098362
ef: 30.587967648573013
reward: 19.835585197589392
episode0,iteration9 selected nodes:[16, 15, 19, 17, 13],center node:15
################################################## episode0,iteration9 ##################################################
node13: train data size:1056
node13 epoch0:node_model train_loss:0.09469212422316725,train_acc:0.972012996673584
node13 epoch1:node_model train_loss:0.06615930135277184,train_acc:0.981103777885437
node13_model on test-dataset: loss:0.12817034303036054,acc:0.957397997379303
node13 weight score:8239.035451047037
node15: train data size:1376
node15 epoch0:node_model train_loss:0.1242789039388299,train_acc:0.9598121643066406
node15 epoch1:node_model train_loss:0.08284698533160346,train_acc:0.9757143259048462
node15_model on test-dataset: loss:0.07701919084123801,acc:0.9749980568885803
node15 weight score:17865.67717695698
node16: train data size:920
node16 epoch0:node_model train_loss:0.1161323631182313,train_acc:0.9669999480247498
node16 epoch1:node_model train_loss:0.08817248567938804,train_acc:0.9699999690055847
node16_model on test-dataset: loss:0.1909728343388997,acc:0.9436957836151123
node16 weight score:4817.439104283133
node17: train data size:719
node17 epoch0:node_model train_loss:0.09368580457521603,train_acc:0.973421037197113
node17 epoch1:node_model train_loss:0.06504352181218565,train_acc:0.9771710634231567
node17_model on test-dataset: loss:0.19183256559772416,acc:0.9434970021247864
node17 weight score:3748.0601782064164
node19: train data size:5781
node19 epoch0:node_model train_loss:0.10824201699217846,train_acc:0.9673733115196228
node19 epoch1:node_model train_loss:0.07508673332631588,train_acc:0.9772415161132812
node19_model on test-dataset: loss:0.08478410933632403,acc:0.9726979732513428
node19 weight score:68184.9469818426
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06478917217580601,acc:0.9789979535341263
total cost energy:5.696820393249937 | all_enery_cp：4.926 | all_enery_tp: 0.7708203932499369
ef: 30.678784470399382
reward: 24.981964077149446
episode0,iteration10 selected nodes:[1, 18, 15, 13, 10],center node:15
################################################## episode0,iteration10 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0918937089449879,train_acc:0.9686933755874634
node1 epoch1:node_model train_loss:0.05977942890473711,train_acc:0.9828357696533203
node1_model on test-dataset: loss:0.08980164823209634,acc:0.9733949303627014
node1 weight score:74130.04249982905
node10: train data size:1915
node10 epoch0:node_model train_loss:0.12400151919573546,train_acc:0.9614998698234558
node10 epoch1:node_model train_loss:0.05873389141634107,train_acc:0.9784998893737793
node10_model on test-dataset: loss:0.0878731019130646,acc:0.9718971252441406
node10 weight score:21792.789355433983
node13: train data size:1056
node13 epoch0:node_model train_loss:0.11292652650312944,train_acc:0.9600648283958435
node13 epoch1:node_model train_loss:0.07268859801644628,train_acc:0.9781818389892578
node13_model on test-dataset: loss:0.2826552291950793,acc:0.922595739364624
node13 weight score:3736.0002254590654
node15: train data size:1376
node15 epoch0:node_model train_loss:0.09665768247629915,train_acc:0.9695488810539246
node15 epoch1:node_model train_loss:0.05391690307962043,train_acc:0.982631504535675
node15_model on test-dataset: loss:0.11356866322283167,acc:0.9659935832023621
node15 weight score:12116.01828314354
node18: train data size:801
node18 epoch0:node_model train_loss:0.07492222221723448,train_acc:0.9722222089767456
node18 epoch1:node_model train_loss:0.04021245199611359,train_acc:0.9877777099609375
node18_model on test-dataset: loss:0.19632638025501364,acc:0.943892776966095
node18 weight score:4079.940754571848
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06142617830555537,acc:0.9806959396600723
D:\pengyubo\pythonProj\SBM-master\experiment_1\SBM_train.py:109: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
total cost energy:7.288114969578876 | all_enery_cp：5.9025 | all_enery_tp: 1.3856149695788764
ef: 30.28241742515456
reward: 22.994302455575685
episode0,iteration11 selected nodes:[16, 5, 7, 12, 17],center node:12
################################################## episode0,iteration11 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.11440269898015018,train_acc:0.9641421437263489
node5 epoch1:node_model train_loss:0.06589389712150608,train_acc:0.9793877005577087
node5_model on test-dataset: loss:0.08962647315318463,acc:0.973097026348114
node5 weight score:53968.42952564769
node7: train data size:3637
node7 epoch0:node_model train_loss:0.10821460076683276,train_acc:0.966376781463623
node7 epoch1:node_model train_loss:0.0725987679262117,train_acc:0.9772971272468567
node7_model on test-dataset: loss:0.11460809847849304,acc:0.9625860452651978
node7 weight score:31734.232120450954
node12: train data size:1406
node12 epoch0:node_model train_loss:0.08575622501472632,train_acc:0.971333384513855
node12 epoch1:node_model train_loss:0.07523690105881542,train_acc:0.9786667227745056
node12_model on test-dataset: loss:0.11458707439349382,acc:0.9649929404258728
node12 weight score:12270.144843490583
node16: train data size:920
node16 epoch0:node_model train_loss:0.08991190348751843,train_acc:0.9739999771118164
node16 epoch1:node_model train_loss:0.08212455161847174,train_acc:0.9749999046325684
node16_model on test-dataset: loss:0.12733232116414,acc:0.9607890248298645
node16 weight score:7225.188322877249
node17: train data size:719
node17 epoch0:node_model train_loss:0.16179086500778794,train_acc:0.9646710157394409
node17 epoch1:node_model train_loss:0.13186102581676096,train_acc:0.9602631330490112
node17_model on test-dataset: loss:0.098011420215189,acc:0.9669969081878662
node17 weight score:7335.879823202228
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.060024924796598496,acc:0.9810959362983703
total cost energy:6.713163105724556 | all_enery_cp：5.7595 | all_enery_tp: 0.9536631057245559
ef: 30.77167437477271
reward: 24.058511269048154
episode0,iteration12 selected nodes:[16, 7, 10, 1, 0],center node:7
################################################## episode0,iteration12 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.10601304617658672,train_acc:0.966858983039856
node0 epoch1:node_model train_loss:0.0640408766662909,train_acc:0.9790811538696289
node0_model on test-dataset: loss:0.12079729511402547,acc:0.9628969430923462
node0 weight score:59314.24203858758
node1: train data size:6657
node1 epoch0:node_model train_loss:0.08666135862803281,train_acc:0.9724247455596924
node1 epoch1:node_model train_loss:0.05125039072234684,train_acc:0.9843285083770752
node1_model on test-dataset: loss:0.09197709888452664,acc:0.9731962084770203
node1 weight score:72376.71203739077
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0856184071131252,train_acc:0.9730532169342041
node7 epoch1:node_model train_loss:0.04981038290881426,train_acc:0.9845945239067078
node7_model on test-dataset: loss:0.09347496638773009,acc:0.9712908864021301
node7 weight score:38908.81313520758
node10: train data size:1915
node10 epoch0:node_model train_loss:0.09985743700526654,train_acc:0.9751664996147156
node10 epoch1:node_model train_loss:0.06452653033193201,train_acc:0.9774999022483826
node10_model on test-dataset: loss:0.09518604677534313,acc:0.9696971774101257
node10 weight score:20118.49493571004
node16: train data size:920
node16 epoch0:node_model train_loss:0.08546014577150345,train_acc:0.9719999432563782
node16 epoch1:node_model train_loss:0.05704686306416988,train_acc:0.9809999465942383
node16_model on test-dataset: loss:0.1772798861935735,acc:0.9493970274925232
node16 weight score:5189.534017386743
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05712807760923169,acc:0.9818969440460205
total cost energy:11.726669127533635 | all_enery_cp：10.147 | all_enery_tp: 1.5796691275336339
ef: 30.7985895678404
reward: 19.071920440306762
episode0,iteration13 selected nodes:[14, 8, 18, 0, 5],center node:8
################################################## episode0,iteration13 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07770248702985959,train_acc:0.9770088195800781
node0 epoch1:node_model train_loss:0.05985781451454386,train_acc:0.980341911315918
node0_model on test-dataset: loss:0.08986537463555579,acc:0.9724939465522766
node0 weight score:79730.37478625415
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08824042354387288,train_acc:0.9735299348831177
node5 epoch1:node_model train_loss:0.05537583976176244,train_acc:0.9820408225059509
node5_model on test-dataset: loss:0.08964244953065645,acc:0.973796010017395
node5 weight score:53958.811091455216
node8: train data size:2290
node8 epoch0:node_model train_loss:0.11652699129089066,train_acc:0.9598066210746765
node8 epoch1:node_model train_loss:0.0736868533751239,train_acc:0.9785988330841064
node8_model on test-dataset: loss:0.12043264245206955,acc:0.9627981185913086
node8 weight score:19014.778330645586
node14: train data size:1540
node14 epoch0:node_model train_loss:0.0999863421311602,train_acc:0.9681248664855957
node14 epoch1:node_model train_loss:0.07939998421352357,train_acc:0.9771873950958252
node14_model on test-dataset: loss:0.09387473980779759,acc:0.9711957573890686
node14 weight score:16404.83907761608
node18: train data size:801
node18 epoch0:node_model train_loss:0.06616673778949513,train_acc:0.9777777194976807
node18 epoch1:node_model train_loss:0.06147431375251876,train_acc:0.9777777194976807
node18_model on test-dataset: loss:0.258136747451174,acc:0.9242861866950989
node18 weight score:3103.006479739997
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05751420944463462,acc:0.9808959370851517
total cost energy:9.723859258196459 | all_enery_cp：8.3165 | all_enery_tp: 1.407359258196459
ef: 30.784558433775334
reward: 21.060699175578875
episode0,iteration14 selected nodes:[0, 1, 7, 4, 11],center node:0
################################################## episode0,iteration14 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.06264216046676868,train_acc:0.97756427526474
node0 epoch1:node_model train_loss:0.0459343306073505,train_acc:0.9847224354743958
node0_model on test-dataset: loss:0.09122821272118017,acc:0.9731959104537964
node0 weight score:78539.30035764609
node1: train data size:6657
node1 epoch0:node_model train_loss:0.08435006556448651,train_acc:0.9738808274269104
node1 epoch1:node_model train_loss:0.03947614860345623,train_acc:0.9873136878013611
node1_model on test-dataset: loss:0.09647468307171948,acc:0.9691950678825378
node1 weight score:69002.55888947748
node4: train data size:4298
node4 epoch0:node_model train_loss:0.09097017131226007,train_acc:0.9746319651603699
node4 epoch1:node_model train_loss:0.05218920839387317,train_acc:0.9839534163475037
node4_model on test-dataset: loss:0.07731714764609933,acc:0.9752970933914185
node4 weight score:55589.22090185042
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0791041106489059,train_acc:0.9727023839950562
node7 epoch1:node_model train_loss:0.058907151373254286,train_acc:0.9824323058128357
node7_model on test-dataset: loss:0.14413491448081914,acc:0.9558987021446228
node7 weight score:25233.303208321508
node11: train data size:1575
node11 epoch0:node_model train_loss:0.10781469382345676,train_acc:0.9662498831748962
node11 epoch1:node_model train_loss:0.06219913437962532,train_acc:0.982499897480011
node11_model on test-dataset: loss:0.11153576752563822,acc:0.9671950340270996
node11 weight score:14121.030723511738
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05137104651890695,acc:0.9825969451665878
total cost energy:13.428780549425378 | all_enery_cp：11.665999999999999 | all_enery_tp: 1.762780549425379
ef: 31.078980696025866
reward: 17.65020014660049
episode0,iteration15 selected nodes:[4, 1, 18, 7, 9],center node:7
################################################## episode0,iteration15 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.06296125261474457,train_acc:0.9814167022705078
node1 epoch1:node_model train_loss:0.051271728080099645,train_acc:0.9829850792884827
node1_model on test-dataset: loss:0.07412305468838895,acc:0.9771960377693176
node1 weight score:89810.11411342699
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07315211487544138,train_acc:0.9785950183868408
node4 epoch1:node_model train_loss:0.05669802368813476,train_acc:0.9795206785202026
node4_model on test-dataset: loss:0.06731058645062149,acc:0.977996826171875
node4 weight score:63853.25439339291
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07487676310277469,train_acc:0.9789991974830627
node7 epoch1:node_model train_loss:0.0635302197188139,train_acc:0.9798901677131653
node7_model on test-dataset: loss:0.09836913952269243,acc:0.9692969918251038
node7 weight score:36972.977680271295
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0843301322311163,train_acc:0.9718179702758789
node9 epoch1:node_model train_loss:0.041077207011932675,train_acc:0.9868180751800537
node9_model on test-dataset: loss:0.10481210343568818,acc:0.9690952897071838
node9 weight score:20274.376053371376
node18: train data size:801
node18 epoch0:node_model train_loss:0.05276723288827472,train_acc:0.9822221398353577
node18 epoch1:node_model train_loss:0.041215677223893486,train_acc:0.9811111092567444
node18_model on test-dataset: loss:0.17961169695510762,acc:0.9492915868759155
node18 weight score:4459.620467815094
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0520441805444716,acc:0.9822979539632797
total cost energy:10.915549317506656 | all_enery_cp：8.759 | all_enery_tp: 2.156549317506656
ef: 31.08341173658417
reward: 20.167862419077515
episode0,iteration16 selected nodes:[14, 0, 1, 17, 10],center node:10
################################################## episode0,iteration16 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.06986492986066474,train_acc:0.9765918850898743
node0 epoch1:node_model train_loss:0.03856578537185366,train_acc:0.9865171909332275
node0_model on test-dataset: loss:0.07349781243014149,acc:0.977196991443634
node0 weight score:97485.89465584734
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05551396117114754,train_acc:0.9798508882522583
node1 epoch1:node_model train_loss:0.05081951979950849,train_acc:0.9824249148368835
node1_model on test-dataset: loss:0.08340419147221838,acc:0.9745960235595703
node1 weight score:79816.13252875213
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0783953454985749,train_acc:0.9789999127388
node10 epoch1:node_model train_loss:0.035292326367925855,train_acc:0.9899998903274536
node10_model on test-dataset: loss:0.08211166019376832,acc:0.9760969281196594
node10 weight score:23321.90087840088
node14: train data size:1540
node14 epoch0:node_model train_loss:0.09177779022138566,train_acc:0.974687397480011
node14 epoch1:node_model train_loss:0.04341790033504367,train_acc:0.9868749380111694
node14_model on test-dataset: loss:0.08912391906400444,acc:0.9723968505859375
node14 weight score:17279.311953214798
node17: train data size:719
node17 epoch0:node_model train_loss:0.11326585453934968,train_acc:0.973421037197113
node17 epoch1:node_model train_loss:0.058516938937827945,train_acc:0.9862499237060547
node17_model on test-dataset: loss:0.12352179318491835,acc:0.9617860317230225
node17 weight score:5820.835185930476
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04548127467918675,acc:0.9852969443798065
total cost energy:10.587544699709127 | all_enery_cp：8.998 | all_enery_tp: 1.589544699709128
ef: 31.464081048485227
reward: 20.8765363487761
episode0,iteration17 selected nodes:[15, 13, 11, 5, 2],center node:11
################################################## episode0,iteration17 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.09270138210280145,train_acc:0.9729787111282349
node2 epoch1:node_model train_loss:0.052763442031449656,train_acc:0.9825531840324402
node2_model on test-dataset: loss:0.11037953038088745,acc:0.9667972326278687
node2 weight score:41764.99015797802
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08675703459552356,train_acc:0.974346399307251
node5 epoch1:node_model train_loss:0.053502422643407266,train_acc:0.9826530814170837
node5_model on test-dataset: loss:0.07942766516644043,acc:0.9777958989143372
node5 weight score:60898.17684888611
node11: train data size:1575
node11 epoch0:node_model train_loss:0.08292059447558131,train_acc:0.9774999022483826
node11 epoch1:node_model train_loss:0.05906852299813181,train_acc:0.9804165363311768
node11_model on test-dataset: loss:0.08820904964144574,acc:0.9739959239959717
node11 weight score:17855.310837177112
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08678163579580458,train_acc:0.975844144821167
node13 epoch1:node_model train_loss:0.043025552125817,train_acc:0.9872727394104004
node13_model on test-dataset: loss:0.09233700331300497,acc:0.9692981243133545
node13 weight score:11436.368542526336
node15: train data size:1376
node15 epoch0:node_model train_loss:0.09554717752949468,train_acc:0.9707518815994263
node15 epoch1:node_model train_loss:0.049284687698153515,train_acc:0.9883458018302917
node15_model on test-dataset: loss:0.06297159450070466,acc:0.9812968969345093
node15 weight score:21851.122095766565
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05319561893455102,acc:0.9832959347963333
total cost energy:7.986524158061725 | all_enery_cp：6.727 | all_enery_tp: 1.259524158061724
ef: 31.539127591917694
reward: 23.55260343385597
episode0,iteration18 selected nodes:[10, 6, 17, 13, 0],center node:6
################################################## episode0,iteration18 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.05601474257936287,train_acc:0.982564389705658
node0 epoch1:node_model train_loss:0.039883122572468385,train_acc:0.9881200790405273
node0_model on test-dataset: loss:0.15014497024472803,acc:0.9568877816200256
node0 weight score:47720.54627152308
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07826426633012791,train_acc:0.9718196988105774
node6 epoch1:node_model train_loss:0.044150961705276534,train_acc:0.9855554699897766
node6_model on test-dataset: loss:0.062076620659790936,acc:0.9804946780204773
node6 weight score:56849.09974949472
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06765817292034626,train_acc:0.9789999127388
node10 epoch1:node_model train_loss:0.04872642667032778,train_acc:0.9860000014305115
node10_model on test-dataset: loss:0.10505129541197675,acc:0.9688958525657654
node10 weight score:18229.189773338803
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08985001881691543,train_acc:0.9705843925476074
node13 epoch1:node_model train_loss:0.09102171113375913,train_acc:0.9722077250480652
node13_model on test-dataset: loss:0.0946548665408045,acc:0.9701980352401733
node13 weight score:11156.320204041194
node17: train data size:719
node17 epoch0:node_model train_loss:0.08987153926864266,train_acc:0.9696710705757141
node17 epoch1:node_model train_loss:0.07369215949438512,train_acc:0.9762499332427979
node17_model on test-dataset: loss:0.11412252943649946,acc:0.9673879742622375
node17 weight score:6300.245915948341
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05032597874844214,acc:0.9835959386825561
total cost energy:8.749684886366493 | all_enery_cp：7.192 | all_enery_tp: 1.5576848863664925
ef: 30.873427925189855
reward: 22.123743038823363
episode0,iteration19 selected nodes:[3, 9, 4, 19, 16],center node:4
################################################## episode0,iteration19 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08144510413618072,train_acc:0.9749404788017273
node3 epoch1:node_model train_loss:0.05912154969318133,train_acc:0.9823090434074402
node3_model on test-dataset: loss:0.08112964832806029,acc:0.9754951000213623
node3 weight score:46370.22441891737
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06719747257180685,train_acc:0.9765068888664246
node4 epoch1:node_model train_loss:0.0385516899053094,train_acc:0.9883720278739929
node4_model on test-dataset: loss:0.0885391307977261,acc:0.9723960161209106
node4 weight score:48543.50795264846
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0666446067943153,train_acc:0.9781817197799683
node9 epoch1:node_model train_loss:0.048399502263319766,train_acc:0.9854544401168823
node9_model on test-dataset: loss:0.09374421114194774,acc:0.9706941843032837
node9 weight score:22668.066370331064
node16: train data size:920
node16 epoch0:node_model train_loss:0.06400935407727956,train_acc:0.9819999933242798
node16 epoch1:node_model train_loss:0.04101460911333561,train_acc:0.9879999160766602
node16_model on test-dataset: loss:0.073153815381811,acc:0.9775949120521545
node16 weight score:12576.240831708545
node19: train data size:5781
node19 epoch0:node_model train_loss:0.08191123271585796,train_acc:0.9769073128700256
node19 epoch1:node_model train_loss:0.06326565541455458,train_acc:0.9803448915481567
node19_model on test-dataset: loss:0.08054231230868027,acc:0.9746990203857422
node19 weight score:71775.93781817667
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0485442546007107,acc:0.9844989615678787
total cost energy:10.37842534212242 | all_enery_cp：8.443000000000001 | all_enery_tp: 1.9354253421224183
ef: 31.396435841840308
reward: 21.01801049971789
episode0,iteration20 selected nodes:[3, 4, 17, 13, 18],center node:17
################################################## episode0,iteration20 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06829669882886504,train_acc:0.9768845438957214
node3 epoch1:node_model train_loss:0.041360192711612113,train_acc:0.9865788817405701
node3_model on test-dataset: loss:0.05953897445113398,acc:0.9825969338417053
node3 weight score:63185.50218038479
node4: train data size:4298
node4 epoch0:node_model train_loss:0.060301189505776696,train_acc:0.9813951849937439
node4 epoch1:node_model train_loss:0.04309457474453158,train_acc:0.9871999025344849
node4_model on test-dataset: loss:0.07660430300136795,acc:0.9761961698532104
node4 weight score:56106.50879394137
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07995748248967258,train_acc:0.9740259051322937
node13 epoch1:node_model train_loss:0.03531160933727568,train_acc:0.9890908598899841
node13_model on test-dataset: loss:0.07739849371952005,acc:0.9751989245414734
node13 weight score:13643.676372136873
node17: train data size:719
node17 epoch0:node_model train_loss:0.07527765328995883,train_acc:0.977171003818512
node17 epoch1:node_model train_loss:0.04572417540475726,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.06763404169701971,acc:0.9781982898712158
node17 weight score:10630.741294759599
node18: train data size:801
node18 epoch0:node_model train_loss:0.047099030136653,train_acc:0.9844444394111633
node18 epoch1:node_model train_loss:0.03302278291994298,train_acc:0.9877777099609375
node18_model on test-dataset: loss:0.1003412311959255,acc:0.9709981083869934
node18 weight score:7982.760331453116
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043393974969221746,acc:0.9863979542255401
total cost energy:7.412632800429791 | all_enery_cp：5.3180000000000005 | all_enery_tp: 2.0946328004297907
ef: 31.60647746738517
reward: 24.19384466695538
episode0,iteration21 selected nodes:[12, 15, 1, 8, 3],center node:8
################################################## episode0,iteration21 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.055029328116702275,train_acc:0.9816784858703613
node1 epoch1:node_model train_loss:0.03906798376632271,train_acc:0.9876121282577515
node1_model on test-dataset: loss:0.14719405597341848,acc:0.9604000449180603
node1 weight score:45226.0110367648
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05345979759371594,train_acc:0.9798386096954346
node3 epoch1:node_model train_loss:0.03630968435716472,train_acc:0.9861543774604797
node3_model on test-dataset: loss:0.07998450598606724,acc:0.9776971340179443
node3 weight score:47034.10933931773
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08457617947588796,train_acc:0.9728984236717224
node8 epoch1:node_model train_loss:0.04301652004537375,train_acc:0.9860868453979492
node8_model on test-dataset: loss:0.06899393051629886,acc:0.9780980348587036
node8 weight score:33191.32542331414
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0536421666542689,train_acc:0.984000027179718
node12 epoch1:node_model train_loss:0.036509182405037185,train_acc:0.9899999499320984
node12_model on test-dataset: loss:0.1619992658978299,acc:0.9531002044677734
node12 weight score:8679.051674757216
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07456999864163143,train_acc:0.9778571128845215
node15 epoch1:node_model train_loss:0.040268854171569855,train_acc:0.9869171977043152
node15_model on test-dataset: loss:0.08963923123912537,acc:0.9751968383789062
node15 weight score:15350.421695711831
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05080958353952156,acc:0.9836969459056855
total cost energy:9.369496489063195 | all_enery_cp：7.7455 | all_enery_tp: 1.6239964890631948
ef: 31.069581146360065
reward: 21.70008465729687
episode0,iteration22 selected nodes:[10, 0, 3, 1, 16],center node:0
################################################## episode0,iteration22 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.05237955539875353,train_acc:0.9829167127609253
node0 epoch1:node_model train_loss:0.040223213069516026,train_acc:0.9872223734855652
node0_model on test-dataset: loss:0.08469874520320446,acc:0.975695013999939
node0 weight score:84593.93327268469
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04518709085714906,train_acc:0.9839174151420593
node1 epoch1:node_model train_loss:0.0327893368024101,train_acc:0.9900734424591064
node1_model on test-dataset: loss:0.12155362490364496,acc:0.9666960835456848
node1 weight score:54765.95210777939
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04529202530944818,train_acc:0.9841510653495789
node3 epoch1:node_model train_loss:0.04588159961674951,train_acc:0.9868420362472534
node3_model on test-dataset: loss:0.07927391384617294,acc:0.9764968156814575
node3 weight score:47455.71168972397
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06540947316680104,train_acc:0.9834999442100525
node10 epoch1:node_model train_loss:0.05663565918803215,train_acc:0.9811664819717407
node10_model on test-dataset: loss:0.07828788388287648,acc:0.9763960242271423
node10 weight score:24461.00092403773
node16: train data size:920
node16 epoch0:node_model train_loss:0.0855517797113862,train_acc:0.9769999384880066
node16 epoch1:node_model train_loss:0.05029486229177564,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.10925406082142217,acc:0.9674909114837646
node16 weight score:8420.739632769873
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051754623190499845,acc:0.9836969482898712
total cost energy:12.151080095137067 | all_enery_cp：10.209500000000002 | all_enery_tp: 1.9415800951370645
ef: 31.055623426519368
reward: 18.9045433313823
episode0,iteration23 selected nodes:[10, 8, 2, 18, 4],center node:8
################################################## episode0,iteration23 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07760385689424708,train_acc:0.9755316972732544
node2 epoch1:node_model train_loss:0.051694368646341436,train_acc:0.9823403358459473
node2_model on test-dataset: loss:0.07875872488846652,acc:0.9763961434364319
node2 weight score:58533.19751593758
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06141808082696137,train_acc:0.9802088141441345
node4 epoch1:node_model train_loss:0.03135856443496291,train_acc:0.9906977415084839
node4_model on test-dataset: loss:0.06522827828826848,acc:0.9796969294548035
node4 weight score:65891.66712335269
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07527249897627727,train_acc:0.9782124757766724
node8 epoch1:node_model train_loss:0.04280344338115791,train_acc:0.986086905002594
node8_model on test-dataset: loss:0.08015341322061431,acc:0.9756969213485718
node8 weight score:28570.211897240137
node10: train data size:1915
node10 epoch0:node_model train_loss:0.059297213552054015,train_acc:0.98499995470047
node10 epoch1:node_model train_loss:0.04459300013259053,train_acc:0.9884999394416809
node10_model on test-dataset: loss:0.08668931302629063,acc:0.974098801612854
node10 weight score:22090.381537793823
node18: train data size:801
node18 epoch0:node_model train_loss:0.04372375944836272,train_acc:0.9844444394111633
node18 epoch1:node_model train_loss:0.02495781197325818,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.12634677342488432,acc:0.9635928869247437
node18 weight score:6339.694938677722
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04836690371608711,acc:0.9844969481229782
total cost energy:8.380587024213298 | all_enery_cp：6.957000000000001 | all_enery_tp: 1.4235870242132966
ef: 31.637348910471353
reward: 23.256761886258055
episode0,iteration24 selected nodes:[7, 17, 18, 13, 19],center node:18
################################################## episode0,iteration24 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07967157028237912,train_acc:0.9736739993095398
node7 epoch1:node_model train_loss:0.037526951756680735,train_acc:0.988648533821106
node7_model on test-dataset: loss:0.05846062589320354,acc:0.9810965657234192
node7 weight score:62212.81322995255
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05336076021194458,train_acc:0.9863635301589966
node13 epoch1:node_model train_loss:0.04860499496995048,train_acc:0.9827272295951843
node13_model on test-dataset: loss:0.07193251141579822,acc:0.9767971634864807
node13 weight score:14680.427239581619
node17: train data size:719
node17 epoch0:node_model train_loss:0.08022826028172858,train_acc:0.9799999594688416
node17 epoch1:node_model train_loss:0.03493043628986925,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.06928134654212044,acc:0.9761940836906433
node17 weight score:10377.973810928677
node18: train data size:801
node18 epoch0:node_model train_loss:0.034445890898091136,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.0246096009375631,train_acc:0.9922221302986145
node18_model on test-dataset: loss:0.09160834312184306,acc:0.9731937050819397
node18 weight score:8743.745085910303
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06895998478385396,train_acc:0.9803043603897095
node19 epoch1:node_model train_loss:0.052810402723543086,train_acc:0.9842700362205505
node19_model on test-dataset: loss:0.07700322558463085,acc:0.9756980538368225
node19 weight score:75074.77714224266
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04189046104205772,acc:0.9865969461202622
total cost energy:7.156524158061724 | all_enery_cp：5.997 | all_enery_tp: 1.1595241580617242
ef: 31.89928614068587
reward: 24.742761982624145
episode0,iteration25 selected nodes:[8, 14, 5, 18, 16],center node:8
################################################## episode0,iteration25 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.060259859885412215,train_acc:0.9795918464660645
node5 epoch1:node_model train_loss:0.0428114427842808,train_acc:0.9863872528076172
node5_model on test-dataset: loss:0.10538498814790728,acc:0.9694956541061401
node5 weight score:45898.37779562394
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07519048492869605,train_acc:0.9781157374382019
node8 epoch1:node_model train_loss:0.049338922754901905,train_acc:0.9838162660598755
node8_model on test-dataset: loss:0.06885285783209838,acc:0.9777960181236267
node8 weight score:33259.331160723865
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08337146899430081,train_acc:0.9699999690055847
node14 epoch1:node_model train_loss:0.03970185783691704,train_acc:0.9887499213218689
node14_model on test-dataset: loss:0.0776269031589618,acc:0.9760950207710266
node14 weight score:19838.48301724003
node16: train data size:920
node16 epoch0:node_model train_loss:0.08271482717245818,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.039288425864651796,train_acc:0.9869999289512634
node16_model on test-dataset: loss:0.08656298453675845,acc:0.9757960438728333
node16 weight score:10628.099353590651
node18: train data size:801
node18 epoch0:node_model train_loss:0.03397084946563053,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.0349809758287544,train_acc:0.9888888597488403
node18_model on test-dataset: loss:0.1000180872396959,acc:0.9698930978775024
node18 weight score:8008.551474098711
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04693353431139258,acc:0.9842959356307983
total cost energy:6.739875738909909 | all_enery_cp：5.194 | all_enery_tp: 1.545875738909909
ef: 31.098542637584817
reward: 24.358666898674908
episode0,iteration26 selected nodes:[10, 18, 17, 7, 14],center node:17
################################################## episode0,iteration26 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06499810654015557,train_acc:0.9784586429595947
node7 epoch1:node_model train_loss:0.03310010306587493,train_acc:0.9886485934257507
node7_model on test-dataset: loss:0.06963626154203667,acc:0.9785939455032349
node7 weight score:52228.53610262358
node10: train data size:1915
node10 epoch0:node_model train_loss:0.052435370674356815,train_acc:0.9811664819717407
node10 epoch1:node_model train_loss:0.0327237700810656,train_acc:0.9894999861717224
node10_model on test-dataset: loss:0.08719432096377204,acc:0.9748990535736084
node10 weight score:21962.439512495937
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07974913262296468,train_acc:0.9765624403953552
node14 epoch1:node_model train_loss:0.03638789104297757,train_acc:0.9881248474121094
node14_model on test-dataset: loss:0.06266886746787349,acc:0.9798991084098816
node14 weight score:24573.605080536425
node17: train data size:719
node17 epoch0:node_model train_loss:0.049185761221451685,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.04559283913113177,train_acc:0.9859210252761841
node17_model on test-dataset: loss:0.0709341879689964,acc:0.9782968759536743
node17 weight score:10136.156070670144
node18: train data size:801
node18 epoch0:node_model train_loss:0.025019299859700875,train_acc:0.9877777099609375
node18 epoch1:node_model train_loss:0.02362486595262049,train_acc:0.9922221302986145
node18_model on test-dataset: loss:0.09124483060269994,acc:0.971191942691803
node18 weight score:8778.579506467935
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04468022557135555,acc:0.9861989653110504
total cost energy:5.748366870536343 | all_enery_cp：4.306 | all_enery_tp: 1.442366870536343
ef: 31.693833109669132
reward: 25.945466239132788
episode0,iteration27 selected nodes:[11, 0, 2, 7, 16],center node:7
################################################## episode0,iteration27 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04873504569857485,train_acc:0.9839532971382141
node0 epoch1:node_model train_loss:0.02739538003839294,train_acc:0.9914534091949463
node0_model on test-dataset: loss:0.07156268250953872,acc:0.9790958762168884
node0 weight score:100122.01539601263
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05926254990342212,train_acc:0.9821276068687439
node2 epoch1:node_model train_loss:0.03962966094308711,train_acc:0.9859575033187866
node2_model on test-dataset: loss:0.08088002239441267,acc:0.9773899912834167
node2 weight score:56998.005978772664
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04520033809003999,train_acc:0.9859459400177002
node7 epoch1:node_model train_loss:0.036778537040526,train_acc:0.989269495010376
node7_model on test-dataset: loss:0.07793502305037692,acc:0.9758968353271484
node7 weight score:46667.08057106824
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06598055036738515,train_acc:0.9785415530204773
node11 epoch1:node_model train_loss:0.041002543817739934,train_acc:0.9868748784065247
node11_model on test-dataset: loss:0.08815810519241495,acc:0.9749971032142639
node11 weight score:17865.628991938815
node16: train data size:920
node16 epoch0:node_model train_loss:0.05359634956694208,train_acc:0.9860000014305115
node16 epoch1:node_model train_loss:0.020395669620484114,train_acc:0.9909999966621399
node16_model on test-dataset: loss:0.06775453507209023,acc:0.9801927804946899
node16 weight score:13578.426876092177
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04798429210524773,acc:0.9838959342241287
total cost energy:10.276177276241437 | all_enery_cp：8.953500000000002 | all_enery_tp: 1.322677276241436
ef: 31.83485882612135
reward: 21.558681549879914
episode0,iteration28 selected nodes:[18, 2, 9, 5, 13],center node:5
################################################## episode0,iteration28 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.051672188909427125,train_acc:0.9842552542686462
node2 epoch1:node_model train_loss:0.03530843619924742,train_acc:0.9876595735549927
node2_model on test-dataset: loss:0.07811011056794087,acc:0.9753983020782471
node2 weight score:59019.24816749786
node5: train data size:4837
node5 epoch0:node_model train_loss:0.056506465811624516,train_acc:0.9806122779846191
node5 epoch1:node_model train_loss:0.02959481355311273,train_acc:0.9893878698348999
node5_model on test-dataset: loss:0.06603248268176685,acc:0.9813968539237976
node5 weight score:73251.82703353983
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04909848383712498,train_acc:0.9831817150115967
node9 epoch1:node_model train_loss:0.033260148484259844,train_acc:0.9909090995788574
node9_model on test-dataset: loss:0.07821965560342506,acc:0.9776947498321533
node9 weight score:27167.084585155746
node13: train data size:1056
node13 epoch0:node_model train_loss:0.044341125673699106,train_acc:0.9847401976585388
node13 epoch1:node_model train_loss:0.02266280772164464,train_acc:0.9903895854949951
node13_model on test-dataset: loss:0.06976327183379909,acc:0.9781961441040039
node13 weight score:15136.904738581748
node18: train data size:801
node18 epoch0:node_model train_loss:0.030724450402582686,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.02372915987184064,train_acc:0.9888888597488403
node18_model on test-dataset: loss:0.07990421966693248,acc:0.9752970933914185
node18 weight score:10024.501876607217
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04763047855682089,acc:0.9852969467639923
total cost energy:8.548758213637267 | all_enery_cp：6.714500000000001 | all_enery_tp: 1.8342582136372654
ef: 31.48269571116613
reward: 22.933937497528863
episode0,iteration29 selected nodes:[18, 16, 0, 7, 8],center node:7
################################################## episode0,iteration29 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03892703142870838,train_acc:0.9863784909248352
node0 epoch1:node_model train_loss:0.026865550679051213,train_acc:0.9913891553878784
node0_model on test-dataset: loss:0.0804773250449216,acc:0.9784919619560242
node0 weight score:89031.28919854845
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0413851918577141,train_acc:0.9870269894599915
node7 epoch1:node_model train_loss:0.028129681252647896,train_acc:0.9910810589790344
node7_model on test-dataset: loss:0.06405616605814429,acc:0.9803982377052307
node7 weight score:56778.29667012331
node8: train data size:2290
node8 epoch0:node_model train_loss:0.0608068142167252,train_acc:0.9816424250602722
node8 epoch1:node_model train_loss:0.021563843312754256,train_acc:0.9942994713783264
node8_model on test-dataset: loss:0.07095224494463764,acc:0.9800971150398254
node8 weight score:32275.22965322426
node16: train data size:920
node16 epoch0:node_model train_loss:0.050408865022473034,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.030320328148081898,train_acc:0.9879999160766602
node16_model on test-dataset: loss:0.06829959046823206,acc:0.9819957613945007
node16 weight score:13470.066126207832
node18: train data size:801
node18 epoch0:node_model train_loss:0.02864276380983534,train_acc:0.992222249507904
node18 epoch1:node_model train_loss:0.0276745923070444,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.10031759311774635,acc:0.9710944294929504
node18 weight score:7984.641328663434
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0436049072255264,acc:0.9862969475984573
total cost energy:9.04734889114037 | all_enery_cp：7.406499999999999 | all_enery_tp: 1.64084889114037
ef: 31.652136988831472
reward: 22.604788097691102
episode0,iteration30 selected nodes:[14, 5, 19, 9, 7],center node:5
################################################## episode0,iteration30 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04984411560189055,train_acc:0.9835080504417419
node5 epoch1:node_model train_loss:0.03803970788282399,train_acc:0.9872643351554871
node5_model on test-dataset: loss:0.08732879530085484,acc:0.9754961133003235
node5 weight score:55388.3742852073
node7: train data size:3637
node7 epoch0:node_model train_loss:0.043244984433545755,train_acc:0.9860261678695679
node7 epoch1:node_model train_loss:0.03339468585201413,train_acc:0.9881882667541504
node7_model on test-dataset: loss:0.07004975842457498,acc:0.9794971942901611
node7 weight score:51920.23615493385
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04768546804552898,train_acc:0.9877271056175232
node9 epoch1:node_model train_loss:0.023147224755534393,train_acc:0.9927272200584412
node9_model on test-dataset: loss:0.05891539602678677,acc:0.9812971949577332
node9 weight score:36068.67038683466
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07390095450682566,train_acc:0.9784374237060547
node14 epoch1:node_model train_loss:0.036714164016302675,train_acc:0.9887499213218689
node14_model on test-dataset: loss:0.08374653790320735,acc:0.9731979966163635
node14 weight score:18388.819867155617
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06516994385586666,train_acc:0.9796149730682373
node19 epoch1:node_model train_loss:0.04493707648462391,train_acc:0.9866837859153748
node19_model on test-dataset: loss:0.06485676859301748,acc:0.9804969429969788
node19 weight score:89134.87559450173
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04195230721219559,acc:0.987696944475174
total cost energy:10.669994144925337 | all_enery_cp：8.959999999999999 | all_enery_tp: 1.7099941449253369
ef: 32.00292517958521
reward: 21.332931034659875
episode0,iteration31 selected nodes:[4, 1, 5, 9, 6],center node:5
################################################## episode0,iteration31 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05684620600694151,train_acc:0.9825741052627563
node1 epoch1:node_model train_loss:0.028780470348533187,train_acc:0.9902988076210022
node1_model on test-dataset: loss:0.06395682572521764,acc:0.9812960028648376
node1 weight score:104085.84110476264
node4: train data size:4298
node4 epoch0:node_model train_loss:0.060509493487865426,train_acc:0.9818605184555054
node4 epoch1:node_model train_loss:0.02896061299780254,train_acc:0.991158127784729
node4_model on test-dataset: loss:0.07252325915149413,acc:0.9777915477752686
node4 weight score:59263.74586974767
node5: train data size:4837
node5 epoch0:node_model train_loss:0.042778900178263385,train_acc:0.9869993329048157
node5 epoch1:node_model train_loss:0.024009524374173915,train_acc:0.9922447800636292
node5_model on test-dataset: loss:0.07457415539567591,acc:0.9798972010612488
node5 weight score:64861.612905112
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05519422886168791,train_acc:0.9820976257324219
node6 epoch1:node_model train_loss:0.03217537865081491,train_acc:0.987777590751648
node6_model on test-dataset: loss:0.06552625882293796,acc:0.9807969927787781
node6 weight score:53856.2717205617
node9: train data size:2125
node9 epoch0:node_model train_loss:0.036973154219925745,train_acc:0.9886361956596375
node9 epoch1:node_model train_loss:0.02608571270089173,train_acc:0.9899999499320984
node9_model on test-dataset: loss:0.0697576853861392,acc:0.9792972207069397
node9 weight score:30462.593307636263
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0419833458570065,acc:0.9876969480514526
total cost energy:12.130768723046359 | all_enery_cp：10.723 | all_enery_tp: 1.407768723046357
ef: 32.35894029667584
reward: 20.22817157362948
episode0,iteration32 selected nodes:[12, 17, 10, 18, 5],center node:12
################################################## episode0,iteration32 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.037964057861542215,train_acc:0.9881632924079895
node5 epoch1:node_model train_loss:0.02308174493076394,train_acc:0.9920408725738525
node5_model on test-dataset: loss:0.06395475009267101,acc:0.9815981984138489
node5 weight score:75631.59879432166
node10: train data size:1915
node10 epoch0:node_model train_loss:0.048513571242801846,train_acc:0.9849998354911804
node10 epoch1:node_model train_loss:0.03202241907711141,train_acc:0.9894999861717224
node10_model on test-dataset: loss:0.058875041848277763,acc:0.9815980195999146
node10 weight score:32526.516158323855
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04800850807126456,train_acc:0.9853333234786987
node12 epoch1:node_model train_loss:0.021970003579432764,train_acc:0.9946666359901428
node12_model on test-dataset: loss:0.07064957919450535,acc:0.9807937145233154
node12 weight score:19901.038562864494
node17: train data size:719
node17 epoch0:node_model train_loss:0.0723816737299785,train_acc:0.979671061038971
node17 epoch1:node_model train_loss:0.03410962858470157,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.08232638498207961,acc:0.9733999967575073
node17 weight score:8733.530570503104
node18: train data size:801
node18 epoch0:node_model train_loss:0.023473320292800456,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.015040497502512708,train_acc:0.9955554604530334
node18_model on test-dataset: loss:0.09235247925644216,acc:0.9740930199623108
node18 weight score:8673.291788689314
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04464495372711099,acc:0.9867979556322097
total cost energy:5.928292222699218 | all_enery_cp：4.839 | all_enery_tp: 1.0892922226992172
ef: 31.92104236234113
reward: 25.992750139641913
episode0,iteration33 selected nodes:[14, 10, 12, 0, 6],center node:10
################################################## episode0,iteration33 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04223994825345775,train_acc:0.9868057370185852
node0 epoch1:node_model train_loss:0.02680238359446068,train_acc:0.9912502765655518
node0_model on test-dataset: loss:0.07356853429256717,acc:0.9791960120201111
node0 weight score:97392.18089497673
node6: train data size:3529
node6 epoch0:node_model train_loss:0.053523669788976096,train_acc:0.9822220206260681
node6 epoch1:node_model train_loss:0.04600002553262231,train_acc:0.9833618998527527
node6_model on test-dataset: loss:0.07004541043934295,acc:0.9787948131561279
node6 weight score:50381.60213303339
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04249125921342056,train_acc:0.9854999780654907
node10 epoch1:node_model train_loss:0.017548878196976148,train_acc:0.9955000281333923
node10_model on test-dataset: loss:0.06642771514962079,acc:0.9813960790634155
node10 weight score:28828.32859276708
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04006961480093499,train_acc:0.9859999418258667
node12 epoch1:node_model train_loss:0.03437570325319636,train_acc:0.9886665940284729
node12_model on test-dataset: loss:0.06525787666920223,acc:0.9808961749076843
node12 weight score:21545.291875295523
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07094605272868648,train_acc:0.9759374856948853
node14 epoch1:node_model train_loss:0.032740917296905536,train_acc:0.9881249070167542
node14_model on test-dataset: loss:0.0669076201516873,acc:0.9795003533363342
node14 weight score:23016.81029019777
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042346633576707975,acc:0.9869969469308854
total cost energy:9.161661925296379 | all_enery_cp：7.7775 | all_enery_tp: 1.384161925296378
ef: 32.26629194853874
reward: 23.104630023242365
episode0,iteration34 selected nodes:[6, 13, 5, 2, 18],center node:6
################################################## episode0,iteration34 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05497161310879474,train_acc:0.983404278755188
node2 epoch1:node_model train_loss:0.040792570419688805,train_acc:0.9859575033187866
node2_model on test-dataset: loss:0.07631432819282054,acc:0.9782969355583191
node2 weight score:60408.0532341461
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03436292313533474,train_acc:0.9886322021484375
node5 epoch1:node_model train_loss:0.031093816467731888,train_acc:0.9884281158447266
node5_model on test-dataset: loss:0.06386240282619837,acc:0.9813989996910095
node5 weight score:75740.96472949669
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03842240597522403,train_acc:0.9872220158576965
node6 epoch1:node_model train_loss:0.03014972564091699,train_acc:0.9902775883674622
node6_model on test-dataset: loss:0.09789934856584295,acc:0.9707972407341003
node6 weight score:36047.22658217225
node13: train data size:1056
node13 epoch0:node_model train_loss:0.042112782396460796,train_acc:0.9863635897636414
node13 epoch1:node_model train_loss:0.022428991802206092,train_acc:0.9936363101005554
node13_model on test-dataset: loss:0.08696482652681879,acc:0.9738962054252625
node13 weight score:12142.840297328066
node18: train data size:801
node18 epoch0:node_model train_loss:0.0966645129956305,train_acc:0.9899998903274536
node18 epoch1:node_model train_loss:0.019181582329717155,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.1576931805724444,acc:0.9611929655075073
node18 weight score:5079.4840784634935
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04686690951071796,acc:0.9860989624261856
total cost energy:8.624268723046358 | all_enery_cp：7.4165 | all_enery_tp: 1.207768723046357
ef: 31.398412238785472
reward: 22.774143515739112
episode0,iteration35 selected nodes:[15, 10, 8, 9, 2],center node:8
################################################## episode0,iteration35 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05370481399462578,train_acc:0.9823405146598816
node2 epoch1:node_model train_loss:0.04150687356261497,train_acc:0.9863830208778381
node2_model on test-dataset: loss:0.058682647953246486,acc:0.9830960631370544
node2 weight score:78558.14556413455
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06345338175964096,train_acc:0.9807727932929993
node8 epoch1:node_model train_loss:0.030079918417512723,train_acc:0.9908695220947266
node8_model on test-dataset: loss:0.08689702646213845,acc:0.9755949974060059
node8 weight score:26353.030629854366
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05019164224028249,train_acc:0.9854544997215271
node9 epoch1:node_model train_loss:0.022911199106601998,train_acc:0.992272675037384
node9_model on test-dataset: loss:0.11088539155462059,acc:0.9701961874961853
node9 weight score:19163.931066187874
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04192613586783409,train_acc:0.9836665987968445
node10 epoch1:node_model train_loss:0.02507246930617839,train_acc:0.9924999475479126
node10_model on test-dataset: loss:0.08830686170971604,acc:0.9751991033554077
node10 weight score:21685.744039857556
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0614440187013575,train_acc:0.9816916584968567
node15 epoch1:node_model train_loss:0.03804940537416509,train_acc:0.9847744107246399
node15_model on test-dataset: loss:0.055590558327967304,acc:0.9829960465431213
node15 weight score:24752.4047497782
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04012510785018094,acc:0.9878969466686249
total cost energy:7.445705430228725 | all_enery_cp：6.158 | all_enery_tp: 1.2877054302287245
ef: 31.867379843526372
reward: 24.421674413297648
episode0,iteration36 selected nodes:[10, 11, 17, 2, 1],center node:11
################################################## episode0,iteration36 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04331515927159631,train_acc:0.9858208894729614
node1 epoch1:node_model train_loss:0.028094198554754257,train_acc:0.9903351068496704
node1_model on test-dataset: loss:0.06860185375669971,acc:0.9790972471237183
node1 weight score:97038.19409325907
node2: train data size:4610
node2 epoch0:node_model train_loss:0.034584811470035386,train_acc:0.988936185836792
node2 epoch1:node_model train_loss:0.02396254727260539,train_acc:0.9914892911911011
node2_model on test-dataset: loss:0.07917271435711883,acc:0.9775950312614441
node2 weight score:58227.13086740964
node10: train data size:1915
node10 epoch0:node_model train_loss:0.039784242335008456,train_acc:0.9851665496826172
node10 epoch1:node_model train_loss:0.025453822842246156,train_acc:0.9914999008178711
node10_model on test-dataset: loss:0.07468964973333642,acc:0.9771971702575684
node10 weight score:25639.42938328272
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06596355291549116,train_acc:0.9774999022483826
node11 epoch1:node_model train_loss:0.0302723650529515,train_acc:0.9918749332427979
node11_model on test-dataset: loss:0.058115240468177946,acc:0.9834968447685242
node11 weight score:27101.32466650327
node17: train data size:719
node17 epoch0:node_model train_loss:0.03746122040320188,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.028468594391597435,train_acc:0.9871710538864136
node17_model on test-dataset: loss:0.060784651473804845,acc:0.9813959002494812
node17 weight score:11828.643951505639
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04076216804151045,acc:0.9869969445466995
total cost energy:9.060212513921044 | all_enery_cp：7.738 | all_enery_tp: 1.3222125139210446
ef: 32.32120144747503
reward: 23.26098893355399
episode0,iteration37 selected nodes:[9, 16, 17, 7, 12],center node:12
################################################## episode0,iteration37 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04568316184009451,train_acc:0.9864864349365234
node7 epoch1:node_model train_loss:0.0230668680899701,train_acc:0.9935134649276733
node7_model on test-dataset: loss:0.0649136595867094,acc:0.9814971089363098
node7 weight score:56028.2692911778
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06903379270806909,train_acc:0.9781817197799683
node9 epoch1:node_model train_loss:0.02870244378308681,train_acc:0.9918181300163269
node9_model on test-dataset: loss:0.06279006958429818,acc:0.9786969423294067
node9 weight score:33842.93112061458
node12: train data size:1406
node12 epoch0:node_model train_loss:0.050173427843643974,train_acc:0.9819998741149902
node12 epoch1:node_model train_loss:0.01911923094497373,train_acc:0.9953334331512451
node12_model on test-dataset: loss:0.05914298220537603,acc:0.9823962450027466
node12 weight score:23772.896590124878
node16: train data size:920
node16 epoch0:node_model train_loss:0.04322273219004273,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.049135623360052706,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.08361990331803099,acc:0.9752970933914185
node16 weight score:11002.165315845565
node17: train data size:719
node17 epoch0:node_model train_loss:0.04496567964088172,train_acc:0.9837499260902405
node17 epoch1:node_model train_loss:0.03018110831908416,train_acc:0.9887499809265137
node17_model on test-dataset: loss:0.07182132913912938,acc:0.9785972237586975
node17 weight score:10010.953690472397
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04408445555382059,acc:0.9861969459056854
total cost energy:5.674320393249937 | all_enery_cp：4.4035 | all_enery_tp: 1.270820393249937
ef: 32.06427726619982
reward: 26.38995687294988
episode0,iteration38 selected nodes:[18, 19, 2, 17, 1],center node:17
################################################## episode0,iteration38 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.035264442400284,train_acc:0.9891412854194641
node1 epoch1:node_model train_loss:0.020358823907019485,train_acc:0.9919403791427612
node1_model on test-dataset: loss:0.08975936061513494,acc:0.9738959074020386
node1 weight score:74164.96679988067
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0355738430875948,train_acc:0.986595869064331
node2 epoch1:node_model train_loss:0.021636736648593176,train_acc:0.9919149875640869
node2_model on test-dataset: loss:0.0697159799002111,acc:0.9794931411743164
node2 weight score:66125.44220992929
node17: train data size:719
node17 epoch0:node_model train_loss:0.04830611977376975,train_acc:0.9809210896492004
node17 epoch1:node_model train_loss:0.024405491349170916,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.06697385049168587,acc:0.9805949330329895
node17 weight score:10735.533267409442
node18: train data size:801
node18 epoch0:node_model train_loss:0.021973974329865895,train_acc:0.992222249507904
node18 epoch1:node_model train_loss:0.4441009314679023,train_acc:0.8855555653572083
node18_model on test-dataset: loss:0.07827988283283048,acc:0.9793962836265564
node18 weight score:10232.514038256348
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06080962545720154,train_acc:0.9830226898193359
node19 epoch1:node_model train_loss:0.03299700690369154,train_acc:0.9898277521133423
node19_model on test-dataset: loss:0.05727178583787463,acc:0.9816977977752686
node19 weight score:100939.75446068498
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03933800760351005,acc:0.9875969499349594
total cost energy:11.040804390453678 | all_enery_cp：9.283999999999999 | all_enery_tp: 1.756804390453679
ef: 32.17328897177578
reward: 21.132484581322103
episode0,iteration39 selected nodes:[16, 14, 3, 6, 9],center node:14
################################################## episode0,iteration39 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.050167258105877986,train_acc:0.9813156127929688
node3 epoch1:node_model train_loss:0.026891304596670364,train_acc:0.9918420910835266
node3_model on test-dataset: loss:0.061986993774007716,acc:0.9820960164070129
node3 weight score:60690.15080349767
node6: train data size:3529
node6 epoch0:node_model train_loss:0.054765689362991705,train_acc:0.9814175367355347
node6 epoch1:node_model train_loss:0.031584990547142096,train_acc:0.990555465221405
node6_model on test-dataset: loss:0.060527533464628504,acc:0.9822961091995239
node6 weight score:58304.044424052096
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03133377530717884,train_acc:0.991363525390625
node9 epoch1:node_model train_loss:0.03129428297556429,train_acc:0.9899998903274536
node9_model on test-dataset: loss:0.06286377910138981,acc:0.9796968698501587
node9 weight score:33803.24934924283
node14: train data size:1540
node14 epoch0:node_model train_loss:0.060288706910796463,train_acc:0.9790624380111694
node14 epoch1:node_model train_loss:0.051693773741135374,train_acc:0.9859374761581421
node14_model on test-dataset: loss:0.06769780992515734,acc:0.9793980121612549
node14 weight score:22748.150962380205
node16: train data size:920
node16 epoch0:node_model train_loss:0.0616437032353133,train_acc:0.9809999465942383
node16 epoch1:node_model train_loss:0.023061421827878802,train_acc:0.9869999885559082
node16_model on test-dataset: loss:0.06872469755220663,acc:0.9786944389343262
node16 weight score:13386.744980596288
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041227913991024254,acc:0.9878979575634003
total cost energy:7.774827108283605 | all_enery_cp：5.938 | all_enery_tp: 1.8368271082836056
ef: 32.21695616258727
reward: 24.442129054303667
episode0,iteration40 selected nodes:[10, 4, 8, 5, 17],center node:8
################################################## episode0,iteration40 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05193853132470056,train_acc:0.984404444694519
node4 epoch1:node_model train_loss:0.033266365663473345,train_acc:0.9888325333595276
node4_model on test-dataset: loss:0.08662186182329606,acc:0.9734917879104614
node4 weight score:49617.959133315424
node5: train data size:4837
node5 epoch0:node_model train_loss:0.0334713360370726,train_acc:0.989183783531189
node5 epoch1:node_model train_loss:0.023172448237179493,train_acc:0.9925097227096558
node5_model on test-dataset: loss:0.07860356729623164,acc:0.9799968600273132
node5 weight score:61536.64733523987
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04555519848414089,train_acc:0.9873428344726562
node8 epoch1:node_model train_loss:0.03033473953584452,train_acc:0.9913042783737183
node8_model on test-dataset: loss:0.09479098001967941,acc:0.9752969145774841
node8 weight score:24158.416755735372
node10: train data size:1915
node10 epoch0:node_model train_loss:0.040245926818170116,train_acc:0.9859998822212219
node10 epoch1:node_model train_loss:0.023801147117046638,train_acc:0.9909998774528503
node10_model on test-dataset: loss:0.06953872634989239,acc:0.9795988202095032
node10 weight score:27538.611943572985
node17: train data size:719
node17 epoch0:node_model train_loss:0.04191981778058107,train_acc:0.9862499833106995
node17 epoch1:node_model train_loss:0.013573514751897164,train_acc:0.9949999451637268
node17_model on test-dataset: loss:0.07941781211280613,acc:0.9767979383468628
node17 weight score:9053.384635914204
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04735486054312787,acc:0.9857979553937912
total cost energy:7.947704249800546 | all_enery_cp：7.0295 | all_enery_tp: 0.9182042498005465
ef: 31.743354347269232
reward: 23.795650097468688
episode0,iteration41 selected nodes:[13, 11, 1, 19, 15],center node:15
################################################## episode0,iteration41 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03502786554632458,train_acc:0.9873136878013611
node1 epoch1:node_model train_loss:0.026179448227526676,train_acc:0.991791307926178
node1_model on test-dataset: loss:0.06796907280251617,acc:0.9790969491004944
node1 weight score:97941.60381357389
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06455470030778088,train_acc:0.9777082204818726
node11 epoch1:node_model train_loss:0.020969305202015676,train_acc:0.9937499165534973
node11_model on test-dataset: loss:0.06538539505148946,acc:0.9814927577972412
node11 weight score:24087.948061791543
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06801501242443919,train_acc:0.9872726202011108
node13 epoch1:node_model train_loss:0.031617544083432716,train_acc:0.9887662529945374
node13_model on test-dataset: loss:0.06211737670135335,acc:0.9811961054801941
node13 weight score:17000.073990197867
node15: train data size:1376
node15 epoch0:node_model train_loss:0.045067557000688145,train_acc:0.98499995470047
node15 epoch1:node_model train_loss:0.038024680955069404,train_acc:0.9904887676239014
node15_model on test-dataset: loss:0.06526613829941198,acc:0.9802950024604797
node15 weight score:21082.908164223303
node19: train data size:5781
node19 epoch0:node_model train_loss:0.05363588465441917,train_acc:0.9843106269836426
node19 epoch1:node_model train_loss:0.03723875340074301,train_acc:0.9889252185821533
node19_model on test-dataset: loss:0.06983018302969868,acc:0.9795988202095032
node19 weight score:82786.55087501847
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.044845657903497343,acc:0.9859969460964203
total cost energy:9.50159664627976 | all_enery_cp：8.2225 | all_enery_tp: 1.2790966462797588
ef: 32.42699254896536
reward: 22.9253959026856
episode0,iteration42 selected nodes:[10, 1, 5, 16, 9],center node:5
################################################## episode0,iteration42 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.027093514231895443,train_acc:0.9904845356941223
node1 epoch1:node_model train_loss:0.0145753234422439,train_acc:0.995671808719635
node1_model on test-dataset: loss:0.06148953220155818,acc:0.9816968441009521
node1 weight score:108262.32956495491
node5: train data size:4837
node5 epoch0:node_model train_loss:0.030270412203152568,train_acc:0.9908164739608765
node5 epoch1:node_model train_loss:0.02057311549421628,train_acc:0.9916932582855225
node5_model on test-dataset: loss:0.0902828228330327,acc:0.9754876494407654
node5 weight score:53576.08289392385
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04088309989310801,train_acc:0.9890908598899841
node9 epoch1:node_model train_loss:0.01458710203455253,train_acc:0.9954545497894287
node9_model on test-dataset: loss:0.07567449726499036,acc:0.9782879948616028
node9 weight score:28080.794412929634
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06046010470890906,train_acc:0.9786664843559265
node10 epoch1:node_model train_loss:0.022657887387322263,train_acc:0.9909999966621399
node10_model on test-dataset: loss:0.05770799554702535,acc:0.9812998175621033
node10 weight score:33184.30976240539
node16: train data size:920
node16 epoch0:node_model train_loss:0.031123159313574435,train_acc:0.9899999499320984
node16 epoch1:node_model train_loss:0.04330795453279279,train_acc:0.9860000014305115
node16_model on test-dataset: loss:0.10242432934479438,acc:0.9729949235916138
node16 weight score:8982.24089808754
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04254483821503527,acc:0.9869969433546066
total cost energy:9.758375520796337 | all_enery_cp：8.227 | all_enery_tp: 1.531375520796336
ef: 31.853844977961717
reward: 22.09546945716538
episode0,iteration43 selected nodes:[7, 19, 15, 18, 10],center node:15
################################################## episode0,iteration43 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04094261176273781,train_acc:0.9889991879463196
node7 epoch1:node_model train_loss:0.034449899066522766,train_acc:0.9889188408851624
node7_model on test-dataset: loss:0.06292076032987097,acc:0.9812968969345093
node7 weight score:57802.86158229039
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04041325307662191,train_acc:0.9874998927116394
node10 epoch1:node_model train_loss:0.016475523777626223,train_acc:0.9959999322891235
node10_model on test-dataset: loss:0.06374147999566049,acc:0.9829952120780945
node10 weight score:30043.230877763945
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06435712204880215,train_acc:0.9812029004096985
node15 epoch1:node_model train_loss:0.03462026030839687,train_acc:0.98906010389328
node15_model on test-dataset: loss:0.06237541087675709,acc:0.9807971119880676
node15 weight score:22059.974926958566
node18: train data size:801
node18 epoch0:node_model train_loss:0.012271597050124051,train_acc:0.995555579662323
node18 epoch1:node_model train_loss:0.021322697879643075,train_acc:0.9944443106651306
node18_model on test-dataset: loss:0.06918889585685975,acc:0.9795960187911987
node18 weight score:11577.00220649763
node19: train data size:5781
node19 epoch0:node_model train_loss:0.044436723039011826,train_acc:0.9847874641418457
node19 epoch1:node_model train_loss:0.02811161879698971,train_acc:0.9905685782432556
node19_model on test-dataset: loss:0.06120227326173335,acc:0.9812979698181152
node19 weight score:94457.27571715157
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.044500476490866275,acc:0.9857969450950622
total cost energy:7.832338716549054 | all_enery_cp：6.754999999999999 | all_enery_tp: 1.0773387165490547
ef: 32.53698802615651
reward: 24.704649309607454
episode0,iteration44 selected nodes:[6, 12, 9, 18, 11],center node:12
################################################## episode0,iteration44 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04231684762311892,train_acc:0.9858331680297852
node6 epoch1:node_model train_loss:0.029244398255185742,train_acc:0.991388738155365
node6_model on test-dataset: loss:0.06006529909645906,acc:0.9814988970756531
node6 weight score:58752.72500238061
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03817580242387273,train_acc:0.9854544401168823
node9 epoch1:node_model train_loss:0.018576279538551302,train_acc:0.9940909147262573
node9_model on test-dataset: loss:0.04983529407400056,acc:0.9844968914985657
node9 weight score:42640.462738006165
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06348589451226871,train_acc:0.979166567325592
node11 epoch1:node_model train_loss:0.028474297985667363,train_acc:0.9906249046325684
node11_model on test-dataset: loss:0.09736418180167675,acc:0.9737939238548279
node11 weight score:16176.379967000106
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06287151386107628,train_acc:0.9833332300186157
node12 epoch1:node_model train_loss:0.03547473554499447,train_acc:0.9906665682792664
node12_model on test-dataset: loss:0.06788271194400294,acc:0.9784958958625793
node12 weight score:20712.195487413966
node18: train data size:801
node18 epoch0:node_model train_loss:0.01770037570450869,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.4763056102157053,train_acc:0.8822221755981445
node18_model on test-dataset: loss:0.0829061779506344,acc:0.9777927994728088
node18 weight score:9661.523662047803
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03958993834574358,acc:0.9876969438791275
total cost energy:5.900842712474619 | all_enery_cp：4.718 | all_enery_tp: 1.182842712474619
ef: 32.13370398247536
reward: 26.23286127000074
episode0,iteration45 selected nodes:[10, 15, 2, 6, 8],center node:6
################################################## episode0,iteration45 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03873761455984192,train_acc:0.9870213866233826
node2 epoch1:node_model train_loss:0.025045619315549512,train_acc:0.990425705909729
node2_model on test-dataset: loss:0.12267077733106817,acc:0.9704931378364563
node2 weight score:37580.26239255313
node6: train data size:3529
node6 epoch0:node_model train_loss:0.028253845737910725,train_acc:0.9901531934738159
node6 epoch1:node_model train_loss:0.02280264814503931,train_acc:0.9922221302986145
node6_model on test-dataset: loss:0.0583761910334033,acc:0.9821958541870117
node6 weight score:60452.72803051983
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04923317518652133,train_acc:0.9842510223388672
node8 epoch1:node_model train_loss:0.023855424131793174,train_acc:0.9920772314071655
node8_model on test-dataset: loss:0.06012924985217978,acc:0.9820960760116577
node8 weight score:38084.626128376425
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0343500343238702,train_acc:0.9869999289512634
node10 epoch1:node_model train_loss:0.022975437977584078,train_acc:0.9944999814033508
node10_model on test-dataset: loss:0.06881396435546776,acc:0.9794999957084656
node10 weight score:27828.6539358176
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04882739956623742,train_acc:0.9828947186470032
node15 epoch1:node_model train_loss:0.03537237091638547,train_acc:0.9876314997673035
node15_model on test-dataset: loss:0.05459943002057116,acc:0.9842970967292786
node15 weight score:25201.728286935802
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03943757890658162,acc:0.9884979557991028
total cost energy:8.053010659580075 | all_enery_cp：6.859999999999999 | all_enery_tp: 1.193010659580075
ef: 32.24557947222296
reward: 24.192568812642882
episode0,iteration46 selected nodes:[9, 4, 2, 0, 18],center node:4
################################################## episode0,iteration46 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04183282973341799,train_acc:0.9865279793739319
node0 epoch1:node_model train_loss:0.02437395998761834,train_acc:0.9920727610588074
node0_model on test-dataset: loss:0.07273675696676946,acc:0.9792958498001099
node0 weight score:98505.90401319934
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03689792022193247,train_acc:0.9870213270187378
node2 epoch1:node_model train_loss:0.022076147326089916,train_acc:0.9929787516593933
node2_model on test-dataset: loss:0.05565489159984281,acc:0.9830942153930664
node2 weight score:82831.89253418689
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04363882839463131,train_acc:0.9853439927101135
node4 epoch1:node_model train_loss:0.024054533374214242,train_acc:0.9930233359336853
node4_model on test-dataset: loss:0.08200418560652906,acc:0.9778950810432434
node4 weight score:52411.95883125506
node9: train data size:2125
node9 epoch0:node_model train_loss:0.025866406234192917,train_acc:0.991363525390625
node9 epoch1:node_model train_loss:0.015856308037076484,train_acc:0.9963635802268982
node9_model on test-dataset: loss:0.058816451556995164,acc:0.9832949638366699
node9 weight score:36129.347210632084
node18: train data size:801
node18 epoch0:node_model train_loss:0.014225594802863069,train_acc:0.9955554604530334
node18 epoch1:node_model train_loss:0.018198031647544768,train_acc:0.9944443106651306
node18_model on test-dataset: loss:0.09692020229363835,acc:0.9764928817749023
node18 weight score:8264.530830973887
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039122040947258935,acc:0.9876959371566773
total cost energy:11.650646791745519 | all_enery_cp：9.4995 | all_enery_tp: 2.1511467917455205
ef: 31.988098958193298
reward: 20.33745216644778
episode0,iteration47 selected nodes:[4, 11, 3, 10, 16],center node:10
################################################## episode0,iteration47 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05193182371409708,train_acc:0.9854668378829956
node3 epoch1:node_model train_loss:0.022710166010687028,train_acc:0.9921053051948547
node3_model on test-dataset: loss:0.057848192440360435,acc:0.9825968146324158
node3 weight score:65032.28262280618
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04102800126176674,train_acc:0.9881299734115601
node4 epoch1:node_model train_loss:0.02474759462110828,train_acc:0.991855800151825
node4_model on test-dataset: loss:0.056779664731875526,acc:0.9826989769935608
node4 weight score:75696.11445041074
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0359804839885328,train_acc:0.9884998202323914
node10 epoch1:node_model train_loss:0.01376299902849496,train_acc:0.9959999322891235
node10_model on test-dataset: loss:0.060859618851377494,acc:0.98419588804245
node10 weight score:31465.855950832265
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04841118429612834,train_acc:0.9862498044967651
node11 epoch1:node_model train_loss:0.037563190708169714,train_acc:0.9908332228660583
node11_model on test-dataset: loss:0.07298328538774512,acc:0.9790979027748108
node11 weight score:21580.283644842108
node16: train data size:920
node16 epoch0:node_model train_loss:0.04588646786287427,train_acc:0.984000027179718
node16 epoch1:node_model train_loss:0.02490163270267658,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.06470888827687304,acc:0.982094943523407
node16 weight score:14217.52134055451
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0432539162725152,acc:0.986496946811676
total cost energy:7.729721147617607 | all_enery_cp：6.235 | all_enery_tp: 1.4947211476176059
ef: 32.406749634911584
reward: 24.677028487293978
episode0,iteration48 selected nodes:[11, 7, 15, 6, 3],center node:6
################################################## episode0,iteration48 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03440839869558419,train_acc:0.9893124103546143
node3 epoch1:node_model train_loss:0.024753477369239062,train_acc:0.9905263185501099
node3_model on test-dataset: loss:0.06616316964942598,acc:0.98089599609375
node3 weight score:56859.42828817661
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0308697069623223,train_acc:0.9908331632614136
node6 epoch1:node_model train_loss:0.02364473081575448,train_acc:0.9918200373649597
node6_model on test-dataset: loss:0.059786766727047504,acc:0.9814991354942322
node6 weight score:59026.4400165912
node7: train data size:3637
node7 epoch0:node_model train_loss:0.036633169347722386,train_acc:0.988728940486908
node7 epoch1:node_model train_loss:0.025228013345858436,train_acc:0.9937838315963745
node7_model on test-dataset: loss:0.07769741404001251,acc:0.9780920743942261
node7 weight score:46809.79470085095
node11: train data size:1575
node11 epoch0:node_model train_loss:0.051780866604531184,train_acc:0.9812498688697815
node11 epoch1:node_model train_loss:0.021617606762447394,train_acc:0.9918748736381531
node11_model on test-dataset: loss:0.07296313674974954,acc:0.9795939326286316
node11 weight score:21586.243001064595
node15: train data size:1376
node15 epoch0:node_model train_loss:0.035422678770763535,train_acc:0.9881202578544617
node15 epoch1:node_model train_loss:0.031992894322944006,train_acc:0.9881202578544617
node15_model on test-dataset: loss:0.05710919435437972,acc:0.984096109867096
node15 weight score:24094.193860650637
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04032192187885812,acc:0.9869969439506531
total cost energy:8.18761081679664 | all_enery_cp：6.939500000000001 | all_enery_tp: 1.248110816796639
ef: 32.363025937157445
reward: 24.175415120360803
episode0,iteration49 selected nodes:[10, 8, 9, 14, 19],center node:10
################################################## episode0,iteration49 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04818167899856749,train_acc:0.9834780693054199
node8 epoch1:node_model train_loss:0.02519383987047426,train_acc:0.9938647150993347
node8_model on test-dataset: loss:0.06830567317367241,acc:0.9804940819740295
node8 weight score:33525.7657761091
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0242724612894976,train_acc:0.9886361956596375
node9 epoch1:node_model train_loss:0.017897279709524646,train_acc:0.9940907955169678
node9_model on test-dataset: loss:0.06291024109837963,acc:0.9820950627326965
node9 weight score:33778.284153718385
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04569797515869141,train_acc:0.9866665005683899
node10 epoch1:node_model train_loss:0.018832296546315776,train_acc:0.9891666769981384
node10_model on test-dataset: loss:0.05394412422247115,acc:0.9848968386650085
node10 weight score:35499.695798236375
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05761798849562183,train_acc:0.9812499284744263
node14 epoch1:node_model train_loss:0.03132591542089358,train_acc:0.9906249642372131
node14_model on test-dataset: loss:0.10209266158621176,acc:0.971697986125946
node14 weight score:15084.335897145289
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04727775901796874,train_acc:0.9865113496780396
node19 epoch1:node_model train_loss:0.028418767093359654,train_acc:0.9903451204299927
node19_model on test-dataset: loss:0.060982306116638935,acc:0.982298731803894
node19 weight score:94797.98925515974
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03686281726106245,acc:0.9890969473123551
total cost energy:7.873370866461908 | all_enery_cp：6.8255 | all_enery_tp: 1.0478708664619076
ef: 32.52716708384248
reward: 24.65379621738057
episode0,iteration50 selected nodes:[15, 9, 3, 18, 5],center node:5
################################################## episode0,iteration50 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.032968546095377714,train_acc:0.9874702095985413
node3 epoch1:node_model train_loss:0.018618602281142222,train_acc:0.9934210777282715
node3_model on test-dataset: loss:0.08218287187933583,acc:0.9775968790054321
node3 weight score:45775.96175421465
node5: train data size:4837
node5 epoch0:node_model train_loss:0.032090351335960915,train_acc:0.9898566007614136
node5 epoch1:node_model train_loss:0.023680597504515354,train_acc:0.9918367862701416
node5_model on test-dataset: loss:0.06091122118550175,acc:0.984196126461029
node5 weight score:79410.6554729741
node9: train data size:2125
node9 epoch0:node_model train_loss:0.027820475584700365,train_acc:0.991363525390625
node9 epoch1:node_model train_loss:0.018385951132090253,train_acc:0.9931817650794983
node9_model on test-dataset: loss:0.05504478219718294,acc:0.9848971366882324
node9 weight score:38604.93066150696
node15: train data size:1376
node15 epoch0:node_model train_loss:0.043912213728097935,train_acc:0.9816916584968567
node15 epoch1:node_model train_loss:0.02092161767983011,train_acc:0.9942857027053833
node15_model on test-dataset: loss:0.06880476333826664,acc:0.9815958738327026
node15 weight score:19998.61540450529
node18: train data size:801
node18 epoch0:node_model train_loss:0.02753111295815971,train_acc:0.992222249507904
node18 epoch1:node_model train_loss:0.0182444421237354,train_acc:0.9955554604530334
node18_model on test-dataset: loss:0.09647959790703681,acc:0.9736960530281067
node18 weight score:8302.273406775657
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045952932608911395,acc:0.9861979550123214
total cost energy:8.29997365172289 | all_enery_cp：6.4505 | all_enery_tp: 1.849473651722891
ef: 31.775603574348295
reward: 23.475629922625405
episode0,iteration51 selected nodes:[10, 8, 9, 11, 18],center node:10
################################################## episode0,iteration51 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04140480964080147,train_acc:0.9852172136306763
node8 epoch1:node_model train_loss:0.0225419561333104,train_acc:0.9921255707740784
node8_model on test-dataset: loss:0.046111580553024396,acc:0.9856969118118286
node8 weight score:49662.14500859919
node9: train data size:2125
node9 epoch0:node_model train_loss:0.013678746652493084,train_acc:0.9954545497894287
node9 epoch1:node_model train_loss:0.011737031920347363,train_acc:0.9945453405380249
node9_model on test-dataset: loss:0.07753691070118293,acc:0.9805981516838074
node9 weight score:27406.302118348136
node10: train data size:1915
node10 epoch0:node_model train_loss:0.03425494760158472,train_acc:0.9879999160766602
node10 epoch1:node_model train_loss:0.01729846637463197,train_acc:0.9950000047683716
node10_model on test-dataset: loss:0.05025396756095688,acc:0.9851970672607422
node10 weight score:38106.44398727623
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05392035789554939,train_acc:0.982916533946991
node11 epoch1:node_model train_loss:0.025004443843499757,train_acc:0.9910415410995483
node11_model on test-dataset: loss:0.07039527665110654,acc:0.979897141456604
node11 weight score:22373.660207431585
node18: train data size:801
node18 epoch0:node_model train_loss:0.024743674038391974,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.008558740684078556,train_acc:0.9977777600288391
node18_model on test-dataset: loss:0.07013468360407388,acc:0.9818958640098572
node18 weight score:11420.882776371043
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04152402250048908,acc:0.9874969488382339
total cost energy:5.49151648071345 | all_enery_cp：4.353 | all_enery_tp: 1.1385164807134505
ef: 32.57702212999841
reward: 27.08550564928496
episode0,iteration52 selected nodes:[7, 13, 5, 18, 2],center node:7
################################################## episode0,iteration52 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04327615042851168,train_acc:0.9859574437141418
node2 epoch1:node_model train_loss:0.01963700978778937,train_acc:0.9931915402412415
node2_model on test-dataset: loss:0.05241367002017796,acc:0.9848994016647339
node2 weight score:87954.15391109351
node5: train data size:4837
node5 epoch0:node_model train_loss:0.025297071480629395,train_acc:0.991224467754364
node5 epoch1:node_model train_loss:0.01897928505727299,train_acc:0.9943464398384094
node5_model on test-dataset: loss:0.06561096909533944,acc:0.982895016670227
node5 weight score:73722.42883611954
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03200217159276174,train_acc:0.9910810589790344
node7 epoch1:node_model train_loss:0.024688125875891764,train_acc:0.9932431578636169
node7_model on test-dataset: loss:0.06871154963431764,acc:0.9811961054801941
node7 weight score:52931.421563857715
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04896281634203412,train_acc:0.9883766174316406
node13 epoch1:node_model train_loss:0.01896726993039589,train_acc:0.993636429309845
node13_model on test-dataset: loss:0.06366673003532924,acc:0.9822949767112732
node13 weight score:16586.370925819752
node18: train data size:801
node18 epoch0:node_model train_loss:0.017705964312578242,train_acc:0.9944444298744202
node18 epoch1:node_model train_loss:0.009203576010678362,train_acc:0.9988888502120972
node18_model on test-dataset: loss:0.07129087540879482,acc:0.9815971255302429
node18 weight score:11235.659478256097
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04164126558282078,acc:0.9883969438076019
total cost energy:8.689260072786364 | all_enery_cp：7.4704999999999995 | all_enery_tp: 1.2187600727863641
ef: 32.343807346334856
reward: 23.65454727354849
episode0,iteration53 selected nodes:[5, 16, 17, 0, 13],center node:16
################################################## episode0,iteration53 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03363859666973844,train_acc:0.9883975982666016
node0 epoch1:node_model train_loss:0.019781994609931845,train_acc:0.9940921068191528
node0_model on test-dataset: loss:0.06286423189314519,acc:0.9819968342781067
node0 weight score:113975.7821614501
node5: train data size:4837
node5 epoch0:node_model train_loss:0.01651337132218997,train_acc:0.9942857027053833
node5 epoch1:node_model train_loss:0.02561695963068276,train_acc:0.9904689192771912
node5_model on test-dataset: loss:0.0610133034226601,acc:0.9826979041099548
node5 weight score:79277.79236099446
node13: train data size:1056
node13 epoch0:node_model train_loss:0.03852451787414876,train_acc:0.9881818294525146
node13 epoch1:node_model train_loss:0.0339783751829104,train_acc:0.9918181300163269
node13_model on test-dataset: loss:0.05931026505146292,acc:0.9828950762748718
node13 weight score:17804.675111192293
node16: train data size:920
node16 epoch0:node_model train_loss:0.04023572504520416,train_acc:0.9860000014305115
node16 epoch1:node_model train_loss:0.05055750478059053,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.07734738431033293,acc:0.9787949919700623
node16 weight score:11894.39058868208
node17: train data size:719
node17 epoch0:node_model train_loss:0.03353967335715424,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.01800553754583234,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.07824817261018324,acc:0.978198230266571
node17 weight score:9188.712988633157
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04338197100714751,acc:0.986496948003769
total cost energy:9.087174089844812 | all_enery_cp：7.3485 | all_enery_tp: 1.7386740898448128
ef: 32.18677123810899
reward: 23.099597148264177
episode0,iteration54 selected nodes:[8, 10, 9, 16, 19],center node:10
################################################## episode0,iteration54 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.042102148027523704,train_acc:0.9890820384025574
node8 epoch1:node_model train_loss:0.025399910974437775,train_acc:0.9913042783737183
node8_model on test-dataset: loss:0.07198705132643227,acc:0.9784980416297913
node8 weight score:31811.276581058624
node9: train data size:2125
node9 epoch0:node_model train_loss:0.027368998571711763,train_acc:0.9899999499320984
node9 epoch1:node_model train_loss:0.018282233754606834,train_acc:0.9936363101005554
node9_model on test-dataset: loss:0.06304240345487415,acc:0.9818960428237915
node9 weight score:33707.47121849627
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0308168859424768,train_acc:0.9884999394416809
node10 epoch1:node_model train_loss:0.012349433406052412,train_acc:0.9959999322891235
node10_model on test-dataset: loss:0.062476653288576926,acc:0.9827969670295715
node10 weight score:30651.44976883923
node16: train data size:920
node16 epoch0:node_model train_loss:0.03886132659390569,train_acc:0.9899999499320984
node16 epoch1:node_model train_loss:0.018811589006509165,train_acc:0.9909999966621399
node16_model on test-dataset: loss:0.07603356977506337,acc:0.9797958135604858
node16 weight score:12099.918532323485
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04046711875005336,train_acc:0.9886208176612854
node19 epoch1:node_model train_loss:0.02287262686375721,train_acc:0.9934484362602234
node19_model on test-dataset: loss:0.05357410752749274,acc:0.9824957847595215
node19 weight score:107906.60389505046
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04394375549356482,acc:0.9869969469308854
total cost energy:7.849666020071206 | all_enery_cp：6.515499999999999 | all_enery_tp: 1.334166020071207
ef: 32.39615980870675
reward: 24.546493788635544
episode0,iteration55 selected nodes:[6, 18, 15, 16, 12],center node:15
################################################## episode0,iteration55 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03243744586911311,train_acc:0.9894443154335022
node6 epoch1:node_model train_loss:0.01747916948968648,train_acc:0.9950000047683716
node6_model on test-dataset: loss:0.05620264608412981,acc:0.9843969345092773
node6 weight score:62790.63791262489
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04361588582396507,train_acc:0.987333357334137
node12 epoch1:node_model train_loss:0.03391177996139352,train_acc:0.9906665682792664
node12_model on test-dataset: loss:0.06825695614643337,acc:0.9789968132972717
node12 weight score:20598.63315591854
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04521784671149882,train_acc:0.9852631092071533
node15 epoch1:node_model train_loss:0.023495476060946072,train_acc:0.9904887080192566
node15_model on test-dataset: loss:0.06787246526429953,acc:0.9803928136825562
node15 weight score:20273.316942913032
node16: train data size:920
node16 epoch0:node_model train_loss:0.05967639092123136,train_acc:0.9859998822212219
node16 epoch1:node_model train_loss:0.02766060350859334,train_acc:0.9909999966621399
node16_model on test-dataset: loss:0.07653259211888326,acc:0.9780972599983215
node16 weight score:12021.022345236937
node18: train data size:801
node18 epoch0:node_model train_loss:0.023140456883185025,train_acc:0.9911110401153564
node18 epoch1:node_model train_loss:0.11175720620667562,train_acc:0.8888888955116272
node18_model on test-dataset: loss:0.059752990276028865,acc:0.9822958707809448
node18 weight score:13405.18685842803
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0411043151103695,acc:0.9887959378957748
total cost energy:4.715070478491457 | all_enery_cp：4.016 | all_enery_tp: 0.6990704784914571
ef: 32.46942760221266
reward: 27.754357123721206
episode0,iteration56 selected nodes:[13, 19, 7, 5, 16],center node:16
################################################## episode0,iteration56 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.024388181840364193,train_acc:0.9922449588775635
node5 epoch1:node_model train_loss:0.015175516346507534,train_acc:0.9953061938285828
node5_model on test-dataset: loss:0.061053917106837614,acc:0.9823969602584839
node5 weight score:79225.05597037754
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03026670145462393,train_acc:0.9918918609619141
node7 epoch1:node_model train_loss:0.018880330398993415,train_acc:0.993783712387085
node7_model on test-dataset: loss:0.05512727357521726,acc:0.9837948679924011
node7 weight score:65974.60320684228
node13: train data size:1056
node13 epoch0:node_model train_loss:0.055105895692990584,train_acc:0.9865584373474121
node13 epoch1:node_model train_loss:0.02200751823627136,train_acc:0.9947402477264404
node13_model on test-dataset: loss:0.059175775639887435,acc:0.980894923210144
node13 weight score:17845.13998475084
node16: train data size:920
node16 epoch0:node_model train_loss:0.03712110442575067,train_acc:0.9889999628067017
node16 epoch1:node_model train_loss:0.024537987826624884,train_acc:0.9919999241828918
node16_model on test-dataset: loss:0.07953332876784316,acc:0.9792919158935547
node16 weight score:11567.477612881878
node19: train data size:5781
node19 epoch0:node_model train_loss:0.030889372222912336,train_acc:0.9913794994354248
node19 epoch1:node_model train_loss:0.022851944359115743,train_acc:0.9913795590400696
node19_model on test-dataset: loss:0.07051953393813165,acc:0.9784950613975525
node19 weight score:81977.28596833607
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0427900729119392,acc:0.9865959352254867
total cost energy:9.355991834728767 | all_enery_cp：8.1155 | all_enery_tp: 1.2404918347287666
ef: 32.602756722664196
reward: 23.246764887935427
episode0,iteration57 selected nodes:[3, 2, 19, 15, 5],center node:5
################################################## episode0,iteration57 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0338553763490091,train_acc:0.9878724217414856
node2 epoch1:node_model train_loss:0.0158736325795465,train_acc:0.9948937892913818
node2_model on test-dataset: loss:0.06534852797831263,acc:0.9819962382316589
node2 weight score:70544.81780415821
node3: train data size:3762
node3 epoch0:node_model train_loss:0.036302217600965185,train_acc:0.987105131149292
node3 epoch1:node_model train_loss:0.019841023141787827,train_acc:0.9934209585189819
node3_model on test-dataset: loss:0.061093396726018906,acc:0.9824958443641663
node3 weight score:61577.84967942062
node5: train data size:4837
node5 epoch0:node_model train_loss:0.018364707626906052,train_acc:0.9942858219146729
node5 epoch1:node_model train_loss:0.014192075909908898,train_acc:0.9953062534332275
node5_model on test-dataset: loss:0.10200727522980742,acc:0.975597083568573
node5 weight score:47418.18648819851
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04554102621373853,train_acc:0.9842856526374817
node15 epoch1:node_model train_loss:0.020453656907193363,train_acc:0.9942856431007385
node15_model on test-dataset: loss:0.048486664429165105,acc:0.9856977462768555
node15 weight score:28378.93709950329
node19: train data size:5781
node19 epoch0:node_model train_loss:0.027971316024596835,train_acc:0.991683840751648
node19 epoch1:node_model train_loss:0.017926433394614864,train_acc:0.9931036233901978
node19_model on test-dataset: loss:0.059415820219310266,acc:0.9835979342460632
node19 weight score:97297.31877236902
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043752577906434456,acc:0.986897953748703
total cost energy:11.840933824461764 | all_enery_cp：10.183 | all_enery_tp: 1.657933824461764
ef: 32.403163120273284
reward: 20.56222929581152
episode0,iteration58 selected nodes:[5, 9, 14, 8, 2],center node:8
################################################## episode0,iteration58 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.027879879459121976,train_acc:0.9912766218185425
node2 epoch1:node_model train_loss:0.016118577191283547,train_acc:0.9934043288230896
node2_model on test-dataset: loss:0.05507684961616178,acc:0.9834973216056824
node2 weight score:83701.22895786035
node5: train data size:4837
node5 epoch0:node_model train_loss:0.0164557572187528,train_acc:0.9948980212211609
node5 epoch1:node_model train_loss:0.015754065801845674,train_acc:0.9944900274276733
node5_model on test-dataset: loss:0.0686057974072122,acc:0.9813961386680603
node5 weight score:70504.24574602363
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03416487783112604,train_acc:0.9877777099609375
node8 epoch1:node_model train_loss:0.019988009397385886,train_acc:0.9938647150993347
node8_model on test-dataset: loss:0.06187759087362792,acc:0.9822970628738403
node8 weight score:37008.55136194373
node9: train data size:2125
node9 epoch0:node_model train_loss:0.021134762627876957,train_acc:0.992272675037384
node9 epoch1:node_model train_loss:0.01798776915529743,train_acc:0.9945454597473145
node9_model on test-dataset: loss:0.0857556114302497,acc:0.979496955871582
node9 weight score:24779.71953740185
node14: train data size:1540
node14 epoch0:node_model train_loss:0.045668695915082935,train_acc:0.981874942779541
node14 epoch1:node_model train_loss:0.027398931517382152,train_acc:0.9912499189376831
node14_model on test-dataset: loss:0.05843898207164784,acc:0.9827970862388611
node14 weight score:26352.27283924824
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04172848491079094,acc:0.9875969451665878
total cost energy:8.865755903440693 | all_enery_cp：7.700999999999999 | all_enery_tp: 1.1647559034406951
ef: 32.550614809723804
reward: 23.68485890628311
episode0,iteration59 selected nodes:[5, 11, 13, 18, 6],center node:11
################################################## episode0,iteration59 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.016898931224167593,train_acc:0.9944900274276733
node5 epoch1:node_model train_loss:0.01080472186345569,train_acc:0.9963265657424927
node5_model on test-dataset: loss:0.07392449701294027,acc:0.9790971279144287
node5 weight score:65431.6254482367
node6: train data size:3529
node6 epoch0:node_model train_loss:0.031059608405080832,train_acc:0.9902777075767517
node6 epoch1:node_model train_loss:0.018813904656174902,train_acc:0.9944443106651306
node6_model on test-dataset: loss:0.08344971541693667,acc:0.979197084903717
node6 weight score:42288.93990073172
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05462084505415987,train_acc:0.983124852180481
node11 epoch1:node_model train_loss:0.01912594799796352,train_acc:0.9920833110809326
node11_model on test-dataset: loss:0.05404286491466337,acc:0.9854958057403564
node11 weight score:29143.532684416547
node13: train data size:1056
node13 epoch0:node_model train_loss:0.060111248696392235,train_acc:0.9849351048469543
node13 epoch1:node_model train_loss:0.0264289659998295,train_acc:0.9938311576843262
node13_model on test-dataset: loss:0.07125945252475503,acc:0.9780949354171753
node13 weight score:14819.086627604289
node18: train data size:801
node18 epoch0:node_model train_loss:0.008099000304961615,train_acc:0.9966666102409363
node18 epoch1:node_model train_loss:0.009826445988235517,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.12883093642828045,acc:0.9705857634544373
node18 weight score:6217.450731997999
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04165746677605057,acc:0.9874979543685913
total cost energy:7.106768723046357 | all_enery_cp：5.899 | all_enery_tp: 1.207768723046357
ef: 31.842866336654833
reward: 24.736097613608475
episode0_cost time: 13369.142199516296
episode1,iteration0 selected nodes:[17, 13, 8, 12, 0],center node:12
################################################## episode1,iteration0 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.7120042737159464,train_acc:0.7767627835273743
node0 epoch1:node_model train_loss:0.24694728716793987,train_acc:0.9215918779373169
node0_model on test-dataset: loss:0.2652420546999201,acc:0.9178938269615173
node0 weight score:27013.06174130673
node8: train data size:2290
node8 epoch0:node_model train_loss:1.4683577703393025,train_acc:0.537004828453064
node8 epoch1:node_model train_loss:0.4715636100458062,train_acc:0.8557488918304443
node8_model on test-dataset: loss:0.4614957838691771,acc:0.8431876301765442
node8 weight score:4962.125505894458
node12: train data size:1406
node12 epoch0:node_model train_loss:1.540633314847946,train_acc:0.5366666316986084
node12 epoch1:node_model train_loss:0.5769816199938457,train_acc:0.8122222423553467
node12_model on test-dataset: loss:0.6410010321065783,acc:0.7799937129020691
node12 weight score:2193.4442061338623
node13: train data size:1056
node13 epoch0:node_model train_loss:1.930846561085094,train_acc:0.3633766174316406
node13 epoch1:node_model train_loss:0.8044466918165033,train_acc:0.7592856884002686
node13_model on test-dataset: loss:0.8826904361881316,acc:0.6953675746917725
node13 weight score:1196.342405793247
node17: train data size:719
node17 epoch0:node_model train_loss:2.1832446604967117,train_acc:0.32269737124443054
node17 epoch1:node_model train_loss:1.0731404423713684,train_acc:0.703026294708252
node17_model on test-dataset: loss:1.0869458938017487,acc:0.6389303207397461
node17 weight score:661.4864678178182
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.30695455882698297,acc:0.9182939165830613
total cost energy:7.740929753251347 | all_enery_cp：6.318 | all_enery_tp: 1.4229297532513472
ef: 26.257713584510725
reward: 18.516783831259378
step 61:loss:454.04974365234375|running q:0.019152499735355377
episode1,iteration1 selected nodes:[8, 3, 17, 18, 13],center node:17
################################################## episode1,iteration1 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.29710922507863297,train_acc:0.9027759432792664
node3 epoch1:node_model train_loss:0.17911242850516973,train_acc:0.9439898133277893
node3_model on test-dataset: loss:0.17407954461639746,acc:0.9456990361213684
node3 weight score:21610.810209148705
node8: train data size:2290
node8 epoch0:node_model train_loss:0.33635991941327636,train_acc:0.8871014714241028
node8 epoch1:node_model train_loss:0.21302554076132568,train_acc:0.9354106187820435
node8_model on test-dataset: loss:0.2505251260334626,acc:0.9204849600791931
node8 weight score:9140.799712417369
node13: train data size:1056
node13 epoch0:node_model train_loss:0.44937267899513245,train_acc:0.8486363887786865
node13 epoch1:node_model train_loss:0.26873948221856897,train_acc:0.905519425868988
node13_model on test-dataset: loss:0.4576542029646225,acc:0.8441847562789917
node13 weight score:2307.4189926791314
node17: train data size:719
node17 epoch0:node_model train_loss:0.4155552200973034,train_acc:0.8552631735801697
node17 epoch1:node_model train_loss:0.28210987336933613,train_acc:0.8952631950378418
node17_model on test-dataset: loss:0.37693000052357095,acc:0.8772969841957092
node17 weight score:1907.51598175067
node18: train data size:801
node18 epoch0:node_model train_loss:0.4733290473620097,train_acc:0.8555555939674377
node18 epoch1:node_model train_loss:0.35848251978556317,train_acc:0.8899999260902405
node18_model on test-dataset: loss:0.316919601354748,acc:0.9007958173751831
node18 weight score:2527.454902050664
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.1491008670674637,acc:0.9534979552030564
total cost energy:6.128875504232904 | all_enery_cp：4.314 | all_enery_tp: 1.8148755042329043
ef: 27.60552428391634
reward: 21.476648779683433
step 62:loss:548.5164184570312|running q:0.3045734167098999
episode1,iteration2 selected nodes:[9, 11, 4, 13, 0],center node:11
################################################## episode1,iteration2 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.2225698885611362,train_acc:0.9311006665229797
node0 epoch1:node_model train_loss:0.1406771764676604,train_acc:0.9561645984649658
node0_model on test-dataset: loss:0.15475633991416543,acc:0.9502959251403809
node0 weight score:46298.58785736351
node4: train data size:4298
node4 epoch0:node_model train_loss:0.23207587783419809,train_acc:0.9288133382797241
node4 epoch1:node_model train_loss:0.14193047531122385,train_acc:0.9544090628623962
node4_model on test-dataset: loss:0.159245095163933,acc:0.9478889107704163
node4 weight score:26989.842265317333
node9: train data size:2125
node9 epoch0:node_model train_loss:0.26081856712698936,train_acc:0.9204545617103577
node9 epoch1:node_model train_loss:0.15158496560020882,train_acc:0.9463635683059692
node9_model on test-dataset: loss:0.2511100806575268,acc:0.9194980263710022
node9 weight score:8462.424106733306
node11: train data size:1575
node11 epoch0:node_model train_loss:0.3049463275820017,train_acc:0.8981249332427979
node11 epoch1:node_model train_loss:0.2115172198973596,train_acc:0.9320833086967468
node11_model on test-dataset: loss:0.19038308694027364,acc:0.9440948367118835
node11 weight score:8272.793688307533
node13: train data size:1056
node13 epoch0:node_model train_loss:0.4058616960590536,train_acc:0.873376727104187
node13 epoch1:node_model train_loss:0.23764339902184226,train_acc:0.9207792282104492
node13_model on test-dataset: loss:0.22725269575603307,acc:0.9252963662147522
node13 weight score:4646.8095636307335
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.10534197834320366,acc:0.9669969487190246
total cost energy:10.0865329614269 | all_enery_cp：8.1095 | all_enery_tp: 1.9770329614269007
ef: 29.00930064835815
reward: 18.922767686931248
step 63:loss:571.1885986328125|running q:0.549564003944397
episode1,iteration3 selected nodes:[9, 6, 0, 14, 7],center node:6
################################################## episode1,iteration3 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.16452491516247392,train_acc:0.9492201209068298
node0 epoch1:node_model train_loss:0.1168726662484308,train_acc:0.9614529013633728
node0_model on test-dataset: loss:0.224930656675715,acc:0.9255899786949158
node0 weight score:31854.261690659
node6: train data size:3529
node6 epoch0:node_model train_loss:0.18624416097170776,train_acc:0.9420976638793945
node6 epoch1:node_model train_loss:0.12264776602387428,train_acc:0.9597509503364563
node6_model on test-dataset: loss:0.17003777999430894,acc:0.9466976523399353
node6 weight score:20754.2112118737
node7: train data size:3637
node7 epoch0:node_model train_loss:0.19309149601975004,train_acc:0.9350255131721497
node7 epoch1:node_model train_loss:0.13362513321477013,train_acc:0.9598098993301392
node7_model on test-dataset: loss:0.21576172129483895,acc:0.9363940358161926
node7 weight score:16856.55814281362
node9: train data size:2125
node9 epoch0:node_model train_loss:0.18383307551795786,train_acc:0.9363635778427124
node9 epoch1:node_model train_loss:0.12031778337603266,train_acc:0.9609090685844421
node9_model on test-dataset: loss:0.14788720836862923,acc:0.9512991309165955
node9 weight score:14369.058848572928
node14: train data size:1540
node14 epoch0:node_model train_loss:0.22676058998331428,train_acc:0.9206249713897705
node14 epoch1:node_model train_loss:0.13977060513570905,train_acc:0.9546874165534973
node14_model on test-dataset: loss:0.1588282015081495,acc:0.9520980715751648
node14 weight score:9696.011069677588
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.09500523794908076,acc:0.9697989642620086
total cost energy:10.737814790230134 | all_enery_cp：8.998 | all_enery_tp: 1.7398147902301346
ef: 29.139700399007097
reward: 18.401885608776965
step 64:loss:553.4512939453125|running q:0.7833391427993774
episode1,iteration4 selected nodes:[19, 15, 5, 1, 3],center node:5
################################################## episode1,iteration4 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.1641298935595733,train_acc:0.9482430219650269
node1 epoch1:node_model train_loss:0.11560742095557612,train_acc:0.963058352470398
node1_model on test-dataset: loss:0.12806757473852484,acc:0.9559948444366455
node1 weight score:51980.3706253638
node3: train data size:3762
node3 epoch0:node_model train_loss:0.15032582288902058,train_acc:0.9502037763595581
node3 epoch1:node_model train_loss:0.10630375695855994,train_acc:0.9658913016319275
node3_model on test-dataset: loss:0.14696509910281746,acc:0.9523962140083313
node3 weight score:25597.914218858776
node5: train data size:4837
node5 epoch0:node_model train_loss:0.16313638188401047,train_acc:0.9464479088783264
node5 epoch1:node_model train_loss:0.11125522471812306,train_acc:0.9644895792007446
node5_model on test-dataset: loss:0.13685484094399725,acc:0.9584909677505493
node5 weight score:35344.01827977252
node15: train data size:1376
node15 epoch0:node_model train_loss:0.18274823443165847,train_acc:0.9454886317253113
node15 epoch1:node_model train_loss:0.1405273683901344,train_acc:0.9490978121757507
node15_model on test-dataset: loss:0.15483936787903077,acc:0.9494869112968445
node15 weight score:8886.628890625598
node19: train data size:5781
node19 epoch0:node_model train_loss:0.15340685003019613,train_acc:0.9517346024513245
node19 epoch1:node_model train_loss:0.11210272210682261,train_acc:0.9655683636665344
node19_model on test-dataset: loss:0.12428849396528677,acc:0.9621968865394592
node19 weight score:46512.75283466391
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0753754092764575,acc:0.9757969468832016
total cost energy:12.942146239533544 | all_enery_cp：11.2065 | all_enery_tp: 1.735646239533544
ef: 30.11836494927306
reward: 17.176218709739516
step 65:loss:450.7524719238281|running q:1.2463619709014893
episode1,iteration5 selected nodes:[1, 8, 12, 18, 6],center node:6
################################################## episode1,iteration5 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.14121130720448138,train_acc:0.955223798751831
node1 epoch1:node_model train_loss:0.09041553883076604,train_acc:0.9717152118682861
node1_model on test-dataset: loss:0.1335061929607764,acc:0.958896815776825
node1 weight score:49862.85544038995
node6: train data size:3529
node6 epoch0:node_model train_loss:0.15117767463541693,train_acc:0.9509867429733276
node6 epoch1:node_model train_loss:0.09693768703275257,train_acc:0.9666664600372314
node6_model on test-dataset: loss:0.11976189041510224,acc:0.9617969393730164
node6 weight score:29466.802734728586
node8: train data size:2290
node8 epoch0:node_model train_loss:0.1577077537127163,train_acc:0.9521738886833191
node8 epoch1:node_model train_loss:0.11032062574573186,train_acc:0.9628500938415527
node8_model on test-dataset: loss:0.14549834756879135,acc:0.9557949900627136
node8 weight score:15739.010361731374
node12: train data size:1406
node12 epoch0:node_model train_loss:0.13043640640874704,train_acc:0.9653334021568298
node12 epoch1:node_model train_loss:0.1400170587003231,train_acc:0.9533332586288452
node12_model on test-dataset: loss:0.35114950063776634,acc:0.8920755386352539
node12 weight score:4003.9925941696865
node18: train data size:801
node18 epoch0:node_model train_loss:0.12036510405596346,train_acc:0.967777669429779
node18 epoch1:node_model train_loss:0.07771754972863062,train_acc:0.9755555391311646
node18_model on test-dataset: loss:0.22046597789274527,acc:0.9320901036262512
node18 weight score:3633.2136489090362
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07788913501659409,acc:0.9752989661693573
total cost energy:8.604941361516795 | all_enery_cp：7.3415 | all_enery_tp: 1.2634413615167959
ef: 29.50113126919661
reward: 20.896189907679812
step 66:loss:361.82012939453125|running q:2.0335848331451416
episode1,iteration6 selected nodes:[3, 1, 19, 18, 5],center node:5
################################################## episode1,iteration6 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.11289653198710128,train_acc:0.9636918902397156
node1 epoch1:node_model train_loss:0.08582016512918383,train_acc:0.9716418385505676
node1_model on test-dataset: loss:0.11314164136434555,acc:0.9664918780326843
node1 weight score:58837.7534542099
node3: train data size:3762
node3 epoch0:node_model train_loss:0.13449538205015032,train_acc:0.9568843841552734
node3 epoch1:node_model train_loss:0.08092279224901607,train_acc:0.9722070097923279
node3_model on test-dataset: loss:0.11534352778922767,acc:0.9638931155204773
node3 weight score:32615.614175374183
node5: train data size:4837
node5 epoch0:node_model train_loss:0.12964596454890406,train_acc:0.9571206569671631
node5 epoch1:node_model train_loss:0.08954882766215169,train_acc:0.9680804014205933
node5_model on test-dataset: loss:0.11149170132281143,acc:0.963398277759552
node5 weight score:43384.39491559126
node18: train data size:801
node18 epoch0:node_model train_loss:0.1155582534071679,train_acc:0.9622222185134888
node18 epoch1:node_model train_loss:0.09869817147652309,train_acc:0.9722222089767456
node18_model on test-dataset: loss:0.29079214529658204,acc:0.9110767245292664
node18 weight score:2754.5448285167795
node19: train data size:5781
node19 epoch0:node_model train_loss:0.1290421631379888,train_acc:0.9614707827568054
node19 epoch1:node_model train_loss:0.08414615005062058,train_acc:0.9750915765762329
node19_model on test-dataset: loss:0.15419366540838383,acc:0.9513929486274719
node19 weight score:37491.81255072282
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06878558084717952,acc:0.9778969460725784
total cost energy:12.796067595770852 | all_enery_cp：10.918999999999999 | all_enery_tp: 1.8770675957708534
ef: 30.141525701947373
reward: 17.345458106176523
step 67:loss:460.2225341796875|running q:2.868788719177246
episode1,iteration7 selected nodes:[4, 0, 11, 8, 13],center node:8
################################################## episode1,iteration7 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.11122268047701153,train_acc:0.9656838178634644
node0 epoch1:node_model train_loss:0.07635321835469869,train_acc:0.976805567741394
node0_model on test-dataset: loss:0.11833679755916819,acc:0.9631907343864441
node0 weight score:60547.52323695014
node4: train data size:4298
node4 epoch0:node_model train_loss:0.13071877572165672,train_acc:0.9641574621200562
node4 epoch1:node_model train_loss:0.08727457920132681,train_acc:0.9732462167739868
node4_model on test-dataset: loss:0.0979004827910103,acc:0.9684960246086121
node4 weight score:43901.7242557936
node8: train data size:2290
node8 epoch0:node_model train_loss:0.13693080534753593,train_acc:0.9576327204704285
node8 epoch1:node_model train_loss:0.09696131029530712,train_acc:0.9677293300628662
node8_model on test-dataset: loss:0.10766709490198992,acc:0.9634981155395508
node8 weight score:21269.265248445707
node11: train data size:1575
node11 epoch0:node_model train_loss:0.1400032783858478,train_acc:0.9589582681655884
node11 epoch1:node_model train_loss:0.093006826704368,train_acc:0.966249942779541
node11_model on test-dataset: loss:0.0923699167300947,acc:0.9716949462890625
node11 weight score:17051.00595253492
node13: train data size:1056
node13 epoch0:node_model train_loss:0.13128644668243147,train_acc:0.9547402262687683
node13 epoch1:node_model train_loss:0.11101495914838531,train_acc:0.9638311266899109
node13_model on test-dataset: loss:0.22634324687358456,acc:0.9261980056762695
node13 weight score:4665.48047969723
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06653863629966508,acc:0.9783949273824691
total cost energy:9.740110816796639 | all_enery_cp：8.192 | all_enery_tp: 1.548110816796639
ef: 30.624432659781256
reward: 20.88432184298462
step 68:loss:438.1767272949219|running q:3.8872084617614746
episode1,iteration8 selected nodes:[4, 1, 0, 18, 7],center node:7
################################################## episode1,iteration8 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.09749542055134144,train_acc:0.9679701924324036
node0 epoch1:node_model train_loss:0.09076859706288411,train_acc:0.9711645841598511
node0_model on test-dataset: loss:0.08759559768077452,acc:0.970696210861206
node0 weight score:81796.3481008655
node1: train data size:6657
node1 epoch0:node_model train_loss:0.10293147716421022,train_acc:0.9679469466209412
node1 epoch1:node_model train_loss:0.06654663163640384,train_acc:0.9780597686767578
node1_model on test-dataset: loss:0.12521627400434227,acc:0.9587950110435486
node1 weight score:53164.01604290787
node4: train data size:4298
node4 epoch0:node_model train_loss:0.1089076723816783,train_acc:0.9662742614746094
node4 epoch1:node_model train_loss:0.07502938103104054,train_acc:0.9781205058097839
node4_model on test-dataset: loss:0.1435800748190377,acc:0.9531838297843933
node4 weight score:29934.515672993057
node7: train data size:3637
node7 epoch0:node_model train_loss:0.11374321811505266,train_acc:0.965405285358429
node7 epoch1:node_model train_loss:0.07571986287429526,train_acc:0.9721618890762329
node7_model on test-dataset: loss:0.1316257373476401,acc:0.9590961337089539
node7 weight score:27631.374177181067
node18: train data size:801
node18 epoch0:node_model train_loss:0.09193456090158886,train_acc:0.9755555391311646
node18 epoch1:node_model train_loss:0.07736506375173728,train_acc:0.9788888096809387
node18_model on test-dataset: loss:0.1713835854502395,acc:0.9494946599006653
node18 weight score:4673.726470920209
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05820175334549276,acc:0.9809969455003739
total cost energy:13.207538328578604 | all_enery_cp：11.279 | all_enery_tp: 1.9285383285786042
ef: 30.339130383406825
reward: 17.13159205482822
step 69:loss:651.9820556640625|running q:4.884200572967529
episode1,iteration9 selected nodes:[17, 12, 6, 7, 19],center node:12
################################################## episode1,iteration9 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.09256964317885125,train_acc:0.969319760799408
node6 epoch1:node_model train_loss:0.07516205854093035,train_acc:0.9770975112915039
node6_model on test-dataset: loss:0.20484039899660275,acc:0.9398877024650574
node6 weight score:17228.04689546874
node7: train data size:3637
node7 epoch0:node_model train_loss:0.09380780681464318,train_acc:0.9718915820121765
node7 epoch1:node_model train_loss:0.06958500746436216,train_acc:0.9775382876396179
node7_model on test-dataset: loss:0.12897080704162364,acc:0.9609978199005127
node7 weight score:28200.180206875855
node12: train data size:1406
node12 epoch0:node_model train_loss:0.09883737315734227,train_acc:0.9699999094009399
node12 epoch1:node_model train_loss:0.13956626942381262,train_acc:0.972222089767456
node12_model on test-dataset: loss:0.09851191454508808,acc:0.9681960344314575
node12 weight score:14272.385289562977
node17: train data size:719
node17 epoch0:node_model train_loss:0.1073743193410337,train_acc:0.9762499332427979
node17 epoch1:node_model train_loss:0.08385556156281382,train_acc:0.981249988079071
node17_model on test-dataset: loss:0.1586219807260204,acc:0.9528931379318237
node17 weight score:4532.789192954864
node19: train data size:5781
node19 epoch0:node_model train_loss:0.10609722818280089,train_acc:0.9674648642539978
node19 epoch1:node_model train_loss:0.0870331368682071,train_acc:0.9726777672767639
node19_model on test-dataset: loss:0.09709732542512939,acc:0.9683980345726013
node19 weight score:59538.20019952724
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06502079726662487,acc:0.9793979549407958
total cost energy:8.483213595499958 | all_enery_cp：7.536 | all_enery_tp: 0.9472135954999581
ef: 30.484789239142007
reward: 22.00157564364205
step 70:loss:621.876220703125|running q:5.843790054321289
episode1,iteration10 selected nodes:[17, 5, 11, 10, 7],center node:11
################################################## episode1,iteration10 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.10676101291058016,train_acc:0.9646111130714417
node5 epoch1:node_model train_loss:0.07726721878030471,train_acc:0.9744070172309875
node5_model on test-dataset: loss:0.07786076658027014,acc:0.9744970202445984
node5 weight score:62123.71406609927
node7: train data size:3637
node7 epoch0:node_model train_loss:0.10788298166684203,train_acc:0.9638640284538269
node7 epoch1:node_model train_loss:0.07396225626202854,train_acc:0.9786485433578491
node7_model on test-dataset: loss:0.11736690420635569,acc:0.9636929631233215
node7 weight score:30988.292863253762
node10: train data size:1915
node10 epoch0:node_model train_loss:0.1178677340503782,train_acc:0.963666558265686
node10 epoch1:node_model train_loss:0.10471768006682396,train_acc:0.9646665453910828
node10_model on test-dataset: loss:0.15094808261503204,acc:0.9566988945007324
node10 weight score:12686.481118702837
node11: train data size:1575
node11 epoch0:node_model train_loss:0.11436906980816275,train_acc:0.9660415649414062
node11 epoch1:node_model train_loss:0.07136470440309495,train_acc:0.9787499308586121
node11_model on test-dataset: loss:0.12778009177112837,acc:0.961392879486084
node11 weight score:12325.863741129882
node17: train data size:719
node17 epoch0:node_model train_loss:0.09932724619284272,train_acc:0.9671710729598999
node17 epoch1:node_model train_loss:0.08194771059788764,train_acc:0.9774999618530273
node17_model on test-dataset: loss:0.21370874404383358,acc:0.9345970153808594
node17 weight score:3364.3920524493215
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06515756785869599,acc:0.9791979575157166
total cost energy:7.247949510224598 | all_enery_cp：6.3415 | all_enery_tp: 0.906449510224598
ef: 30.553119031244805
reward: 23.305169521020208
step 71:loss:662.3842163085938|running q:7.053310394287109
episode1,iteration11 selected nodes:[12, 13, 7, 4, 19],center node:12
################################################## episode1,iteration11 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.09812005565956582,train_acc:0.9695301651954651
node4 epoch1:node_model train_loss:0.057764543426158124,train_acc:0.9820882081985474
node4_model on test-dataset: loss:0.11242646881000837,acc:0.964492917060852
node4 weight score:38229.43160532127
node7: train data size:3637
node7 epoch0:node_model train_loss:0.08388487843645585,train_acc:0.9739443063735962
node7 epoch1:node_model train_loss:0.0646907532889698,train_acc:0.9792695045471191
node7_model on test-dataset: loss:0.07238409606623464,acc:0.9754981994628906
node7 weight score:50245.844013469265
node12: train data size:1406
node12 epoch0:node_model train_loss:0.08989757976184289,train_acc:0.9740000367164612
node12 epoch1:node_model train_loss:0.0816293302923441,train_acc:0.9726666808128357
node12_model on test-dataset: loss:0.11116273032384924,acc:0.9658969044685364
node12 weight score:12648.124024157329
node13: train data size:1056
node13 epoch0:node_model train_loss:0.11339617266573689,train_acc:0.9629220366477966
node13 epoch1:node_model train_loss:0.05313698092306202,train_acc:0.9801948070526123
node13_model on test-dataset: loss:0.1441387678598403,acc:0.9532898664474487
node13 weight score:7326.273255137356
node19: train data size:5781
node19 epoch0:node_model train_loss:0.09354772566464441,train_acc:0.9707407355308533
node19 epoch1:node_model train_loss:0.06205696103580553,train_acc:0.981207013130188
node19_model on test-dataset: loss:0.0912060086830752,acc:0.97138512134552
node19 weight score:63383.981861194654
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05160316891560797,acc:0.9833969438076019
total cost energy:9.545062329783656 | all_enery_cp：8.089 | all_enery_tp: 1.456062329783655
ef: 31.11503385014017
reward: 21.56997152035651
step 72:loss:594.403076171875|running q:8.698114395141602
episode1,iteration12 selected nodes:[12, 0, 1, 5, 9],center node:5
################################################## episode1,iteration12 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07882021872016291,train_acc:0.9750533699989319
node0 epoch1:node_model train_loss:0.06152509690986739,train_acc:0.9805343747138977
node0_model on test-dataset: loss:0.07899706228810828,acc:0.975000262260437
node0 weight score:90699.57530659433
node1: train data size:6657
node1 epoch0:node_model train_loss:0.07984703917528933,train_acc:0.9757081866264343
node1 epoch1:node_model train_loss:0.06214730438095198,train_acc:0.9788427352905273
node1_model on test-dataset: loss:0.11052913052597432,acc:0.9668799638748169
node1 weight score:60228.46618191397
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08181108992394744,train_acc:0.9734693169593811
node5 epoch1:node_model train_loss:0.06307349157310566,train_acc:0.9781631231307983
node5_model on test-dataset: loss:0.12830734974093502,acc:0.960693895816803
node5 weight score:37698.54189776636
node9: train data size:2125
node9 epoch0:node_model train_loss:0.08461962301622737,train_acc:0.976363480091095
node9 epoch1:node_model train_loss:0.055508461272851986,train_acc:0.9849998354911804
node9_model on test-dataset: loss:0.10294449752786022,acc:0.9687936902046204
node9 weight score:20642.19119069384
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07932502571493387,train_acc:0.9753333330154419
node12 epoch1:node_model train_loss:0.04664109093137085,train_acc:0.9819999933242798
node12_model on test-dataset: loss:0.12757750192351522,acc:0.9628972411155701
node12 weight score:11020.751925703325
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.050529619136214024,acc:0.9839979577064514
total cost energy:12.501839201537813 | all_enery_cp：11.094999999999999 | all_enery_tp: 1.406839201537814
ef: 30.926095636843883
reward: 18.42425643530607
step 73:loss:667.4140014648438|running q:10.028833389282227
episode1,iteration13 selected nodes:[17, 0, 13, 10, 11],center node:11
################################################## episode1,iteration13 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0710367818440621,train_acc:0.9781196713447571
node0 epoch1:node_model train_loss:0.053258959126348294,train_acc:0.9828421473503113
node0_model on test-dataset: loss:0.10234207014051208,acc:0.9699906706809998
node0 weight score:70010.309447158
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0802177653182298,train_acc:0.9785000681877136
node10 epoch1:node_model train_loss:0.06348293730989099,train_acc:0.9834999442100525
node10_model on test-dataset: loss:0.09959829126455588,acc:0.9688969850540161
node10 weight score:19227.23749259233
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0918996743275784,train_acc:0.9720832705497742
node11 epoch1:node_model train_loss:0.06486075988505036,train_acc:0.9837498664855957
node11_model on test-dataset: loss:0.121642726184582,acc:0.9615896940231323
node11 weight score:12947.753222910162
node13: train data size:1056
node13 epoch0:node_model train_loss:0.09197796898132021,train_acc:0.972012996673584
node13 epoch1:node_model train_loss:0.03303271024064584,train_acc:0.9899999499320984
node13_model on test-dataset: loss:0.1059897675702814,acc:0.9669979810714722
node13 weight score:9963.225924613624
node17: train data size:719
node17 epoch0:node_model train_loss:0.09000257856678218,train_acc:0.973421037197113
node17 epoch1:node_model train_loss:0.07042923429980874,train_acc:0.9759210348129272
node17_model on test-dataset: loss:0.1463409966789186,acc:0.9529961347579956
node17 weight score:4913.182336577436
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06162278378775227,acc:0.9798959374427796
total cost energy:7.5535164807134505 | all_enery_cp：6.215 | all_enery_tp: 1.3385164807134504
ef: 30.49546193489287
reward: 22.941945454179418
step 74:loss:957.2020263671875|running q:11.397624969482422
episode1,iteration14 selected nodes:[16, 10, 5, 18, 15],center node:15
################################################## episode1,iteration14 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08061163737533653,train_acc:0.9732653498649597
node5 epoch1:node_model train_loss:0.04708720957480219,train_acc:0.9843465089797974
node5_model on test-dataset: loss:0.12567792516740156,acc:0.9624947309494019
node5 weight score:38487.26809865115
node10: train data size:1915
node10 epoch0:node_model train_loss:0.09553608261048793,train_acc:0.9739999175071716
node10 epoch1:node_model train_loss:0.034146793326362966,train_acc:0.9904999136924744
node10_model on test-dataset: loss:0.0780379096447723,acc:0.9747990369796753
node10 weight score:24539.355407097126
node15: train data size:1376
node15 epoch0:node_model train_loss:0.10475353111646005,train_acc:0.9690977334976196
node15 epoch1:node_model train_loss:0.056224625723968656,train_acc:0.9798120260238647
node15_model on test-dataset: loss:0.08084318694862304,acc:0.9752936959266663
node15 weight score:17020.60559382038
node16: train data size:920
node16 epoch0:node_model train_loss:0.09605691821780056,train_acc:0.9660000205039978
node16 epoch1:node_model train_loss:0.07077482915483416,train_acc:0.9719999432563782
node16_model on test-dataset: loss:0.11577309196116403,acc:0.9650939106941223
node16 weight score:7946.578815642352
node18: train data size:801
node18 epoch0:node_model train_loss:0.04391578810949189,train_acc:0.9844443202018738
node18 epoch1:node_model train_loss:0.046043530106544495,train_acc:0.9855555295944214
node18_model on test-dataset: loss:0.10569733556592836,acc:0.9685961008071899
node18 weight score:7578.242116617774
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.052938417424156795,acc:0.9828979521989822
total cost energy:6.002495987511003 | all_enery_cp：4.924499999999999 | all_enery_tp: 1.0779959875110041
ef: 31.089740815962987
reward: 25.087244828451983
step 75:loss:551.0455322265625|running q:12.634293556213379
episode1,iteration15 selected nodes:[15, 16, 9, 3, 0],center node:15
################################################## episode1,iteration15 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.06441676669702348,train_acc:0.976164698600769
node0 epoch1:node_model train_loss:0.04158476974892741,train_acc:0.9861007332801819
node0_model on test-dataset: loss:0.10074069699490792,acc:0.9701939821243286
node0 weight score:71123.19264936358
node3: train data size:3762
node3 epoch0:node_model train_loss:0.10258213048310656,train_acc:0.9675720930099487
node3 epoch1:node_model train_loss:0.06477461649889224,train_acc:0.9815192818641663
node3_model on test-dataset: loss:0.11421494551534124,acc:0.9648921489715576
node3 weight score:32937.896026003815
node9: train data size:2125
node9 epoch0:node_model train_loss:0.07972629918632182,train_acc:0.9704543352127075
node9 epoch1:node_model train_loss:0.04301579525186257,train_acc:0.9863635301589966
node9_model on test-dataset: loss:0.08202813267169404,acc:0.9763898849487305
node9 weight score:25905.746367590873
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0989170792911734,train_acc:0.9688345789909363
node15 epoch1:node_model train_loss:0.03530231863260269,train_acc:0.9942857027053833
node15_model on test-dataset: loss:0.07401272745570169,acc:0.9767971038818359
node15 weight score:18591.39701105553
node16: train data size:920
node16 epoch0:node_model train_loss:0.07655146676115691,train_acc:0.9729999899864197
node16 epoch1:node_model train_loss:0.07704855035990477,train_acc:0.968999981880188
node16_model on test-dataset: loss:0.09629257124695868,acc:0.9717840552330017
node16 weight score:9554.215741529048
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05708237994811498,acc:0.981295936703682
total cost energy:10.0084428111092 | all_enery_cp：7.6739999999999995 | all_enery_tp: 2.3344428111092004
ef: 31.203546819123307
reward: 21.195104008014106
step 76:loss:563.7014770507812|running q:14.607279777526855
episode1,iteration16 selected nodes:[14, 7, 12, 13, 9],center node:12
################################################## episode1,iteration16 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.08031020731337972,train_acc:0.9719721078872681
node7 epoch1:node_model train_loss:0.04849591726638578,train_acc:0.9849450588226318
node7_model on test-dataset: loss:0.06446269225212746,acc:0.9811992645263672
node7 weight score:56420.231190079845
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06181415780024095,train_acc:0.9786363244056702
node9 epoch1:node_model train_loss:0.04275500624101947,train_acc:0.9840909242630005
node9_model on test-dataset: loss:0.07675953196456248,acc:0.9755980968475342
node9 weight score:27683.858220775073
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06314363611163572,train_acc:0.9753332734107971
node12 epoch1:node_model train_loss:0.040402141073718666,train_acc:0.984000027179718
node12_model on test-dataset: loss:0.08954323364188894,acc:0.972195029258728
node12 weight score:15701.912280977347
node13: train data size:1056
node13 epoch0:node_model train_loss:0.11772334236990321,train_acc:0.9603896737098694
node13 epoch1:node_model train_loss:0.04404540300707926,train_acc:0.9890909194946289
node13_model on test-dataset: loss:0.10321914131287485,acc:0.9662972092628479
node13 weight score:10230.660578730098
node14: train data size:1540
node14 epoch0:node_model train_loss:0.1093485135352239,train_acc:0.9646874666213989
node14 epoch1:node_model train_loss:0.06145714607555419,train_acc:0.9793749451637268
node14_model on test-dataset: loss:0.07686155126335507,acc:0.9757950901985168
node14 weight score:20036.025485920924
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04792699426659965,acc:0.9838979536294937
total cost energy:6.515508749109257 | all_enery_cp：4.882 | all_enery_tp: 1.6335087491092577
ef: 31.448586588056628
reward: 24.93307783894737
step 77:loss:813.1901245117188|running q:16.16216278076172
episode1,iteration17 selected nodes:[8, 17, 16, 2, 4],center node:8
################################################## episode1,iteration17 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.09583464603712584,train_acc:0.9682978987693787
node2 epoch1:node_model train_loss:0.07511750124692124,train_acc:0.9772341251373291
node2_model on test-dataset: loss:0.09706272326322506,acc:0.9705878496170044
node2 weight score:47495.06138930504
node4: train data size:4298
node4 epoch0:node_model train_loss:0.08221856087749434,train_acc:0.9748835563659668
node4 epoch1:node_model train_loss:0.05199814042033151,train_acc:0.9848836660385132
node4_model on test-dataset: loss:0.07363233574782498,acc:0.9771950840950012
node4 weight score:58371.09411712446
node8: train data size:2290
node8 epoch0:node_model train_loss:0.10398769678305024,train_acc:0.9693717956542969
node8 epoch1:node_model train_loss:0.058982636496100735,train_acc:0.9816423654556274
node8_model on test-dataset: loss:0.10821810308407294,acc:0.9652949571609497
node8 weight score:21160.969696733042
node16: train data size:920
node16 epoch0:node_model train_loss:0.07528314590454102,train_acc:0.9750000238418579
node16 epoch1:node_model train_loss:0.05088589616061654,train_acc:0.9819999933242798
node16_model on test-dataset: loss:0.12678731378575323,acc:0.9635979533195496
node16 weight score:7256.246484996342
node17: train data size:719
node17 epoch0:node_model train_loss:0.08375533201615326,train_acc:0.9774999618530273
node17 epoch1:node_model train_loss:0.05321005964651704,train_acc:0.9824999570846558
node17_model on test-dataset: loss:0.08789626043319004,acc:0.974197268486023
node17 weight score:8180.09772493691
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04722886058443691,acc:0.9844959318637848
total cost energy:8.058063442988615 | all_enery_cp：6.4185 | all_enery_tp: 1.639563442988616
ef: 30.854200604817475
reward: 22.79613716182886
step 78:loss:582.0104370117188|running q:17.868789672851562
episode1,iteration18 selected nodes:[0, 9, 7, 17, 15],center node:17
################################################## episode1,iteration18 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.06452868630488713,train_acc:0.9790920615196228
node0 epoch1:node_model train_loss:0.038138160191010684,train_acc:0.9877034425735474
node0_model on test-dataset: loss:0.06917056119695189,acc:0.9791958928108215
node0 weight score:103584.52896744371
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0692483855260385,train_acc:0.9776477813720703
node7 epoch1:node_model train_loss:0.05283529493278144,train_acc:0.9824323058128357
node7_model on test-dataset: loss:0.07500014129007468,acc:0.9758988618850708
node7 weight score:48493.241978482925
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05540204391052777,train_acc:0.9809090495109558
node9 epoch1:node_model train_loss:0.03464938748881898,train_acc:0.9895453453063965
node9_model on test-dataset: loss:0.0883761339513876,acc:0.9717958569526672
node9 weight score:24044.953145029885
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0881431685494525,train_acc:0.9716918468475342
node15 epoch1:node_model train_loss:0.051800443790853024,train_acc:0.9804887771606445
node15_model on test-dataset: loss:0.09250135353096994,acc:0.971596896648407
node15 weight score:14875.458006561039
node17: train data size:719
node17 epoch0:node_model train_loss:0.06690174486720935,train_acc:0.9837499856948853
node17 epoch1:node_model train_loss:0.0351430190494284,train_acc:0.9887499213218689
node17_model on test-dataset: loss:0.08356280001462438,acc:0.9750959277153015
node17 weight score:8604.307178243995
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0543054182013293,acc:0.9833969455957413
total cost energy:9.448347862891438 | all_enery_cp：7.510999999999999 | all_enery_tp: 1.9373478628914391
ef: 31.64497683912349
reward: 22.196628976232052
step 79:loss:512.4713134765625|running q:19.660961151123047
episode1,iteration19 selected nodes:[0, 18, 8, 4, 14],center node:8
################################################## episode1,iteration19 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.047831313529362283,train_acc:0.9848612546920776
node0 epoch1:node_model train_loss:0.027744414042293403,train_acc:0.9902034401893616
node0_model on test-dataset: loss:0.06765907053340925,acc:0.9781960248947144
node0 weight score:105898.58748446754
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07416172012643413,train_acc:0.9806928634643555
node4 epoch1:node_model train_loss:0.04609137177900519,train_acc:0.9862790703773499
node4_model on test-dataset: loss:0.0923623697651783,acc:0.9717949628829956
node4 weight score:46534.102697096416
node8: train data size:2290
node8 epoch0:node_model train_loss:0.0948737528094131,train_acc:0.969903290271759
node8 epoch1:node_model train_loss:0.05660012315796769,train_acc:0.9815940856933594
node8_model on test-dataset: loss:0.09097656086436473,acc:0.9707981944084167
node8 weight score:25171.318614847605
node14: train data size:1540
node14 epoch0:node_model train_loss:0.09150959143880755,train_acc:0.9737499356269836
node14 epoch1:node_model train_loss:0.05108314554672688,train_acc:0.9837498664855957
node14_model on test-dataset: loss:0.07737062979998882,acc:0.9746999740600586
node14 weight score:19904.193671178084
node18: train data size:801
node18 epoch0:node_model train_loss:0.0492611164938555,train_acc:0.9799999594688416
node18 epoch1:node_model train_loss:0.03670706979917466,train_acc:0.992222249507904
node18_model on test-dataset: loss:0.12940934009704505,acc:0.9623971581459045
node18 weight score:6189.661421651049
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051163036707766875,acc:0.9835989648103713
total cost energy:9.629165667975988 | all_enery_cp：8.047 | all_enery_tp: 1.5821656679759872
ef: 31.382247732837264
reward: 21.753082064861275
step 80:loss:597.8326416015625|running q:20.895936965942383
episode1,iteration20 selected nodes:[0, 5, 11, 1, 7],center node:5
################################################## episode1,iteration20 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03759775800022504,train_acc:0.9873612523078918
node0 epoch1:node_model train_loss:0.03139602930039271,train_acc:0.9897759556770325
node0_model on test-dataset: loss:0.07571613228312345,acc:0.9776960015296936
node0 weight score:94629.76757988764
node1: train data size:6657
node1 epoch0:node_model train_loss:0.07568445142858954,train_acc:0.9785441756248474
node1 epoch1:node_model train_loss:0.044801611557547284,train_acc:0.985373318195343
node1_model on test-dataset: loss:0.11012430655988283,acc:0.9673940539360046
node1 weight score:60449.86986029366
node5: train data size:4837
node5 epoch0:node_model train_loss:0.0712456869303572,train_acc:0.9753668308258057
node5 epoch1:node_model train_loss:0.04748730275932015,train_acc:0.9829177856445312
node5_model on test-dataset: loss:0.08224093950571841,acc:0.9739930033683777
node5 weight score:58814.98957904867
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06259718464335075,train_acc:0.9804308414459229
node7 epoch1:node_model train_loss:0.04032848696098537,train_acc:0.9851351976394653
node7_model on test-dataset: loss:0.1037211294088047,acc:0.967697024345398
node7 weight score:35065.179300788266
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0785095295868814,train_acc:0.9743749499320984
node11 epoch1:node_model train_loss:0.04718308482551947,train_acc:0.9868749380111694
node11_model on test-dataset: loss:0.11673852641968552,acc:0.966395914554596
node11 weight score:13491.689918525553
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05036873563469271,acc:0.9846979582309723
total cost energy:13.135889691313215 | all_enery_cp：11.9355 | all_enery_tp: 1.200389691313216
ef: 31.45675999626641
reward: 18.320870304953196
step 81:loss:825.16796875|running q:22.56766128540039
episode1,iteration21 selected nodes:[19, 3, 13, 15, 17],center node:15
################################################## episode1,iteration21 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.0778173686152226,train_acc:0.974677324295044
node3 epoch1:node_model train_loss:0.03710635261928761,train_acc:0.9886842370033264
node3_model on test-dataset: loss:0.07240310533728916,acc:0.9760971069335938
node3 weight score:51959.097368472794
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07700925489718263,train_acc:0.971103847026825
node13 epoch1:node_model train_loss:0.029984614769504828,train_acc:0.9881817698478699
node13_model on test-dataset: loss:0.08317414343968267,acc:0.9724977612495422
node13 weight score:12696.253382709063
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07361851306632161,train_acc:0.979285717010498
node15 epoch1:node_model train_loss:0.03908355041806187,train_acc:0.9862029552459717
node15_model on test-dataset: loss:0.08163369051704648,acc:0.9756952524185181
node15 weight score:16855.785782619594
node17: train data size:719
node17 epoch0:node_model train_loss:0.05729178333422169,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.0390327880741097,train_acc:0.9871710538864136
node17_model on test-dataset: loss:0.11089302310632775,acc:0.9682952165603638
node17 weight score:6483.726206206859
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07555042053893979,train_acc:0.978620707988739
node19 epoch1:node_model train_loss:0.04881569826654319,train_acc:0.9848278760910034
node19_model on test-dataset: loss:0.07573431382581475,acc:0.9774958491325378
node19 weight score:76332.6385090914
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.050624417497165265,acc:0.9841979539394379
total cost energy:7.824046168079791 | all_enery_cp：6.3469999999999995 | all_enery_tp: 1.4770461680797917
ef: 31.50569359981737
reward: 23.68164743173758
step 82:loss:656.9283447265625|running q:24.0809383392334
episode1,iteration22 selected nodes:[3, 17, 11, 10, 15],center node:11
################################################## episode1,iteration22 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06186442409249905,train_acc:0.9807299375534058
node3 epoch1:node_model train_loss:0.03795255300294804,train_acc:0.9894736409187317
node3_model on test-dataset: loss:0.08330544154217932,acc:0.9741949439048767
node3 weight score:45159.11482319218
node10: train data size:1915
node10 epoch0:node_model train_loss:0.07029931889846922,train_acc:0.9774999022483826
node10 epoch1:node_model train_loss:0.04685562808008399,train_acc:0.9859998822212219
node10_model on test-dataset: loss:0.07437549458976718,acc:0.975998044013977
node10 weight score:25747.72793865188
node11: train data size:1575
node11 epoch0:node_model train_loss:0.07461487408727407,train_acc:0.9772915840148926
node11 epoch1:node_model train_loss:0.041824308631476015,train_acc:0.9860416054725647
node11_model on test-dataset: loss:0.09601780132681598,acc:0.9692898988723755
node11 weight score:16403.208345077277
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06267388311347791,train_acc:0.9831202626228333
node15 epoch1:node_model train_loss:0.03768589470668563,train_acc:0.98906010389328
node15_model on test-dataset: loss:0.07976128969632555,acc:0.9779958724975586
node15 weight score:17251.476314372958
node17: train data size:719
node17 epoch0:node_model train_loss:0.08504016228835098,train_acc:0.9762499332427979
node17 epoch1:node_model train_loss:0.03275763326018932,train_acc:0.9887499809265137
node17_model on test-dataset: loss:0.07444233710262779,acc:0.9764000773429871
node17 weight score:9658.482363453624
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05135763729995233,acc:0.98309694647789
total cost energy:5.880201987234509 | all_enery_cp：4.6735 | all_enery_tp: 1.206701987234509
ef: 31.545646769222195
reward: 25.665444781987688
step 83:loss:605.7199096679688|running q:25.479787826538086
episode1,iteration23 selected nodes:[19, 6, 16, 3, 17],center node:17
################################################## episode1,iteration23 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05977245904260168,train_acc:0.982631504535675
node3 epoch1:node_model train_loss:0.04096500810823942,train_acc:0.984677255153656
node3_model on test-dataset: loss:0.1151298224479433,acc:0.9665958881378174
node3 weight score:32676.155665062477
node6: train data size:3529
node6 epoch0:node_model train_loss:0.061853006741633486,train_acc:0.9780554175376892
node6 epoch1:node_model train_loss:0.03970349569701486,train_acc:0.987930953502655
node6_model on test-dataset: loss:0.10422161800128378,acc:0.969793975353241
node6 weight score:33860.53745544932
node16: train data size:920
node16 epoch0:node_model train_loss:0.08717102892696857,train_acc:0.9739999175071716
node16 epoch1:node_model train_loss:0.03632445312105119,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.08806820478886948,acc:0.973097026348114
node16 weight score:10446.448888172119
node17: train data size:719
node17 epoch0:node_model train_loss:0.07671402289997786,train_acc:0.9787499308586121
node17 epoch1:node_model train_loss:0.04228584555676207,train_acc:0.9821710586547852
node17_model on test-dataset: loss:0.08048686524736695,acc:0.9744969606399536
node17 weight score:8933.1345902246
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06293643015468943,train_acc:0.981988251209259
node19 epoch1:node_model train_loss:0.04233417691711079,train_acc:0.9858216643333435
node19_model on test-dataset: loss:0.0798545157875924,acc:0.9741981029510498
node19 weight score:72394.15257838479
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04890923383798509,acc:0.9847989666461945
total cost energy:8.932566466002461 | all_enery_cp：7.355499999999999 | all_enery_tp: 1.5770664660024611
ef: 31.304002705302352
reward: 22.37143623929989
step 84:loss:636.6676025390625|running q:27.246238708496094
episode1,iteration24 selected nodes:[14, 10, 6, 7, 15],center node:6
################################################## episode1,iteration24 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04978557657321087,train_acc:0.9833330512046814
node6 epoch1:node_model train_loss:0.04464738991292608,train_acc:0.9854310154914856
node6_model on test-dataset: loss:0.07522057393216527,acc:0.9766948223114014
node6 weight score:46915.35593948658
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06170532512604385,train_acc:0.9807010293006897
node7 epoch1:node_model train_loss:0.03181420192374168,train_acc:0.9883782863616943
node7_model on test-dataset: loss:0.06781279960283428,acc:0.9798979759216309
node7 weight score:53632.94276745933
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06269512110156938,train_acc:0.9839999079704285
node10 epoch1:node_model train_loss:0.03463090400910005,train_acc:0.9889999628067017
node10_model on test-dataset: loss:0.07373116849339567,acc:0.9776980876922607
node10 weight score:25972.733636678124
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08994195086415857,train_acc:0.9699999690055847
node14 epoch1:node_model train_loss:0.0418142179842107,train_acc:0.9859373569488525
node14_model on test-dataset: loss:0.08736135283121257,acc:0.9714991450309753
node14 weight score:17627.932147243337
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05807312259899585,train_acc:0.9833458662033081
node15 epoch1:node_model train_loss:0.030899997792273228,train_acc:0.991428554058075
node15_model on test-dataset: loss:0.08021239804460492,acc:0.9754968881607056
node15 weight score:17154.455340368044
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04369873244300834,acc:0.9860979533195495
total cost energy:7.358378083047767 | all_enery_cp：5.9985 | all_enery_tp: 1.359878083047767
ef: 31.758006299517938
reward: 24.39962821647017
step 85:loss:414.1062927246094|running q:28.67066764831543
episode1,iteration25 selected nodes:[5, 14, 3, 12, 17],center node:5
################################################## episode1,iteration25 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06544743040821661,train_acc:0.9783614277839661
node3 epoch1:node_model train_loss:0.038014890497403316,train_acc:0.9886246919631958
node3_model on test-dataset: loss:0.07856867080903612,acc:0.9769971966743469
node3 weight score:47881.68058924747
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06751269330175555,train_acc:0.9812244176864624
node5 epoch1:node_model train_loss:0.043021650832830646,train_acc:0.9871429204940796
node5_model on test-dataset: loss:0.06535649543380714,acc:0.9787977933883667
node5 weight score:74009.47630215115
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04503939925537755,train_acc:0.9866665601730347
node12 epoch1:node_model train_loss:0.04859696049243212,train_acc:0.984666645526886
node12_model on test-dataset: loss:0.11683265847841995,acc:0.9669958353042603
node12 weight score:12034.306317353045
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07309516111854464,train_acc:0.9765623807907104
node14 epoch1:node_model train_loss:0.04274709228775464,train_acc:0.9846874475479126
node14_model on test-dataset: loss:0.07774750005281021,acc:0.9751969575881958
node14 weight score:19807.710845415615
node17: train data size:719
node17 epoch0:node_model train_loss:0.0446886652498506,train_acc:0.98499995470047
node17 epoch1:node_model train_loss:0.022394183848518878,train_acc:0.9912499189376831
node17_model on test-dataset: loss:0.09503866209764965,acc:0.9713971018791199
node17 weight score:7565.342189489652
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.047622145021668984,acc:0.9843979555368424
total cost energy:7.663727906310079 | all_enery_cp：6.132 | all_enery_tp: 1.5317279063100797
ef: 31.29395811771386
reward: 23.63023021140378
step 86:loss:566.065673828125|running q:30.14409828186035
episode1,iteration26 selected nodes:[9, 12, 3, 2, 11],center node:11
################################################## episode1,iteration26 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.08692758011215544,train_acc:0.9725531339645386
node2 epoch1:node_model train_loss:0.05518185564916224,train_acc:0.9829787015914917
node2_model on test-dataset: loss:0.06931282927282155,acc:0.9789982438087463
node2 weight score:66510.05374278726
node3: train data size:3762
node3 epoch0:node_model train_loss:0.043557758075430206,train_acc:0.9849405884742737
node3 epoch1:node_model train_loss:0.037153364857658744,train_acc:0.9881578683853149
node3_model on test-dataset: loss:0.08025157891490381,acc:0.9759936928749084
node3 weight score:46877.58235871101
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05955325052226809,train_acc:0.9840908050537109
node9 epoch1:node_model train_loss:0.034699743169105866,train_acc:0.9899998903274536
node9_model on test-dataset: loss:0.07339042194143985,acc:0.9761947989463806
node9 weight score:28954.73201796814
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06514695650548674,train_acc:0.9774998426437378
node11 epoch1:node_model train_loss:0.03448397063766606,train_acc:0.9889582991600037
node11_model on test-dataset: loss:0.07133977257864899,acc:0.9770938754081726
node11 weight score:22077.44632580138
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06000486041108767,train_acc:0.9755555391311646
node12 epoch1:node_model train_loss:0.050275690191968654,train_acc:0.9833332300186157
node12_model on test-dataset: loss:0.08698450069525279,acc:0.9760940074920654
node12 weight score:16163.79916838142
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05122918188033509,acc:0.9837969440221787
total cost energy:8.334405752046296 | all_enery_cp：6.739 | all_enery_tp: 1.5954057520462963
ef: 31.93129382507909
reward: 23.596888073032794
step 87:loss:574.7860107421875|running q:31.494495391845703
episode1,iteration27 selected nodes:[16, 17, 12, 4, 19],center node:17
################################################## episode1,iteration27 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06796861539573171,train_acc:0.981390655040741
node4 epoch1:node_model train_loss:0.031152439791016107,train_acc:0.9892929196357727
node4_model on test-dataset: loss:0.0769934084804845,acc:0.9777969717979431
node4 weight score:55822.960495240484
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03776903363565604,train_acc:0.9886666536331177
node12 epoch1:node_model train_loss:0.030941772204823793,train_acc:0.9879999756813049
node12_model on test-dataset: loss:0.07583135522028897,acc:0.9781968593597412
node12 weight score:18541.14298650724
node16: train data size:920
node16 epoch0:node_model train_loss:0.055929150385782125,train_acc:0.9819999933242798
node16 epoch1:node_model train_loss:0.0408274850808084,train_acc:0.9879999160766602
node16_model on test-dataset: loss:0.08838005414960208,acc:0.9732957482337952
node16 weight score:10409.588553121996
node17: train data size:719
node17 epoch0:node_model train_loss:0.06362199666909873,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.04555267297109822,train_acc:0.9862499833106995
node17_model on test-dataset: loss:0.06772775457240642,acc:0.9791969656944275
node17 weight score:10616.031854877621
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06468906710406058,train_acc:0.9811664819717407
node19 epoch1:node_model train_loss:0.046460836090083266,train_acc:0.98517245054245
node19_model on test-dataset: loss:0.06953621663298691,acc:0.9769992828369141
node19 weight score:83136.53344863722
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04425478640914662,acc:0.9859979546070099
total cost energy:7.8835683437474104 | all_enery_cp：6.561999999999999 | all_enery_tp: 1.3215683437474113
ef: 31.936735491067076
reward: 24.053167147319666
step 88:loss:320.3269348144531|running q:32.744876861572266
episode1,iteration28 selected nodes:[14, 8, 4, 19, 1],center node:8
################################################## episode1,iteration28 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05794865844437658,train_acc:0.9828725457191467
node1 epoch1:node_model train_loss:0.036377304469916355,train_acc:0.9892538189888
node1_model on test-dataset: loss:0.07726798222807701,acc:0.9758960604667664
node1 weight score:86154.70221999705
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05405857781121551,train_acc:0.9837066531181335
node4 epoch1:node_model train_loss:0.038754143113227085,train_acc:0.9867395162582397
node4_model on test-dataset: loss:0.0903014622056071,acc:0.972697913646698
node4 weight score:47596.12851244754
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08592663008881651,train_acc:0.9728501439094543
node8 epoch1:node_model train_loss:0.0433552659764562,train_acc:0.9860868453979492
node8_model on test-dataset: loss:0.0638293493175297,acc:0.9810991287231445
node8 weight score:35876.91280711659
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08174204989336431,train_acc:0.9765625
node14 epoch1:node_model train_loss:0.026042094046715647,train_acc:0.9924999475479126
node14_model on test-dataset: loss:0.07324676146599814,acc:0.9771973490715027
node14 weight score:21024.820335775297
node19: train data size:5781
node19 epoch0:node_model train_loss:0.05063791803469689,train_acc:0.9847063422203064
node19 epoch1:node_model train_loss:0.03479793231235817,train_acc:0.989148736000061
node19_model on test-dataset: loss:0.0661808695412401,acc:0.9797878861427307
node19 weight score:87351.5268093843
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04368843192147324,acc:0.9854969447851181
total cost energy:11.882070478491457 | all_enery_cp：10.283 | all_enery_tp: 1.5990704784914571
ef: 32.174356582145705
reward: 20.29228610365425
step 89:loss:465.9904479980469|running q:33.986061096191406
episode1,iteration29 selected nodes:[19, 10, 12, 5, 7],center node:12
################################################## episode1,iteration29 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.050908397094403604,train_acc:0.9828571677207947
node5 epoch1:node_model train_loss:0.029833551344214653,train_acc:0.9912245273590088
node5_model on test-dataset: loss:0.06898910683288705,acc:0.9811960458755493
node5 weight score:70112.51807791497
node7: train data size:3637
node7 epoch0:node_model train_loss:0.053386591396898636,train_acc:0.9822424054145813
node7 epoch1:node_model train_loss:0.031007885366577555,train_acc:0.9910810589790344
node7_model on test-dataset: loss:0.06264640651308583,acc:0.9812990427017212
node7 weight score:58056.0035672643
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06545711359940469,train_acc:0.9801666140556335
node10 epoch1:node_model train_loss:0.04579136389074847,train_acc:0.9869999289512634
node10_model on test-dataset: loss:0.07916939454713429,acc:0.9739980697631836
node10 weight score:24188.640205652777
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04457856597922121,train_acc:0.9846667051315308
node12 epoch1:node_model train_loss:0.024334260448813438,train_acc:0.9933333396911621
node12_model on test-dataset: loss:0.09690876722248504,acc:0.9705936908721924
node12 weight score:14508.49123662958
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04202634009837719,train_acc:0.9868563413619995
node19 epoch1:node_model train_loss:0.032016720133030724,train_acc:0.9901320338249207
node19_model on test-dataset: loss:0.0775463294700603,acc:0.9749980568885803
node19 weight score:74548.98303383881
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04480679779968341,acc:0.9859979516267776
total cost energy:9.894449510224598 | all_enery_cp：8.788 | all_enery_tp: 1.1064495102245981
ef: 32.14026311043369
reward: 22.245813600209097
step 90:loss:561.6785278320312|running q:35.045902252197266
episode1,iteration30 selected nodes:[16, 10, 17, 15, 9],center node:17
################################################## episode1,iteration30 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05159767463125966,train_acc:0.9813635945320129
node9 epoch1:node_model train_loss:0.03667096622203561,train_acc:0.987727165222168
node9_model on test-dataset: loss:0.07991661681619007,acc:0.9770979881286621
node9 weight score:26590.214709508356
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05447612138523254,train_acc:0.9806665778160095
node10 epoch1:node_model train_loss:0.03935305244522169,train_acc:0.9869999289512634
node10_model on test-dataset: loss:0.06363422737413202,acc:0.9807969331741333
node10 weight score:30093.86738902196
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06135410403034517,train_acc:0.9774059057235718
node15 epoch1:node_model train_loss:0.028567696023466333,train_acc:0.9919173121452332
node15_model on test-dataset: loss:0.08994937279290753,acc:0.9728939533233643
node15 weight score:15297.49410446692
node16: train data size:920
node16 epoch0:node_model train_loss:0.04499175928067416,train_acc:0.9869999885559082
node16 epoch1:node_model train_loss:0.038901748368516564,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.09223341167256877,acc:0.9714958667755127
node16 weight score:9974.693371053281
node17: train data size:719
node17 epoch0:node_model train_loss:0.05499405658338219,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.03751167554582935,train_acc:0.9871711134910583
node17_model on test-dataset: loss:0.0714778020495578,acc:0.9795001149177551
node17 weight score:10059.067002389005
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04110804230163922,acc:0.9874979519844055
total cost energy:4.888693756954886 | all_enery_cp：3.5275 | all_enery_tp: 1.3611937569548864
ef: 31.5744616086243
reward: 26.685767851669414
step 91:loss:541.0781860351562|running q:36.04360580444336
episode1,iteration31 selected nodes:[4, 12, 11, 14, 8],center node:8
################################################## episode1,iteration31 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05125096244710432,train_acc:0.9865069389343262
node4 epoch1:node_model train_loss:0.03661574168823833,train_acc:0.9883531332015991
node4_model on test-dataset: loss:0.09194215521391015,acc:0.9728949666023254
node4 weight score:46746.78323561579
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06861285711436169,train_acc:0.977342963218689
node8 epoch1:node_model train_loss:0.03708510135259965,train_acc:0.9891303777694702
node8_model on test-dataset: loss:0.07869422930700239,acc:0.9762968420982361
node8 weight score:29099.97365964712
node11: train data size:1575
node11 epoch0:node_model train_loss:0.050167164248705376,train_acc:0.9862498641014099
node11 epoch1:node_model train_loss:0.033114716556156054,train_acc:0.9897914528846741
node11_model on test-dataset: loss:0.07498988693019783,acc:0.978393018245697
node11 weight score:21002.83204141971
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05050002199908098,train_acc:0.9806666374206543
node12 epoch1:node_model train_loss:0.03961741901390876,train_acc:0.9886666536331177
node12_model on test-dataset: loss:0.07654090089825331,acc:0.9781960844993591
node12 weight score:18369.26379882844
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07678349904017523,train_acc:0.978437602519989
node14 epoch1:node_model train_loss:0.0401460615103133,train_acc:0.9843748807907104
node14_model on test-dataset: loss:0.07018081305301166,acc:0.9799979329109192
node14 weight score:21943.319448817845
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04467555658746278,acc:0.9866969430446625
total cost energy:6.693405042258275 | all_enery_cp：5.554500000000001 | all_enery_tp: 1.138905042258274
ef: 31.747477459024815
reward: 25.054072416766537
step 92:loss:496.9442138671875|running q:36.9727783203125
episode1,iteration32 selected nodes:[2, 4, 9, 14, 15],center node:14
################################################## episode1,iteration32 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0681019390318939,train_acc:0.9776595830917358
node2 epoch1:node_model train_loss:0.049581666295590035,train_acc:0.9851064085960388
node2_model on test-dataset: loss:0.05408707963651978,acc:0.9827990531921387
node2 weight score:85232.92496064647
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04615074768662453,train_acc:0.9844092130661011
node4 epoch1:node_model train_loss:0.023791128106220343,train_acc:0.9927906394004822
node4_model on test-dataset: loss:0.058582450605536,acc:0.9824928045272827
node4 weight score:73366.68158422588
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0393483625640246,train_acc:0.9872726202011108
node9 epoch1:node_model train_loss:0.026143900288099594,train_acc:0.9913636445999146
node9_model on test-dataset: loss:0.08013063974009128,acc:0.9758979678153992
node9 weight score:26519.19424196
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05599715845892206,train_acc:0.9774999618530273
node14 epoch1:node_model train_loss:0.0354252899705898,train_acc:0.9849998950958252
node14_model on test-dataset: loss:0.0933644120162353,acc:0.9728960394859314
node14 weight score:16494.507561748545
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05780353017949632,train_acc:0.9857518076896667
node15 epoch1:node_model train_loss:0.03948447021788785,train_acc:0.9892857074737549
node15_model on test-dataset: loss:0.07660514801915269,acc:0.9783959984779358
node15 weight score:17962.23929566685
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04092357819630706,acc:0.9859969449043274
total cost energy:8.739255903440695 | all_enery_cp：6.9745 | all_enery_tp: 1.764755903440695
ef: 32.08042783239421
reward: 23.341171928953514
step 93:loss:227.2578887939453|running q:37.81821823120117
episode1,iteration33 selected nodes:[13, 14, 18, 5, 16],center node:16
################################################## episode1,iteration33 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.055692480515916735,train_acc:0.9800000786781311
node5 epoch1:node_model train_loss:0.027444708417644913,train_acc:0.9900001287460327
node5_model on test-dataset: loss:0.05785662609385327,acc:0.9820980429649353
node5 weight score:83603.21585558006
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06411930376833136,train_acc:0.978376567363739
node13 epoch1:node_model train_loss:0.03476736313578757,train_acc:0.9890909194946289
node13_model on test-dataset: loss:0.0905209794176335,acc:0.9710961580276489
node13 weight score:11665.80395830639
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06196361355250701,train_acc:0.9762498736381531
node14 epoch1:node_model train_loss:0.036405319173354656,train_acc:0.9868749380111694
node14_model on test-dataset: loss:0.09690885914918908,acc:0.9696981310844421
node14 weight score:15891.219992892533
node16: train data size:920
node16 epoch0:node_model train_loss:0.054626225307583806,train_acc:0.9869999885559082
node16 epoch1:node_model train_loss:0.03303356631658971,train_acc:0.9929999709129333
node16_model on test-dataset: loss:0.10542002089554443,acc:0.9684970378875732
node16 weight score:8726.995045007467
node18: train data size:801
node18 epoch0:node_model train_loss:0.045865665500362716,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.13181126503170365,train_acc:0.8799999356269836
node18_model on test-dataset: loss:0.09213450356238355,acc:0.9737961888313293
node18 weight score:8693.8114281763
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0435126285610022,acc:0.9855969429016114
total cost energy:6.01842135623731 | all_enery_cp：4.577 | all_enery_tp: 1.4414213562373097
ef: 31.4017052618141
reward: 25.38328390557679
step 94:loss:249.50631713867188|running q:38.65774154663086
episode1,iteration34 selected nodes:[19, 1, 16, 14, 7],center node:7
################################################## episode1,iteration34 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05504621158062077,train_acc:0.9831344485282898
node1 epoch1:node_model train_loss:0.03471408347918916,train_acc:0.9885075092315674
node1_model on test-dataset: loss:0.07297614282622816,acc:0.9772909879684448
node1 weight score:91221.59300542568
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04924970994565032,train_acc:0.9840539693832397
node7 epoch1:node_model train_loss:0.03481964615953935,train_acc:0.988648533821106
node7_model on test-dataset: loss:0.06745731781586073,acc:0.9789950251579285
node7 weight score:53915.573843715145
node14: train data size:1540
node14 epoch0:node_model train_loss:0.043097946618217975,train_acc:0.9843749403953552
node14 epoch1:node_model train_loss:0.037593455010210164,train_acc:0.9868749380111694
node14_model on test-dataset: loss:0.07569445155357243,acc:0.9792969822883606
node14 weight score:20344.952217667782
node16: train data size:920
node16 epoch0:node_model train_loss:0.05299235302954912,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.014763369213324041,train_acc:0.994999885559082
node16_model on test-dataset: loss:0.0660342908597886,acc:0.9817939400672913
node16 weight score:13932.155369903905
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04986769923386324,train_acc:0.9831950664520264
node19 epoch1:node_model train_loss:0.029318104863391613,train_acc:0.9902236461639404
node19_model on test-dataset: loss:0.061531936130631945,acc:0.9802990555763245
node19 weight score:93951.21238712479
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04067727603425737,acc:0.9867979544401169
total cost energy:11.080677876642891 | all_enery_cp：9.2675 | all_enery_tp: 1.8131778766428917
ef: 32.050378150845695
reward: 20.969700274202804
step 95:loss:221.53431701660156|running q:39.21373748779297
episode1,iteration35 selected nodes:[15, 4, 5, 8, 14],center node:8
################################################## episode1,iteration35 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04289716389030218,train_acc:0.9872044324874878
node4 epoch1:node_model train_loss:0.01961666658564016,train_acc:0.9941813349723816
node4_model on test-dataset: loss:0.06036079169411096,acc:0.9811971187591553
node4 weight score:71205.16281133088
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04113336140289903,train_acc:0.9879591464996338
node5 epoch1:node_model train_loss:0.027280957537836263,train_acc:0.9895918369293213
node5_model on test-dataset: loss:0.06368573541694786,acc:0.9804989695549011
node5 weight score:75951.07394665953
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06188262579962611,train_acc:0.9790337085723877
node8 epoch1:node_model train_loss:0.03181511916868065,train_acc:0.9895169138908386
node8_model on test-dataset: loss:0.06421921892375394,acc:0.9793977737426758
node8 weight score:35659.10701465968
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04997688118601218,train_acc:0.9799999594688416
node14 epoch1:node_model train_loss:0.04198922011710238,train_acc:0.982499897480011
node14_model on test-dataset: loss:0.09427272318087489,acc:0.9738991856575012
node14 weight score:16335.584122729786
node15: train data size:1376
node15 epoch0:node_model train_loss:0.039312902837991714,train_acc:0.9852631092071533
node15 epoch1:node_model train_loss:0.018331936635409614,train_acc:0.9940601587295532
node15_model on test-dataset: loss:0.07157864484936,acc:0.9796978831291199
node15 weight score:19223.610657841382
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04260318954893592,acc:0.9861979550123214
total cost energy:8.358205430228724 | all_enery_cp：7.170499999999999 | all_enery_tp: 1.1877054302287244
ef: 32.26175129983099
reward: 23.90354586960227
step 96:loss:633.5880126953125|running q:40.44027328491211
episode1,iteration36 selected nodes:[7, 12, 1, 11, 18],center node:12
################################################## episode1,iteration36 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04970223897435605,train_acc:0.9855958819389343
node1 epoch1:node_model train_loss:0.0250313338100799,train_acc:0.9917911887168884
node1_model on test-dataset: loss:0.07348956881702179,acc:0.9783970713615417
node1 weight score:90584.28437068328
node7: train data size:3637
node7 epoch0:node_model train_loss:0.043573109741398205,train_acc:0.9856756329536438
node7 epoch1:node_model train_loss:0.03064208248365872,train_acc:0.990810751914978
node7_model on test-dataset: loss:0.07348693272972014,acc:0.9761961102485657
node7 weight score:49491.792144552215
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05299875565106049,train_acc:0.9812499284744263
node11 epoch1:node_model train_loss:0.03378937461820897,train_acc:0.9887499809265137
node11_model on test-dataset: loss:0.06893109932483639,acc:0.9789959788322449
node11 weight score:22848.90296871438
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05029602504024903,train_acc:0.9866666197776794
node12 epoch1:node_model train_loss:0.02324514804252734,train_acc:0.9893333911895752
node12_model on test-dataset: loss:0.07921151934820955,acc:0.9760971665382385
node12 weight score:17749.943588625036
node18: train data size:801
node18 epoch0:node_model train_loss:0.04518873168207291,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.02042560465633869,train_acc:0.9944443106651306
node18_model on test-dataset: loss:0.08175717602483928,acc:0.9760991930961609
node18 weight score:9797.305128991271
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04317933570848254,acc:0.9870969438552857
total cost energy:8.144449510224597 | all_enery_cp：7.037999999999999 | all_enery_tp: 1.106449510224598
ef: 32.01807557614669
reward: 23.873626065922096
step 97:loss:325.73114013671875|running q:41.27058410644531
episode1,iteration37 selected nodes:[10, 13, 15, 19, 1],center node:15
################################################## episode1,iteration37 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.040120432778859315,train_acc:0.9879106879234314
node1 epoch1:node_model train_loss:0.026554869484406576,train_acc:0.9925374984741211
node1_model on test-dataset: loss:0.07040832945944203,acc:0.9784941077232361
node1 weight score:94548.47247632389
node10: train data size:1915
node10 epoch0:node_model train_loss:0.055314907385036347,train_acc:0.9829999208450317
node10 epoch1:node_model train_loss:0.03107858975417912,train_acc:0.9889998435974121
node10_model on test-dataset: loss:0.07457410730305128,acc:0.9775971174240112
node10 weight score:25679.154189776615
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05410700511525978,train_acc:0.981103777885437
node13 epoch1:node_model train_loss:0.04470032563602382,train_acc:0.9881817698478699
node13_model on test-dataset: loss:0.05655888682929799,acc:0.9825959801673889
node13 weight score:18670.805936954595
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05231642623298934,train_acc:0.9831202626228333
node15 epoch1:node_model train_loss:0.044367065437005034,train_acc:0.9897744655609131
node15_model on test-dataset: loss:0.0786952846515669,acc:0.9757980704307556
node15 weight score:17485.164531679504
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04405433096087955,train_acc:0.9846553206443787
node19 epoch1:node_model train_loss:0.020959948342100812,train_acc:0.9932355880737305
node19_model on test-dataset: loss:0.06793173507252505,acc:0.9789979457855225
node19 weight score:85100.13756940122
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0431172620441248,acc:0.9864969462156296
total cost energy:9.860300411091545 | all_enery_cp：8.3925 | all_enery_tp: 1.467800411091546
ef: 32.2138949299534
reward: 22.353594518861854
step 98:loss:353.7091369628906|running q:42.02265167236328
episode1,iteration38 selected nodes:[6, 19, 9, 12, 18],center node:12
################################################## episode1,iteration38 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05235942330586517,train_acc:0.9840420484542847
node6 epoch1:node_model train_loss:0.03197257286713769,train_acc:0.9898754954338074
node6_model on test-dataset: loss:0.10320004676817916,acc:0.970595121383667
node6 weight score:34195.720937290665
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04244005822957578,train_acc:0.983636200428009
node9 epoch1:node_model train_loss:0.03083991622142176,train_acc:0.991363525390625
node9_model on test-dataset: loss:0.0785121297146543,acc:0.9773930907249451
node9 weight score:27065.881510578467
node12: train data size:1406
node12 epoch0:node_model train_loss:0.042354961042292416,train_acc:0.9853333830833435
node12 epoch1:node_model train_loss:0.02073342523459966,train_acc:0.994666576385498
node12_model on test-dataset: loss:0.06940569506274187,acc:0.9804980158805847
node12 weight score:20257.70361825487
node18: train data size:801
node18 epoch0:node_model train_loss:0.022212614693292682,train_acc:0.9888888597488403
node18 epoch1:node_model train_loss:0.03519284207141027,train_acc:0.9911110401153564
node18_model on test-dataset: loss:0.07573090067409793,acc:0.9764977693557739
node18 weight score:10576.92425245332
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03669681663199424,train_acc:0.9887527823448181
node19 epoch1:node_model train_loss:0.02202057105625562,train_acc:0.9940976500511169
node19_model on test-dataset: loss:0.057524737002895565,acc:0.9823001027107239
node19 weight score:100495.89622129012
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04218342959138681,acc:0.9867979550361633
total cost energy:8.20384271247462 | all_enery_cp：6.821 | all_enery_tp: 1.3828427124746192
ef: 31.910485540968534
reward: 23.706642828493912
step 99:loss:282.7502746582031|running q:42.87963104248047
episode1,iteration39 selected nodes:[17, 14, 7, 15, 18],center node:15
################################################## episode1,iteration39 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.042477582414584186,train_acc:0.9876477122306824
node7 epoch1:node_model train_loss:0.029823655155613214,train_acc:0.989810049533844
node7_model on test-dataset: loss:0.06801491720996637,acc:0.9797968864440918
node7 weight score:53473.56358271157
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05648812680738047,train_acc:0.9796874523162842
node14 epoch1:node_model train_loss:0.025586931573343463,train_acc:0.9887499809265137
node14_model on test-dataset: loss:0.07957765801898858,acc:0.9772000908851624
node14 weight score:19352.16539839021
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05369299419024693,train_acc:0.9809775352478027
node15 epoch1:node_model train_loss:0.030943971759240543,train_acc:0.991428554058075
node15_model on test-dataset: loss:0.06771861998829991,acc:0.9790949821472168
node15 weight score:20319.37449755678
node17: train data size:719
node17 epoch0:node_model train_loss:0.05529305736126844,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.0324047758767847,train_acc:0.9834210276603699
node17_model on test-dataset: loss:0.05274001263547689,acc:0.9831958413124084
node17 weight score:13632.91292646272
node18: train data size:801
node18 epoch0:node_model train_loss:0.03646188583742413,train_acc:0.9922221302986145
node18 epoch1:node_model train_loss:0.026649248669855297,train_acc:0.992222249507904
node18_model on test-dataset: loss:0.08372061286855569,acc:0.9752938747406006
node18 weight score:9567.536268010821
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04700061604333314,acc:0.9856989651918411
total cost energy:5.301528153987289 | all_enery_cp：4.0365 | all_enery_tp: 1.2650281539872885
ef: 31.923742566491747
reward: 26.62221441250446
step 100:loss:242.78614807128906|running q:43.870059967041016
episode1,iteration40 selected nodes:[19, 14, 5, 16, 3],center node:5
################################################## episode1,iteration40 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05167755188028279,train_acc:0.9855262637138367
node3 epoch1:node_model train_loss:0.02933326062109125,train_acc:0.9905261993408203
node3_model on test-dataset: loss:0.07121502569338191,acc:0.9771969318389893
node3 weight score:52825.930530410624
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03533138674968968,train_acc:0.9879591464996338
node5 epoch1:node_model train_loss:0.024441351116235768,train_acc:0.9902650117874146
node5_model on test-dataset: loss:0.05418423893293948,acc:0.9839958548545837
node5 weight score:89269.50152398486
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05141862756499904,train_acc:0.982499897480011
node14 epoch1:node_model train_loss:0.026376151741715148,train_acc:0.9912499189376831
node14_model on test-dataset: loss:0.06638028232260694,acc:0.9807982444763184
node14 weight score:23199.660292428835
node16: train data size:920
node16 epoch0:node_model train_loss:0.059960805613081904,train_acc:0.9809999465942383
node16 epoch1:node_model train_loss:0.02214878679951653,train_acc:0.9909999966621399
node16_model on test-dataset: loss:0.06099104832101147,acc:0.9821929931640625
node16 weight score:15084.180799087188
node19: train data size:5781
node19 epoch0:node_model train_loss:0.02664439258133543,train_acc:0.9912071228027344
node19 epoch1:node_model train_loss:0.024512247687229757,train_acc:0.9918966293334961
node19_model on test-dataset: loss:0.06524116164793668,acc:0.9808971881866455
node19 weight score:88609.70365911366
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04211892842060479,acc:0.9875979536771774
total cost energy:10.295091111987144 | all_enery_cp：8.42 | all_enery_tp: 1.875091111987145
ef: 32.22937144954649
reward: 21.934280337559347
step 101:loss:309.4233703613281|running q:44.894832611083984
episode1,iteration41 selected nodes:[11, 6, 18, 1, 17],center node:11
################################################## episode1,iteration41 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03379477240117406,train_acc:0.9892539381980896
node1 epoch1:node_model train_loss:0.026962062938194444,train_acc:0.9905973672866821
node1_model on test-dataset: loss:0.07147024362295269,acc:0.9802958369255066
node1 weight score:93143.65899072019
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04587815633082452,train_acc:0.9847220182418823
node6 epoch1:node_model train_loss:0.02673456573716572,train_acc:0.9918199181556702
node6_model on test-dataset: loss:0.07332920837100573,acc:0.9789942502975464
node6 weight score:48125.43430368412
node11: train data size:1575
node11 epoch0:node_model train_loss:0.047288801666582,train_acc:0.9833332300186157
node11 epoch1:node_model train_loss:0.02653976163128391,train_acc:0.9889582395553589
node11_model on test-dataset: loss:0.06582982256782997,acc:0.9824951887130737
node11 weight score:23925.32652472435
node17: train data size:719
node17 epoch0:node_model train_loss:0.09325703885406256,train_acc:0.9821709990501404
node17 epoch1:node_model train_loss:0.0317941535031423,train_acc:0.9912499189376831
node17_model on test-dataset: loss:0.07268937980523332,acc:0.9773980379104614
node17 weight score:9891.403695099832
node18: train data size:801
node18 epoch0:node_model train_loss:0.03110832032851047,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.020838499418459833,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.08217540287762859,acc:0.9774981737136841
node18 weight score:9747.442323986026
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04570150931733224,acc:0.9868959373235703
total cost energy:7.934563876655657 | all_enery_cp：6.6405 | all_enery_tp: 1.2940638766556565
ef: 31.906439484535884
reward: 23.97187560788023
step 102:loss:221.05262756347656|running q:45.703372955322266
episode1,iteration42 selected nodes:[14, 5, 16, 18, 11],center node:11
################################################## episode1,iteration42 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.035740264624889405,train_acc:0.9879373908042908
node5 epoch1:node_model train_loss:0.027996336566094234,train_acc:0.9914894104003906
node5_model on test-dataset: loss:0.06196654841507552,acc:0.9819971919059753
node5 weight score:78058.24470970906
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0452770485135261,train_acc:0.9843749403953552
node11 epoch1:node_model train_loss:0.026575261843390763,train_acc:0.9931249022483826
node11_model on test-dataset: loss:0.06655022500141058,acc:0.9808979630470276
node11 weight score:23666.33621398901
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06154312880244106,train_acc:0.9790624380111694
node14 epoch1:node_model train_loss:0.022871577173646074,train_acc:0.9937499165534973
node14_model on test-dataset: loss:0.06627638575577294,acc:0.9795960187911987
node14 weight score:23236.02867656162
node16: train data size:920
node16 epoch0:node_model train_loss:0.050822020741179585,train_acc:0.9789999127388
node16 epoch1:node_model train_loss:0.0277586932759732,train_acc:0.9930000305175781
node16_model on test-dataset: loss:0.08273808581827324,acc:0.9773922562599182
node16 weight score:11119.42572639035
node18: train data size:801
node18 epoch0:node_model train_loss:0.034590491818057165,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.02205402582572131,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.0748517398063541,acc:0.9789979457855225
node18 weight score:10701.154068993383
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04523046415401041,acc:0.9862969452142716
total cost energy:6.149200253874981 | all_enery_cp：4.836499999999999 | all_enery_tp: 1.312700253874982
ef: 31.85631306261645
reward: 25.70711280874147
step 103:loss:352.8321228027344|running q:46.31671905517578
episode1,iteration43 selected nodes:[6, 3, 13, 1, 14],center node:6
################################################## episode1,iteration43 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.02931472047583075,train_acc:0.9905211925506592
node1 epoch1:node_model train_loss:0.02246111236797618,train_acc:0.9926868081092834
node1_model on test-dataset: loss:0.06273337478201939,acc:0.9825960993766785
node1 weight score:106115.76410692361
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04144628649565244,train_acc:0.9857893586158752
node3 epoch1:node_model train_loss:0.026115922269558434,train_acc:0.9916808605194092
node3_model on test-dataset: loss:0.07270079702720977,acc:0.981096088886261
node3 weight score:51746.33778212905
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04226517402437619,train_acc:0.9854309558868408
node6 epoch1:node_model train_loss:0.027032152959792357,train_acc:0.9894443154335022
node6_model on test-dataset: loss:0.0640780030845417,acc:0.9796969294548035
node6 weight score:55073.501515707234
node13: train data size:1056
node13 epoch0:node_model train_loss:0.0676004175435413,train_acc:0.9774674773216248
node13 epoch1:node_model train_loss:0.04181929982521317,train_acc:0.9885714650154114
node13_model on test-dataset: loss:0.07477774210608913,acc:0.9777958393096924
node13 weight score:14121.849232915127
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04743002352915937,train_acc:0.9828124642372131
node14 epoch1:node_model train_loss:0.026160416135098785,train_acc:0.9884374141693115
node14_model on test-dataset: loss:0.08826189657740542,acc:0.9758960008621216
node14 weight score:17448.07283457165
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043281385557493196,acc:0.987397952079773
total cost energy:10.123926570060751 | all_enery_cp：8.272 | all_enery_tp: 1.8519265700607512
ef: 31.92656431704777
reward: 21.802637746987017
step 104:loss:270.5078430175781|running q:46.900543212890625
episode1,iteration44 selected nodes:[16, 6, 11, 5, 2],center node:6
################################################## episode1,iteration44 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05422295175401117,train_acc:0.9821276068687439
node2 epoch1:node_model train_loss:0.0235855619483776,train_acc:0.992553174495697
node2_model on test-dataset: loss:0.05795492118428228,acc:0.9825971126556396
node2 weight score:79544.58233738845
node5: train data size:4837
node5 epoch0:node_model train_loss:0.028015610990019476,train_acc:0.9900000095367432
node5 epoch1:node_model train_loss:0.02333923983470803,train_acc:0.992449164390564
node5_model on test-dataset: loss:0.07501513184190117,acc:0.9793941974639893
node5 weight score:64480.323919102935
node6: train data size:3529
node6 epoch0:node_model train_loss:0.049506333035727344,train_acc:0.984319806098938
node6 epoch1:node_model train_loss:0.027583635530188784,train_acc:0.9905555844306946
node6_model on test-dataset: loss:0.07984449058040809,acc:0.9784971475601196
node6 weight score:44198.41587499503
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04441076211514883,train_acc:0.9849998950958252
node11 epoch1:node_model train_loss:0.03532233918667771,train_acc:0.9874998927116394
node11_model on test-dataset: loss:0.10073137124220466,acc:0.9727868437767029
node11 weight score:15635.645386112872
node16: train data size:920
node16 epoch0:node_model train_loss:0.03746918257384095,train_acc:0.9879999160766602
node16 epoch1:node_model train_loss:0.032155993266496805,train_acc:0.9919999241828918
node16_model on test-dataset: loss:0.07050215183306136,acc:0.9792940020561218
node16 weight score:13049.247095016668
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.046939819497347345,acc:0.9867979544401169
total cost energy:8.719661925296379 | all_enery_cp：7.7355 | all_enery_tp: 0.984161925296378
ef: 32.00581529255681
reward: 23.286153367260432
step 105:loss:229.46800231933594|running q:47.3934440612793
episode1,iteration45 selected nodes:[1, 15, 16, 17, 11],center node:15
################################################## episode1,iteration45 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.026416931724446868,train_acc:0.9913435578346252
node1 epoch1:node_model train_loss:0.01932992361687513,train_acc:0.9937315583229065
node1_model on test-dataset: loss:0.0674401138004032,acc:0.9799970984458923
node1 weight score:98709.79784675571
node11: train data size:1575
node11 epoch0:node_model train_loss:0.039706065588688944,train_acc:0.9874999523162842
node11 epoch1:node_model train_loss:0.02295101097843144,train_acc:0.9922915697097778
node11_model on test-dataset: loss:0.07810085091334258,acc:0.9781928062438965
node11 weight score:20166.233550355984
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04925793625547418,train_acc:0.9857141971588135
node15 epoch1:node_model train_loss:0.03829705834920917,train_acc:0.9900376200675964
node15_model on test-dataset: loss:0.06470363197964617,acc:0.9800998568534851
node15 weight score:21266.19415171081
node16: train data size:920
node16 epoch0:node_model train_loss:0.058654300821945074,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.06285736123099923,train_acc:0.9799999594688416
node16_model on test-dataset: loss:0.07389410807198146,acc:0.977496862411499
node16 weight score:12450.248389273647
node17: train data size:719
node17 epoch0:node_model train_loss:0.05208749696612358,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.014277136302553117,train_acc:0.9962499737739563
node17_model on test-dataset: loss:0.07397789229187765,acc:0.9783949255943298
node17 weight score:9719.119830600284
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04475990973875014,acc:0.985698961019516
total cost energy:6.778989848529779 | all_enery_cp：5.623499999999999 | all_enery_tp: 1.15548984852978
ef: 32.06307543472729
reward: 25.28408558619751
step 106:loss:270.5136413574219|running q:48.2712287902832
episode1,iteration46 selected nodes:[5, 4, 10, 0, 3],center node:4
################################################## episode1,iteration46 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04663687122309865,train_acc:0.9853421449661255
node0 epoch1:node_model train_loss:0.03148969506340412,train_acc:0.989508867263794
node0_model on test-dataset: loss:0.06961148180005694,acc:0.9805939793586731
node0 weight score:102928.42236256116
node3: train data size:3762
node3 epoch0:node_model train_loss:0.043089571672393696,train_acc:0.9868420958518982
node3 epoch1:node_model train_loss:0.026282950887291105,train_acc:0.9899999499320984
node3_model on test-dataset: loss:0.07114512799283944,acc:0.9797980785369873
node3 weight score:52877.83023425912
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04379894268471574,train_acc:0.9855765104293823
node4 epoch1:node_model train_loss:0.019971450368418943,train_acc:0.9937210083007812
node4_model on test-dataset: loss:0.0667989179572396,acc:0.9810979962348938
node4 weight score:64342.35959856871
node5: train data size:4837
node5 epoch0:node_model train_loss:0.027525615636069253,train_acc:0.9900001287460327
node5 epoch1:node_model train_loss:0.015534500675798603,train_acc:0.9946939945220947
node5_model on test-dataset: loss:0.10230574579456515,acc:0.9704920053482056
node5 weight score:47279.84691801112
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05659747022436932,train_acc:0.9834999442100525
node10 epoch1:node_model train_loss:0.027988541102968157,train_acc:0.9919999241828918
node10_model on test-dataset: loss:0.05945426526997835,acc:0.9821000695228577
node10 weight score:32209.631912935034
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041904421964572974,acc:0.9865979522466659
total cost energy:12.273319196258328 | all_enery_cp：10.9885 | all_enery_tp: 1.2848191962583275
ef: 32.29822021611703
reward: 20.024901019858703
step 107:loss:182.12826538085938|running q:49.23904800415039
episode1,iteration47 selected nodes:[4, 13, 10, 5, 2],center node:5
################################################## episode1,iteration47 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05190805705252445,train_acc:0.9844680428504944
node2 epoch1:node_model train_loss:0.03196226946137687,train_acc:0.9900001287460327
node2_model on test-dataset: loss:0.07859045375415008,acc:0.9760960936546326
node2 weight score:58658.52377467107
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03300581221620357,train_acc:0.9897626638412476
node4 epoch1:node_model train_loss:0.02296152259060723,train_acc:0.9920788407325745
node4_model on test-dataset: loss:0.07881146043720946,acc:0.9786950945854187
node4 weight score:54535.2157688322
node5: train data size:4837
node5 epoch0:node_model train_loss:0.031276316617197375,train_acc:0.9895918369293213
node5 epoch1:node_model train_loss:0.02290792754323849,train_acc:0.9918369054794312
node5_model on test-dataset: loss:0.0734007504885085,acc:0.9799948930740356
node5 weight score:65898.50877283976
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0481774951913394,train_acc:0.9861665964126587
node10 epoch1:node_model train_loss:0.023448361913324334,train_acc:0.9914999008178711
node10_model on test-dataset: loss:0.09517337469413178,acc:0.9718950390815735
node10 weight score:20121.173659696608
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05797023156827146,train_acc:0.9820130467414856
node13 epoch1:node_model train_loss:0.021332008637149225,train_acc:0.9929220080375671
node13_model on test-dataset: loss:0.07603301373768773,acc:0.9771969318389893
node13 weight score:13888.703710248517
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04208365072368906,acc:0.987695934176445
total cost energy:9.802965990938048 | all_enery_cp：8.358 | all_enery_tp: 1.4449659909380483
ef: 31.665764477744695
reward: 21.862798486806646
step 108:loss:276.4036865234375|running q:50.56964111328125
episode1,iteration48 selected nodes:[7, 5, 16, 1, 9],center node:5
################################################## episode1,iteration48 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.021766508697469565,train_acc:0.9929853677749634
node1 epoch1:node_model train_loss:0.021936737267654945,train_acc:0.9926866888999939
node1_model on test-dataset: loss:0.08699602549880979,acc:0.9755958318710327
node1 weight score:76520.73714667662
node5: train data size:4837
node5 epoch0:node_model train_loss:0.019195497404232775,train_acc:0.9930613040924072
node5 epoch1:node_model train_loss:0.016578467010415862,train_acc:0.9946939945220947
node5_model on test-dataset: loss:0.0904130946960504,acc:0.9771931171417236
node5 weight score:53498.88770272676
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04480705454920393,train_acc:0.9867565035820007
node7 epoch1:node_model train_loss:0.026937027765450546,train_acc:0.9916216135025024
node7_model on test-dataset: loss:0.06640551071353912,acc:0.9804941415786743
node7 weight score:54769.55091406998
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03808998149311678,train_acc:0.9877272844314575
node9 epoch1:node_model train_loss:0.023129095513881606,train_acc:0.991363525390625
node9_model on test-dataset: loss:0.05789437325693143,acc:0.9832930564880371
node9 weight score:36704.775964486034
node16: train data size:920
node16 epoch0:node_model train_loss:0.047291984580806454,train_acc:0.9829999208450317
node16 epoch1:node_model train_loss:0.02224246223922819,train_acc:0.9919999241828918
node16_model on test-dataset: loss:0.07609868127416121,acc:0.9789959788322449
node16 weight score:12089.565608706282
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043979531803515784,acc:0.9863959336280823
total cost energy:10.695768723046358 | all_enery_cp：9.088000000000001 | all_enery_tp: 1.607768723046357
ef: 31.796216659176316
reward: 21.10044793612996
step 109:loss:170.4973907470703|running q:51.64268112182617
episode1,iteration49 selected nodes:[0, 15, 16, 17, 8],center node:15
################################################## episode1,iteration49 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03864465626732757,train_acc:0.9883335828781128
node0 epoch1:node_model train_loss:0.026041786158910125,train_acc:0.9912501573562622
node0_model on test-dataset: loss:0.06497098970245134,acc:0.9814959764480591
node0 weight score:110279.98854278907
node8: train data size:2290
node8 epoch0:node_model train_loss:0.047365093315222664,train_acc:0.9842509627342224
node8 epoch1:node_model train_loss:0.028597188830051735,train_acc:0.99173903465271
node8_model on test-dataset: loss:0.05840314004744869,acc:0.9808979630470276
node8 weight score:39210.22051450532
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04300874944393789,train_acc:0.9871428608894348
node15 epoch1:node_model train_loss:0.0223125399622534,train_acc:0.993571400642395
node15_model on test-dataset: loss:0.07293661930115831,acc:0.9803969264030457
node15 weight score:18865.694807137126
node16: train data size:920
node16 epoch0:node_model train_loss:0.04373870916897431,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.018278325046412646,train_acc:0.9919999241828918
node16_model on test-dataset: loss:0.07195137875094587,acc:0.9806939959526062
node16 weight score:12786.412379733663
node17: train data size:719
node17 epoch0:node_model train_loss:0.028832625481300056,train_acc:0.9874998927116394
node17 epoch1:node_model train_loss:0.00881416657648515,train_acc:0.9975000023841858
node17_model on test-dataset: loss:0.0890358803273557,acc:0.9751980900764465
node17 weight score:8075.39609151359
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04036767337875062,acc:0.9875959330797195
total cost energy:7.7269306483427345 | all_enery_cp：6.234999999999999 | all_enery_tp: 1.4919306483427348
ef: 32.0497885989597
reward: 24.322857950616967
step 110:loss:257.7229919433594|running q:52.63090515136719
episode1,iteration50 selected nodes:[17, 18, 14, 0, 1],center node:17
################################################## episode1,iteration50 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.027761614879839018,train_acc:0.990833580493927
node0 epoch1:node_model train_loss:0.023955406976812,train_acc:0.9921369552612305
node0_model on test-dataset: loss:0.08318577431695302,acc:0.9756957292556763
node0 weight score:86132.51555127729
node1: train data size:6657
node1 epoch0:node_model train_loss:0.023943467180804114,train_acc:0.9919770359992981
node1 epoch1:node_model train_loss:0.01602295810337623,train_acc:0.9958211183547974
node1_model on test-dataset: loss:0.06000734085078875,acc:0.9804979562759399
node1 weight score:110936.4272040143
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05578218027949333,train_acc:0.9824998378753662
node14 epoch1:node_model train_loss:0.030075593465880957,train_acc:0.9893749356269836
node14_model on test-dataset: loss:0.06753888953436217,acc:0.9807000756263733
node14 weight score:22801.677827653428
node17: train data size:719
node17 epoch0:node_model train_loss:0.03220734580827411,train_acc:0.9912500381469727
node17 epoch1:node_model train_loss:0.024513576936442405,train_acc:0.9925000071525574
node17_model on test-dataset: loss:0.06394470081435429,acc:0.9807950258255005
node17 weight score:11244.090453834748
node18: train data size:801
node18 epoch0:node_model train_loss:0.032774535990837544,train_acc:0.9888888597488403
node18 epoch1:node_model train_loss:0.025766120856941497,train_acc:0.9888888597488403
node18_model on test-dataset: loss:0.0618714771733994,acc:0.9809961318969727
node18 weight score:12946.191631325339
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.044203813317071766,acc:0.9864969420433044
total cost energy:10.588428332676365 | all_enery_cp：8.440999999999999 | all_enery_tp: 2.1474283326763652
ef: 32.109855637643065
reward: 21.5214273049667
step 111:loss:301.72003173828125|running q:53.5489387512207
episode1,iteration51 selected nodes:[11, 4, 1, 7, 8],center node:8
################################################## episode1,iteration51 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.02099745795512989,train_acc:0.9910449981689453
node1 epoch1:node_model train_loss:0.015675179046533528,train_acc:0.9946269392967224
node1_model on test-dataset: loss:0.06908594033906411,acc:0.9815980195999146
node1 weight score:96358.24550304125
node4: train data size:4298
node4 epoch0:node_model train_loss:0.043748702358476124,train_acc:0.9869672060012817
node4 epoch1:node_model train_loss:0.018119510835805517,train_acc:0.993948757648468
node4_model on test-dataset: loss:0.07328383648632325,acc:0.9798908233642578
node4 weight score:58648.67624393714
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03669853761597108,train_acc:0.9889188408851624
node7 epoch1:node_model train_loss:0.025205349559090227,train_acc:0.9918918609619141
node7_model on test-dataset: loss:0.08592136138817295,acc:0.9766898155212402
node7 weight score:42329.40378550184
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04396394270209029,train_acc:0.9877293705940247
node8 epoch1:node_model train_loss:0.02869842651948009,train_acc:0.9926086068153381
node8_model on test-dataset: loss:0.06541786618640799,acc:0.9811979532241821
node8 weight score:35005.72754046506
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0567212563182693,train_acc:0.9843748807907104
node11 epoch1:node_model train_loss:0.02522894393769093,train_acc:0.9947916269302368
node11_model on test-dataset: loss:0.07279261013254654,acc:0.9796916842460632
node11 weight score:21636.81171937805
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04490503251287009,acc:0.9872979521751404
total cost energy:10.680645126328583 | all_enery_cp：9.2285 | all_enery_tp: 1.452145126328583
ef: 32.20208588673429
reward: 21.521440760405703
step 112:loss:131.0798797607422|running q:54.50336837768555
episode1,iteration52 selected nodes:[12, 2, 18, 19, 9],center node:12
################################################## episode1,iteration52 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05211280209922289,train_acc:0.9840425252914429
node2 epoch1:node_model train_loss:0.022535906757842353,train_acc:0.9931917190551758
node2_model on test-dataset: loss:0.059005899736985154,acc:0.9825940728187561
node2 weight score:78127.7807905441
node9: train data size:2125
node9 epoch0:node_model train_loss:0.047182808423795825,train_acc:0.98499995470047
node9 epoch1:node_model train_loss:0.019659070134035905,train_acc:0.9945454597473145
node9_model on test-dataset: loss:0.06520900927840557,acc:0.98099684715271
node9 weight score:32587.521624925357
node12: train data size:1406
node12 epoch0:node_model train_loss:0.039698220292727154,train_acc:0.9859999418258667
node12 epoch1:node_model train_loss:0.020245913593801863,train_acc:0.9939998984336853
node12_model on test-dataset: loss:0.07586177071887505,acc:0.9787949323654175
node12 weight score:18533.709227672633
node18: train data size:801
node18 epoch0:node_model train_loss:0.008593916585798271,train_acc:0.9988888502120972
node18 epoch1:node_model train_loss:0.014552894902105132,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.0792073302381823,acc:0.9804958701133728
node18 weight score:10112.70039769468
node19: train data size:5781
node19 epoch0:node_model train_loss:0.040571428436468805,train_acc:0.9896553158760071
node19 epoch1:node_model train_loss:0.019708798644293486,train_acc:0.993793249130249
node19_model on test-dataset: loss:0.05872928398763179,acc:0.98069828748703
node19 weight score:98434.70935585494
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03664345733557638,acc:0.9885979557037353
total cost energy:8.944342712474619 | all_enery_cp：7.3614999999999995 | all_enery_tp: 1.5828427124746192
ef: 32.21799063225643
reward: 23.273647919781812
step 113:loss:142.4247589111328|running q:55.6159782409668
episode1,iteration53 selected nodes:[6, 9, 17, 12, 19],center node:17
################################################## episode1,iteration53 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.039982392630513966,train_acc:0.9872220158576965
node6 epoch1:node_model train_loss:0.028811731970765524,train_acc:0.9902775883674622
node6_model on test-dataset: loss:0.05999626237444318,acc:0.9830971956253052
node6 weight score:58820.33080619469
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03160275722621009,train_acc:0.9899998903274536
node9 epoch1:node_model train_loss:0.02297365910040257,train_acc:0.991363525390625
node9_model on test-dataset: loss:0.05116374854620517,acc:0.9844968914985657
node9 weight score:41533.31333963824
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03055838724806866,train_acc:0.9886665940284729
node12 epoch1:node_model train_loss:0.01567574224609416,train_acc:0.9960000514984131
node12_model on test-dataset: loss:0.060745840741528806,acc:0.9838968515396118
node12 weight score:23145.617590222766
node17: train data size:719
node17 epoch0:node_model train_loss:0.030883802261087112,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.01666195780853741,train_acc:0.9924999475479126
node17_model on test-dataset: loss:0.05541796870034887,acc:0.9841999411582947
node17 weight score:12974.131258540225
node19: train data size:5781
node19 epoch0:node_model train_loss:0.027952719566107183,train_acc:0.9915115833282471
node19 epoch1:node_model train_loss:0.024667286283962815,train_acc:0.9925863742828369
node19_model on test-dataset: loss:0.09926723548456722,acc:0.9728968739509583
node19 weight score:58236.73815212427
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0381983037748887,acc:0.9893969470262527
total cost energy:8.095855197262503 | all_enery_cp：6.779999999999999 | all_enery_tp: 1.3158551972625048
ef: 32.543423686708444
reward: 24.447568489445942
step 114:loss:247.13577270507812|running q:56.527278900146484
episode1,iteration54 selected nodes:[1, 17, 3, 13, 0],center node:0
################################################## episode1,iteration54 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.026203130900588196,train_acc:0.992361307144165
node0 epoch1:node_model train_loss:0.019757027128636966,train_acc:0.993611216545105
node0_model on test-dataset: loss:0.07225061385681329,acc:0.9806920289993286
node0 weight score:99168.7076071027
node1: train data size:6657
node1 epoch0:node_model train_loss:0.019025454866055246,train_acc:0.9937681555747986
node1 epoch1:node_model train_loss:0.013109396801264123,train_acc:0.9958576560020447
node1_model on test-dataset: loss:0.06544402226075363,acc:0.9839982986450195
node1 weight score:101720.52037810275
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05569678108134356,train_acc:0.9814599752426147
node3 epoch1:node_model train_loss:0.02664372713096734,train_acc:0.9902631044387817
node3_model on test-dataset: loss:0.06330093980403034,acc:0.981097936630249
node3 weight score:59430.39726813779
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04877227165905589,train_acc:0.9840258955955505
node13 epoch1:node_model train_loss:0.02615689606913789,train_acc:0.9903895854949951
node13_model on test-dataset: loss:0.08127818177350492,acc:0.9771952033042908
node13 weight score:12992.416623476132
node17: train data size:719
node17 epoch0:node_model train_loss:0.04171761544421315,train_acc:0.9846710562705994
node17 epoch1:node_model train_loss:0.03208557207835838,train_acc:0.9874999523162842
node17_model on test-dataset: loss:0.05071546014572959,acc:0.9848940968513489
node17 weight score:14177.136477397064
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039307144365884596,acc:0.9883979558944702
total cost energy:11.849091084065115 | all_enery_cp：9.679499999999999 | all_enery_tp: 2.1695910840651162
ef: 31.783180850225634
reward: 19.93408976616052
step 115:loss:218.84092712402344|running q:57.35698699951172
episode1,iteration55 selected nodes:[18, 10, 6, 5, 14],center node:10
################################################## episode1,iteration55 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.026348007036603473,train_acc:0.9918367862701416
node5 epoch1:node_model train_loss:0.020264728924934278,train_acc:0.9918974041938782
node5_model on test-dataset: loss:0.05764411533216389,acc:0.9844991564750671
node5 weight score:83911.42742199535
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03153409259781862,train_acc:0.9898754954338074
node6 epoch1:node_model train_loss:0.028703708091699,train_acc:0.9901531934738159
node6_model on test-dataset: loss:0.05018639251880813,acc:0.9850970506668091
node6 weight score:70317.86551857562
node10: train data size:1915
node10 epoch0:node_model train_loss:0.046423326910007744,train_acc:0.9864999055862427
node10 epoch1:node_model train_loss:0.024914462625747546,train_acc:0.9929999709129333
node10_model on test-dataset: loss:0.06577526353765278,acc:0.9811972379684448
node10 weight score:29114.288518262885
node14: train data size:1540
node14 epoch0:node_model train_loss:0.060674978885799646,train_acc:0.9815624356269836
node14 epoch1:node_model train_loss:0.0207956480953726,train_acc:0.9931249618530273
node14_model on test-dataset: loss:0.049044406620014344,acc:0.9848950505256653
node14 weight score:31400.114837387948
node18: train data size:801
node18 epoch0:node_model train_loss:0.020301181291061867,train_acc:0.9933333992958069
node18 epoch1:node_model train_loss:0.012091664097372105,train_acc:0.9977777600288391
node18_model on test-dataset: loss:0.07914301325015118,acc:0.9795919060707092
node18 weight score:10120.918664901477
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03668389593719439,acc:0.9880969446897506
total cost energy:7.657285203759807 | all_enery_cp：6.311 | all_enery_tp: 1.3462852037598072
ef: 32.64925250986137
reward: 24.991967306101564
step 116:loss:202.8938751220703|running q:58.29396438598633
episode1,iteration56 selected nodes:[0, 8, 19, 10, 9],center node:8
################################################## episode1,iteration56 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.021809635366985783,train_acc:0.9924998879432678
node0 epoch1:node_model train_loss:0.017062601802333828,train_acc:0.9948611855506897
node0_model on test-dataset: loss:0.08186526237896033,acc:0.9787958264350891
node0 weight score:87521.85959940722
node8: train data size:2290
node8 epoch0:node_model train_loss:0.050805854469375765,train_acc:0.9860868453979492
node8 epoch1:node_model train_loss:0.01919028771858986,train_acc:0.994782567024231
node8_model on test-dataset: loss:0.07695272419670801,acc:0.9796971678733826
node8 weight score:29758.53062909454
node9: train data size:2125
node9 epoch0:node_model train_loss:0.027072429122530262,train_acc:0.992272675037384
node9 epoch1:node_model train_loss:0.026298342695967716,train_acc:0.9904545545578003
node9_model on test-dataset: loss:0.08182906124029614,acc:0.9781968593597412
node9 weight score:25968.769136429477
node10: train data size:1915
node10 epoch0:node_model train_loss:0.03485215654363856,train_acc:0.9874998927116394
node10 epoch1:node_model train_loss:0.021562573756091295,train_acc:0.9915000200271606
node10_model on test-dataset: loss:0.0707911392042206,acc:0.979692816734314
node10 weight score:27051.40814976215
node19: train data size:5781
node19 epoch0:node_model train_loss:0.024764544261290274,train_acc:0.9923736453056335
node19 epoch1:node_model train_loss:0.02417293551392002,train_acc:0.9923735857009888
node19_model on test-dataset: loss:0.06288117776672152,acc:0.9811973571777344
node19 weight score:91935.30091701729
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04096991087403694,acc:0.9873989617824555
total cost energy:10.954227766016839 | all_enery_cp：9.638 | all_enery_tp: 1.316227766016838
ef: 32.19111813519791
reward: 21.236890369181072
step 117:loss:263.3875427246094|running q:59.35161209106445
episode1,iteration57 selected nodes:[4, 2, 10, 1, 0],center node:0
################################################## episode1,iteration57 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.021150723292117216,train_acc:0.9927778244018555
node0 epoch1:node_model train_loss:0.011587286452090161,train_acc:0.9962501525878906
node0_model on test-dataset: loss:0.06340689200943415,acc:0.9814981818199158
node0 weight score:113000.33439478373
node1: train data size:6657
node1 epoch0:node_model train_loss:0.023676262091178058,train_acc:0.9923882484436035
node1 epoch1:node_model train_loss:0.01895342365084494,train_acc:0.994627058506012
node1_model on test-dataset: loss:0.07448276010807603,acc:0.9801963567733765
node1 weight score:89376.38710408362
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03908775608269547,train_acc:0.9891490340232849
node2 epoch1:node_model train_loss:0.023825847105360887,train_acc:0.9927659034729004
node2_model on test-dataset: loss:0.06320632399918395,acc:0.9810950756072998
node2 weight score:72935.7397854607
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03694813860970181,train_acc:0.9879069328308105
node4 epoch1:node_model train_loss:0.01983994674506099,train_acc:0.9934788942337036
node4_model on test-dataset: loss:0.04719690159876336,acc:0.9863958358764648
node4 weight score:91065.29993300694
node10: train data size:1915
node10 epoch0:node_model train_loss:0.03646452201064676,train_acc:0.9856664538383484
node10 epoch1:node_model train_loss:0.019315931951859967,train_acc:0.9950000047683716
node10_model on test-dataset: loss:0.062499009580551504,acc:0.9832969307899475
node10 weight score:30640.485550924815
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038120681384043564,acc:0.9881969445943832
total cost energy:13.862991834728767 | all_enery_cp：12.3225 | all_enery_tp: 1.5404918347287666
ef: 32.71448151725034
reward: 18.851489682521574
step 118:loss:216.26144409179688|running q:60.19526672363281
episode1,iteration58 selected nodes:[16, 6, 2, 11, 15],center node:6
################################################## episode1,iteration58 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0387612655337781,train_acc:0.9859575033187866
node2 epoch1:node_model train_loss:0.03222714063211126,train_acc:0.9891490340232849
node2_model on test-dataset: loss:0.05033314986918413,acc:0.9848992824554443
node2 weight score:91589.73781655611
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0326963444846721,train_acc:0.9905555844306946
node6 epoch1:node_model train_loss:0.018584749716359913,train_acc:0.9941664934158325
node6_model on test-dataset: loss:0.05471244305791515,acc:0.9846981763839722
node6 weight score:64500.86676378941
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04407967338192975,train_acc:0.9856249094009399
node11 epoch1:node_model train_loss:0.026957244648656342,train_acc:0.9931249022483826
node11_model on test-dataset: loss:0.07578299382290425,acc:0.9782907366752625
node11 weight score:20783.02691076821
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04086593925707608,train_acc:0.9900000095367432
node15 epoch1:node_model train_loss:0.025247961981222034,train_acc:0.9919173121452332
node15_model on test-dataset: loss:0.06370659020442872,acc:0.982496976852417
node15 weight score:21599.021319215793
node16: train data size:920
node16 epoch0:node_model train_loss:0.06633113969583064,train_acc:0.9759999513626099
node16 epoch1:node_model train_loss:0.029362652054987847,train_acc:0.9920000433921814
node16_model on test-dataset: loss:0.0713885352696525,acc:0.9791958332061768
node16 weight score:12887.223368919505
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03741272341241711,acc:0.9884979540109634
total cost energy:7.105389691313215 | all_enery_cp：6.005 | all_enery_tp: 1.1003896913132158
ef: 32.48547834753944
reward: 25.380088656226228
step 119:loss:181.912109375|running q:61.47957229614258
episode1,iteration59 selected nodes:[14, 12, 5, 11, 19],center node:11
################################################## episode1,iteration59 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.02419061758032791,train_acc:0.9916326999664307
node5 epoch1:node_model train_loss:0.016094430311754043,train_acc:0.9959183931350708
node5_model on test-dataset: loss:0.06872282334559714,acc:0.9797949194908142
node5 weight score:70384.18627935914
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04110898757062387,train_acc:0.9891665577888489
node11 epoch1:node_model train_loss:0.013145101838745177,train_acc:0.9968749284744263
node11_model on test-dataset: loss:0.07575541830097791,acc:0.9810949563980103
node11 weight score:20790.592083360836
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04228648371839275,train_acc:0.9860000014305115
node12 epoch1:node_model train_loss:0.008887120658861629,train_acc:0.9960000514984131
node12_model on test-dataset: loss:0.07256529321563904,acc:0.9790949821472168
node12 weight score:19375.653810449752
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04361171880009351,train_acc:0.9849998950958252
node14 epoch1:node_model train_loss:0.029277669396833517,train_acc:0.9903124570846558
node14_model on test-dataset: loss:0.06764645665532953,acc:0.9819969534873962
node14 weight score:22765.42004035138
node19: train data size:5781
node19 epoch0:node_model train_loss:0.028252238883428146,train_acc:0.9918965697288513
node19 epoch1:node_model train_loss:0.01999457209603861,train_acc:0.9934080243110657
node19_model on test-dataset: loss:0.05861812453396851,acc:0.9825970530509949
node19 weight score:98621.37429268961
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0403947029083065,acc:0.9883969473838806
total cost energy:8.621645126328582 | all_enery_cp：7.5695 | all_enery_tp: 1.052145126328583
ef: 32.39227656410024
reward: 23.770631437771662
step 120:loss:259.43560791015625|running q:62.68221664428711
episode1_cost time: 13455.465304136276
episode2,iteration0 selected nodes:[13, 9, 14, 6, 18],center node:6
################################################## episode2,iteration0 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.9040829795930121,train_acc:0.7215708494186401
node6 epoch1:node_model train_loss:0.3013371316095193,train_acc:0.9032374620437622
node6_model on test-dataset: loss:0.3054249959765002,acc:0.9016890525817871
node6 weight score:11554.391574000467
node9: train data size:2125
node9 epoch0:node_model train_loss:1.2800389745018699,train_acc:0.6140909194946289
node9 epoch1:node_model train_loss:0.41000567918474023,train_acc:0.8736364245414734
node9_model on test-dataset: loss:0.4069255957007408,acc:0.8611989617347717
node9 weight score:5222.084878540687
node13: train data size:1056
node13 epoch0:node_model train_loss:1.9258378960869529,train_acc:0.3973376750946045
node13 epoch1:node_model train_loss:0.8307744589718905,train_acc:0.7362337112426758
node13_model on test-dataset: loss:0.8441131766326726,acc:0.6988756656646729
node13 weight score:1251.017078317133
node14: train data size:1540
node14 epoch0:node_model train_loss:1.6839218474924564,train_acc:0.484687477350235
node14 epoch1:node_model train_loss:0.6372226737439632,train_acc:0.7959374785423279
node14_model on test-dataset: loss:0.5539463832974434,acc:0.8042809367179871
node14 weight score:2780.0524499012604
node18: train data size:801
node18 epoch0:node_model train_loss:2.086000442504883,train_acc:0.3122222125530243
node18 epoch1:node_model train_loss:1.364618592792087,train_acc:0.49666664004325867
node18_model on test-dataset: loss:1.398768288642168,acc:0.5395696759223938
node18 weight score:572.6466681465578
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.48836009291931987,acc:0.8696908903121948
total cost energy:6.548819444564563 | all_enery_cp：4.5255 | all_enery_tp: 2.023319444564563
ef: 25.93490637079027
reward: 19.38608692622571
step 121:loss:181.57379150390625|running q:0.9697710275650024
episode2,iteration1 selected nodes:[3, 18, 11, 17, 16],center node:11
################################################## episode2,iteration1 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.4006244234348598,train_acc:0.8719863891601562
node3 epoch1:node_model train_loss:0.22984151777468229,train_acc:0.9244142770767212
node3_model on test-dataset: loss:0.2921880463976413,acc:0.9052715301513672
node3 weight score:12875.27004058291
node11: train data size:1575
node11 epoch0:node_model train_loss:0.5696151498705149,train_acc:0.815208375453949
node11 epoch1:node_model train_loss:0.3004055256024003,train_acc:0.9018750190734863
node11_model on test-dataset: loss:0.2780568518768996,acc:0.9120919108390808
node11 weight score:5664.309256789251
node16: train data size:920
node16 epoch0:node_model train_loss:0.6634955018758774,train_acc:0.7820000052452087
node16 epoch1:node_model train_loss:0.32925466895103456,train_acc:0.9000000357627869
node16_model on test-dataset: loss:0.3943713414296508,acc:0.869799017906189
node16 weight score:2332.826712673574
node17: train data size:719
node17 epoch0:node_model train_loss:0.5288150999695063,train_acc:0.8346710205078125
node17 epoch1:node_model train_loss:0.44769106432795525,train_acc:0.8427631855010986
node17_model on test-dataset: loss:0.3264282965660095,acc:0.8929958939552307
node17 weight score:2202.627675246915
node18: train data size:801
node18 epoch0:node_model train_loss:0.6543056550953124,train_acc:0.8211111426353455
node18 epoch1:node_model train_loss:0.5012993331895106,train_acc:0.8277777433395386
node18_model on test-dataset: loss:0.7915667234733701,acc:0.7425856590270996
node18 weight score:1011.917222196048
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.2177381638204679,acc:0.9308918982744216
total cost energy:5.348378083047767 | all_enery_cp：3.8885 | all_enery_tp: 1.4598780830477671
ef: 27.01315726933635
reward: 21.664779186288584
step 122:loss:176.95260620117188|running q:2.3777451515197754
episode2,iteration2 selected nodes:[5, 16, 0, 2, 7],center node:7
################################################## episode2,iteration2 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.2671384279512697,train_acc:0.9124785661697388
node0 epoch1:node_model train_loss:0.17608257362412083,train_acc:0.9414210915565491
node0_model on test-dataset: loss:0.18339588940143586,acc:0.9415998458862305
node0 weight score:39068.48743112507
node2: train data size:4610
node2 epoch0:node_model train_loss:0.2701792346828796,train_acc:0.9193618297576904
node2 epoch1:node_model train_loss:0.19150379601311177,train_acc:0.9382978081703186
node2_model on test-dataset: loss:0.3220753072109073,acc:0.8998959064483643
node2 weight score:14313.422658574673
node5: train data size:4837
node5 epoch0:node_model train_loss:0.27638526960295073,train_acc:0.911224365234375
node5 epoch1:node_model train_loss:0.17101007623940098,train_acc:0.9463045597076416
node5_model on test-dataset: loss:0.2197599082067609,acc:0.9293982982635498
node5 weight score:22010.384148181904
node7: train data size:3637
node7 epoch0:node_model train_loss:0.3394852824307777,train_acc:0.887837827205658
node7 epoch1:node_model train_loss:0.1836083690459664,train_acc:0.9415120482444763
node7_model on test-dataset: loss:0.17511451381607912,acc:0.9450939893722534
node7 weight score:20769.26646879711
node16: train data size:920
node16 epoch0:node_model train_loss:0.36319768726825713,train_acc:0.8830000162124634
node16 epoch1:node_model train_loss:0.21919808760285378,train_acc:0.9229999780654907
node16_model on test-dataset: loss:0.28210637307260183,acc:0.9058718681335449
node16 weight score:3261.1811990622145
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.11440228348597885,acc:0.964398964047432
total cost energy:11.924334563766816 | all_enery_cp：10.5845 | all_enery_tp: 1.339834563766817
ef: 28.777293173971717
reward: 16.8529586102049
step 123:loss:203.51248168945312|running q:4.072574138641357
episode2,iteration3 selected nodes:[1, 16, 9, 13, 15],center node:15
################################################## episode2,iteration3 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.19536720572122887,train_acc:0.9410418272018433
node1 epoch1:node_model train_loss:0.13807354800736726,train_acc:0.9571642279624939
node1_model on test-dataset: loss:0.11428898700047284,acc:0.9643959403038025
node1 weight score:58247.08202175647
node9: train data size:2125
node9 epoch0:node_model train_loss:0.21163927933031862,train_acc:0.9281817674636841
node9 epoch1:node_model train_loss:0.15998014706102284,train_acc:0.9422726631164551
node9_model on test-dataset: loss:0.17674355613766238,acc:0.9421888589859009
node9 weight score:12023.069165502566
node13: train data size:1056
node13 epoch0:node_model train_loss:0.25793122161518445,train_acc:0.9153246879577637
node13 epoch1:node_model train_loss:0.1653333448550918,train_acc:0.950389564037323
node13_model on test-dataset: loss:0.20677394890924916,acc:0.9339989423751831
node13 weight score:5107.02632304743
node15: train data size:1376
node15 epoch0:node_model train_loss:0.19861507628645217,train_acc:0.9376691579818726
node15 epoch1:node_model train_loss:0.12980072359953607,train_acc:0.9590600728988647
node15_model on test-dataset: loss:0.15431225397624077,acc:0.9492929577827454
node15 weight score:8916.984649915494
node16: train data size:920
node16 epoch0:node_model train_loss:0.28501798063516615,train_acc:0.9039999842643738
node16 epoch1:node_model train_loss:0.22131490856409072,train_acc:0.9169999361038208
node16_model on test-dataset: loss:0.2620316519273911,acc:0.9196909666061401
node16 weight score:3511.0262185231413
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.09233040792169049,acc:0.969795937538147
total cost energy:7.705989831966348 | all_enery_cp：6.066999999999999 | all_enery_tp: 1.6389898319663485
ef: 29.36124320961954
reward: 21.655253377653192
step 124:loss:175.9842071533203|running q:5.68858528137207
episode2,iteration4 selected nodes:[7, 4, 2, 3, 10],center node:10
################################################## episode2,iteration4 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.16916062730423947,train_acc:0.9485107064247131
node2 epoch1:node_model train_loss:0.14431123911066257,train_acc:0.9529786109924316
node2_model on test-dataset: loss:0.201762840799056,acc:0.9342940449714661
node2 weight score:22848.60771063038
node3: train data size:3762
node3 epoch0:node_model train_loss:0.17697974039535774,train_acc:0.9456282258033752
node3 epoch1:node_model train_loss:0.13148618420880093,train_acc:0.9547792077064514
node3_model on test-dataset: loss:0.2043889712309465,acc:0.9313942790031433
node3 weight score:18406.081195786148
node4: train data size:4298
node4 epoch0:node_model train_loss:0.17483743124229964,train_acc:0.9460323452949524
node4 epoch1:node_model train_loss:0.1093147345060526,train_acc:0.9662647843360901
node4_model on test-dataset: loss:0.16405482702306473,acc:0.9440990090370178
node4 weight score:26198.558603799796
node7: train data size:3637
node7 epoch0:node_model train_loss:0.16841782659694954,train_acc:0.9452154040336609
node7 epoch1:node_model train_loss:0.11786334568987021,train_acc:0.9627830386161804
node7_model on test-dataset: loss:0.1571762923791539,acc:0.9456982016563416
node7 weight score:23139.622044439897
node10: train data size:1915
node10 epoch0:node_model train_loss:0.1677160806953907,train_acc:0.9494999051094055
node10 epoch1:node_model train_loss:0.12906542466953397,train_acc:0.9564999938011169
node10_model on test-dataset: loss:0.2647445170255378,acc:0.9114939570426941
node10 weight score:7233.388708160764
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.08753048860002309,acc:0.9705979532003403
total cost energy:10.843032791758286 | all_enery_cp：9.111 | all_enery_tp: 1.7320327917582854
ef: 29.152553381325667
reward: 18.30952058956738
step 125:loss:123.79409790039062|running q:7.10453987121582
episode2,iteration5 selected nodes:[16, 19, 15, 6, 17],center node:15
################################################## episode2,iteration5 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.15581593662500381,train_acc:0.94987553358078
node6 epoch1:node_model train_loss:0.09809643640700313,train_acc:0.967499852180481
node6_model on test-dataset: loss:0.14568749558064156,acc:0.95469069480896
node6 weight score:24223.080957875434
node15: train data size:1376
node15 epoch0:node_model train_loss:0.14843552293522017,train_acc:0.9553006887435913
node15 epoch1:node_model train_loss:0.13315577538950102,train_acc:0.9602631330490112
node15_model on test-dataset: loss:0.13650004518858622,acc:0.9566929340362549
node15 weight score:10080.582743389872
node16: train data size:920
node16 epoch0:node_model train_loss:0.12261018678545951,train_acc:0.9649999737739563
node16 epoch1:node_model train_loss:0.10742892138659954,train_acc:0.9599999785423279
node16_model on test-dataset: loss:0.18132489116251235,acc:0.9424806833267212
node16 weight score:5073.765626449214
node17: train data size:719
node17 epoch0:node_model train_loss:0.1614521867595613,train_acc:0.9587499499320984
node17 epoch1:node_model train_loss:0.11449707485735416,train_acc:0.9559209942817688
node17_model on test-dataset: loss:0.13289402504335157,acc:0.9581982493400574
node17 weight score:5410.3260080011405
node19: train data size:5781
node19 epoch0:node_model train_loss:0.14755230987894125,train_acc:0.9557003974914551
node19 epoch1:node_model train_loss:0.08747374719201491,train_acc:0.973884642124176
node19_model on test-dataset: loss:0.1400516437110491,acc:0.9542940855026245
node19 weight score:41277.63049984053
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.08706812787771924,acc:0.9711959409713745
total cost energy:7.025941361516796 | all_enery_cp：6.1625 | all_enery_tp: 0.863441361516796
ef: 29.940457580371028
reward: 22.91451621885423
step 126:loss:152.87107849121094|running q:8.756969451904297
episode2,iteration6 selected nodes:[9, 1, 15, 19, 18],center node:15
################################################## episode2,iteration6 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.12444812643216617,train_acc:0.9600731730461121
node1 epoch1:node_model train_loss:0.08779087017721204,train_acc:0.97149258852005
node1_model on test-dataset: loss:0.142409015766425,acc:0.9560908079147339
node1 weight score:46745.63590073968
node9: train data size:2125
node9 epoch0:node_model train_loss:0.1282384419305758,train_acc:0.9618180990219116
node9 epoch1:node_model train_loss:0.08415477079423991,train_acc:0.9709089994430542
node9_model on test-dataset: loss:0.12999593853310215,acc:0.9604946970939636
node9 weight score:16346.664549515062
node15: train data size:1376
node15 epoch0:node_model train_loss:0.14705196688217775,train_acc:0.9509774446487427
node15 epoch1:node_model train_loss:0.09427791062210288,train_acc:0.9688345789909363
node15_model on test-dataset: loss:0.13434716856339946,acc:0.9580929279327393
node15 weight score:10242.121324281241
node18: train data size:801
node18 epoch0:node_model train_loss:0.11881268658261332,train_acc:0.9622222185134888
node18 epoch1:node_model train_loss:0.07770436686567134,train_acc:0.9744443893432617
node18_model on test-dataset: loss:0.17719444330839906,acc:0.9429899454116821
node18 weight score:4520.4577809807215
node19: train data size:5781
node19 epoch0:node_model train_loss:0.11771828411468144,train_acc:0.9649296998977661
node19 epoch1:node_model train_loss:0.10172179261029794,train_acc:0.9690571427345276
node19_model on test-dataset: loss:0.12492410553386435,acc:0.9610909223556519
node19 weight score:46276.09679729018
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07096804549102671,acc:0.9767969483137131
total cost energy:10.050411188203658 | all_enery_cp：8.37 | all_enery_tp: 1.6804111882036579
ef: 30.007664779647516
reward: 19.95725359144386
step 127:loss:122.11990356445312|running q:10.406025886535645
episode2,iteration7 selected nodes:[19, 1, 14, 13, 15],center node:15
################################################## episode2,iteration7 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.11911026736957703,train_acc:0.9634327292442322
node1 epoch1:node_model train_loss:0.07750856740149989,train_acc:0.9760434627532959
node1_model on test-dataset: loss:0.11975213015677583,acc:0.9628877639770508
node1 weight score:55589.82534410752
node13: train data size:1056
node13 epoch0:node_model train_loss:0.14529156278480182,train_acc:0.9585714340209961
node13 epoch1:node_model train_loss:0.12660215456377377,train_acc:0.9567532539367676
node13_model on test-dataset: loss:0.17839456880581564,acc:0.9458885788917542
node13 weight score:5919.462722822393
node14: train data size:1540
node14 epoch0:node_model train_loss:0.14498963148798794,train_acc:0.9524999856948853
node14 epoch1:node_model train_loss:0.10623427503742278,train_acc:0.9640624523162842
node14_model on test-dataset: loss:0.10474750194873195,acc:0.9660990834236145
node14 weight score:14702.021254442363
node15: train data size:1376
node15 epoch0:node_model train_loss:0.1494386579309191,train_acc:0.956954836845398
node15 epoch1:node_model train_loss:0.10007580116923366,train_acc:0.970714271068573
node15_model on test-dataset: loss:0.13026383951379103,acc:0.9583921432495117
node15 weight score:10563.177050023332
node19: train data size:5781
node19 epoch0:node_model train_loss:0.10655444642079287,train_acc:0.9674647450447083
node19 epoch1:node_model train_loss:0.0738777748373305,train_acc:0.9773329496383667
node19_model on test-dataset: loss:0.10490784192785213,acc:0.9663991928100586
node19 weight score:55105.50873761892
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06986645863609738,acc:0.9771969503164292
total cost energy:9.860489848529781 | all_enery_cp：8.205 | all_enery_tp: 1.65548984852978
ef: 30.325446503647072
reward: 20.46495665511729
step 128:loss:157.74221801757812|running q:12.372986793518066
episode2,iteration8 selected nodes:[2, 3, 1, 8, 17],center node:8
################################################## episode2,iteration8 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.10116834823153358,train_acc:0.9677608609199524
node1 epoch1:node_model train_loss:0.07562291171790948,train_acc:0.9743651151657104
node1_model on test-dataset: loss:0.11907269201794407,acc:0.9628871083259583
node1 weight score:55907.02525644419
node2: train data size:4610
node2 epoch0:node_model train_loss:0.11962930420215459,train_acc:0.9621275663375854
node2 epoch1:node_model train_loss:0.07998574204108816,train_acc:0.9757446646690369
node2_model on test-dataset: loss:0.13120497468160466,acc:0.9551963806152344
node2 weight score:35135.862883149784
node3: train data size:3762
node3 epoch0:node_model train_loss:0.12782192372373843,train_acc:0.9603648781776428
node3 epoch1:node_model train_loss:0.08377779704077463,train_acc:0.9687687754631042
node3_model on test-dataset: loss:0.09936515029752627,acc:0.968097984790802
node3 weight score:37860.35635970508
node8: train data size:2290
node8 epoch0:node_model train_loss:0.1468111702605434,train_acc:0.9529467821121216
node8 epoch1:node_model train_loss:0.09200521858166093,train_acc:0.9724153876304626
node8_model on test-dataset: loss:0.1121204633306479,acc:0.9634969830513
node8 weight score:20424.460727090423
node17: train data size:719
node17 epoch0:node_model train_loss:0.11453021201305091,train_acc:0.9687499403953552
node17 epoch1:node_model train_loss:0.07358562806621194,train_acc:0.9712499976158142
node17_model on test-dataset: loss:0.2703137877068366,acc:0.9159998893737793
node17 weight score:2659.871722043927
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06241165905958042,acc:0.9796969449520111
total cost energy:10.664374323804726 | all_enery_cp：9.019 | all_enery_tp: 1.6453743238047265
ef: 30.331069627149596
reward: 19.66669530334487
step 129:loss:121.59402465820312|running q:14.145075798034668
episode2,iteration9 selected nodes:[6, 17, 7, 4, 13],center node:6
################################################## episode2,iteration9 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.11147179584517035,train_acc:0.9665020704269409
node4 epoch1:node_model train_loss:0.071771917496483,train_acc:0.9746367931365967
node4_model on test-dataset: loss:0.09998610932205337,acc:0.9673970341682434
node4 weight score:42985.971042799785
node6: train data size:3529
node6 epoch0:node_model train_loss:0.11105593490517801,train_acc:0.9651530385017395
node6 epoch1:node_model train_loss:0.067376879904057,train_acc:0.9744442701339722
node6_model on test-dataset: loss:0.12416823901061434,acc:0.961787760257721
node6 weight score:28421.116608558234
node7: train data size:3637
node7 epoch0:node_model train_loss:0.10596240331997743,train_acc:0.9661064147949219
node7 epoch1:node_model train_loss:0.07668837119598647,train_acc:0.9740539193153381
node7_model on test-dataset: loss:0.1465784791263286,acc:0.9522767663002014
node7 weight score:24812.64658821745
node13: train data size:1056
node13 epoch0:node_model train_loss:0.11940227178010074,train_acc:0.9585714340209961
node13 epoch1:node_model train_loss:0.07423245017840104,train_acc:0.9747403264045715
node13_model on test-dataset: loss:0.17162280157441273,acc:0.9416959285736084
node13 weight score:6153.028562129237
node17: train data size:719
node17 epoch0:node_model train_loss:0.09809603315079585,train_acc:0.9775000214576721
node17 epoch1:node_model train_loss:0.07692651986144483,train_acc:0.9737499356269836
node17_model on test-dataset: loss:0.1670157093880698,acc:0.9486978054046631
node17 weight score:4304.98425947086
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0631007478426909,acc:0.9783969509601593
total cost energy:8.092365690108165 | all_enery_cp：6.6195 | all_enery_tp: 1.4728656901081651
ef: 29.903665528407483
reward: 21.81129983829932
step 130:loss:106.72956848144531|running q:15.719573020935059
episode2,iteration10 selected nodes:[2, 1, 11, 19, 7],center node:7
################################################## episode2,iteration10 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.09025776190961253,train_acc:0.9702985286712646
node1 epoch1:node_model train_loss:0.06164103961869407,train_acc:0.9802988171577454
node1_model on test-dataset: loss:0.14997009451151824,acc:0.9548979997634888
node1 weight score:44388.8498015764
node2: train data size:4610
node2 epoch0:node_model train_loss:0.10359247238870631,train_acc:0.9693617224693298
node2 epoch1:node_model train_loss:0.08703282523028394,train_acc:0.9736170172691345
node2_model on test-dataset: loss:0.09432050011353567,acc:0.9705957770347595
node2 weight score:48875.907087545565
node7: train data size:3637
node7 epoch0:node_model train_loss:0.09096055976241021,train_acc:0.9714315533638
node7 epoch1:node_model train_loss:0.04824144986609148,train_acc:0.9840539693832397
node7_model on test-dataset: loss:0.1731687061410048,acc:0.9514948725700378
node7 weight score:21002.64003265421
node11: train data size:1575
node11 epoch0:node_model train_loss:0.12702753860503435,train_acc:0.9574998617172241
node11 epoch1:node_model train_loss:0.0752154371002689,train_acc:0.9787498116493225
node11_model on test-dataset: loss:0.19319436738092918,acc:0.9396929740905762
node11 weight score:8152.411591247423
node19: train data size:5781
node19 epoch0:node_model train_loss:0.09235600231151128,train_acc:0.971551775932312
node19 epoch1:node_model train_loss:0.0648412926058317,train_acc:0.9790166020393372
node19_model on test-dataset: loss:0.1455329532750875,acc:0.9533981084823608
node19 weight score:39722.96218762708
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06243077519800863,acc:0.9800989651679992
total cost energy:12.462579227600713 | all_enery_cp：11.129999999999999 | all_enery_tp: 1.3325792276007147
ef: 30.163923850450402
reward: 17.70134462284969
step 131:loss:96.13675689697266|running q:17.306018829345703
episode2,iteration11 selected nodes:[11, 15, 6, 12, 2],center node:12
################################################## episode2,iteration11 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.09667793372051513,train_acc:0.9712767601013184
node2 epoch1:node_model train_loss:0.05845203590480254,train_acc:0.9821276068687439
node2_model on test-dataset: loss:0.09113065683530294,acc:0.9720951914787292
node2 weight score:50586.70879911995
node6: train data size:3529
node6 epoch0:node_model train_loss:0.08116612442406929,train_acc:0.9709864854812622
node6 epoch1:node_model train_loss:0.07382042186024289,train_acc:0.9737642407417297
node6_model on test-dataset: loss:0.11235988053580513,acc:0.96579909324646
node6 weight score:31408.007761946956
node11: train data size:1575
node11 epoch0:node_model train_loss:0.11902562185423449,train_acc:0.9660415649414062
node11 epoch1:node_model train_loss:0.05596215010154992,train_acc:0.9831249713897705
node11_model on test-dataset: loss:0.0978349973022705,acc:0.9691928029060364
node11 weight score:16098.53368865426
node12: train data size:1406
node12 epoch0:node_model train_loss:0.1052442488571008,train_acc:0.959555447101593
node12 epoch1:node_model train_loss:0.063063431395373,train_acc:0.9766666293144226
node12_model on test-dataset: loss:0.08383825483499095,acc:0.9737971425056458
node12 weight score:16770.38725063237
node15: train data size:1376
node15 epoch0:node_model train_loss:0.11913081098880086,train_acc:0.9612029790878296
node15 epoch1:node_model train_loss:0.06265937497041055,train_acc:0.9797744750976562
node15_model on test-dataset: loss:0.0780890967726009,acc:0.9735971689224243
node15 weight score:17620.89788292694
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0529279000396491,acc:0.9814969456195831
total cost energy:7.0894213562373105 | all_enery_cp：6.248000000000001 | all_enery_tp: 0.8414213562373096
ef: 31.55664909581085
reward: 24.46722773957354
step 132:loss:108.07494354248047|running q:19.110321044921875
episode2,iteration12 selected nodes:[17, 16, 2, 19, 12],center node:12
################################################## episode2,iteration12 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07451535662279483,train_acc:0.9778721332550049
node2 epoch1:node_model train_loss:0.0579199453399695,train_acc:0.981489360332489
node2_model on test-dataset: loss:0.09734504150721478,acc:0.9682960510253906
node2 weight score:47357.31711263719
node12: train data size:1406
node12 epoch0:node_model train_loss:0.09762689415365458,train_acc:0.9746667146682739
node12 epoch1:node_model train_loss:0.06282992670603563,train_acc:0.981999933719635
node12_model on test-dataset: loss:0.13746458518289728,acc:0.9588948488235474
node12 weight score:10228.088915623688
node16: train data size:920
node16 epoch0:node_model train_loss:0.10781607087701559,train_acc:0.9599999785423279
node16 epoch1:node_model train_loss:0.05478319628164172,train_acc:0.9860000014305115
node16_model on test-dataset: loss:0.10305013831355608,acc:0.9702928066253662
node16 weight score:8927.693014837763
node17: train data size:719
node17 epoch0:node_model train_loss:0.105824233032763,train_acc:0.9540131092071533
node17 epoch1:node_model train_loss:0.101536761270836,train_acc:0.9605920314788818
node17_model on test-dataset: loss:0.11115562896186021,acc:0.9647889733314514
node17 weight score:6468.408363257103
node19: train data size:5781
node19 epoch0:node_model train_loss:0.08222724982248299,train_acc:0.9737122058868408
node19 epoch1:node_model train_loss:0.06530314833246942,train_acc:0.9794423580169678
node19_model on test-dataset: loss:0.11311979451362277,acc:0.9653979539871216
node19 weight score:51105.11405060771
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.053354027768291415,acc:0.9825969475507736
total cost energy:7.865213595499959 | all_enery_cp：6.718000000000001 | all_enery_tp: 1.147213595499958
ef: 30.610472565210628
reward: 22.74525896971067
step 133:loss:138.83145141601562|running q:20.983549118041992
episode2,iteration13 selected nodes:[16, 9, 11, 2, 3],center node:11
################################################## episode2,iteration13 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07559837906205273,train_acc:0.9761700630187988
node2 epoch1:node_model train_loss:0.04808104953708801,train_acc:0.9831914901733398
node2_model on test-dataset: loss:0.11013019495248955,acc:0.9666917324066162
node2 weight score:41859.54634865367
node3: train data size:3762
node3 epoch0:node_model train_loss:0.1142453466983218,train_acc:0.9636247754096985
node3 epoch1:node_model train_loss:0.06975736983708646,train_acc:0.9771050810813904
node3_model on test-dataset: loss:0.07730594860564452,acc:0.9740980267524719
node3 weight score:48663.7841958428
node9: train data size:2125
node9 epoch0:node_model train_loss:0.09415225574577396,train_acc:0.965908944606781
node9 epoch1:node_model train_loss:0.051789250447076156,train_acc:0.9813635945320129
node9_model on test-dataset: loss:0.09074161281285342,acc:0.9707930684089661
node9 weight score:23418.14228475997
node11: train data size:1575
node11 epoch0:node_model train_loss:0.09884688619058579,train_acc:0.96937495470047
node11 epoch1:node_model train_loss:0.06799311452778056,train_acc:0.9785415530204773
node11_model on test-dataset: loss:0.12176197582564782,acc:0.9611859917640686
node11 weight score:12935.072622796939
node16: train data size:920
node16 epoch0:node_model train_loss:0.08457032022997737,train_acc:0.9709998965263367
node16 epoch1:node_model train_loss:0.05342389512807131,train_acc:0.9799999594688416
node16_model on test-dataset: loss:0.11619461988317198,acc:0.9657948613166809
node16 weight score:7917.750416714776
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05438664104265627,acc:0.9821979528665543
total cost energy:8.307633518063135 | all_enery_cp：6.496 | all_enery_tp: 1.8116335180631342
ef: 30.797653891330732
reward: 22.490020373267598
step 134:loss:95.38609313964844|running q:22.36579704284668
episode2,iteration14 selected nodes:[13, 8, 6, 10, 1],center node:6
################################################## episode2,iteration14 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0836963711111848,train_acc:0.9737679362297058
node1 epoch1:node_model train_loss:0.05252493734457599,train_acc:0.9821630120277405
node1_model on test-dataset: loss:0.126126557419193,acc:0.9617959856987
node1 weight score:52780.319515697716
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07657329303522904,train_acc:0.9740420579910278
node6 epoch1:node_model train_loss:0.05749925742081056,train_acc:0.979722261428833
node6_model on test-dataset: loss:0.10268630850943737,acc:0.9662961363792419
node6 weight score:34366.8016819951
node8: train data size:2290
node8 epoch0:node_model train_loss:0.11770503168277767,train_acc:0.9653622508049011
node8 epoch1:node_model train_loss:0.0776829639368731,train_acc:0.9754104614257812
node8_model on test-dataset: loss:0.0772318326956156,acc:0.9756991863250732
node8 weight score:29650.986129324392
node10: train data size:1915
node10 epoch0:node_model train_loss:0.10151952872984112,train_acc:0.9714999198913574
node10 epoch1:node_model train_loss:0.052032514661550525,train_acc:0.9854998588562012
node10_model on test-dataset: loss:0.06645782906896784,acc:0.9772979617118835
node10 weight score:28815.265662871312
node13: train data size:1056
node13 epoch0:node_model train_loss:0.10429475456476212,train_acc:0.9700648784637451
node13 epoch1:node_model train_loss:0.06478734585371884,train_acc:0.9820130467414856
node13_model on test-dataset: loss:0.08533287945610937,acc:0.9719991087913513
node13 weight score:12375.06582141236
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.052753213272226276,acc:0.9814979559183121
total cost energy:9.060838021109635 | all_enery_cp：7.7235 | all_enery_tp: 1.337338021109636
ef: 31.29495363850672
reward: 22.234115617397084
step 135:loss:179.38645935058594|running q:23.706884384155273
episode2,iteration15 selected nodes:[14, 2, 13, 0, 7],center node:7
################################################## episode2,iteration15 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.08163194785205026,train_acc:0.9749254584312439
node0 epoch1:node_model train_loss:0.060966043409684464,train_acc:0.9813143610954285
node0_model on test-dataset: loss:0.07670662685544812,acc:0.9756981730461121
node0 weight score:93407.83572066439
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0672875818538856,train_acc:0.9789361953735352
node2 epoch1:node_model train_loss:0.05700222443556413,train_acc:0.979999840259552
node2_model on test-dataset: loss:0.11284528870863142,acc:0.963296115398407
node2 weight score:40852.39226870253
node7: train data size:3637
node7 epoch0:node_model train_loss:0.09330640541943344,train_acc:0.9717018604278564
node7 epoch1:node_model train_loss:0.05253309397832365,train_acc:0.9827026128768921
node7_model on test-dataset: loss:0.08119304598360032,acc:0.9731960296630859
node7 weight score:44794.47661976637
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06497317738831043,train_acc:0.9820130467414856
node13 epoch1:node_model train_loss:0.0383519891818816,train_acc:0.9847403764724731
node13_model on test-dataset: loss:0.07746330902446061,acc:0.9753981828689575
node13 weight score:13632.260399133564
node14: train data size:1540
node14 epoch0:node_model train_loss:0.09950634832784999,train_acc:0.9721875190734863
node14 epoch1:node_model train_loss:0.05409156990936026,train_acc:0.9821873903274536
node14_model on test-dataset: loss:0.06681114155828255,acc:0.9784991145133972
node14 weight score:23050.047702845855
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051544659495702944,acc:0.9824969464540482
total cost energy:10.681269903474535 | all_enery_cp：9.004 | all_enery_tp: 1.677269903474535
ef: 31.509632147176692
reward: 20.82836224370216
step 136:loss:165.80215454101562|running q:25.322072982788086
episode2,iteration16 selected nodes:[5, 18, 17, 1, 0],center node:5
################################################## episode2,iteration16 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07609859539984933,train_acc:0.9761109948158264
node0 epoch1:node_model train_loss:0.0608139867900819,train_acc:0.9821475148200989
node0_model on test-dataset: loss:0.06236826951382682,acc:0.9807971119880676
node0 weight score:114882.13567335784
node1: train data size:6657
node1 epoch0:node_model train_loss:0.06449946918900111,train_acc:0.9782823920249939
node1 epoch1:node_model train_loss:0.048257228230306905,train_acc:0.9841398596763611
node1_model on test-dataset: loss:0.10555047783709597,acc:0.968694806098938
node1 weight score:63069.349721696686
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08228349438583364,train_acc:0.9753667116165161
node5 epoch1:node_model train_loss:0.059779472202442736,train_acc:0.9810811281204224
node5_model on test-dataset: loss:0.06193739212030778,acc:0.9806961417198181
node5 weight score:78094.9897051617
node17: train data size:719
node17 epoch0:node_model train_loss:0.07528870739042759,train_acc:0.981249988079071
node17 epoch1:node_model train_loss:0.05150844334275462,train_acc:0.9862499833106995
node17_model on test-dataset: loss:0.07581364939454943,acc:0.9755958318710327
node17 weight score:9483.78037123869
node18: train data size:801
node18 epoch0:node_model train_loss:0.07461458475639422,train_acc:0.975555419921875
node18 epoch1:node_model train_loss:0.031227420549839735,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.10082233125649509,acc:0.9686939120292664
node18 weight score:7944.66850763678
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0492800474181422,acc:0.9849969452619552
total cost energy:11.74427888107424 | all_enery_cp：10.0895 | all_enery_tp: 1.654778881074241
ef: 31.602606895937157
reward: 19.858328014862916
step 137:loss:196.04591369628906|running q:26.556655883789062
episode2,iteration17 selected nodes:[0, 14, 3, 11, 10],center node:10
################################################## episode2,iteration17 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.059821681746850826,train_acc:0.980138897895813
node0 epoch1:node_model train_loss:0.04234782850512096,train_acc:0.984722375869751
node0_model on test-dataset: loss:0.08027249154751188,acc:0.9755960702896118
node0 weight score:89258.47275787075
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08082284963395643,train_acc:0.9748385548591614
node3 epoch1:node_model train_loss:0.050000868063714156,train_acc:0.9858319163322449
node3_model on test-dataset: loss:0.07805825413030107,acc:0.9760969281196594
node3 weight score:48194.775067863666
node10: train data size:1915
node10 epoch0:node_model train_loss:0.08292537261731922,train_acc:0.9769998788833618
node10 epoch1:node_model train_loss:0.04739349959418178,train_acc:0.984666645526886
node10_model on test-dataset: loss:0.1213365255331155,acc:0.9607970118522644
node10 weight score:15782.55180446347
node11: train data size:1575
node11 epoch0:node_model train_loss:0.1008719690144062,train_acc:0.9712499380111694
node11 epoch1:node_model train_loss:0.06676595154567622,train_acc:0.9785415530204773
node11_model on test-dataset: loss:0.06593041578278644,acc:0.9796975255012512
node11 weight score:23888.822500209557
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08393014001194388,train_acc:0.9721873998641968
node14 epoch1:node_model train_loss:0.050996758160181344,train_acc:0.9862499237060547
node14_model on test-dataset: loss:0.07749634167303157,acc:0.976996898651123
node14 weight score:19871.905779726814
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.052703325827023945,acc:0.9840969455242157
total cost energy:9.326370866461907 | all_enery_cp：7.9785 | all_enery_tp: 1.3478708664619075
ef: 31.730037932141187
reward: 22.40366706567928
step 138:loss:125.84546661376953|running q:28.16359519958496
episode2,iteration18 selected nodes:[13, 9, 16, 15, 4],center node:15
################################################## episode2,iteration18 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07466248347055773,train_acc:0.9771997928619385
node4 epoch1:node_model train_loss:0.05002407458080196,train_acc:0.9839439392089844
node4_model on test-dataset: loss:0.07238691870326874,acc:0.9773969650268555
node4 weight score:59375.368878713125
node9: train data size:2125
node9 epoch0:node_model train_loss:0.07202691343528303,train_acc:0.9781817197799683
node9 epoch1:node_model train_loss:0.032560449580407956,train_acc:0.9881816506385803
node9_model on test-dataset: loss:0.07651933944376652,acc:0.9747991561889648
node9 weight score:27770.757241855787
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07995362630621954,train_acc:0.9747403264045715
node13 epoch1:node_model train_loss:0.03130749113519083,train_acc:0.9918181896209717
node13_model on test-dataset: loss:0.10011391735170036,acc:0.9682929515838623
node13 weight score:10547.984015951251
node15: train data size:1376
node15 epoch0:node_model train_loss:0.08065290542851601,train_acc:0.9747743606567383
node15 epoch1:node_model train_loss:0.03282117101896022,train_acc:0.9935714602470398
node15_model on test-dataset: loss:0.07268991064003785,acc:0.9783968329429626
node15 weight score:18929.724742873663
node16: train data size:920
node16 epoch0:node_model train_loss:0.06756059813778847,train_acc:0.9779999852180481
node16 epoch1:node_model train_loss:0.03174281546380371,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.13499380185836343,acc:0.9601858258247375
node16 weight score:6815.127712050597
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05403919160700752,acc:0.9829979556798935
total cost energy:6.679790889522918 | all_enery_cp：4.8875 | all_enery_tp: 1.7922908895229175
ef: 31.284767568927666
reward: 24.60497667940475
step 139:loss:133.08917236328125|running q:29.55422592163086
episode2,iteration19 selected nodes:[6, 7, 10, 1, 0],center node:6
################################################## episode2,iteration19 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.054798813546464466,train_acc:0.9816561937332153
node0 epoch1:node_model train_loss:0.05334481963008228,train_acc:0.9820834398269653
node0_model on test-dataset: loss:0.06961373209138401,acc:0.9781959056854248
node0 weight score:102925.09516074059
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05995360063388944,train_acc:0.9800366759300232
node1 epoch1:node_model train_loss:0.042174919223440674,train_acc:0.9851113557815552
node1_model on test-dataset: loss:0.06318625108964625,acc:0.9796969890594482
node1 weight score:105355.1980881933
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07270392467681733,train_acc:0.9794443249702454
node6 epoch1:node_model train_loss:0.048518936938813165,train_acc:0.9840420484542847
node6_model on test-dataset: loss:0.06587837484650663,acc:0.9782981872558594
node6 weight score:53568.41312832012
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07812585460173117,train_acc:0.9758361577987671
node7 epoch1:node_model train_loss:0.061031452546877835,train_acc:0.9810810685157776
node7_model on test-dataset: loss:0.10467330536605005,acc:0.9681981205940247
node7 weight score:34746.20379361433
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0839827683288604,train_acc:0.9769998788833618
node10 epoch1:node_model train_loss:0.04104475611820817,train_acc:0.9859998822212219
node10_model on test-dataset: loss:0.08049720538896508,acc:0.9736969470977783
node10 weight score:23789.645749149407
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0461125051540148,acc:0.9841979521512986
total cost energy:12.636319196258327 | all_enery_cp：11.4515 | all_enery_tp: 1.1848191962583274
ef: 32.23780580585021
reward: 19.60148660959188
step 140:loss:106.4278335571289|running q:30.916444778442383
episode2,iteration20 selected nodes:[0, 8, 5, 9, 6],center node:5
################################################## episode2,iteration20 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04834345834872996,train_acc:0.9850000739097595
node0 epoch1:node_model train_loss:0.03669459585378516,train_acc:0.9877034425735474
node0_model on test-dataset: loss:0.07304758765472798,acc:0.9769997000694275
node0 weight score:98086.7435878459
node5: train data size:4837
node5 epoch0:node_model train_loss:0.0765271233394742,train_acc:0.9753665924072266
node5 epoch1:node_model train_loss:0.0503078521400386,train_acc:0.9835300445556641
node5_model on test-dataset: loss:0.10146635618206347,acc:0.967896044254303
node5 weight score:47670.9737296652
node6: train data size:3529
node6 epoch0:node_model train_loss:0.06791298863633226,train_acc:0.9779309034347534
node6 epoch1:node_model train_loss:0.04112349978337685,train_acc:0.9870976209640503
node6_model on test-dataset: loss:0.0661458824139845,acc:0.9791958928108215
node6 weight score:53351.77143625046
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08021165300970493,train_acc:0.9751206636428833
node8 epoch1:node_model train_loss:0.047550364275989326,train_acc:0.9834299087524414
node8_model on test-dataset: loss:0.08728030297061196,acc:0.9738979339599609
node8 weight score:26237.305807371715
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05638838457790288,train_acc:0.9813634753227234
node9 epoch1:node_model train_loss:0.039120370649139986,train_acc:0.9881816506385803
node9_model on test-dataset: loss:0.07842373161831347,acc:0.9753971099853516
node9 weight score:27096.389780867954
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.047159497848551835,acc:0.9850989645719528
total cost energy:11.077862717754106 | all_enery_cp：9.973 | all_enery_tp: 1.1048627177541055
ef: 31.93637100590664
reward: 20.858508288152535
step 141:loss:146.33056640625|running q:32.53711700439453
episode2,iteration21 selected nodes:[6, 3, 19, 16, 8],center node:8
################################################## episode2,iteration21 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.0667764579411596,train_acc:0.9780983328819275
node3 epoch1:node_model train_loss:0.03868454731510658,train_acc:0.9857894778251648
node3_model on test-dataset: loss:0.06766917359025683,acc:0.9788958430290222
node3 weight score:55593.9994594771
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04817595490668383,train_acc:0.98194420337677
node6 epoch1:node_model train_loss:0.04575696373811095,train_acc:0.9852777123451233
node6_model on test-dataset: loss:0.08699962403683457,acc:0.9727950096130371
node6 weight score:40563.393682090675
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07428812672910483,train_acc:0.9776326417922974
node8 epoch1:node_model train_loss:0.0411573024061711,train_acc:0.9843478202819824
node8_model on test-dataset: loss:0.08672671626525698,acc:0.971697986125946
node8 weight score:26404.78157844634
node16: train data size:920
node16 epoch0:node_model train_loss:0.08800977393984795,train_acc:0.9669999480247498
node16 epoch1:node_model train_loss:0.02546170987188816,train_acc:0.9939999580383301
node16_model on test-dataset: loss:0.08540420754899969,acc:0.9746937155723572
node16 weight score:10772.302986034505
node19: train data size:5781
node19 epoch0:node_model train_loss:0.08144754155314174,train_acc:0.9776778221130371
node19 epoch1:node_model train_loss:0.04797470429510777,train_acc:0.9830632209777832
node19_model on test-dataset: loss:0.06229236062499695,acc:0.9810971617698669
node19 weight score:92804.317287025
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04404927680094261,acc:0.9849969440698624
total cost energy:9.856299374276688 | all_enery_cp：8.141 | all_enery_tp: 1.7152993742766873
ef: 31.793067605752483
reward: 21.936768231475796
step 142:loss:120.57575225830078|running q:33.98107147216797
episode2,iteration22 selected nodes:[19, 17, 9, 14, 4],center node:14
################################################## episode2,iteration22 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06731080815066086,train_acc:0.9788326025009155
node4 epoch1:node_model train_loss:0.04236137568084306,train_acc:0.9846510887145996
node4_model on test-dataset: loss:0.11903089027073292,acc:0.9641921520233154
node4 weight score:36108.27399697928
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06502560725096952,train_acc:0.979090690612793
node9 epoch1:node_model train_loss:0.03830628689717163,train_acc:0.987727165222168
node9_model on test-dataset: loss:0.08044634198427957,acc:0.9750959277153015
node9 weight score:26415.122771092032
node14: train data size:1540
node14 epoch0:node_model train_loss:0.10136269079521298,train_acc:0.9681248068809509
node14 epoch1:node_model train_loss:0.05240677483379841,train_acc:0.9812498688697815
node14_model on test-dataset: loss:0.06360222183335282,acc:0.9784958958625793
node14 weight score:24212.9906095895
node17: train data size:719
node17 epoch0:node_model train_loss:0.09944196935975924,train_acc:0.9705920219421387
node17 epoch1:node_model train_loss:0.043846745509654284,train_acc:0.9862499833106995
node17_model on test-dataset: loss:0.08404213500441983,acc:0.9756930470466614
node17 weight score:8555.23244337126
node19: train data size:5781
node19 epoch0:node_model train_loss:0.060855594351245414,train_acc:0.9811666011810303
node19 epoch1:node_model train_loss:0.04176515649907805,train_acc:0.9870287775993347
node19_model on test-dataset: loss:0.06786731385447638,acc:0.9789990186691284
node19 weight score:85180.91658078344
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04741552616258559,acc:0.9837959343194962
total cost energy:8.639976165529363 | all_enery_cp：7.2315 | all_enery_tp: 1.408476165529364
ef: 31.745801752796346
reward: 23.105825587266985
step 143:loss:135.5321502685547|running q:35.21297073364258
episode2,iteration23 selected nodes:[19, 15, 5, 9, 6],center node:5
################################################## episode2,iteration23 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06779588029092672,train_acc:0.9790403246879578
node5 epoch1:node_model train_loss:0.0469756588548878,train_acc:0.9827744364738464
node5_model on test-dataset: loss:0.09599348709351034,acc:0.9722980856895447
node5 weight score:50388.8351851217
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05493086632082446,train_acc:0.9816664457321167
node6 epoch1:node_model train_loss:0.040721044474695295,train_acc:0.9852777123451233
node6_model on test-dataset: loss:0.07789501953466242,acc:0.9755929112434387
node6 weight score:45304.56531215881
node9: train data size:2125
node9 epoch0:node_model train_loss:0.054502805610272015,train_acc:0.9813634753227234
node9 epoch1:node_model train_loss:0.025103521094107153,train_acc:0.9927272200584412
node9_model on test-dataset: loss:0.0775490431144135,acc:0.9758970141410828
node9 weight score:27402.014449937695
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07233607030606695,train_acc:0.9764285087585449
node15 epoch1:node_model train_loss:0.046898327102618556,train_acc:0.9866917133331299
node15_model on test-dataset: loss:0.06609260172725044,acc:0.9793928265571594
node15 weight score:20819.274231001644
node19: train data size:5781
node19 epoch0:node_model train_loss:0.052605259918672,train_acc:0.9852641224861145
node19 epoch1:node_model train_loss:0.03442678802871499,train_acc:0.9908219575881958
node19_model on test-dataset: loss:0.0832962179649303,acc:0.9740970134735107
node19 weight score:69402.9109753091
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04821473792024562,acc:0.9843969464302063
total cost energy:10.433994144925336 | all_enery_cp：8.824 | all_enery_tp: 1.609994144925337
ef: 31.654403946153316
reward: 21.22040980122798
step 144:loss:210.06619262695312|running q:36.86402893066406
episode2,iteration24 selected nodes:[15, 3, 1, 14, 6],center node:6
################################################## episode2,iteration24 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.056849904427888676,train_acc:0.9812673926353455
node1 epoch1:node_model train_loss:0.041412770581334385,train_acc:0.9867530465126038
node1_model on test-dataset: loss:0.06155229417068767,acc:0.9817930459976196
node1 weight score:108151.93957742334
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05656046628657924,train_acc:0.9792104363441467
node3 epoch1:node_model train_loss:0.04696635555132831,train_acc:0.9857893586158752
node3_model on test-dataset: loss:0.08267481850667537,acc:0.9763960242271423
node3 weight score:45503.57736432463
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04106943780789152,train_acc:0.9841952919960022
node6 epoch1:node_model train_loss:0.030984907475714054,train_acc:0.9897220730781555
node6_model on test-dataset: loss:0.07136121265808469,acc:0.9776961207389832
node6 weight score:49452.63496163684
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08878526545595378,train_acc:0.9724998474121094
node14 epoch1:node_model train_loss:0.053182319417828694,train_acc:0.9837499856948853
node14_model on test-dataset: loss:0.0695163158934156,acc:0.9783968925476074
node14 weight score:22153.072702546146
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07223765246037926,train_acc:0.98003751039505
node15 epoch1:node_model train_loss:0.04029411902385099,train_acc:0.9907143115997314
node15_model on test-dataset: loss:0.05798393797696917,acc:0.9817958474159241
node15 weight score:23730.709710446677
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04573036950692767,acc:0.9851959359645843
total cost energy:10.23959920853119 | all_enery_cp：8.432 | all_enery_tp: 1.8075992085311903
ef: 32.098815420860106
reward: 21.859216212328917
step 145:loss:152.54800415039062|running q:38.2856559753418
episode2,iteration25 selected nodes:[3, 8, 10, 17, 5],center node:8
################################################## episode2,iteration25 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.051488244085033476,train_acc:0.9826313853263855
node3 epoch1:node_model train_loss:0.03755306882024007,train_acc:0.9872069954872131
node3_model on test-dataset: loss:0.08214303901142557,acc:0.976193904876709
node3 weight score:45798.15947005236
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06896495152673475,train_acc:0.9789795875549316
node5 epoch1:node_model train_loss:0.03267340784018137,train_acc:0.990877091884613
node5_model on test-dataset: loss:0.06480909096875621,acc:0.980396032333374
node5 weight score:74634.59103803612
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07051423376264132,train_acc:0.9763766527175903
node8 epoch1:node_model train_loss:0.041672417387852205,train_acc:0.9895650744438171
node8_model on test-dataset: loss:0.07202194322453578,acc:0.9762979745864868
node8 weight score:31795.865224861962
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06106577401515097,train_acc:0.9819998741149902
node10 epoch1:node_model train_loss:0.030045799084473402,train_acc:0.9899998903274536
node10_model on test-dataset: loss:0.0651953756709554,acc:0.9804959893226624
node10 weight score:29373.24895043337
node17: train data size:719
node17 epoch0:node_model train_loss:0.07644124034413835,train_acc:0.9799999594688416
node17 epoch1:node_model train_loss:0.032264473324175924,train_acc:0.9874999523162842
node17_model on test-dataset: loss:0.06424040561672882,acc:0.9811989665031433
node17 weight score:11192.332817599232
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0447926873599863,acc:0.9862979567050933
total cost energy:7.7240316113301075 | all_enery_cp：6.7615 | all_enery_tp: 0.9625316113301075
ef: 32.222701608984515
reward: 24.498669997654407
step 146:loss:101.5682373046875|running q:39.828372955322266
episode2,iteration26 selected nodes:[16, 5, 7, 14, 17],center node:17
################################################## episode2,iteration26 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.054335746210904756,train_acc:0.9812851548194885
node5 epoch1:node_model train_loss:0.03486811898040528,train_acc:0.9887754917144775
node5_model on test-dataset: loss:0.06191169549390906,acc:0.982296884059906
node5 weight score:78127.40325413749
node7: train data size:3637
node7 epoch0:node_model train_loss:0.058761757081122815,train_acc:0.9816215634346008
node7 epoch1:node_model train_loss:0.03766714335998168,train_acc:0.9894595146179199
node7_model on test-dataset: loss:0.08993839156915782,acc:0.9731978178024292
node7 weight score:40438.79300646978
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07065682741813362,train_acc:0.9765624403953552
node14 epoch1:node_model train_loss:0.04283399507403374,train_acc:0.9868749976158142
node14_model on test-dataset: loss:0.08144109080269119,acc:0.973499059677124
node14 weight score:18909.373447993054
node16: train data size:920
node16 epoch0:node_model train_loss:0.062177551444619894,train_acc:0.9839999079704285
node16 epoch1:node_model train_loss:0.048027834366075696,train_acc:0.9880000352859497
node16_model on test-dataset: loss:0.08004876029852312,acc:0.9749948382377625
node16 weight score:11492.994976675158
node17: train data size:719
node17 epoch0:node_model train_loss:0.0587818666244857,train_acc:0.9837499856948853
node17 epoch1:node_model train_loss:0.03167988537461497,train_acc:0.9887499809265137
node17_model on test-dataset: loss:0.07758861829934176,acc:0.9757998585700989
node17 weight score:9266.823095444912
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041220231836778115,acc:0.9861969435214997
total cost energy:7.414562486640328 | all_enery_cp：5.8265 | all_enery_tp: 1.5880624866403281
ef: 31.68620138523829
reward: 24.271638898597963
step 147:loss:84.25086975097656|running q:41.35474395751953
episode2,iteration27 selected nodes:[18, 6, 14, 0, 1],center node:6
################################################## episode2,iteration27 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.056173007704071805,train_acc:0.9822865724563599
node0 epoch1:node_model train_loss:0.029187365118155464,train_acc:0.9900000095367432
node0_model on test-dataset: loss:0.06287102172209416,acc:0.9805958867073059
node0 weight score:113963.47321459344
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04908765879556982,train_acc:0.9841400384902954
node1 epoch1:node_model train_loss:0.03526743045493738,train_acc:0.9875361919403076
node1_model on test-dataset: loss:0.06713938943885296,acc:0.9795970916748047
node1 weight score:99151.9293761652
node6: train data size:3529
node6 epoch0:node_model train_loss:0.050308161143523954,train_acc:0.9824997782707214
node6 epoch1:node_model train_loss:0.02250483846809301,train_acc:0.9922221302986145
node6_model on test-dataset: loss:0.06413947092514719,acc:0.9799968004226685
node6 weight score:55020.722015597166
node14: train data size:1540
node14 epoch0:node_model train_loss:0.056333064851060044,train_acc:0.979374885559082
node14 epoch1:node_model train_loss:0.04725213555502705,train_acc:0.9859373569488525
node14_model on test-dataset: loss:0.07115862554768683,acc:0.976495087146759
node14 weight score:21641.789567281223
node18: train data size:801
node18 epoch0:node_model train_loss:0.04364118662649869,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.020843548415642645,train_acc:0.995555579662323
node18_model on test-dataset: loss:0.12634677060093055,acc:0.9662888646125793
node18 weight score:6339.695080375094
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04277280855865683,acc:0.9868979549407959
total cost energy:11.600572853696416 | all_enery_cp：9.846 | all_enery_tp: 1.7545728536964167
ef: 31.93788650299372
reward: 20.337313649297304
step 148:loss:197.2140655517578|running q:42.95874786376953
episode2,iteration28 selected nodes:[7, 4, 18, 19, 13],center node:18
################################################## episode2,iteration28 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05617854417150104,train_acc:0.9844138622283936
node4 epoch1:node_model train_loss:0.038249048133662276,train_acc:0.9890555143356323
node4_model on test-dataset: loss:0.07415742860233877,acc:0.9778958559036255
node4 weight score:57957.78091292192
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06056878231214108,train_acc:0.9792693853378296
node7 epoch1:node_model train_loss:0.03660791890217444,train_acc:0.9868369698524475
node7_model on test-dataset: loss:0.06401550432987278,acc:0.9800978899002075
node7 weight score:56814.36142810792
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06674825789576228,train_acc:0.9772726893424988
node13 epoch1:node_model train_loss:0.04599026967348023,train_acc:0.9890908598899841
node13_model on test-dataset: loss:0.06342537169461139,acc:0.9803968071937561
node13 weight score:16649.488553012572
node18: train data size:801
node18 epoch0:node_model train_loss:0.04200415016768804,train_acc:0.9855554699897766
node18 epoch1:node_model train_loss:0.01939856711153867,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.06130134704464581,acc:0.9810960292816162
node18 weight score:13066.597042584255
node19: train data size:5781
node19 epoch0:node_model train_loss:0.053533715830216634,train_acc:0.9832356572151184
node19 epoch1:node_model train_loss:0.03357935217931738,train_acc:0.9889655709266663
node19_model on test-dataset: loss:0.09051548788891522,acc:0.973494827747345
node19 weight score:63867.52294916323
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04364811605919385,acc:0.9852959340810776
total cost energy:9.540451349061641 | all_enery_cp：7.7865 | all_enery_tp: 1.7539513490616399
ef: 32.19919865281348
reward: 22.658747303751838
step 149:loss:317.5299987792969|running q:44.10266876220703
episode2,iteration29 selected nodes:[0, 6, 14, 7, 1],center node:6
################################################## episode2,iteration29 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.041848863764446124,train_acc:0.9851391911506653
node0 epoch1:node_model train_loss:0.025536401546560228,train_acc:0.9918057322502136
node0_model on test-dataset: loss:0.07902816210757009,acc:0.9777960181236267
node0 weight score:90663.88245556412
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03934578344907,train_acc:0.9873135685920715
node1 epoch1:node_model train_loss:0.034869380420379674,train_acc:0.9889921545982361
node1_model on test-dataset: loss:0.0893140420576674,acc:0.97409588098526
node1 weight score:74534.75228118972
node6: train data size:3529
node6 epoch0:node_model train_loss:0.040200978504597314,train_acc:0.9866665005683899
node6 epoch1:node_model train_loss:0.023188779395746276,train_acc:0.9922221302986145
node6_model on test-dataset: loss:0.06385143378240173,acc:0.981293261051178
node6 weight score:55268.923357718515
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04492305889348122,train_acc:0.9856755137443542
node7 epoch1:node_model train_loss:0.02750893065869506,train_acc:0.9900803565979004
node7_model on test-dataset: loss:0.08145018396251544,acc:0.9754979610443115
node7 weight score:44653.0605955881
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07154017628636211,train_acc:0.9787499308586121
node14 epoch1:node_model train_loss:0.029250558698549867,train_acc:0.9890624284744263
node14_model on test-dataset: loss:0.08016326947980361,acc:0.9770979285240173
node14 weight score:19210.793297147004
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0413704692688043,acc:0.9863979536294937
total cost energy:12.671359258196459 | all_enery_cp：11.264 | all_enery_tp: 1.4073592581964587
ef: 31.982179235322818
reward: 19.310819977126357
step 150:loss:267.79241943359375|running q:45.75205612182617
episode2,iteration30 selected nodes:[10, 2, 18, 9, 17],center node:10
################################################## episode2,iteration30 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05473494915628528,train_acc:0.9838297963142395
node2 epoch1:node_model train_loss:0.03411938097374235,train_acc:0.9897873997688293
node2_model on test-dataset: loss:0.06495117787024356,acc:0.9805970788002014
node2 weight score:70976.38797574454
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05659376298585399,train_acc:0.9804544448852539
node9 epoch1:node_model train_loss:0.0416667263912545,train_acc:0.9872726202011108
node9_model on test-dataset: loss:0.0860690983337554,acc:0.9755919575691223
node9 weight score:24689.465105812516
node10: train data size:1915
node10 epoch0:node_model train_loss:0.059796181484125556,train_acc:0.982499897480011
node10 epoch1:node_model train_loss:0.04075950410915539,train_acc:0.9899998903274536
node10_model on test-dataset: loss:0.06254754005647556,acc:0.9821949005126953
node10 weight score:30616.71167676465
node17: train data size:719
node17 epoch0:node_model train_loss:0.12354408111423254,train_acc:0.9796710014343262
node17 epoch1:node_model train_loss:0.05525041569489986,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.07023866702569649,acc:0.978996992111206
node17 weight score:10236.526836948047
node18: train data size:801
node18 epoch0:node_model train_loss:0.04026509329883589,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.023297344631929364,train_acc:0.9944445490837097
node18_model on test-dataset: loss:0.10162688880285714,acc:0.9713982939720154
node18 weight score:7881.772328520606
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045460509925324,acc:0.9859969472885132
total cost energy:6.706359193188069 | all_enery_cp：5.085 | all_enery_tp: 1.6213591931880695
ef: 31.614221614097236
reward: 24.90786242090917
step 151:loss:238.43801879882812|running q:47.09803009033203
episode2,iteration31 selected nodes:[9, 18, 11, 6, 1],center node:11
################################################## episode2,iteration31 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04560792325551052,train_acc:0.9847004413604736
node1 epoch1:node_model train_loss:0.02380529238230813,train_acc:0.9918644428253174
node1_model on test-dataset: loss:0.06768293377826921,acc:0.9790970683097839
node1 weight score:98355.66557750701
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03895949281609824,train_acc:0.9838888645172119
node6 epoch1:node_model train_loss:0.028384491856235802,train_acc:0.9888888597488403
node6_model on test-dataset: loss:0.06059169039479457,acc:0.9808948040008545
node6 weight score:58242.30974587856
node9: train data size:2125
node9 epoch0:node_model train_loss:0.045602144207805395,train_acc:0.9881817698478699
node9 epoch1:node_model train_loss:0.014735051430761814,train_acc:0.9945453405380249
node9_model on test-dataset: loss:0.06722394739579612,acc:0.9804930686950684
node9 weight score:31610.75899766171
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06119002850027755,train_acc:0.9797915816307068
node11 epoch1:node_model train_loss:0.03536062847706489,train_acc:0.9897915720939636
node11_model on test-dataset: loss:0.06539282726065722,acc:0.9810980558395386
node11 weight score:24085.210350701247
node18: train data size:801
node18 epoch0:node_model train_loss:0.0425188070965103,train_acc:0.9877777099609375
node18 epoch1:node_model train_loss:0.02481097017880529,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.07830849997655605,acc:0.9783930778503418
node18 weight score:10228.774657154752
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04197143377885368,acc:0.987196946144104
total cost energy:8.937563876655657 | all_enery_cp：7.343500000000001 | all_enery_tp: 1.5940638766556565
ef: 32.18240690834596
reward: 23.2448430316903
step 152:loss:151.4670867919922|running q:48.615394592285156
episode2,iteration32 selected nodes:[3, 8, 11, 4, 0],center node:8
################################################## episode2,iteration32 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.041518771002301946,train_acc:0.9861007332801819
node0 epoch1:node_model train_loss:0.021906205904087983,train_acc:0.9933335185050964
node0_model on test-dataset: loss:0.06270532856833597,acc:0.9843960404396057
node0 weight score:114264.61137495864
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05515877815502647,train_acc:0.9807299375534058
node3 epoch1:node_model train_loss:0.038511802485261704,train_acc:0.9871053099632263
node3_model on test-dataset: loss:0.06490564882342369,acc:0.9818957448005676
node3 weight score:57961.05682934546
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04447445804036634,train_acc:0.9876744151115417
node4 epoch1:node_model train_loss:0.024564058898926475,train_acc:0.992315948009491
node4_model on test-dataset: loss:0.0643743099195126,acc:0.9799980521202087
node4 weight score:66765.76425244485
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07626267077158326,train_acc:0.9765215516090393
node8 epoch1:node_model train_loss:0.036071709863355624,train_acc:0.9873429536819458
node8_model on test-dataset: loss:0.06932223817886551,acc:0.9786959886550903
node8 weight score:33034.132482729896
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06639128105598502,train_acc:0.981666624546051
node11 epoch1:node_model train_loss:0.03661461670708377,train_acc:0.9881248474121094
node11_model on test-dataset: loss:0.05714020852610702,acc:0.9836969375610352
node11 weight score:27563.77760295348
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04299140175578941,acc:0.9861979556083679
total cost energy:10.845389691313216 | all_enery_cp：9.545 | all_enery_tp: 1.3003896913132158
ef: 32.65180980618405
reward: 21.806420114870832
step 153:loss:161.2986602783203|running q:50.11564636230469
episode2,iteration33 selected nodes:[6, 4, 8, 9, 11],center node:8
################################################## episode2,iteration33 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04505379502336646,train_acc:0.9846511483192444
node4 epoch1:node_model train_loss:0.022929380390641473,train_acc:0.9930233359336853
node4_model on test-dataset: loss:0.05407965264384984,acc:0.980790913105011
node4 weight score:79475.36254171534
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03291739408612355,train_acc:0.9886110424995422
node6 epoch1:node_model train_loss:0.021887100654162675,train_acc:0.9933332800865173
node6_model on test-dataset: loss:0.07132694048517806,acc:0.9789959788322449
node6 weight score:49476.39666015587
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06556322670582196,train_acc:0.9804346561431885
node8 epoch1:node_model train_loss:0.04632138890092787,train_acc:0.98429936170578
node8_model on test-dataset: loss:0.067234984163224,acc:0.978895902633667
node8 weight score:34059.64957827085
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04110032188790766,train_acc:0.9859089851379395
node9 epoch1:node_model train_loss:0.029290699093094605,train_acc:0.9918181300163269
node9_model on test-dataset: loss:0.06558870687167655,acc:0.979695200920105
node9 weight score:32398.87019205204
node11: train data size:1575
node11 epoch0:node_model train_loss:0.056750371819362044,train_acc:0.9804165363311768
node11 epoch1:node_model train_loss:0.024174842445063405,train_acc:0.9937499165534973
node11_model on test-dataset: loss:0.060864772874629124,acc:0.981391966342926
node11 weight score:25877.03733396372
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04008347738847078,acc:0.9868959385156632
total cost energy:8.080790095800493 | all_enery_cp：6.9085 | all_enery_tp: 1.172290095800493
ef: 32.582679074618255
reward: 24.50188897881776
step 154:loss:126.44572448730469|running q:51.55525588989258
episode2,iteration34 selected nodes:[4, 16, 3, 5, 14],center node:4
################################################## episode2,iteration34 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05126701558842078,train_acc:0.9835228323936462
node3 epoch1:node_model train_loss:0.03017931093927473,train_acc:0.9918420910835266
node3_model on test-dataset: loss:0.0591275066313392,acc:0.9826989769935608
node3 weight score:63625.20955696848
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03883697522457603,train_acc:0.9872046113014221
node4 epoch1:node_model train_loss:0.023897868246760566,train_acc:0.9920930862426758
node4_model on test-dataset: loss:0.06686932322147186,acc:0.9786971211433411
node4 weight score:64274.614919684194
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04114216094722553,train_acc:0.9848982095718384
node5 epoch1:node_model train_loss:0.029198338165974284,train_acc:0.9910204410552979
node5_model on test-dataset: loss:0.08191697680187644,acc:0.974797785282135
node5 weight score:59047.589264661445
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06750926742097363,train_acc:0.9765624403953552
node14 epoch1:node_model train_loss:0.05050182869308628,train_acc:0.9868749976158142
node14_model on test-dataset: loss:0.12693651076901005,acc:0.9623978137969971
node14 weight score:12132.04924785101
node16: train data size:920
node16 epoch0:node_model train_loss:0.05415636200341396,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.019488963345065714,train_acc:0.9959999918937683
node16_model on test-dataset: loss:0.07465058712441533,acc:0.9773918986320496
node16 weight score:12324.08257508672
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03731464979995508,acc:0.9887989664077759
total cost energy:9.34912814054859 | all_enery_cp：7.6785 | all_enery_tp: 1.6706281405485912
ef: 31.732424740288568
reward: 22.383296599739978
step 155:loss:127.60421752929688|running q:53.010581970214844
episode2,iteration35 selected nodes:[0, 14, 9, 15, 11],center node:11
################################################## episode2,iteration35 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03716001779604186,train_acc:0.9880557656288147
node0 epoch1:node_model train_loss:0.023425312371627014,train_acc:0.9913890957832336
node0_model on test-dataset: loss:0.07219930245686555,acc:0.9793928265571594
node0 weight score:99239.18592261508
node9: train data size:2125
node9 epoch0:node_model train_loss:0.047308472990566355,train_acc:0.9849998354911804
node9 epoch1:node_model train_loss:0.023401276076170194,train_acc:0.991363525390625
node9_model on test-dataset: loss:0.07575542270140431,acc:0.9770960807800293
node9 weight score:28050.797213235113
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05184556180029176,train_acc:0.9818748831748962
node11 epoch1:node_model train_loss:0.039823480845370796,train_acc:0.9877082705497742
node11_model on test-dataset: loss:0.07199689625413157,acc:0.9796949028968811
node11 weight score:21875.943018996713
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06235783617012203,train_acc:0.9784373641014099
node14 epoch1:node_model train_loss:0.03744222469686065,train_acc:0.9874999523162842
node14_model on test-dataset: loss:0.08524291943115714,acc:0.9737987518310547
node14 weight score:18066.01662961246
node15: train data size:1376
node15 epoch0:node_model train_loss:0.054543078145278354,train_acc:0.982631504535675
node15 epoch1:node_model train_loss:0.02238193174291934,train_acc:0.9921428561210632
node15_model on test-dataset: loss:0.08011132994205582,acc:0.9781948924064636
node15 weight score:17176.09732599939
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04605535988455813,acc:0.985797957777977
total cost energy:8.564933841025194 | all_enery_cp：6.890499999999999 | all_enery_tp: 1.6744338410251953
ef: 31.745048491865063
reward: 23.18011465083987
step 156:loss:122.62953186035156|running q:54.309356689453125
episode2,iteration36 selected nodes:[17, 3, 7, 11, 1],center node:11
################################################## episode2,iteration36 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03386606425463931,train_acc:0.9894031882286072
node1 epoch1:node_model train_loss:0.02306919482036202,train_acc:0.9925374984741211
node1_model on test-dataset: loss:0.06595726546482183,acc:0.980090856552124
node1 weight score:100928.9871720121
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04101612630888427,train_acc:0.9851018190383911
node3 epoch1:node_model train_loss:0.023260541125445774,train_acc:0.9918421506881714
node3_model on test-dataset: loss:0.0704499227803899,acc:0.9793958067893982
node3 weight score:53399.63269693139
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04627320145231646,train_acc:0.9851349592208862
node7 epoch1:node_model train_loss:0.04278097808285541,train_acc:0.9871073961257935
node7_model on test-dataset: loss:0.06473020735371392,acc:0.9798968434333801
node7 weight score:56187.05931414456
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0472634843463311,train_acc:0.9841666221618652
node11 epoch1:node_model train_loss:0.02883643204404507,train_acc:0.9918749332427979
node11_model on test-dataset: loss:0.07849307048280024,acc:0.9783961176872253
node11 weight score:20065.465528515935
node17: train data size:719
node17 epoch0:node_model train_loss:0.05484770795010263,train_acc:0.9862499833106995
node17 epoch1:node_model train_loss:0.02507914713351056,train_acc:0.9925000071525574
node17_model on test-dataset: loss:0.06101919813925633,acc:0.9807968735694885
node17 weight score:11783.176802145415
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04258127266410156,acc:0.9870969444513321
total cost energy:9.750839853318428 | all_enery_cp：8.175 | all_enery_tp: 1.5758398533184277
ef: 32.237857802821125
reward: 22.487017949502697
step 157:loss:272.1790466308594|running q:55.77482604980469
episode2,iteration37 selected nodes:[8, 1, 2, 14, 17],center node:8
################################################## episode2,iteration37 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.027232987646347108,train_acc:0.9911943078041077
node1 epoch1:node_model train_loss:0.02240979533224217,train_acc:0.9911941885948181
node1_model on test-dataset: loss:0.09232760587452503,acc:0.9764937162399292
node1 weight score:72101.94542515257
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04919028531839239,train_acc:0.9838297963142395
node2 epoch1:node_model train_loss:0.03108896671279155,train_acc:0.9893616437911987
node2_model on test-dataset: loss:0.06351771511079278,acc:0.9815980195999146
node2 weight score:72578.17747314842
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05657653511582833,train_acc:0.9820771813392639
node8 epoch1:node_model train_loss:0.036279551381164274,train_acc:0.9885989427566528
node8_model on test-dataset: loss:0.06818949106025685,acc:0.9792959094047546
node8 weight score:33582.88739794819
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06269543702364899,train_acc:0.9828125238418579
node14 epoch1:node_model train_loss:0.029932891338830814,train_acc:0.9899999499320984
node14_model on test-dataset: loss:0.0742431186157046,acc:0.9773998260498047
node14 weight score:20742.663141230772
node17: train data size:719
node17 epoch0:node_model train_loss:0.041591833636630327,train_acc:0.98499995470047
node17 epoch1:node_model train_loss:0.02837745733995689,train_acc:0.9924999475479126
node17_model on test-dataset: loss:0.06972577147666016,acc:0.9787961840629578
node17 weight score:10311.825667510562
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04350467506388668,acc:0.986997955441475
total cost energy:9.475661908732945 | all_enery_cp：7.907999999999999 | all_enery_tp: 1.5676619087329466
ef: 31.88997046553055
reward: 22.414308556797604
step 158:loss:98.12348175048828|running q:57.22382736206055
episode2,iteration38 selected nodes:[7, 8, 1, 10, 9],center node:8
################################################## episode2,iteration38 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.024134376909889614,train_acc:0.9929853677749634
node1 epoch1:node_model train_loss:0.02124775480001178,train_acc:0.9937315583229065
node1_model on test-dataset: loss:0.09135826491954503,acc:0.9742969274520874
node1 weight score:72866.97055665965
node7: train data size:3637
node7 epoch0:node_model train_loss:0.053234223477743765,train_acc:0.9854052066802979
node7 epoch1:node_model train_loss:0.03794240110830681,train_acc:0.989269495010376
node7_model on test-dataset: loss:0.0834481798048364,acc:0.9758942127227783
node7 weight score:43583.934466947
node8: train data size:2290
node8 epoch0:node_model train_loss:0.052941259239678795,train_acc:0.9821255207061768
node8 epoch1:node_model train_loss:0.028717945846121595,train_acc:0.991642415523529
node8_model on test-dataset: loss:0.06839960383273137,acc:0.9795979261398315
node8 weight score:33479.72607560868
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03446328483352607,train_acc:0.9881817698478699
node9 epoch1:node_model train_loss:0.02401034807553515,train_acc:0.9931817650794983
node9_model on test-dataset: loss:0.07271282521229296,acc:0.9811941385269165
node9 weight score:29224.555555307234
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04842575712900725,train_acc:0.9844999313354492
node10 epoch1:node_model train_loss:0.042441553832031784,train_acc:0.9884998202323914
node10_model on test-dataset: loss:0.05842582599492743,acc:0.9813938140869141
node10 weight score:32776.60122710565
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04335849394865363,acc:0.9869979536533355
total cost energy:9.640538328578604 | all_enery_cp：8.312 | all_enery_tp: 1.328538328578604
ef: 32.20464681575044
reward: 22.56410848717184
step 159:loss:139.06748962402344|running q:58.585880279541016
episode2,iteration39 selected nodes:[8, 7, 11, 0, 6],center node:6
################################################## episode2,iteration39 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03511840322688739,train_acc:0.9876390099525452
node0 epoch1:node_model train_loss:0.022233872161046345,train_acc:0.9927032589912415
node0_model on test-dataset: loss:0.06352722683252068,acc:0.981296718120575
node0 weight score:112786.28640424318
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03818615242521951,train_acc:0.986944317817688
node6 epoch1:node_model train_loss:0.01738569467205606,train_acc:0.9944443106651306
node6_model on test-dataset: loss:0.05824250203266274,acc:0.9824957251548767
node6 weight score:60591.490352198736
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04479180590085987,train_acc:0.9846748113632202
node7 epoch1:node_model train_loss:0.023712499542287678,train_acc:0.9933236837387085
node7_model on test-dataset: loss:0.0610582166968743,acc:0.981796145439148
node7 weight score:59566.10259444714
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04680460482916754,train_acc:0.9856520891189575
node8 epoch1:node_model train_loss:0.02903431756219462,train_acc:0.9899999499320984
node8_model on test-dataset: loss:0.08860490013203162,acc:0.974994957447052
node8 weight score:25845.07173517078
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04580678965430707,train_acc:0.9843748807907104
node11 epoch1:node_model train_loss:0.03419028190546669,train_acc:0.9899999499320984
node11_model on test-dataset: loss:0.06597870777724893,acc:0.9806930422782898
node11 weight score:23871.337482349092
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04162097168191394,acc:0.9871969437599182
total cost energy:10.162098632478745 | all_enery_cp：9.097999999999999 | all_enery_tp: 1.0640986324787456
ef: 32.582135503126736
reward: 22.42003687064799
step 160:loss:84.28174591064453|running q:60.16728210449219
episode2,iteration40 selected nodes:[4, 3, 5, 19, 17],center node:5
################################################## episode2,iteration40 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04226246353937313,train_acc:0.9842104911804199
node3 epoch1:node_model train_loss:0.0233437715306584,train_acc:0.9916807413101196
node3_model on test-dataset: loss:0.0522919762715901,acc:0.9844967722892761
node3 weight score:71942.20353159364
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04151505980299526,train_acc:0.9874277114868164
node4 epoch1:node_model train_loss:0.025013332471771295,train_acc:0.9927859306335449
node4_model on test-dataset: loss:0.0645991729493835,acc:0.9803959727287292
node4 weight score:66533.35954266298
node5: train data size:4837
node5 epoch0:node_model train_loss:0.047561847162908136,train_acc:0.9863266348838806
node5 epoch1:node_model train_loss:0.026656653848476708,train_acc:0.9921016693115234
node5_model on test-dataset: loss:0.05571443177264882,acc:0.9828940629959106
node5 weight score:86817.72112005219
node17: train data size:719
node17 epoch0:node_model train_loss:0.04708249280520249,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.026658412418328226,train_acc:0.9950000047683716
node17_model on test-dataset: loss:0.05516393768542912,acc:0.982996940612793
node17 weight score:13033.877387435217
node19: train data size:5781
node19 epoch0:node_model train_loss:0.05334458590067666,train_acc:0.9842700362205505
node19 epoch1:node_model train_loss:0.03287800120045271,train_acc:0.9896150231361389
node19_model on test-dataset: loss:0.05833666410064325,acc:0.9811940789222717
node19 weight score:99097.19880496657
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038019730278174395,acc:0.988197957277298
total cost energy:11.461637605836984 | all_enery_cp：9.698500000000001 | all_enery_tp: 1.7631376058369825
ef: 32.685141253284456
reward: 21.223503647447473
step 161:loss:124.17770385742188|running q:61.4765625
episode2,iteration41 selected nodes:[0, 8, 4, 10, 13],center node:8
################################################## episode2,iteration41 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.027112956805568602,train_acc:0.9904809594154358
node0 epoch1:node_model train_loss:0.017307705216808245,train_acc:0.995138943195343
node0_model on test-dataset: loss:0.06645353143787361,acc:0.9834958910942078
node0 weight score:107819.7026549063
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03560490287342217,train_acc:0.9892975687980652
node4 epoch1:node_model train_loss:0.025516227748634858,train_acc:0.991855800151825
node4_model on test-dataset: loss:0.06698946659496868,acc:0.9803959727287292
node4 weight score:64159.3405420966
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04640403236060039,train_acc:0.9826085567474365
node8 epoch1:node_model train_loss:0.028473730968392414,train_acc:0.9890337586402893
node8_model on test-dataset: loss:0.06336613884701364,acc:0.9813971519470215
node8 weight score:36139.17530195111
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05870808752952143,train_acc:0.9834999442100525
node10 epoch1:node_model train_loss:0.02547338958538603,train_acc:0.9929999709129333
node10_model on test-dataset: loss:0.051361227021297966,acc:0.984700083732605
node10 weight score:37284.934785648846
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06490397140045058,train_acc:0.9827272295951843
node13 epoch1:node_model train_loss:0.03554970804940571,train_acc:0.9883766174316406
node13_model on test-dataset: loss:0.055632245657034216,acc:0.982695996761322
node13 weight score:18981.79711295688
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03994551245787079,acc:0.9879959356784821
total cost energy:9.78650401904666 | all_enery_cp：8.362 | all_enery_tp: 1.42450401904666
ef: 32.625177634204285
reward: 22.838673615157624
step 162:loss:112.44214630126953|running q:62.70194625854492
episode2,iteration42 selected nodes:[4, 0, 14, 17, 3],center node:4
################################################## episode2,iteration42 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.021246248730070267,train_acc:0.993194580078125
node0 epoch1:node_model train_loss:0.02327836108613863,train_acc:0.9915279746055603
node0_model on test-dataset: loss:0.0922185170899138,acc:0.9766949415206909
node0 weight score:77695.89260488831
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03563367427743383,train_acc:0.9894736409187317
node3 epoch1:node_model train_loss:0.027348609919978405,train_acc:0.9918419718742371
node3_model on test-dataset: loss:0.062216057303885464,acc:0.9818958044052124
node3 weight score:60466.70526910837
node4: train data size:4298
node4 epoch0:node_model train_loss:0.02869379015904743,train_acc:0.991158127784729
node4 epoch1:node_model train_loss:0.02434633762120854,train_acc:0.9909254312515259
node4_model on test-dataset: loss:0.11224678827272783,acc:0.967992901802063
node4 weight score:38290.62787575784
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07098264916567132,train_acc:0.979374885559082
node14 epoch1:node_model train_loss:0.020707450108602643,train_acc:0.99281245470047
node14_model on test-dataset: loss:0.0658070179844799,acc:0.979197084903717
node14 weight score:23401.75937410198
node17: train data size:719
node17 epoch0:node_model train_loss:0.038462226977571845,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.02262736568809487,train_acc:0.9884210228919983
node17_model on test-dataset: loss:0.06462770354992245,acc:0.9816949367523193
node17 weight score:11125.25991960398
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04114432617694547,acc:0.9874969464540482
total cost energy:10.22280425847205 | all_enery_cp：8.741999999999999 | all_enery_tp: 1.4808042584720513
ef: 31.635522684304103
reward: 21.412718425832054
step 163:loss:97.4404067993164|running q:63.847652435302734
episode2,iteration43 selected nodes:[4, 1, 0, 11, 16],center node:11
################################################## episode2,iteration43 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.02472990079129684,train_acc:0.9918057322502136
node0 epoch1:node_model train_loss:0.012979504496999854,train_acc:0.9948611855506897
node0_model on test-dataset: loss:0.06914310031224886,acc:0.9805960655212402
node0 weight score:103625.66861542226
node1: train data size:6657
node1 epoch0:node_model train_loss:0.023828940056855166,train_acc:0.9916785955429077
node1 epoch1:node_model train_loss:0.023271723527899153,train_acc:0.992872416973114
node1_model on test-dataset: loss:0.07108140207086762,acc:0.9808958768844604
node1 weight score:93653.1892458033
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03070557715241299,train_acc:0.9893022775650024
node4 epoch1:node_model train_loss:0.015188130052614048,train_acc:0.9953488111495972
node4_model on test-dataset: loss:0.05714238433232822,acc:0.9829979538917542
node4 weight score:75215.62234791825
node11: train data size:1575
node11 epoch0:node_model train_loss:0.043859312412678264,train_acc:0.9856249094009399
node11 epoch1:node_model train_loss:0.033339700021315366,train_acc:0.9902083277702332
node11_model on test-dataset: loss:0.059770052829699126,acc:0.9820950031280518
node11 weight score:26350.9889222885
node16: train data size:920
node16 epoch0:node_model train_loss:0.07773731093038805,train_acc:0.9760000109672546
node16 epoch1:node_model train_loss:0.03324641026556492,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.07411263958663766,acc:0.9796966910362244
node16 weight score:12413.537085324295
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039984447666429335,acc:0.9875979548692704
total cost energy:12.210662678803018 | all_enery_cp：10.307500000000001 | all_enery_tp: 1.9031626788030174
ef: 32.38734747808218
reward: 20.17668479927916
step 164:loss:92.6012191772461|running q:65.26724243164062
episode2,iteration44 selected nodes:[4, 19, 9, 14, 1],center node:14
################################################## episode2,iteration44 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.024285009085659438,train_acc:0.9916419386863708
node1 epoch1:node_model train_loss:0.021400628614920512,train_acc:0.9922389388084412
node1_model on test-dataset: loss:0.093047937887859,acc:0.9751918315887451
node1 weight score:71543.76712811185
node4: train data size:4298
node4 epoch0:node_model train_loss:0.02811499968439146,train_acc:0.9920930862426758
node4 epoch1:node_model train_loss:0.012579262366269304,train_acc:0.9965069890022278
node4_model on test-dataset: loss:0.05218433215770347,acc:0.9854978919029236
node4 weight score:82361.88569034946
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05142738535703922,train_acc:0.9854544401168823
node9 epoch1:node_model train_loss:0.0216401148672131,train_acc:0.9904544353485107
node9_model on test-dataset: loss:0.06460137298025075,acc:0.9805980324745178
node9 weight score:32894.0377265609
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06352669588522986,train_acc:0.9790624380111694
node14 epoch1:node_model train_loss:0.03682929487695219,train_acc:0.9865625500679016
node14_model on test-dataset: loss:0.06887078650688636,acc:0.9807969331741333
node14 weight score:22360.71458028167
node19: train data size:5781
node19 epoch0:node_model train_loss:0.05259293164849153,train_acc:0.9846552610397339
node19 epoch1:node_model train_loss:0.032901648732086904,train_acc:0.9905175566673279
node19_model on test-dataset: loss:0.06199510559963528,acc:0.9814960360527039
node19 weight score:93249.29676438861
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04018228195564007,acc:0.9872979545593261
total cost energy:11.977690570558263 | all_enery_cp：10.2005 | all_enery_tp: 1.7771905705582631
ef: 32.57106104951942
reward: 20.59337047896116
step 165:loss:77.80302429199219|running q:66.49311828613281
episode2,iteration45 selected nodes:[7, 14, 12, 9, 5],center node:5
################################################## episode2,iteration45 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05049455791179623,train_acc:0.9869387149810791
node5 epoch1:node_model train_loss:0.028610643626627873,train_acc:0.9896525740623474
node5_model on test-dataset: loss:0.054555753697277394,acc:0.9844947457313538
node5 weight score:88661.59244797291
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04551546717679279,train_acc:0.9856755137443542
node7 epoch1:node_model train_loss:0.025704015295707143,train_acc:0.9921621680259705
node7_model on test-dataset: loss:0.06190958430022874,acc:0.9812949299812317
node7 weight score:58746.96205941997
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03136104510419748,train_acc:0.987727165222168
node9 epoch1:node_model train_loss:0.016721240459777287,train_acc:0.9940909147262573
node9_model on test-dataset: loss:0.052164560392093336,acc:0.9858970642089844
node9 weight score:40736.46905154576
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04545966861769557,train_acc:0.9853333234786987
node12 epoch1:node_model train_loss:0.018906509446484657,train_acc:0.9939998984336853
node12_model on test-dataset: loss:0.0653020795845805,acc:0.9808939695358276
node12 weight score:21530.70788777748
node14: train data size:1540
node14 epoch0:node_model train_loss:0.057347990601556376,train_acc:0.9824999570846558
node14 epoch1:node_model train_loss:0.022767034781281836,train_acc:0.9937499761581421
node14_model on test-dataset: loss:0.06758707726698049,acc:0.9806990623474121
node14 weight score:22785.420856663728
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.036393592954227644,acc:0.9885979551076889
total cost energy:8.226820376686506 | all_enery_cp：6.7725 | all_enery_tp: 1.4543203766865056
ef: 32.52021267348447
reward: 24.29339229679797
step 166:loss:78.53486633300781|running q:67.73332214355469
episode2,iteration46 selected nodes:[8, 2, 11, 14, 16],center node:11
################################################## episode2,iteration46 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04616264951851298,train_acc:0.9870211482048035
node2 epoch1:node_model train_loss:0.024429930329441706,train_acc:0.9912766218185425
node2_model on test-dataset: loss:0.05838352437236608,acc:0.9826971888542175
node2 weight score:78960.6322940996
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05038331789167031,train_acc:0.9877294898033142
node8 epoch1:node_model train_loss:0.02910061337499191,train_acc:0.9934782385826111
node8_model on test-dataset: loss:0.06347955271034152,acc:0.9825969338417053
node8 weight score:36074.60831441766
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04952114490515669,train_acc:0.9835415482521057
node11 epoch1:node_model train_loss:0.025071997566556092,train_acc:0.9933332800865173
node11_model on test-dataset: loss:0.06717042051903263,acc:0.9815969467163086
node11 weight score:23447.821047267767
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05756609565287363,train_acc:0.981249988079071
node14 epoch1:node_model train_loss:0.028400888346368447,train_acc:0.9918749928474426
node14_model on test-dataset: loss:0.055307671436603416,acc:0.9840949773788452
node14 weight score:27844.238601967354
node16: train data size:920
node16 epoch0:node_model train_loss:0.1028548552945722,train_acc:0.9769998788833618
node16 epoch1:node_model train_loss:0.02659403485013172,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.09012799587915651,acc:0.9734960794448853
node16 weight score:10207.705064623147
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04025737101495906,acc:0.9871969425678253
total cost energy:6.83195568889035 | all_enery_cp：5.4675 | all_enery_tp: 1.3644556888903492
ef: 32.25506484199682
reward: 25.42310915310647
step 167:loss:91.81892395019531|running q:68.96674346923828
episode2,iteration47 selected nodes:[9, 5, 15, 17, 18],center node:17
################################################## episode2,iteration47 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03402123320966541,train_acc:0.988163411617279
node5 epoch1:node_model train_loss:0.023164161720921343,train_acc:0.9922449588775635
node5_model on test-dataset: loss:0.06402236806679866,acc:0.9811980724334717
node5 weight score:75551.71959514597
node9: train data size:2125
node9 epoch0:node_model train_loss:0.030016481289005078,train_acc:0.9918181300163269
node9 epoch1:node_model train_loss:0.018801290340806274,train_acc:0.9931817650794983
node9_model on test-dataset: loss:0.07862750251486432,acc:0.9764980673789978
node9 weight score:27026.16682500216
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0410591431427747,train_acc:0.98691725730896
node15 epoch1:node_model train_loss:0.020625808276236057,train_acc:0.9933459162712097
node15_model on test-dataset: loss:0.07952781344938557,acc:0.9788968563079834
node15 weight score:17302.122871462285
node17: train data size:719
node17 epoch0:node_model train_loss:0.04148883918605861,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.019093400915153325,train_acc:0.9950000047683716
node17_model on test-dataset: loss:0.06372169219888747,acc:0.9816979765892029
node17 weight score:11283.441716454498
node18: train data size:801
node18 epoch0:node_model train_loss:0.06882547121495008,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.01620684390783822,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.09114941189135607,acc:0.9746919274330139
node18 weight score:8787.76926125138
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03785049555135629,acc:0.988497958779335
total cost energy:6.403433841025195 | all_enery_cp：4.928999999999999 | all_enery_tp: 1.4744338410251954
ef: 31.821980905468116
reward: 25.418547064442922
step 168:loss:88.66796875|running q:70.3575439453125
episode2,iteration48 selected nodes:[5, 14, 4, 17, 3],center node:4
################################################## episode2,iteration48 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04016841770958548,train_acc:0.9855263233184814
node3 epoch1:node_model train_loss:0.02744944543442934,train_acc:0.9899998903274536
node3_model on test-dataset: loss:0.07207178713346366,acc:0.9789971113204956
node3 weight score:52197.95636583105
node4: train data size:4298
node4 epoch0:node_model train_loss:0.030389251336347053,train_acc:0.9904651045799255
node4 epoch1:node_model train_loss:0.016985828181468817,train_acc:0.9946369528770447
node4_model on test-dataset: loss:0.07135293882980477,acc:0.9796969294548035
node4 weight score:60235.78104122442
node5: train data size:4837
node5 epoch0:node_model train_loss:0.02330843090170006,train_acc:0.9906123876571655
node5 epoch1:node_model train_loss:0.015083488048415403,train_acc:0.9937342405319214
node5_model on test-dataset: loss:0.07160284914629302,acc:0.9804960489273071
node5 weight score:67553.17780885285
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04692703444015933,train_acc:0.9815623760223389
node14 epoch1:node_model train_loss:0.042053582412336254,train_acc:0.9859374165534973
node14_model on test-dataset: loss:0.0840123422868055,acc:0.9757969975471497
node14 weight score:18330.639976001046
node17: train data size:719
node17 epoch0:node_model train_loss:0.04683776115416549,train_acc:0.9862499833106995
node17 epoch1:node_model train_loss:0.013360570745135192,train_acc:0.9950000047683716
node17_model on test-dataset: loss:0.06355848132059919,acc:0.9827961325645447
node17 weight score:11312.416298514883
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042699838411281235,acc:0.9869959336519242
total cost energy:9.034540189760124 | all_enery_cp：7.578 | all_enery_tp: 1.456540189760123
ef: 31.823648018470553
reward: 22.789107828710428
step 169:loss:82.07352447509766|running q:71.4813232421875
episode2,iteration49 selected nodes:[1, 7, 3, 15, 9],center node:7
################################################## episode2,iteration49 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.028697116002475202,train_acc:0.9905972480773926
node1 epoch1:node_model train_loss:0.014696379753399585,train_acc:0.9952239990234375
node1_model on test-dataset: loss:0.0620483050205803,acc:0.9808979034423828
node1 weight score:107287.37872520438
node3: train data size:3762
node3 epoch0:node_model train_loss:0.02985079600904627,train_acc:0.9894735217094421
node3 epoch1:node_model train_loss:0.024180289232294615,train_acc:0.9931578636169434
node3_model on test-dataset: loss:0.05986472228639741,acc:0.9809958338737488
node3 weight score:62841.68465698887
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03688968095072621,train_acc:0.9891891479492188
node7 epoch1:node_model train_loss:0.024613977938487724,train_acc:0.9924324154853821
node7_model on test-dataset: loss:0.062175857106194596,acc:0.9811980128288269
node7 weight score:58495.37375557376
node9: train data size:2125
node9 epoch0:node_model train_loss:0.027414680379231206,train_acc:0.991363525390625
node9 epoch1:node_model train_loss:0.013642875498838046,train_acc:0.9959090948104858
node9_model on test-dataset: loss:0.06522458082588856,acc:0.9821950793266296
node9 weight score:32579.741764420163
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05154776375275105,train_acc:0.9869173169136047
node15 epoch1:node_model train_loss:0.03179746547747137,train_acc:0.9897743463516235
node15_model on test-dataset: loss:0.06653212970104505,acc:0.9809920191764832
node15 weight score:20681.736871837827
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039555860540131105,acc:0.9867979556322097
total cost energy:10.829845536131439 | all_enery_cp：8.778500000000001 | all_enery_tp: 2.0513455361314374
ef: 32.33668159785896
reward: 21.506836061727526
step 170:loss:67.9734878540039|running q:72.77159118652344
episode2,iteration50 selected nodes:[10, 18, 8, 9, 19],center node:10
################################################## episode2,iteration50 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05389529489405939,train_acc:0.9846376776695251
node8 epoch1:node_model train_loss:0.020524818855135338,train_acc:0.9934782385826111
node8_model on test-dataset: loss:0.06518077256710968,acc:0.9805969595909119
node8 weight score:35133.06010054164
node9: train data size:2125
node9 epoch0:node_model train_loss:0.029777724989054895,train_acc:0.9922725558280945
node9 epoch1:node_model train_loss:0.013461979301858017,train_acc:0.9963635802268982
node9_model on test-dataset: loss:0.06110183211421827,acc:0.9818971157073975
node9 weight score:34778.00790044587
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0497195516119973,train_acc:0.9889998435974121
node10 epoch1:node_model train_loss:0.030808250047266485,train_acc:0.9891666769981384
node10_model on test-dataset: loss:0.05678532979606644,acc:0.9833939075469971
node10 weight score:33723.49877824701
node18: train data size:801
node18 epoch0:node_model train_loss:0.12059068866074085,train_acc:0.992222249507904
node18 epoch1:node_model train_loss:0.014551438300663399,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.09664883136319986,acc:0.9763939380645752
node18 weight score:8287.73600986333
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04820652719153541,train_acc:0.9873735308647156
node19 epoch1:node_model train_loss:0.02357142114337405,train_acc:0.9918562173843384
node19_model on test-dataset: loss:0.05898791544677806,acc:0.9839961528778076
node19 weight score:98003.12413507672
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03968431744078771,acc:0.9877979534864426
total cost energy:7.818780549425378 | all_enery_cp：6.4559999999999995 | all_enery_tp: 1.3627805494253789
ef: 32.39679845803944
reward: 24.578017908614065
step 171:loss:68.35951232910156|running q:74.14349365234375
episode2,iteration51 selected nodes:[14, 8, 1, 10, 12],center node:8
################################################## episode2,iteration51 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.024554845652848815,train_acc:0.9917910695075989
node1 epoch1:node_model train_loss:0.00749422941589728,train_acc:0.9974629282951355
node1_model on test-dataset: loss:0.06699538308035699,acc:0.9828938245773315
node1 weight score:99365.0561265591
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03464382918774272,train_acc:0.9886955618858337
node8 epoch1:node_model train_loss:0.019930283371966496,train_acc:0.9943478107452393
node8_model on test-dataset: loss:0.09084503065433182,acc:0.9768968820571899
node8 weight score:25207.762972897453
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0439594689727528,train_acc:0.9854998588562012
node10 epoch1:node_model train_loss:0.01704544511740096,train_acc:0.9965000152587891
node10_model on test-dataset: loss:0.04942462591414369,acc:0.9850970506668091
node10 weight score:38745.86736026242
node12: train data size:1406
node12 epoch0:node_model train_loss:0.08665696207220511,train_acc:0.9762222170829773
node12 epoch1:node_model train_loss:0.045578382678831984,train_acc:0.981999933719635
node12_model on test-dataset: loss:0.07119356515708204,acc:0.9780958890914917
node12 weight score:19748.975864571337
node14: train data size:1540
node14 epoch0:node_model train_loss:0.0629808680532733,train_acc:0.9784374237060547
node14 epoch1:node_model train_loss:0.022289346597972326,train_acc:0.9931249022483826
node14_model on test-dataset: loss:0.059565205623148355,acc:0.9827989935874939
node14 weight score:25854.019706456314
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04048833801611181,acc:0.9882979571819306
total cost energy:8.103070478491457 | all_enery_cp：6.904000000000001 | all_enery_tp: 1.199070478491457
ef: 32.42681765102814
reward: 24.32374717253668
step 172:loss:50.49591827392578|running q:75.4727554321289
episode2,iteration52 selected nodes:[12, 19, 10, 13, 2],center node:12
################################################## episode2,iteration52 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03791599372956664,train_acc:0.9874467849731445
node2 epoch1:node_model train_loss:0.019114056356285918,train_acc:0.9934042692184448
node2_model on test-dataset: loss:0.07400870288982332,acc:0.9789928793907166
node2 weight score:62289.971584327075
node10: train data size:1915
node10 epoch0:node_model train_loss:0.02935174125159392,train_acc:0.9909998774528503
node10 epoch1:node_model train_loss:0.02358119544223882,train_acc:0.9919999241828918
node10_model on test-dataset: loss:0.05229908280780364,acc:0.9843977093696594
node10 weight score:36616.32092167894
node12: train data size:1406
node12 epoch0:node_model train_loss:0.02616737764462111,train_acc:0.9886665940284729
node12 epoch1:node_model train_loss:0.021274135339384277,train_acc:0.9933332204818726
node12_model on test-dataset: loss:0.06247100738845802,acc:0.9826988577842712
node12 weight score:22506.440327705823
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05826408068903468,train_acc:0.9854544997215271
node13 epoch1:node_model train_loss:0.04022554200227288,train_acc:0.9847401976585388
node13_model on test-dataset: loss:0.061298416223762614,acc:0.9823950529098511
node13 weight score:17227.198760653082
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03984932674913956,train_acc:0.9883675575256348
node19 epoch1:node_model train_loss:0.0283968829643182,train_acc:0.9910346865653992
node19_model on test-dataset: loss:0.0700152720024198,acc:0.9790969491004944
node19 weight score:82567.70036971652
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04028063638426829,acc:0.987897954583168
total cost energy:8.684000000000001 | all_enery_cp：7.384 | all_enery_tp: 1.3
ef: 32.48448804586088
reward: 23.800488045860877
step 173:loss:63.472145080566406|running q:76.74540710449219
episode2,iteration53 selected nodes:[8, 4, 16, 17, 3],center node:8
################################################## episode2,iteration53 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.029932349411721685,train_acc:0.9886841177940369
node3 epoch1:node_model train_loss:0.012506647625215058,train_acc:0.9963157176971436
node3_model on test-dataset: loss:0.06999580492214591,acc:0.9800939559936523
node3 weight score:53746.07812831572
node4: train data size:4298
node4 epoch0:node_model train_loss:0.028920035001139568,train_acc:0.9904603958129883
node4 epoch1:node_model train_loss:0.014070368792894188,train_acc:0.9958092570304871
node4_model on test-dataset: loss:0.07430325251705654,acc:0.9789971113204956
node4 weight score:57844.035818127624
node8: train data size:2290
node8 epoch0:node_model train_loss:0.0460395362916524,train_acc:0.9868597984313965
node8 epoch1:node_model train_loss:0.022939856579203322,train_acc:0.9925603866577148
node8_model on test-dataset: loss:0.07201056300382333,acc:0.9790958166122437
node8 weight score:31800.890098282034
node16: train data size:920
node16 epoch0:node_model train_loss:0.0632283657672815,train_acc:0.9879999160766602
node16 epoch1:node_model train_loss:0.030692230304703118,train_acc:0.9869999289512634
node16_model on test-dataset: loss:0.05971944097589585,acc:0.9827970862388611
node16 weight score:15405.368586275505
node17: train data size:719
node17 epoch0:node_model train_loss:0.061978198238648474,train_acc:0.9834210872650146
node17 epoch1:node_model train_loss:0.011249095696257427,train_acc:0.9962499737739563
node17_model on test-dataset: loss:0.07158540773198183,acc:0.9778918027877808
node17 weight score:10043.946423996915
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041418482265980855,acc:0.9875969439744949
total cost energy:7.570354501823086 | all_enery_cp：5.9945 | all_enery_tp: 1.5758545018230863
ef: 31.7650700203967
reward: 24.194715518573613
step 174:loss:51.10825729370117|running q:77.891845703125
episode2,iteration54 selected nodes:[2, 13, 17, 11, 0],center node:11
################################################## episode2,iteration54 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.026727218536608335,train_acc:0.9915280938148499
node0 epoch1:node_model train_loss:0.01812305097579762,train_acc:0.9951390027999878
node0_model on test-dataset: loss:0.06333511855616053,acc:0.9833977222442627
node0 weight score:113128.39011498257
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03429211878562668,train_acc:0.9878724217414856
node2 epoch1:node_model train_loss:0.02351414849808281,train_acc:0.9919149875640869
node2_model on test-dataset: loss:0.06700937243702355,acc:0.9815959930419922
node2 weight score:68796.3464265025
node11: train data size:1575
node11 epoch0:node_model train_loss:0.058901722251903266,train_acc:0.982499897480011
node11 epoch1:node_model train_loss:0.022246438078582287,train_acc:0.9943749308586121
node11_model on test-dataset: loss:0.06950939409456623,acc:0.9783939123153687
node11 weight score:22658.807784416043
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05679169280285185,train_acc:0.9847401976585388
node13 epoch1:node_model train_loss:0.037742823843886567,train_acc:0.9874675273895264
node13_model on test-dataset: loss:0.06704375323319255,acc:0.981096088886261
node13 weight score:15750.908161823902
node17: train data size:719
node17 epoch0:node_model train_loss:0.030997086450952338,train_acc:0.9912499785423279
node17 epoch1:node_model train_loss:0.037173279575654306,train_acc:0.9884210228919983
node17_model on test-dataset: loss:0.05599844535950979,acc:0.9834979772567749
node17 weight score:12839.642161206852
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04205821365612792,acc:0.9866979521512985
total cost energy:9.113327043275216 | all_enery_cp：7.5625 | all_enery_tp: 1.5508270432752165
ef: 32.24368769157624
reward: 23.130360648301025
step 175:loss:55.65830993652344|running q:79.15745544433594
episode2,iteration55 selected nodes:[0, 3, 6, 1, 7],center node:6
################################################## episode2,iteration55 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.018366414321159635,train_acc:0.9945834279060364
node0 epoch1:node_model train_loss:0.015162379372971676,train_acc:0.9944446682929993
node0_model on test-dataset: loss:0.06657445697134108,acc:0.983596920967102
node0 weight score:107623.85944934381
node1: train data size:6657
node1 epoch0:node_model train_loss:0.02152270423973834,train_acc:0.9935427904129028
node1 epoch1:node_model train_loss:0.012206313015246736,train_acc:0.9964180588722229
node1_model on test-dataset: loss:0.07554050608854596,acc:0.9788969159126282
node1 weight score:88124.90602322541
node3: train data size:3762
node3 epoch0:node_model train_loss:0.02713215792104357,train_acc:0.990789532661438
node3 epoch1:node_model train_loss:0.016959319873038975,train_acc:0.9939472675323486
node3_model on test-dataset: loss:0.056755154959955686,acc:0.9841969609260559
node3 weight score:66284.72783933594
node6: train data size:3529
node6 epoch0:node_model train_loss:0.046151554570921384,train_acc:0.9854310154914856
node6 epoch1:node_model train_loss:0.0247165610894121,train_acc:0.9911110997200012
node6_model on test-dataset: loss:0.06414293347238072,acc:0.9813969731330872
node6 weight score:55017.75190122153
node7: train data size:3637
node7 epoch0:node_model train_loss:0.038671312532765234,train_acc:0.9887288212776184
node7 epoch1:node_model train_loss:0.026974514645927056,train_acc:0.9921620488166809
node7_model on test-dataset: loss:0.07713026631315188,acc:0.980096161365509
node7 weight score:47153.99250968017
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03792643379372748,acc:0.9889979553222656
total cost energy:13.807540321741751 | all_enery_cp：12.375 | all_enery_tp: 1.4325403217417507
ef: 32.550625322052404
reward: 18.743085000310653
step 176:loss:41.989784240722656|running q:80.48210144042969
episode2,iteration56 selected nodes:[5, 6, 4, 11, 12],center node:11
################################################## episode2,iteration56 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.025023950445249157,train_acc:0.9920835494995117
node4 epoch1:node_model train_loss:0.009797610478207664,train_acc:0.9958092570304871
node4_model on test-dataset: loss:0.050333831356765585,acc:0.985196053981781
node4 weight score:85389.88358616749
node5: train data size:4837
node5 epoch0:node_model train_loss:0.02719330015635042,train_acc:0.991897463798523
node5 epoch1:node_model train_loss:0.019153192985033124,train_acc:0.9929179549217224
node5_model on test-dataset: loss:0.0632651708198432,acc:0.9822981953620911
node5 weight score:76455.96996448586
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03855697232453773,train_acc:0.9870976209640503
node6 epoch1:node_model train_loss:0.01999716517053053,train_acc:0.9938887357711792
node6_model on test-dataset: loss:0.07135535141453148,acc:0.9800958633422852
node6 weight score:49456.69708076472
node11: train data size:1575
node11 epoch0:node_model train_loss:0.039845296978455735,train_acc:0.9868749380111694
node11 epoch1:node_model train_loss:0.021299911590176634,train_acc:0.9918749332427979
node11_model on test-dataset: loss:0.05475792354234727,acc:0.9845966696739197
node11 weight score:28762.960647731044
node12: train data size:1406
node12 epoch0:node_model train_loss:0.030609428400809217,train_acc:0.9906665682792664
node12 epoch1:node_model train_loss:0.033490309367577235,train_acc:0.9919999837875366
node12_model on test-dataset: loss:0.06761602364662395,acc:0.9799978137016296
node12 weight score:20793.887664084803
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038482429807190785,acc:0.9885979551076889
total cost energy:8.908230076213409 | all_enery_cp：7.8225 | all_enery_tp: 1.0857300762134083
ef: 32.88728290290756
reward: 23.97905282669415
step 177:loss:48.88972473144531|running q:81.78065490722656
episode2,iteration57 selected nodes:[6, 19, 16, 15, 3],center node:15
################################################## episode2,iteration57 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.023018108010307372,train_acc:0.991052508354187
node3 epoch1:node_model train_loss:0.01699712940554539,train_acc:0.9936842918395996
node3_model on test-dataset: loss:0.06678089461051968,acc:0.9810950756072998
node3 weight score:56333.47714104132
node6: train data size:3529
node6 epoch0:node_model train_loss:0.02019988816431982,train_acc:0.9938888549804688
node6 epoch1:node_model train_loss:0.014749896333442949,train_acc:0.9947221875190735
node6_model on test-dataset: loss:0.06210507434501778,acc:0.982996940612793
node6 weight score:56823.05410979843
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04900263611593151,train_acc:0.984323263168335
node15 epoch1:node_model train_loss:0.02368891512742266,train_acc:0.9912028908729553
node15_model on test-dataset: loss:0.05862074897944694,acc:0.9838990569114685
node15 weight score:23472.917421823462
node16: train data size:920
node16 epoch0:node_model train_loss:0.041228951700031755,train_acc:0.9899999499320984
node16 epoch1:node_model train_loss:0.020298085210379214,train_acc:0.9939999580383301
node16_model on test-dataset: loss:0.07289661058206548,acc:0.9790961146354675
node16 weight score:12620.614218603252
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03869359388622327,train_acc:0.9898277521133423
node19 epoch1:node_model train_loss:0.020532302835410268,train_acc:0.9936208128929138
node19_model on test-dataset: loss:0.06146991288027493,acc:0.9838971495628357
node19 weight score:94046.00932589029
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041692293866835824,acc:0.9879969471693039
total cost energy:9.130060338596671 | all_enery_cp：7.683999999999999 | all_enery_tp: 1.446060338596672
ef: 32.58924030986551
reward: 23.45917997126884
step 178:loss:59.309959411621094|running q:83.00511932373047
episode2,iteration58 selected nodes:[6, 3, 7, 18, 5],center node:6
################################################## episode2,iteration58 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.02518350605571054,train_acc:0.9907894134521484
node3 epoch1:node_model train_loss:0.010830264917642222,train_acc:0.9963158965110779
node3_model on test-dataset: loss:0.061067171580016295,acc:0.9825957417488098
node3 weight score:61604.29413487167
node5: train data size:4837
node5 epoch0:node_model train_loss:0.021668511028259004,train_acc:0.9920410513877869
node5 epoch1:node_model train_loss:0.018744388816174955,train_acc:0.9931219816207886
node5_model on test-dataset: loss:0.07320142826284609,acc:0.9803980588912964
node5 weight score:66077.94567384219
node6: train data size:3529
node6 epoch0:node_model train_loss:0.02243048049664746,train_acc:0.991388738155365
node6 epoch1:node_model train_loss:0.015710387530513497,train_acc:0.9944443106651306
node6_model on test-dataset: loss:0.08198906803510908,acc:0.9798979163169861
node6 weight score:43042.3236240327
node7: train data size:3637
node7 epoch0:node_model train_loss:0.037039316468557494,train_acc:0.9900802373886108
node7 epoch1:node_model train_loss:0.02435436788191264,train_acc:0.9921621680259705
node7_model on test-dataset: loss:0.05518290459032869,acc:0.9838939309120178
node7 weight score:65908.09285956684
node18: train data size:801
node18 epoch0:node_model train_loss:0.0595145461977356,train_acc:0.9933333992958069
node18 epoch1:node_model train_loss:0.013670958216404787,train_acc:0.995555579662323
node18_model on test-dataset: loss:0.11374723886705695,acc:0.9739971160888672
node18 weight score:7041.929175407726
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04177929360021153,acc:0.9875979542732238
total cost energy:9.638489848529781 | all_enery_cp：8.283000000000001 | all_enery_tp: 1.35548984852978
ef: 32.124209253729006
reward: 22.485719405199227
step 179:loss:60.0201416015625|running q:84.46906280517578
episode2,iteration59 selected nodes:[19, 1, 16, 2, 4],center node:2
################################################## episode2,iteration59 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.020463015652199122,train_acc:0.9928359985351562
node1 epoch1:node_model train_loss:0.010468073200118213,train_acc:0.9971643686294556
node1_model on test-dataset: loss:0.05712123778081604,acc:0.9845961332321167
node1 weight score:116541.59221030973
node2: train data size:4610
node2 epoch0:node_model train_loss:0.02794897013890032,train_acc:0.9889361262321472
node2 epoch1:node_model train_loss:0.02257267275002805,train_acc:0.9925532937049866
node2_model on test-dataset: loss:0.05415115190131473,acc:0.9837971329689026
node2 weight score:85132.07638502837
node4: train data size:4298
node4 epoch0:node_model train_loss:0.024120233610521482,train_acc:0.9934790134429932
node4 epoch1:node_model train_loss:0.017244608021005556,train_acc:0.9946416020393372
node4_model on test-dataset: loss:0.06666751588267744,acc:0.9824958443641663
node4 weight score:64469.178776117726
node16: train data size:920
node16 epoch0:node_model train_loss:0.04689163016737439,train_acc:0.9860000014305115
node16 epoch1:node_model train_loss:0.030970001663081347,train_acc:0.9959999918937683
node16_model on test-dataset: loss:0.07475147980410839,acc:0.9793950319290161
node16 weight score:12307.448660694423
node19: train data size:5781
node19 epoch0:node_model train_loss:0.028063732955568663,train_acc:0.990821897983551
node19 epoch1:node_model train_loss:0.02543184603664799,train_acc:0.9927588701248169
node19_model on test-dataset: loss:0.07582177815623253,acc:0.980595052242279
node19 weight score:76244.58487491701
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041580847005971006,acc:0.98859694480896
total cost energy:13.103972012747125 | all_enery_cp：11.133 | all_enery_tp: 1.9709720127471266
ef: 32.4046312052227
reward: 19.300659192475575
step 180:loss:67.74504852294922|running q:85.7056884765625
episode2_cost time: 13421.16884469986
episode3,iteration0 selected nodes:[18, 13, 14, 10, 19],center node:19
################################################## episode3,iteration0 ##################################################
node10: train data size:1915
node10 epoch0:node_model train_loss:1.4763898849487305,train_acc:0.5663334131240845
node10 epoch1:node_model train_loss:0.4736946240067482,train_acc:0.8575000762939453
node10_model on test-dataset: loss:0.8032675103098154,acc:0.7281836271286011
node10 weight score:2384.012767130836
node13: train data size:1056
node13 epoch0:node_model train_loss:1.935698476704684,train_acc:0.3627922236919403
node13 epoch1:node_model train_loss:0.850124716758728,train_acc:0.7470129132270813
node13_model on test-dataset: loss:0.8231126015633344,acc:0.7438837885856628
node13 weight score:1282.935041930282
node14: train data size:1540
node14 epoch0:node_model train_loss:1.5743281356990337,train_acc:0.5190625190734863
node14 epoch1:node_model train_loss:0.5798775106668472,train_acc:0.8221874833106995
node14_model on test-dataset: loss:0.6043604941363446,acc:0.7923960089683533
node14 weight score:2548.1480257916623
node18: train data size:801
node18 epoch0:node_model train_loss:2.078796770837572,train_acc:0.28777778148651123
node18 epoch1:node_model train_loss:1.194859395424525,train_acc:0.6044444441795349
node18_model on test-dataset: loss:1.4379361256957055,acc:0.4804999828338623
node18 weight score:557.0483873979161
node19: train data size:5781
node19 epoch0:node_model train_loss:0.777039762457897,train_acc:0.7518773674964905
node19 epoch1:node_model train_loss:0.24035916952737446,train_acc:0.922688364982605
node19_model on test-dataset: loss:0.34747930054552856,acc:0.8845870494842529
node19 weight score:16636.96223321522
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.35604352712631226,acc:0.9005868464708329
total cost energy:7.157151415887286 | all_enery_cp：5.5465 | all_enery_tp: 1.6106514158872864
ef: 25.509416050228307
reward: 18.35226463434102
step 181:loss:18.057109832763672|running q:1.653985857963562
episode3,iteration1 selected nodes:[13, 17, 12, 19, 9],center node:17
################################################## episode3,iteration1 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:0.37675198912620544,train_acc:0.8827271461486816
node9 epoch1:node_model train_loss:0.21108202602375636,train_acc:0.9300000667572021
node9_model on test-dataset: loss:0.2868002246087417,acc:0.9021868705749512
node9 weight score:7409.33868827671
node12: train data size:1406
node12 epoch0:node_model train_loss:0.4990088810523351,train_acc:0.8475555777549744
node12 epoch1:node_model train_loss:0.3327067087093989,train_acc:0.8948889970779419
node12_model on test-dataset: loss:0.45559402925893666,acc:0.8493928909301758
node12 weight score:3086.08083009117
node13: train data size:1056
node13 epoch0:node_model train_loss:0.4943605119531805,train_acc:0.8381169438362122
node13 epoch1:node_model train_loss:0.24815145405856046,train_acc:0.9220130443572998
node13_model on test-dataset: loss:0.2908996557863429,acc:0.9090930819511414
node13 weight score:3630.117736459614
node17: train data size:719
node17 epoch0:node_model train_loss:0.6322860289365053,train_acc:0.7977631092071533
node17 epoch1:node_model train_loss:0.42188114672899246,train_acc:0.853026270866394
node17_model on test-dataset: loss:0.41459289303049446,acc:0.8651979565620422
node17 weight score:1734.2313678954347
node19: train data size:5781
node19 epoch0:node_model train_loss:0.2744817385642693,train_acc:0.9104468822479248
node19 epoch1:node_model train_loss:0.16029248868340049,train_acc:0.9513090252876282
node19_model on test-dataset: loss:0.1540337549801916,acc:0.9491969347000122
node19 weight score:37530.734745403206
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.14353766113054006,acc:0.9546979546546936
total cost energy:6.8942582302006965 | all_enery_cp：5.5435 | all_enery_tp: 1.3507582302006969
ef: 27.929601448975912
reward: 21.035343218775218
step 182:loss:90.15486145019531|running q:3.631298542022705
episode3,iteration2 selected nodes:[10, 13, 17, 7, 8],center node:10
################################################## episode3,iteration2 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.22270202878359202,train_acc:0.9282687902450562
node7 epoch1:node_model train_loss:0.14457001104145437,train_acc:0.952972948551178
node7_model on test-dataset: loss:0.1588205752230715,acc:0.9496952891349792
node7 weight score:22900.05558090726
node8: train data size:2290
node8 epoch0:node_model train_loss:0.2745891407780025,train_acc:0.9153623580932617
node8 epoch1:node_model train_loss:0.16264152915581412,train_acc:0.9459421038627625
node8_model on test-dataset: loss:0.14084415306220763,acc:0.9540931582450867
node8 weight score:16259.105899756873
node10: train data size:1915
node10 epoch0:node_model train_loss:0.2783826470375061,train_acc:0.9123333096504211
node10 epoch1:node_model train_loss:0.16512335874140263,train_acc:0.9446666836738586
node10_model on test-dataset: loss:0.1992817026306875,acc:0.9355930685997009
node10 weight score:9609.512437521236
node13: train data size:1056
node13 epoch0:node_model train_loss:0.2853150875730948,train_acc:0.9124026298522949
node13 epoch1:node_model train_loss:0.20924846760251306,train_acc:0.9356493949890137
node13_model on test-dataset: loss:0.30230919312045446,acc:0.9013000130653381
node13 weight score:3493.112429363797
node17: train data size:719
node17 epoch0:node_model train_loss:0.394044840708375,train_acc:0.882763147354126
node17 epoch1:node_model train_loss:0.20958474278450012,train_acc:0.9318420886993408
node17_model on test-dataset: loss:0.31547784339636564,acc:0.8990958333015442
node17 weight score:2279.0823984955737
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.1141877827438293,acc:0.9620969480276108
total cost energy:6.238556307974577 | all_enery_cp：4.8085 | all_enery_tp: 1.4300563079745772
ef: 28.842985276504265
reward: 22.60442896852969
step 183:loss:92.2371597290039|running q:5.733658313751221
episode3,iteration3 selected nodes:[12, 4, 0, 9, 15],center node:12
################################################## episode3,iteration3 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.17907601796711484,train_acc:0.9442201256752014
node0 epoch1:node_model train_loss:0.1280733676523798,train_acc:0.9606838226318359
node0_model on test-dataset: loss:0.1589052024006378,acc:0.9498931169509888
node0 weight score:45089.77611655112
node4: train data size:4298
node4 epoch0:node_model train_loss:0.18244876418002817,train_acc:0.9413666725158691
node4 epoch1:node_model train_loss:0.10975301205072292,train_acc:0.9660415649414062
node4_model on test-dataset: loss:0.14126549817621709,acc:0.9522959589958191
node4 weight score:30424.98030650484
node9: train data size:2125
node9 epoch0:node_model train_loss:0.21269605207172307,train_acc:0.9350000023841858
node9 epoch1:node_model train_loss:0.12191140871833671,train_acc:0.9586364030838013
node9_model on test-dataset: loss:0.3170848021877464,acc:0.903288722038269
node9 weight score:6701.67723378235
node12: train data size:1406
node12 epoch0:node_model train_loss:0.27740054403742154,train_acc:0.930888831615448
node12 epoch1:node_model train_loss:0.19958397100369135,train_acc:0.933555543422699
node12_model on test-dataset: loss:0.22689130953396672,acc:0.932697057723999
node12 weight score:6196.799705056641
node15: train data size:1376
node15 epoch0:node_model train_loss:0.2551416462021215,train_acc:0.917218029499054
node15 epoch1:node_model train_loss:0.1499294899404049,train_acc:0.9526315927505493
node15_model on test-dataset: loss:0.14993749424815178,acc:0.9524950981140137
node15 weight score:9177.157500862806
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.09369210173026658,acc:0.9695969468355179
total cost energy:10.141972077755517 | all_enery_cp：8.185 | all_enery_tp: 1.9569720777555155
ef: 29.239247357672383
reward: 19.097275279916865
step 184:loss:66.42972564697266|running q:7.734442710876465
episode3,iteration4 selected nodes:[12, 9, 4, 14, 8],center node:8
################################################## episode3,iteration4 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.16354720238163029,train_acc:0.9520739912986755
node4 epoch1:node_model train_loss:0.09419995204134043,train_acc:0.9688323140144348
node4_model on test-dataset: loss:0.189360743452562,acc:0.9412838816642761
node4 weight score:22697.418280239905
node8: train data size:2290
node8 epoch0:node_model train_loss:0.17872936970999706,train_acc:0.948115885257721
node8 epoch1:node_model train_loss:0.13057519584570243,train_acc:0.958405613899231
node8_model on test-dataset: loss:0.13582840471994131,acc:0.9560982584953308
node8 weight score:16859.50744044776
node9: train data size:2125
node9 epoch0:node_model train_loss:0.1610678444531831,train_acc:0.9504544138908386
node9 epoch1:node_model train_loss:0.09668778941373933,train_acc:0.9663634896278381
node9_model on test-dataset: loss:0.12931924309348689,acc:0.9561958909034729
node9 weight score:16432.202579965648
node12: train data size:1406
node12 epoch0:node_model train_loss:0.15160100559393566,train_acc:0.9473333358764648
node12 epoch1:node_model train_loss:0.09509093115727106,train_acc:0.9686666131019592
node12_model on test-dataset: loss:0.12393605896737427,acc:0.9600942134857178
node12 weight score:11344.559539125936
node14: train data size:1540
node14 epoch0:node_model train_loss:0.1872086855582893,train_acc:0.9403125047683716
node14 epoch1:node_model train_loss:0.09755278658121824,train_acc:0.9706249237060547
node14_model on test-dataset: loss:0.1910542968381196,acc:0.9368998408317566
node14 weight score:8060.535803101266
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07793653138447552,acc:0.9742979556322098
total cost energy:7.061026010525133 | all_enery_cp：5.8294999999999995 | all_enery_tp: 1.231526010525133
ef: 29.76789987772013
reward: 22.706873867194997
step 185:loss:76.5302734375|running q:9.784134864807129
episode3,iteration5 selected nodes:[2, 14, 3, 17, 8],center node:8
################################################## episode3,iteration5 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.16003717733745246,train_acc:0.9512764811515808
node2 epoch1:node_model train_loss:0.1002969632440425,train_acc:0.9702127575874329
node2_model on test-dataset: loss:0.1515787458815612,acc:0.950588047504425
node2 weight score:30413.23487134606
node3: train data size:3762
node3 epoch0:node_model train_loss:0.17314135596940392,train_acc:0.9469438791275024
node3 epoch1:node_model train_loss:0.11201257008667055,train_acc:0.9645754098892212
node3_model on test-dataset: loss:0.1588133646780625,acc:0.9542961120605469
node3 weight score:23688.182714509665
node8: train data size:2290
node8 epoch0:node_model train_loss:0.16035193480227306,train_acc:0.9475845694541931
node8 epoch1:node_model train_loss:0.10562419138200906,train_acc:0.9664734601974487
node8_model on test-dataset: loss:0.10739338448503986,acc:0.9657981395721436
node8 weight score:21323.473610415935
node14: train data size:1540
node14 epoch0:node_model train_loss:0.1676696208305657,train_acc:0.9387499094009399
node14 epoch1:node_model train_loss:0.11021285504102707,train_acc:0.9593749642372131
node14_model on test-dataset: loss:0.15696532820002176,acc:0.9466970562934875
node14 weight score:9811.08387221393
node17: train data size:719
node17 epoch0:node_model train_loss:0.2043129140511155,train_acc:0.9446710348129272
node17 epoch1:node_model train_loss:0.11519354721531272,train_acc:0.9587499499320984
node17_model on test-dataset: loss:0.284200523327454,acc:0.9111988544464111
node17 weight score:2529.9038565512174
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07202069490682333,acc:0.9766979509592056
total cost energy:7.8887170362793455 | all_enery_cp：6.4605 | all_enery_tp: 1.4282170362793456
ef: 29.68682292802364
reward: 21.798105891744296
step 186:loss:19.470714569091797|running q:12.000873565673828
episode3,iteration6 selected nodes:[18, 2, 9, 10, 19],center node:10
################################################## episode3,iteration6 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.16518917085325463,train_acc:0.9527658820152283
node2 epoch1:node_model train_loss:0.09056650042692398,train_acc:0.9672338962554932
node2_model on test-dataset: loss:0.16927258986746893,acc:0.9440829753875732
node2 weight score:27234.178927665576
node9: train data size:2125
node9 epoch0:node_model train_loss:0.13680045027285814,train_acc:0.9604544639587402
node9 epoch1:node_model train_loss:0.07262724024158987,train_acc:0.9777272343635559
node9_model on test-dataset: loss:0.12029762153426418,acc:0.9609980583190918
node9 weight score:17664.522148467746
node10: train data size:1915
node10 epoch0:node_model train_loss:0.159984016045928,train_acc:0.9515000581741333
node10 epoch1:node_model train_loss:0.09400584334507585,train_acc:0.9670000076293945
node10_model on test-dataset: loss:0.27541603086865507,acc:0.9133989214897156
node10 weight score:6953.117412810501
node18: train data size:801
node18 epoch0:node_model train_loss:0.10976194611026181,train_acc:0.9655556082725525
node18 epoch1:node_model train_loss:0.20416213551329243,train_acc:0.8499999642372131
node18_model on test-dataset: loss:0.177670499055821,acc:0.9491000771522522
node18 weight score:4508.345528698828
node19: train data size:5781
node19 epoch0:node_model train_loss:0.13033375471573452,train_acc:0.9588845372200012
node19 epoch1:node_model train_loss:0.09556108308506422,train_acc:0.9696657657623291
node19_model on test-dataset: loss:0.127404855468194,acc:0.9572981595993042
node19 weight score:45375.03675786673
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0776911042642314,acc:0.9741989624500275
total cost energy:9.378780549425379 | all_enery_cp：7.616 | all_enery_tp: 1.762780549425379
ef: 29.41084411108102
reward: 20.03206356165564
step 187:loss:67.11540222167969|running q:14.154134750366211
episode3,iteration7 selected nodes:[4, 7, 9, 0, 15],center node:4
################################################## episode3,iteration7 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.1271369515452534,train_acc:0.9590173363685608
node0 epoch1:node_model train_loss:0.09327610879619089,train_acc:0.9682479500770569
node0_model on test-dataset: loss:0.10897390106827515,acc:0.9632970690727234
node0 weight score:65749.68804237751
node4: train data size:4298
node4 epoch0:node_model train_loss:0.11504869877772275,train_acc:0.9669433236122131
node4 epoch1:node_model train_loss:0.0801981812299684,train_acc:0.9748789668083191
node4_model on test-dataset: loss:0.09864409504341892,acc:0.9684938788414001
node4 weight score:43570.77834317608
node7: train data size:3637
node7 epoch0:node_model train_loss:0.1488443704674373,train_acc:0.9505112171173096
node7 epoch1:node_model train_loss:0.08369193928366578,train_acc:0.9725125432014465
node7_model on test-dataset: loss:0.11256945739234653,acc:0.9624941945075989
node7 weight score:32308.941379398315
node9: train data size:2125
node9 epoch0:node_model train_loss:0.1327346165410497,train_acc:0.9531816244125366
node9 epoch1:node_model train_loss:0.08428737402639606,train_acc:0.9718180894851685
node9_model on test-dataset: loss:0.1171395294820104,acc:0.9625959396362305
node9 weight score:18140.759224462698
node15: train data size:1376
node15 epoch0:node_model train_loss:0.12045235213424478,train_acc:0.9619548916816711
node15 epoch1:node_model train_loss:0.09696806408464909,train_acc:0.9693232774734497
node15_model on test-dataset: loss:0.12751466465182604,acc:0.9595961570739746
node15 weight score:10790.91572531768
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06646488777187187,acc:0.9778979539871215
total cost energy:11.38634137929832 | all_enery_cp：9.300500000000001 | all_enery_tp: 2.0858413792983193
ef: 30.57207982639586
reward: 19.185738447097542
step 188:loss:62.74000930786133|running q:16.2204532623291
episode3,iteration8 selected nodes:[9, 5, 3, 6, 12],center node:5
################################################## episode3,iteration8 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.11935469978734066,train_acc:0.9633613228797913
node3 epoch1:node_model train_loss:0.11021561190289886,train_acc:0.9652034640312195
node3_model on test-dataset: loss:0.15396586279843177,acc:0.95489901304245
node3 weight score:24433.987714050065
node5: train data size:4837
node5 epoch0:node_model train_loss:0.13625206714686083,train_acc:0.959448516368866
node5 epoch1:node_model train_loss:0.09006733975696321,train_acc:0.9710203409194946
node5_model on test-dataset: loss:0.12653770145960153,acc:0.9605959057807922
node5 weight score:38225.76152566089
node6: train data size:3529
node6 epoch0:node_model train_loss:0.09721541363332006,train_acc:0.9701530933380127
node6 epoch1:node_model train_loss:0.058617000099426754,train_acc:0.979999840259552
node6_model on test-dataset: loss:0.1511171576444758,acc:0.9545949101448059
node6 weight score:23352.74203808455
node9: train data size:2125
node9 epoch0:node_model train_loss:0.11962159198116172,train_acc:0.9604544639587402
node9 epoch1:node_model train_loss:0.05269484734162688,train_acc:0.9840908050537109
node9_model on test-dataset: loss:0.1718936702114297,acc:0.9508940577507019
node9 weight score:12362.293488679623
node12: train data size:1406
node12 epoch0:node_model train_loss:0.11754402505854765,train_acc:0.9646666646003723
node12 epoch1:node_model train_loss:0.0776845628550897,train_acc:0.9733332991600037
node12_model on test-dataset: loss:0.17621819246560336,acc:0.9460907578468323
node12 weight score:7978.744874905252
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0641781640208501,acc:0.9792969459295273
total cost energy:9.171866870536343 | all_enery_cp：7.8295 | all_enery_tp: 1.3423668705363432
ef: 29.767509745859456
reward: 20.595642875323115
step 189:loss:55.19459533691406|running q:18.14011001586914
episode3,iteration9 selected nodes:[0, 11, 1, 19, 10],center node:11
################################################## episode3,iteration9 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.10516840059103237,train_acc:0.9661005139350891
node0 epoch1:node_model train_loss:0.0786317887250334,train_acc:0.9743484854698181
node0_model on test-dataset: loss:0.10121715747693089,acc:0.9677916765213013
node0 weight score:70788.39377239996
node1: train data size:6657
node1 epoch0:node_model train_loss:0.12660050413001384,train_acc:0.9620896577835083
node1 epoch1:node_model train_loss:0.08146738816997898,train_acc:0.9735820293426514
node1_model on test-dataset: loss:0.13629740100062918,acc:0.9577970504760742
node1 weight score:48841.723694858054
node10: train data size:1915
node10 epoch0:node_model train_loss:0.10322963409125804,train_acc:0.964999794960022
node10 epoch1:node_model train_loss:0.0854062233818695,train_acc:0.9695000052452087
node10_model on test-dataset: loss:0.12239431244437583,acc:0.963287889957428
node10 weight score:15646.151865678434
node11: train data size:1575
node11 epoch0:node_model train_loss:0.13883098145015538,train_acc:0.9572916626930237
node11 epoch1:node_model train_loss:0.07347025524359196,train_acc:0.9743748903274536
node11_model on test-dataset: loss:0.0830582644668175,acc:0.9728931188583374
node11 weight score:18962.592224994376
node19: train data size:5781
node19 epoch0:node_model train_loss:0.1176303649513886,train_acc:0.9660857319831848
node19 epoch1:node_model train_loss:0.0672179430819534,train_acc:0.9794827699661255
node19_model on test-dataset: loss:0.1021530299862934,acc:0.9671962261199951
node19 weight score:56591.566601359526
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05983619530830765,acc:0.9806969439983368
total cost energy:13.111146198089568 | all_enery_cp：11.5465 | all_enery_tp: 1.564646198089567
ef: 31.118352772192313
reward: 18.007206574102746
step 190:loss:62.83032989501953|running q:20.20303726196289
episode3,iteration10 selected nodes:[7, 4, 1, 9, 3],center node:4
################################################## episode3,iteration10 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0995294322996442,train_acc:0.9702591300010681
node1 epoch1:node_model train_loss:0.0861415522339851,train_acc:0.9726471304893494
node1_model on test-dataset: loss:0.13810427721968155,acc:0.9574940204620361
node1 weight score:48202.70692565701
node3: train data size:3762
node3 epoch0:node_model train_loss:0.10762678910242884,train_acc:0.9649405479431152
node3 epoch1:node_model train_loss:0.0848106423049773,train_acc:0.9753649830818176
node3_model on test-dataset: loss:0.10228117714621476,acc:0.9674942493438721
node3 weight score:36780.96112075519
node4: train data size:4298
node4 epoch0:node_model train_loss:0.10492489491264488,train_acc:0.9683719873428345
node4 epoch1:node_model train_loss:0.060846030885397,train_acc:0.9834789037704468
node4_model on test-dataset: loss:0.07762033929786412,acc:0.9749961495399475
node4 weight score:55372.084673665784
node7: train data size:3637
node7 epoch0:node_model train_loss:0.10166830599710748,train_acc:0.9703505635261536
node7 epoch1:node_model train_loss:0.06730692740529776,train_acc:0.9795397520065308
node7_model on test-dataset: loss:0.09486941628507338,acc:0.97039794921875
node7 weight score:38336.907113154026
node9: train data size:2125
node9 epoch0:node_model train_loss:0.11073055482384833,train_acc:0.9672725200653076
node9 epoch1:node_model train_loss:0.07602779050780968,train_acc:0.9718179702758789
node9_model on test-dataset: loss:0.08611341579700821,acc:0.9738960266113281
node9 weight score:24676.758903736667
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.056264638849970655,acc:0.9805969458818435
total cost energy:11.910320393249936 | all_enery_cp：10.2395 | all_enery_tp: 1.670820393249937
ef: 31.331242680659642
reward: 19.420922287409706
step 191:loss:46.619258880615234|running q:22.10852813720703
episode3,iteration11 selected nodes:[6, 8, 13, 3, 19],center node:6
################################################## episode3,iteration11 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.09405267942010571,train_acc:0.9698385000228882
node3 epoch1:node_model train_loss:0.0585168710615682,train_acc:0.9818421602249146
node3_model on test-dataset: loss:0.10119350797904189,acc:0.9692979454994202
node3 weight score:37176.29791803586
node6: train data size:3529
node6 epoch0:node_model train_loss:0.09763321404655774,train_acc:0.966695249080658
node6 epoch1:node_model train_loss:0.057105270964610905,train_acc:0.9811397194862366
node6_model on test-dataset: loss:0.08752404991944786,acc:0.97039395570755
node6 weight score:40320.34627337161
node8: train data size:2290
node8 epoch0:node_model train_loss:0.11552183042563823,train_acc:0.9645892381668091
node8 epoch1:node_model train_loss:0.07144767985395763,train_acc:0.9746856689453125
node8_model on test-dataset: loss:0.12960467726094066,acc:0.9614948630332947
node8 weight score:17669.11540846176
node13: train data size:1056
node13 epoch0:node_model train_loss:0.09933650578287515,train_acc:0.971103847026825
node13 epoch1:node_model train_loss:0.0735768394714052,train_acc:0.9765585660934448
node13_model on test-dataset: loss:0.13742201611050406,acc:0.9566948413848877
node13 weight score:7684.35822649296
node19: train data size:5781
node19 epoch0:node_model train_loss:0.09482824842943328,train_acc:0.9718157052993774
node19 epoch1:node_model train_loss:0.07337453058150051,train_acc:0.9784482717514038
node19_model on test-dataset: loss:0.07652569075929933,acc:0.9757950901985168
node19 weight score:75543.25799140205
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0532588661788759,acc:0.9817959386110305
total cost energy:9.99405914659306 | all_enery_cp：8.209 | all_enery_tp: 1.785059146593059
ef: 30.955982059028845
reward: 20.961922912435785
step 192:loss:42.56000900268555|running q:24.053442001342773
episode3,iteration12 selected nodes:[19, 2, 5, 1, 11],center node:5
################################################## episode3,iteration12 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.09640785729262366,train_acc:0.9722391366958618
node1 epoch1:node_model train_loss:0.05683700299696691,train_acc:0.9814165830612183
node1_model on test-dataset: loss:0.08894727276230696,acc:0.9731948375701904
node1 weight score:74842.09232349871
node2: train data size:4610
node2 epoch0:node_model train_loss:0.10909355411979746,train_acc:0.9668084383010864
node2 epoch1:node_model train_loss:0.07242192596474543,train_acc:0.9755317568778992
node2_model on test-dataset: loss:0.06928408825013321,acc:0.9775971174240112
node2 weight score:66537.64401657022
node5: train data size:4837
node5 epoch0:node_model train_loss:0.09779588292276829,train_acc:0.9691007733345032
node5 epoch1:node_model train_loss:0.06751950009136784,train_acc:0.9777330756187439
node5_model on test-dataset: loss:0.07779796974908094,acc:0.9744971990585327
node5 weight score:62173.85897859039
node11: train data size:1575
node11 epoch0:node_model train_loss:0.13309502066113055,train_acc:0.9591666460037231
node11 epoch1:node_model train_loss:0.06408552010543644,train_acc:0.9810416102409363
node11_model on test-dataset: loss:0.10450061469600769,acc:0.9693967700004578
node11 weight score:15071.681679401365
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07547479049013607,train_acc:0.9768562912940979
node19 epoch1:node_model train_loss:0.07119326860290663,train_acc:0.9785805940628052
node19_model on test-dataset: loss:0.14672013230505399,acc:0.9555971026420593
node19 weight score:39401.5457127615
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.050362202118849383,acc:0.983696944117546
total cost energy:13.135521118484446 | all_enery_cp：11.729999999999999 | all_enery_tp: 1.4055211184844474
ef: 31.562288873608953
reward: 18.42676775512451
step 193:loss:40.362388610839844|running q:25.766799926757812
episode3,iteration13 selected nodes:[7, 10, 6, 11, 1],center node:6
################################################## episode3,iteration13 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0755150237245791,train_acc:0.976006805896759
node1 epoch1:node_model train_loss:0.06447060069819885,train_acc:0.9797383546829224
node1_model on test-dataset: loss:0.0848341436800547,acc:0.9736960530281067
node1 weight score:78470.7632000902
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0827891204195718,train_acc:0.977375328540802
node6 epoch1:node_model train_loss:0.05346272654262268,train_acc:0.9826532006263733
node6_model on test-dataset: loss:0.0629046566868783,acc:0.9797949194908142
node6 weight score:56100.77513921379
node7: train data size:3637
node7 epoch0:node_model train_loss:0.08733985422028077,train_acc:0.9725930690765381
node7 epoch1:node_model train_loss:0.06826064624899142,train_acc:0.9744847416877747
node7_model on test-dataset: loss:0.11384117664238147,acc:0.963892936706543
node7 weight score:31948.018346869372
node10: train data size:1915
node10 epoch0:node_model train_loss:0.07904668636620045,train_acc:0.9734998941421509
node10 epoch1:node_model train_loss:0.05447713037137873,train_acc:0.9794999957084656
node10_model on test-dataset: loss:0.12121028194036626,acc:0.9643930792808533
node10 weight score:15798.989733743487
node11: train data size:1575
node11 epoch0:node_model train_loss:0.09984154719859362,train_acc:0.9691665768623352
node11 epoch1:node_model train_loss:0.05786234640982002,train_acc:0.9806248545646667
node11_model on test-dataset: loss:0.09821764493070077,acc:0.9713947772979736
node11 weight score:16035.8151644877
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05260402434956632,acc:0.9833969455957413
total cost energy:9.640661925296378 | all_enery_cp：8.6565 | all_enery_tp: 0.984161925296378
ef: 31.55635269640153
reward: 21.91569077110515
step 194:loss:58.04560470581055|running q:27.688949584960938
episode3,iteration14 selected nodes:[3, 14, 5, 4, 9],center node:4
################################################## episode3,iteration14 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08800191461647812,train_acc:0.9706279635429382
node3 epoch1:node_model train_loss:0.057971519074941934,train_acc:0.9823089241981506
node3_model on test-dataset: loss:0.09542792375432327,acc:0.9705966711044312
node3 weight score:39422.42324882989
node4: train data size:4298
node4 epoch0:node_model train_loss:0.09033448353063228,train_acc:0.9730088710784912
node4 epoch1:node_model train_loss:0.05469134010287911,train_acc:0.9823111295700073
node4_model on test-dataset: loss:0.15952543378516565,acc:0.9523939490318298
node4 weight score:26942.4122412239
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08147708701007829,train_acc:0.9716105461120605
node5 epoch1:node_model train_loss:0.053769641850447775,train_acc:0.9823055267333984
node5_model on test-dataset: loss:0.07737257852568291,acc:0.9753969311714172
node5 weight score:62515.68827313174
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0858826637903059,train_acc:0.9709089994430542
node9 epoch1:node_model train_loss:0.045729398261755705,train_acc:0.9859090447425842
node9_model on test-dataset: loss:0.1689087774373911,acc:0.9495828747749329
node9 weight score:12580.755317986166
node14: train data size:1540
node14 epoch0:node_model train_loss:0.11895583919249475,train_acc:0.9618748426437378
node14 epoch1:node_model train_loss:0.06294168753083795,train_acc:0.97718745470047
node14_model on test-dataset: loss:0.09050229165353812,acc:0.9716969728469849
node14 weight score:17016.14370048711
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05509676874178695,acc:0.9820969426631927
total cost energy:9.297227766016837 | all_enery_cp：8.280999999999999 | all_enery_tp: 1.016227766016838
ef: 30.934532173744802
reward: 21.637304407727967
step 195:loss:50.59687423706055|running q:29.794485092163086
episode3,iteration15 selected nodes:[12, 8, 14, 6, 2],center node:6
################################################## episode3,iteration15 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.09280188390588824,train_acc:0.9714893102645874
node2 epoch1:node_model train_loss:0.050856260404466315,train_acc:0.9840425848960876
node2_model on test-dataset: loss:0.16333446085445757,acc:0.949286937713623
node2 weight score:28224.294958231945
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0698720177105214,train_acc:0.9777776002883911
node6 epoch1:node_model train_loss:0.05227082261505226,train_acc:0.9811108708381653
node6_model on test-dataset: loss:0.1298944813691196,acc:0.9623879790306091
node6 weight score:27168.205783675156
node8: train data size:2290
node8 epoch0:node_model train_loss:0.09898244670551756,train_acc:0.9706761240959167
node8 epoch1:node_model train_loss:0.04790769439474072,train_acc:0.9829468131065369
node8_model on test-dataset: loss:0.10538527954136953,acc:0.9686970114707947
node8 weight score:21729.790061438787
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0723998316709185,train_acc:0.9759999513626099
node12 epoch1:node_model train_loss:0.061230258519450825,train_acc:0.9766666293144226
node12_model on test-dataset: loss:0.1337494527315721,acc:0.9590970873832703
node12 weight score:10512.192545727763
node14: train data size:1540
node14 epoch0:node_model train_loss:0.10975768574280664,train_acc:0.9624999761581421
node14 epoch1:node_model train_loss:0.05839537805877626,train_acc:0.9790623784065247
node14_model on test-dataset: loss:0.0896853273420129,acc:0.9723989367485046
node14 weight score:17171.147674214822
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05026538995225564,acc:0.984996948838234
total cost energy:7.986822955501368 | all_enery_cp：6.6875 | all_enery_tp: 1.299322955501368
ef: 30.32960237824407
reward: 22.3427794227427
step 196:loss:62.52565002441406|running q:31.674001693725586
episode3,iteration16 selected nodes:[15, 8, 18, 3, 13],center node:15
################################################## episode3,iteration16 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.09455031185950104,train_acc:0.9698386788368225
node3 epoch1:node_model train_loss:0.046948821577978764,train_acc:0.984414279460907
node3_model on test-dataset: loss:0.06399728922871872,acc:0.9794968366622925
node3 weight score:58783.73983240224
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08639008316980756,train_acc:0.9720772504806519
node8 epoch1:node_model train_loss:0.0389484708075938,train_acc:0.9877777099609375
node8_model on test-dataset: loss:0.11279209012529463,acc:0.9649885892868042
node8 weight score:20302.842135970375
node13: train data size:1056
node13 epoch0:node_model train_loss:0.09650655548003587,train_acc:0.9690909385681152
node13 epoch1:node_model train_loss:0.0728368323973634,train_acc:0.9760388731956482
node13_model on test-dataset: loss:0.11350119435141096,acc:0.9644977450370789
node13 weight score:9303.8668538634
node15: train data size:1376
node15 epoch0:node_model train_loss:0.09402288032496083,train_acc:0.9707142114639282
node15 epoch1:node_model train_loss:0.06633453689781683,train_acc:0.9807518124580383
node15_model on test-dataset: loss:0.08763332989256015,acc:0.9722949266433716
node15 weight score:15701.788368500864
node18: train data size:801
node18 epoch0:node_model train_loss:0.37025248052345383,train_acc:0.8699999451637268
node18 epoch1:node_model train_loss:0.2704105602266888,train_acc:0.8722221851348877
node18_model on test-dataset: loss:0.270123419941192,acc:0.9236757159233093
node18 weight score:2965.3111906193994
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.053100583306222686,acc:0.9821969485282898
total cost energy:6.260967524317101 | all_enery_cp：4.6425 | all_enery_tp: 1.6184675243171012
ef: 30.738252264494832
reward: 24.47728474017773
step 197:loss:64.67222595214844|running q:33.28258514404297
episode3,iteration17 selected nodes:[2, 1, 8, 4, 7],center node:2
################################################## episode3,iteration17 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.07854623265508841,train_acc:0.9761561751365662
node1 epoch1:node_model train_loss:0.044705267495183804,train_acc:0.9855958819389343
node1_model on test-dataset: loss:0.09494110153136717,acc:0.9723947644233704
node1 weight score:70117.15571680642
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07999168846141944,train_acc:0.9759573936462402
node2 epoch1:node_model train_loss:0.04997035044621914,train_acc:0.9846808910369873
node2_model on test-dataset: loss:0.10392685480532236,acc:0.9678959250450134
node2 weight score:44358.12099418898
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07700958807931044,train_acc:0.9767441153526306
node4 epoch1:node_model train_loss:0.04501017088850224,train_acc:0.9858090281486511
node4_model on test-dataset: loss:0.09057177764363587,acc:0.9713976979255676
node4 weight score:47454.075781872474
node7: train data size:3637
node7 epoch0:node_model train_loss:0.08011798973421792,train_acc:0.9743243455886841
node7 epoch1:node_model train_loss:0.055759919152871984,train_acc:0.9829728603363037
node7_model on test-dataset: loss:0.05954224641551264,acc:0.9808980226516724
node7 weight score:61082.68026401581
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08627594582250585,train_acc:0.9733814597129822
node8 epoch1:node_model train_loss:0.04186233060191507,train_acc:0.9860386252403259
node8_model on test-dataset: loss:0.08365692480423605,acc:0.9734969139099121
node8 weight score:27373.705229528634
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.049749572105065455,acc:0.9836969459056855
total cost energy:12.126326398495582 | all_enery_cp：10.745999999999999 | all_enery_tp: 1.3803263984955836
ef: 31.750455127872332
reward: 19.62412872937675
step 198:loss:55.960792541503906|running q:35.085880279541016
episode3,iteration18 selected nodes:[5, 6, 4, 9, 16],center node:5
################################################## episode3,iteration18 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06511996496945273,train_acc:0.9797530770301819
node4 epoch1:node_model train_loss:0.03902475544533064,train_acc:0.9881396293640137
node4_model on test-dataset: loss:0.08904462152626365,acc:0.9726938605308533
node4 weight score:48267.93495587275
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07203341551049024,train_acc:0.97551029920578
node5 epoch1:node_model train_loss:0.04996490196267865,train_acc:0.9839383363723755
node5_model on test-dataset: loss:0.08180252058999031,acc:0.9744968414306641
node5 weight score:59130.207298182875
node6: train data size:3529
node6 epoch0:node_model train_loss:0.06262987967218375,train_acc:0.9822221398353577
node6 epoch1:node_model train_loss:0.03954942063945863,train_acc:0.9883331656455994
node6_model on test-dataset: loss:0.07219720735723968,acc:0.9779950380325317
node6 weight score:48880.006986116816
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06650132770565423,train_acc:0.9768180847167969
node9 epoch1:node_model train_loss:0.047122342766008594,train_acc:0.9831816554069519
node9_model on test-dataset: loss:0.08222826638491824,acc:0.9752965569496155
node9 weight score:25842.69489584853
node16: train data size:920
node16 epoch0:node_model train_loss:0.10387024767696858,train_acc:0.9689998626708984
node16 epoch1:node_model train_loss:0.08364101275801658,train_acc:0.9750000238418579
node16_model on test-dataset: loss:0.10777548216261494,acc:0.9670949578285217
node16 weight score:8536.264292577005
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04831595217387075,acc:0.9841969442367554
total cost energy:9.401713595499958 | all_enery_cp：7.8545 | all_enery_tp: 1.547213595499958
ef: 31.43694108463331
reward: 22.035227489133355
step 199:loss:45.4481086730957|running q:37.033912658691406
episode3,iteration19 selected nodes:[18, 5, 9, 8, 6],center node:5
################################################## episode3,iteration19 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06564645770443034,train_acc:0.9817540645599365
node5 epoch1:node_model train_loss:0.05166015685393418,train_acc:0.9832652807235718
node5_model on test-dataset: loss:0.09654169447429012,acc:0.9724982380867004
node5 weight score:50102.70460177323
node6: train data size:3529
node6 epoch0:node_model train_loss:0.06550261652511027,train_acc:0.9774998426437378
node6 epoch1:node_model train_loss:0.0336997443680755,train_acc:0.9886110424995422
node6_model on test-dataset: loss:0.06188861441856716,acc:0.9796969890594482
node6 weight score:57021.79687094864
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07351178890499084,train_acc:0.9777776598930359
node8 epoch1:node_model train_loss:0.050282938486855965,train_acc:0.9847342371940613
node8_model on test-dataset: loss:0.0638714724953752,acc:0.9794983267784119
node8 weight score:35853.25201584814
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0578510130289942,train_acc:0.9818179607391357
node9 epoch1:node_model train_loss:0.05922405324368314,train_acc:0.9786361455917358
node9_model on test-dataset: loss:0.08061217933238368,acc:0.9729971885681152
node9 weight score:26360.780934083257
node18: train data size:801
node18 epoch0:node_model train_loss:0.0404353609515561,train_acc:0.9833332896232605
node18 epoch1:node_model train_loss:0.054613959380528994,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.10643378470907919,acc:0.9684956669807434
node18 weight score:7525.80585374666
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04316762214555638,acc:0.9858979541063309
total cost energy:8.145320376686506 | all_enery_cp：6.791 | all_enery_tp: 1.3543203766865055
ef: 31.71216881817798
reward: 23.566848441491473
step 200:loss:38.25251770019531|running q:38.93221664428711
episode3,iteration20 selected nodes:[14, 4, 19, 12, 6],center node:12
################################################## episode3,iteration20 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05737515856230328,train_acc:0.9820882081985474
node4 epoch1:node_model train_loss:0.03723793987964475,train_acc:0.9881395101547241
node4_model on test-dataset: loss:0.08545103766940883,acc:0.9714938998222351
node4 weight score:50297.80933296576
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04599285093394832,train_acc:0.9837643504142761
node6 epoch1:node_model train_loss:0.024316468964874122,train_acc:0.9926531910896301
node6_model on test-dataset: loss:0.07455696111932411,acc:0.9786970019340515
node6 weight score:47332.93775683855
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07676307049890359,train_acc:0.9759999513626099
node12 epoch1:node_model train_loss:0.030288365591938298,train_acc:0.9913333058357239
node12_model on test-dataset: loss:0.08536452309879677,acc:0.973598062992096
node12 weight score:16470.542433333383
node14: train data size:1540
node14 epoch0:node_model train_loss:0.09337367955595255,train_acc:0.9712499976158142
node14 epoch1:node_model train_loss:0.050028250785544515,train_acc:0.9849998950958252
node14_model on test-dataset: loss:0.12711975766229444,acc:0.9621977806091309
node14 weight score:12114.560539764043
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07809570663737068,train_acc:0.9798274636268616
node19 epoch1:node_model train_loss:0.06286392209184324,train_acc:0.9817242622375488
node19_model on test-dataset: loss:0.0714355528559463,acc:0.9770957827568054
node19 weight score:80926.09028529117
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04361838145603542,acc:0.9851969498395919
total cost energy:9.919357483392956 | all_enery_cp：8.277000000000001 | all_enery_tp: 1.6423574833929544
ef: 31.636545845419437
reward: 21.717188362026484
step 201:loss:50.47677230834961|running q:40.72711944580078
episode3,iteration21 selected nodes:[18, 8, 5, 16, 4],center node:8
################################################## episode3,iteration21 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05167740266661831,train_acc:0.9848790764808655
node4 epoch1:node_model train_loss:0.032110951039506945,train_acc:0.9890697598457336
node4_model on test-dataset: loss:0.10394303389470223,acc:0.9685991406440735
node4 weight score:41349.57234704172
node5: train data size:4837
node5 epoch0:node_model train_loss:0.061489664037160725,train_acc:0.9795091152191162
node5 epoch1:node_model train_loss:0.03805890727826223,train_acc:0.9880198240280151
node5_model on test-dataset: loss:0.07702675624899712,acc:0.9779930710792542
node5 weight score:62796.36110293789
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07055656206996544,train_acc:0.9769080877304077
node8 epoch1:node_model train_loss:0.04006830218207577,train_acc:0.9872944951057434
node8_model on test-dataset: loss:0.06015420563155203,acc:0.9802979826927185
node8 weight score:38068.826210196865
node16: train data size:920
node16 epoch0:node_model train_loss:0.07470255349762737,train_acc:0.9760000109672546
node16 epoch1:node_model train_loss:0.04046422755345702,train_acc:0.9829999208450317
node16_model on test-dataset: loss:0.08735045051333146,acc:0.9733960032463074
node16 weight score:10532.286835310475
node18: train data size:801
node18 epoch0:node_model train_loss:0.05529328311481802,train_acc:0.9799999594688416
node18 epoch1:node_model train_loss:0.03227997006615624,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.11086885625583819,acc:0.9668950438499451
node18 weight score:7224.752081428823
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0470046597057808,acc:0.98559694647789
total cost energy:8.152260792452129 | all_enery_cp：6.573 | all_enery_tp: 1.579260792452128
ef: 31.375325251990706
reward: 23.223064459538577
step 202:loss:47.19800567626953|running q:42.457794189453125
episode3,iteration22 selected nodes:[0, 17, 10, 19, 11],center node:11
################################################## episode3,iteration22 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07104328811530852,train_acc:0.9793056845664978
node0 epoch1:node_model train_loss:0.05057450060849078,train_acc:0.9840278625488281
node0_model on test-dataset: loss:0.0714331331779249,acc:0.9779909253120422
node0 weight score:100303.59416201852
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0796378474755329,train_acc:0.9729998707771301
node10 epoch1:node_model train_loss:0.047493223147466776,train_acc:0.9860000014305115
node10_model on test-dataset: loss:0.09006283037422691,acc:0.972395658493042
node10 weight score:21262.933799024944
node11: train data size:1575
node11 epoch0:node_model train_loss:0.08192852401407436,train_acc:0.9783333539962769
node11 epoch1:node_model train_loss:0.04650082619627938,train_acc:0.9839583039283752
node11_model on test-dataset: loss:0.0639887689324678,acc:0.9809971451759338
node11 weight score:24613.694344740667
node17: train data size:719
node17 epoch0:node_model train_loss:0.1016693883575499,train_acc:0.9705920219421387
node17 epoch1:node_model train_loss:0.04344239798956551,train_acc:0.9824999570846558
node17_model on test-dataset: loss:0.10437703786388738,acc:0.9693971276283264
node17 weight score:6888.488260584767
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06985901207020827,train_acc:0.979614794254303
node19 epoch1:node_model train_loss:0.045104191297311975,train_acc:0.9866029024124146
node19_model on test-dataset: loss:0.07363177070611528,acc:0.9764952659606934
node19 weight score:78512.3044653315
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04711646421976184,acc:0.9841969484090805
total cost energy:9.832244246730287 | all_enery_cp：8.577499999999999 | all_enery_tp: 1.2547442467302883
ef: 31.947609994460365
reward: 22.115365747730078
step 203:loss:56.51165771484375|running q:44.09320831298828
episode3,iteration23 selected nodes:[5, 11, 4, 15, 7],center node:11
################################################## episode3,iteration23 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05065924061332331,train_acc:0.9830231666564941
node4 epoch1:node_model train_loss:0.04112715220219607,train_acc:0.9860320687294006
node4_model on test-dataset: loss:0.11506526171413498,acc:0.964796781539917
node4 weight score:37352.715632610605
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05925303984585465,train_acc:0.9792442321777344
node5 epoch1:node_model train_loss:0.0381940825063051,train_acc:0.9880198836326599
node5_model on test-dataset: loss:0.062009393910993825,acc:0.9799991250038147
node5 weight score:78004.3102331054
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07649579715940195,train_acc:0.9771074056625366
node7 epoch1:node_model train_loss:0.04213938400828959,train_acc:0.9849451780319214
node7_model on test-dataset: loss:0.056071158367267344,acc:0.9823962450027466
node7 weight score:64864.00684247627
node11: train data size:1575
node11 epoch0:node_model train_loss:0.07678956486051902,train_acc:0.9764582514762878
node11 epoch1:node_model train_loss:0.030503745059832,train_acc:0.9893749356269836
node11_model on test-dataset: loss:0.07453775132962619,acc:0.9771947264671326
node11 weight score:21130.23229041244
node15: train data size:1376
node15 epoch0:node_model train_loss:0.08237650351864952,train_acc:0.9738346338272095
node15 epoch1:node_model train_loss:0.05088384669008,train_acc:0.9854887127876282
node15_model on test-dataset: loss:0.11420222621774884,acc:0.9641959071159363
node15 weight score:12048.801897927868
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04491207106555521,acc:0.9855979537963867
total cost energy:9.130072788688027 | all_enery_cp：7.8615 | all_enery_tp: 1.2685727886880274
ef: 32.04937012378542
reward: 22.919297335097397
step 204:loss:53.74663162231445|running q:45.6888313293457
episode3,iteration24 selected nodes:[14, 0, 10, 19, 11],center node:10
################################################## episode3,iteration24 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.06244311975832614,train_acc:0.980694591999054
node0 epoch1:node_model train_loss:0.03608361105499272,train_acc:0.9865172505378723
node0_model on test-dataset: loss:0.08530500523724185,acc:0.9736971259117126
node0 weight score:83992.72680510844
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0642276115657296,train_acc:0.979999840259552
node10 epoch1:node_model train_loss:0.04340300069889054,train_acc:0.9849998354911804
node10_model on test-dataset: loss:0.093360012729172,acc:0.9708991646766663
node10 weight score:20511.993775699477
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05798411933938041,train_acc:0.9847915768623352
node11 epoch1:node_model train_loss:0.038265574039542116,train_acc:0.9904165863990784
node11_model on test-dataset: loss:0.0776209205656778,acc:0.9765951037406921
node11 weight score:20290.921423269345
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08046670205658302,train_acc:0.9749999046325684
node14 epoch1:node_model train_loss:0.03632662317249924,train_acc:0.987187385559082
node14_model on test-dataset: loss:0.07366502303360903,acc:0.9767991900444031
node14 weight score:20905.443812830796
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06617519979622087,train_acc:0.9789251685142517
node19 epoch1:node_model train_loss:0.041214163730258185,train_acc:0.988235592842102
node19_model on test-dataset: loss:0.09897867872534334,acc:0.9684939384460449
node19 weight score:58406.51819612322
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04500898851758393,acc:0.9845969444513321
total cost energy:10.335870866461907 | all_enery_cp：8.988 | all_enery_tp: 1.3478708664619075
ef: 31.660649835699893
reward: 21.324778969237986
step 205:loss:45.35220718383789|running q:47.4500846862793
episode3,iteration25 selected nodes:[9, 13, 18, 7, 3],center node:7
################################################## episode3,iteration25 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.062042736387076344,train_acc:0.9807893633842468
node3 epoch1:node_model train_loss:0.047141938837931344,train_acc:0.9824702739715576
node3_model on test-dataset: loss:0.072006113631287,acc:0.9781968593597412
node3 weight score:52245.56374842862
node7: train data size:3637
node7 epoch0:node_model train_loss:0.052674436211787364,train_acc:0.9814316034317017
node7 epoch1:node_model train_loss:0.03845336990481293,train_acc:0.986997663974762
node7_model on test-dataset: loss:0.07653156702806882,acc:0.975395917892456
node7 weight score:47522.873779209156
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06398589624388312,train_acc:0.9809091091156006
node9 epoch1:node_model train_loss:0.04772016067396511,train_acc:0.9863635897636414
node9_model on test-dataset: loss:0.07951603094275925,acc:0.9727991223335266
node9 weight score:26724.17089743465
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06238311317495324,train_acc:0.9778571128845215
node13 epoch1:node_model train_loss:0.038547959686680275,train_acc:0.9890908598899841
node13_model on test-dataset: loss:0.07369376300135627,acc:0.9761978983879089
node13 weight score:14329.570875361123
node18: train data size:801
node18 epoch0:node_model train_loss:0.03992619573869484,train_acc:0.98333340883255
node18 epoch1:node_model train_loss:0.044087161128926605,train_acc:0.9855555295944214
node18_model on test-dataset: loss:0.10008172373170965,acc:0.9709991216659546
node18 weight score:8003.459274415086
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043768462684529365,acc:0.9860969454050064
total cost energy:7.820771045150985 | all_enery_cp：5.6905 | all_enery_tp: 2.1302710451509848
ef: 31.2481166724702
reward: 23.427345627319216
step 206:loss:56.15830993652344|running q:49.18358612060547
episode3,iteration26 selected nodes:[16, 17, 18, 9, 8],center node:17
################################################## episode3,iteration26 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06109558096479462,train_acc:0.9825602769851685
node8 epoch1:node_model train_loss:0.027237821097040305,train_acc:0.9925603866577148
node8_model on test-dataset: loss:0.058170724446536044,acc:0.9821978807449341
node8 weight score:39366.88122398595
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05210829213452102,train_acc:0.9840907454490662
node9 epoch1:node_model train_loss:0.05048050163102082,train_acc:0.9804545640945435
node9_model on test-dataset: loss:0.06748699033836601,acc:0.9795958995819092
node9 weight score:31487.550257400475
node16: train data size:920
node16 epoch0:node_model train_loss:0.05049981037154794,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.09053775826469064,train_acc:0.9779999852180481
node16_model on test-dataset: loss:0.08343694955101455,acc:0.9749969840049744
node16 weight score:11026.289970458458
node17: train data size:719
node17 epoch0:node_model train_loss:0.07421233900822699,train_acc:0.977171003818512
node17 epoch1:node_model train_loss:0.03461208878434263,train_acc:0.9874999523162842
node17_model on test-dataset: loss:0.08542506811900238,acc:0.9707990884780884
node17 weight score:8416.733118648366
node18: train data size:801
node18 epoch0:node_model train_loss:0.0436029724207477,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.020510593209312193,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.0958769881315311,acc:0.9732938408851624
node18 weight score:8354.455178557855
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042459203634498406,acc:0.9858969473838806
total cost energy:4.942799374276687 | all_enery_cp：3.4274999999999998 | all_enery_tp: 1.5152993742766874
ef: 31.472846837706115
reward: 26.530047463429426
step 207:loss:39.11070251464844|running q:50.59633255004883
episode3,iteration27 selected nodes:[11, 18, 1, 17, 8],center node:11
################################################## episode3,iteration27 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05866617512708502,train_acc:0.9820898175239563
node1 epoch1:node_model train_loss:0.040359658022313866,train_acc:0.9863056540489197
node1_model on test-dataset: loss:0.09906159509426288,acc:0.971996009349823
node1 weight score:67200.61385712068
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04192719555905332,train_acc:0.9873429536819458
node8 epoch1:node_model train_loss:0.03323823657980108,train_acc:0.988260805606842
node8_model on test-dataset: loss:0.07864834196050652,acc:0.9766969680786133
node8 weight score:29116.952028689044
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06078121799509972,train_acc:0.978958249092102
node11 epoch1:node_model train_loss:0.049393776163924485,train_acc:0.9845832586288452
node11_model on test-dataset: loss:0.08751394456223352,acc:0.9738897681236267
node11 weight score:17997.131861425525
node17: train data size:719
node17 epoch0:node_model train_loss:0.06269597675418481,train_acc:0.9837499856948853
node17 epoch1:node_model train_loss:0.028902195335831493,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.06780811288233962,acc:0.97819584608078
node17 weight score:10603.450965338116
node18: train data size:801
node18 epoch0:node_model train_loss:0.05047836123331864,train_acc:0.9866666793823242
node18 epoch1:node_model train_loss:0.014702003242203117,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.0902760333729384,acc:0.9726940989494324
node18 weight score:8872.786830265317
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0511321706421586,acc:0.9845969468355179
total cost energy:7.315063876655655 | all_enery_cp：6.020999999999999 | all_enery_tp: 1.2940638766556565
ef: 31.368573090962403
reward: 24.05350921430675
step 208:loss:81.06603240966797|running q:51.9849739074707
episode3,iteration28 selected nodes:[13, 5, 18, 3, 6],center node:6
################################################## episode3,iteration28 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05691807141135398,train_acc:0.9790490865707397
node3 epoch1:node_model train_loss:0.03130480948541509,train_acc:0.9897367358207703
node3_model on test-dataset: loss:0.07168149809818715,acc:0.9792969226837158
node3 weight score:52482.16206149774
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05809673110061154,train_acc:0.9804689884185791
node5 epoch1:node_model train_loss:0.02804567114145932,train_acc:0.9909377098083496
node5_model on test-dataset: loss:0.07378289126994787,acc:0.9757968187332153
node5 weight score:65557.20325871986
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05483447603829619,train_acc:0.9816664457321167
node6 epoch1:node_model train_loss:0.025144029708624456,train_acc:0.9911110401153564
node6_model on test-dataset: loss:0.056077751849588825,acc:0.9827979207038879
node6 weight score:62930.482831505935
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08954429931261322,train_acc:0.9700648784637451
node13 epoch1:node_model train_loss:0.03912939994849942,train_acc:0.9872727394104004
node13_model on test-dataset: loss:0.0651169913582271,acc:0.978998064994812
node13 weight score:16216.965464369254
node18: train data size:801
node18 epoch0:node_model train_loss:0.03726330068820971,train_acc:0.9844444394111633
node18 epoch1:node_model train_loss:0.2600761789104177,train_acc:0.874444305896759
node18_model on test-dataset: loss:0.092226774789724,acc:0.9742928743362427
node18 weight score:8685.113426402158
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04402687330271874,acc:0.9855969458818435
total cost energy:8.608544976076178 | all_enery_cp：6.9925 | all_enery_tp: 1.616044976076179
ef: 31.933746872639745
reward: 23.325201896563566
step 209:loss:39.368019104003906|running q:53.42616271972656
episode3,iteration29 selected nodes:[17, 6, 14, 4, 8],center node:8
################################################## episode3,iteration29 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.0547723283457873,train_acc:0.983483612537384
node4 epoch1:node_model train_loss:0.026879634107138183,train_acc:0.9913953542709351
node4_model on test-dataset: loss:0.05734418319785618,acc:0.9821958541870117
node4 weight score:74950.93242797608
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03869517058612675,train_acc:0.9872220158576965
node6 epoch1:node_model train_loss:0.03384573417457028,train_acc:0.9886110424995422
node6_model on test-dataset: loss:0.07071749548165826,acc:0.9794957041740417
node6 weight score:49902.78538519939
node8: train data size:2290
node8 epoch0:node_model train_loss:0.043689565854552,train_acc:0.9869081377983093
node8 epoch1:node_model train_loss:0.032471875346306224,train_acc:0.9899998903274536
node8_model on test-dataset: loss:0.10033588177981073,acc:0.9716971516609192
node8 weight score:22823.34055752313
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06550900096772239,train_acc:0.9787499904632568
node14 epoch1:node_model train_loss:0.02764672553166747,train_acc:0.9912498593330383
node14_model on test-dataset: loss:0.06707239703537198,acc:0.979496955871582
node14 weight score:22960.264849157695
node17: train data size:719
node17 epoch0:node_model train_loss:0.05123598878708435,train_acc:0.9771710634231567
node17 epoch1:node_model train_loss:0.06922408659011126,train_acc:0.9796710014343262
node17_model on test-dataset: loss:0.06934347801099648,acc:0.9783958792686462
node17 weight score:10368.675189409754
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04321566826896742,acc:0.9855989652872086
total cost energy:7.463853372054695 | all_enery_cp：6.188000000000001 | all_enery_tp: 1.275853372054694
ef: 31.940887018845967
reward: 24.47703364679127
step 210:loss:46.95859146118164|running q:54.979671478271484
episode3,iteration30 selected nodes:[6, 3, 2, 8, 13],center node:6
################################################## episode3,iteration30 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06555875003872876,train_acc:0.9810637831687927
node2 epoch1:node_model train_loss:0.048307153099077814,train_acc:0.9859575629234314
node2_model on test-dataset: loss:0.060471267551765774,acc:0.9815961122512817
node2 weight score:76234.55215410623
node3: train data size:3762
node3 epoch0:node_model train_loss:0.055069417349602044,train_acc:0.98299640417099
node3 epoch1:node_model train_loss:0.02603507352576248,train_acc:0.9920457601547241
node3_model on test-dataset: loss:0.07126993068110096,acc:0.979496955871582
node3 weight score:52785.23444667234
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03123585540904767,train_acc:0.9902777075767517
node6 epoch1:node_model train_loss:0.03500976230457632,train_acc:0.9871264696121216
node6_model on test-dataset: loss:0.08727044827464851,acc:0.9723938703536987
node6 weight score:40437.51429915768
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04375658747132705,train_acc:0.9845892786979675
node8 epoch1:node_model train_loss:0.03120495165374292,train_acc:0.9908694624900818
node8_model on test-dataset: loss:0.09183223532031662,acc:0.9738971590995789
node8 weight score:24936.777287543264
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05768269994719462,train_acc:0.9811038970947266
node13 epoch1:node_model train_loss:0.0695310306142677,train_acc:0.9776622653007507
node13_model on test-dataset: loss:0.07876562095363625,acc:0.9759949445724487
node13 weight score:13406.864406256538
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04323984487375128,acc:0.9856969445943833
total cost energy:9.108559146593059 | all_enery_cp：7.6235 | all_enery_tp: 1.485059146593059
ef: 31.82879702551566
reward: 22.7202378789226
step 211:loss:43.89346694946289|running q:56.53687286376953
episode3,iteration31 selected nodes:[12, 4, 18, 14, 1],center node:12
################################################## episode3,iteration31 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05448275678026587,train_acc:0.9828360080718994
node1 epoch1:node_model train_loss:0.029337391014030174,train_acc:0.9898508191108704
node1_model on test-dataset: loss:0.0898450537523604,acc:0.9725960493087769
node1 weight score:74094.22914196996
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03908221052179849,train_acc:0.9874370694160461
node4 epoch1:node_model train_loss:0.02389287415534518,train_acc:0.99395352602005
node4_model on test-dataset: loss:0.06121173759689555,acc:0.9813970923423767
node4 weight score:70215.29152307514
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06639057096714775,train_acc:0.9722222089767456
node12 epoch1:node_model train_loss:0.042449339466596334,train_acc:0.9879999756813049
node12_model on test-dataset: loss:0.08980636024993145,acc:0.9734918475151062
node12 weight score:15655.906731851692
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06524497595091816,train_acc:0.9803124666213989
node14 epoch1:node_model train_loss:0.03113085450604558,train_acc:0.9912499189376831
node14_model on test-dataset: loss:0.050822963435784914,acc:0.984199047088623
node14 weight score:30301.26336386894
node18: train data size:801
node18 epoch0:node_model train_loss:0.02875548896069328,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.01641831994574103,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.11755086810495413,acc:0.9660972356796265
node18 weight score:6814.071328548888
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042821740869585484,acc:0.9863979542255401
total cost energy:9.276200195867574 | all_enery_cp：7.351000000000001 | all_enery_tp: 1.9252001958675737
ef: 31.63899012800036
reward: 22.362789932132785
step 212:loss:42.86576843261719|running q:58.14250946044922
episode3,iteration32 selected nodes:[3, 4, 7, 10, 1],center node:10
################################################## episode3,iteration32 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04864866387641141,train_acc:0.9842156767845154
node1 epoch1:node_model train_loss:0.025813514036612948,train_acc:0.9901859760284424
node1_model on test-dataset: loss:0.06194736695382744,acc:0.9801971316337585
node1 weight score:107462.19455883902
node3: train data size:3762
node3 epoch0:node_model train_loss:0.0466157857414433,train_acc:0.9854668974876404
node3 epoch1:node_model train_loss:0.026947922661508385,train_acc:0.9901017546653748
node3_model on test-dataset: loss:0.06313812086245889,acc:0.9798970818519592
node3 weight score:59583.65482867636
node4: train data size:4298
node4 epoch0:node_model train_loss:0.036694772533400984,train_acc:0.9876696467399597
node4 epoch1:node_model train_loss:0.020988018844861452,train_acc:0.9932464957237244
node4_model on test-dataset: loss:0.05348511130963743,acc:0.9838958382606506
node4 weight score:80358.81191529927
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04734291769627079,train_acc:0.9859459400177002
node7 epoch1:node_model train_loss:0.03410125201618349,train_acc:0.9881080389022827
node7_model on test-dataset: loss:0.11754801922754268,acc:0.9664990901947021
node7 weight score:30940.54688373528
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0645956196822226,train_acc:0.9809998869895935
node10 epoch1:node_model train_loss:0.04265377473202534,train_acc:0.9834998250007629
node10_model on test-dataset: loss:0.0762512465607142,acc:0.9766960144042969
node10 weight score:25114.343520603335
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04085004545820994,acc:0.9860979545116425
total cost energy:11.949627981242816 | all_enery_cp：10.134500000000001 | all_enery_tp: 1.8151279812428154
ef: 32.34218451700733
reward: 20.392556535764516
step 213:loss:35.88994598388672|running q:59.65113067626953
episode3,iteration33 selected nodes:[6, 12, 19, 0, 8],center node:12
################################################## episode3,iteration33 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.053306522821205564,train_acc:0.981731116771698
node0 epoch1:node_model train_loss:0.0359154587398128,train_acc:0.9879810810089111
node0_model on test-dataset: loss:0.08391999039205984,acc:0.975596010684967
node0 weight score:85378.94209146527
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03501679834961477,train_acc:0.9880554676055908
node6 epoch1:node_model train_loss:0.020311317128491484,train_acc:0.9925000071525574
node6_model on test-dataset: loss:0.06552243203237594,acc:0.9797970652580261
node6 weight score:53859.41715741337
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04457095913265063,train_acc:0.9864733219146729
node8 epoch1:node_model train_loss:0.037737870745806264,train_acc:0.9873429536819458
node8_model on test-dataset: loss:0.07894290693611765,acc:0.9765980839729309
node8 weight score:29008.305988188637
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0425646015011201,train_acc:0.9860000014305115
node12 epoch1:node_model train_loss:0.03670067926868796,train_acc:0.9873332977294922
node12_model on test-dataset: loss:0.06612996420866693,acc:0.979892909526825
node12 weight score:21261.16378293353
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06099267788472232,train_acc:0.9812581539154053
node19 epoch1:node_model train_loss:0.03881899656824254,train_acc:0.9880631566047668
node19_model on test-dataset: loss:0.10574889568404615,acc:0.9677960872650146
node19 weight score:54667.23754044983
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04573959837200164,acc:0.9863969475030899
total cost energy:11.484822955501368 | all_enery_cp：10.0855 | all_enery_tp: 1.3993229555013682
ef: 32.0718150621075
reward: 20.58699210660613
step 214:loss:35.021663665771484|running q:61.019065856933594
episode3,iteration34 selected nodes:[7, 15, 3, 10, 17],center node:10
################################################## episode3,iteration34 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04273230124166921,train_acc:0.9837860465049744
node3 epoch1:node_model train_loss:0.019610417110686142,train_acc:0.9934210777282715
node3_model on test-dataset: loss:0.06224811690743081,acc:0.9811959862709045
node3 weight score:60435.563144736916
node7: train data size:3637
node7 epoch0:node_model train_loss:0.055184160466180056,train_acc:0.9819720983505249
node7 epoch1:node_model train_loss:0.02933860902762594,train_acc:0.991431713104248
node7_model on test-dataset: loss:0.06325993035090505,acc:0.9812948107719421
node7 weight score:57492.9497997458
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05356713662622496,train_acc:0.9814998507499695
node10 epoch1:node_model train_loss:0.03547518556006253,train_acc:0.9914999008178711
node10_model on test-dataset: loss:0.0616962640423435,acc:0.9830958247184753
node10 weight score:31039.15657981646
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07270099585210639,train_acc:0.975263237953186
node15 epoch1:node_model train_loss:0.03715290900851999,train_acc:0.985037624835968
node15_model on test-dataset: loss:0.07485130730630772,acc:0.9745969772338867
node15 weight score:18383.112460135275
node17: train data size:719
node17 epoch0:node_model train_loss:0.051349943270906806,train_acc:0.9849998950958252
node17 epoch1:node_model train_loss:0.02425727978697978,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.05708033553117275,acc:0.9822958111763
node17 weight score:12596.281947350139
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043558905500867696,acc:0.9862969487905502
total cost energy:7.2711309392482715 | all_enery_cp：5.7045 | all_enery_tp: 1.5666309392482716
ef: 32.21334140892425
reward: 24.942210469675977
step 215:loss:27.75016975402832|running q:62.392494201660156
episode3,iteration35 selected nodes:[8, 11, 4, 1, 6],center node:8
################################################## episode3,iteration35 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.047130921011123414,train_acc:0.9851114749908447
node1 epoch1:node_model train_loss:0.029642691915688007,train_acc:0.9916419386863708
node1_model on test-dataset: loss:0.055622651996163765,acc:0.9814968109130859
node1 weight score:119681.45640483173
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03780046118921492,train_acc:0.989534854888916
node4 epoch1:node_model train_loss:0.023731961123493694,train_acc:0.9920883178710938
node4_model on test-dataset: loss:0.06308216254590661,acc:0.9788969159126282
node4 weight score:68133.36490917267
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03616832029850533,train_acc:0.9864174723625183
node6 epoch1:node_model train_loss:0.026361779807808086,train_acc:0.9908621311187744
node6_model on test-dataset: loss:0.05881096590164816,acc:0.981195867061615
node6 weight score:60005.81602250305
node8: train data size:2290
node8 epoch0:node_model train_loss:0.042970246149469975,train_acc:0.9860868453979492
node8 epoch1:node_model train_loss:0.03506291433966354,train_acc:0.9904346466064453
node8_model on test-dataset: loss:0.08937243886024589,acc:0.9740929007530212
node8 weight score:25623.111881068115
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06897055434819777,train_acc:0.9789583086967468
node11 epoch1:node_model train_loss:0.024449925258522853,train_acc:0.9899998903274536
node11_model on test-dataset: loss:0.06065270154795144,acc:0.9808968901634216
node11 weight score:25967.516034793938
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03774723996310058,acc:0.9886979568004608
total cost energy:10.530562329783654 | all_enery_cp：9.1745 | all_enery_tp: 1.356062329783655
ef: 32.605380870844535
reward: 22.07481854106088
step 216:loss:44.69576644897461|running q:63.72771453857422
episode3,iteration36 selected nodes:[2, 16, 6, 18, 15],center node:15
################################################## episode3,iteration36 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07350287374426076,train_acc:0.9774467349052429
node2 epoch1:node_model train_loss:0.044332074680108025,train_acc:0.9855319857597351
node2_model on test-dataset: loss:0.05457967138441745,acc:0.9825970530509949
node2 weight score:84463.68186299046
node6: train data size:3529
node6 epoch0:node_model train_loss:0.026517266045427985,train_acc:0.9913888573646545
node6 epoch1:node_model train_loss:0.020654618103031278,train_acc:0.9930554628372192
node6_model on test-dataset: loss:0.0631127616338199,acc:0.9821969866752625
node6 weight score:55915.791175091494
node15: train data size:1376
node15 epoch0:node_model train_loss:0.050095501322565336,train_acc:0.982631504535675
node15 epoch1:node_model train_loss:0.02909579304313021,train_acc:0.991428554058075
node15_model on test-dataset: loss:0.11526619233918609,acc:0.9660989046096802
node15 weight score:11937.585271758931
node16: train data size:920
node16 epoch0:node_model train_loss:0.06614077491685748,train_acc:0.9829999804496765
node16 epoch1:node_model train_loss:0.05120904128998518,train_acc:0.9869999885559082
node16_model on test-dataset: loss:0.07537923528812826,acc:0.9765982031822205
node16 weight score:12204.952683367086
node18: train data size:801
node18 epoch0:node_model train_loss:0.02526015522299632,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.033609548648298765,train_acc:0.9888888597488403
node18_model on test-dataset: loss:0.10135654673998942,acc:0.9699838161468506
node18 weight score:7902.794893504119
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04123816023318796,acc:0.9862969428300857
total cost energy:6.685551073613427 | all_enery_cp：5.618 | all_enery_tp: 1.067551073613426
ef: 31.943208868034493
reward: 25.257657794421064
step 217:loss:39.477596282958984|running q:65.27693939208984
episode3,iteration37 selected nodes:[10, 1, 3, 12, 6],center node:6
################################################## episode3,iteration37 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.036293392104388617,train_acc:0.9879471063613892
node1 epoch1:node_model train_loss:0.026597190061028102,train_acc:0.991230845451355
node1_model on test-dataset: loss:0.06225231444477686,acc:0.9810971021652222
node1 weight score:106935.78318128765
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04742837149502808,train_acc:0.982572078704834
node3 epoch1:node_model train_loss:0.02522547421731839,train_acc:0.9913158416748047
node3_model on test-dataset: loss:0.0712561250532599,acc:0.9770981669425964
node3 weight score:52795.46140332665
node6: train data size:3529
node6 epoch0:node_model train_loss:0.031634156621294096,train_acc:0.989319920539856
node6 epoch1:node_model train_loss:0.019051560896009,train_acc:0.9933332800865173
node6_model on test-dataset: loss:0.06871575195278638,acc:0.9791971445083618
node6 weight score:51356.492502981346
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05503005511127412,train_acc:0.9839998483657837
node10 epoch1:node_model train_loss:0.03165138208787539,train_acc:0.9889999628067017
node10_model on test-dataset: loss:0.06834351199097,acc:0.9805960655212402
node10 weight score:28020.216465507692
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04696894328808412,train_acc:0.9846665859222412
node12 epoch1:node_model train_loss:0.03201061857980676,train_acc:0.9899999499320984
node12_model on test-dataset: loss:0.06409101193570678,acc:0.9799960255622864
node12 weight score:21937.553449935163
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040888215793384,acc:0.9866969430446625
total cost energy:10.10333138057622 | all_enery_cp：8.6345 | all_enery_tp: 1.468831380576221
ef: 32.34509582045031
reward: 22.24176443987409
step 218:loss:40.30173873901367|running q:66.70075225830078
episode3,iteration38 selected nodes:[2, 8, 14, 7, 1],center node:2
################################################## episode3,iteration38 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.030332937448306372,train_acc:0.9902986884117126
node1 epoch1:node_model train_loss:0.017637246819178283,train_acc:0.994328498840332
node1_model on test-dataset: loss:0.08153865834363387,acc:0.9766972661018372
node1 weight score:81642.25577449356
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04722893822435389,train_acc:0.9870213270187378
node2 epoch1:node_model train_loss:0.02810342455198592,train_acc:0.9906383156776428
node2_model on test-dataset: loss:0.06445743098927778,acc:0.9799960851669312
node2 weight score:71520.0703665471
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04933870567411587,train_acc:0.9825127124786377
node7 epoch1:node_model train_loss:0.02034915772870787,train_acc:0.9940540194511414
node7_model on test-dataset: loss:0.06270185653676891,acc:0.9822961091995239
node7 weight score:58004.66207674779
node8: train data size:2290
node8 epoch0:node_model train_loss:0.039183943066745996,train_acc:0.9859902262687683
node8 epoch1:node_model train_loss:0.023373155636996355,train_acc:0.9907245635986328
node8_model on test-dataset: loss:0.06536521829799312,acc:0.9803988933563232
node8 weight score:35033.922621662976
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05979176996152091,train_acc:0.9793750047683716
node14 epoch1:node_model train_loss:0.033084323920775205,train_acc:0.9874998927116394
node14_model on test-dataset: loss:0.059747066679992716,acc:0.9808990359306335
node14 weight score:25775.32397110458
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0423257212875069,acc:0.9867979520559311
total cost energy:10.821977647648456 | all_enery_cp：9.367 | all_enery_tp: 1.4549776476484553
ef: 32.28476891166007
reward: 21.462791264011617
step 219:loss:40.29438781738281|running q:68.00206756591797
episode3,iteration39 selected nodes:[19, 3, 16, 15, 17],center node:15
################################################## episode3,iteration39 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04518894939438293,train_acc:0.9853650331497192
node3 epoch1:node_model train_loss:0.02307935762617394,train_acc:0.9908913969993591
node3_model on test-dataset: loss:0.06097293691840605,acc:0.9806949496269226
node3 weight score:61699.50456928631
node15: train data size:1376
node15 epoch0:node_model train_loss:0.052543562304760726,train_acc:0.9862029552459717
node15 epoch1:node_model train_loss:0.031228275770055398,train_acc:0.9883458018302917
node15_model on test-dataset: loss:0.07891799529927085,acc:0.976495087146759
node15 weight score:17435.820496731667
node16: train data size:920
node16 epoch0:node_model train_loss:0.04503645233344287,train_acc:0.9909999966621399
node16 epoch1:node_model train_loss:0.0228181246900931,train_acc:0.9919999241828918
node16_model on test-dataset: loss:0.11552384652080945,acc:0.9669980406761169
node16 weight score:7963.723748016642
node17: train data size:719
node17 epoch0:node_model train_loss:0.04697430490341503,train_acc:0.9887499213218689
node17 epoch1:node_model train_loss:0.05199919566439348,train_acc:0.9874999523162842
node17_model on test-dataset: loss:0.06496172185070463,acc:0.9785982370376587
node17 weight score:11068.056380223567
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06542114790826073,train_acc:0.9808216691017151
node19 epoch1:node_model train_loss:0.02824417160470681,train_acc:0.9904770851135254
node19_model on test-dataset: loss:0.0576280179592959,acc:0.9827999472618103
node19 weight score:100315.78743664693
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045275408202833205,acc:0.9858989644050599
total cost energy:7.632439370329812 | all_enery_cp：6.278999999999999 | all_enery_tp: 1.3534393703298129
ef: 32.03193096703963
reward: 24.399491596709815
step 220:loss:27.231882095336914|running q:69.42393493652344
episode3,iteration40 selected nodes:[10, 2, 16, 5, 15],center node:5
################################################## episode3,iteration40 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03813588174377033,train_acc:0.9872340559959412
node2 epoch1:node_model train_loss:0.02554022366716031,train_acc:0.9914893507957458
node2_model on test-dataset: loss:0.08331422587187262,acc:0.9743968844413757
node2 weight score:55332.68720625973
node5: train data size:4837
node5 epoch0:node_model train_loss:0.054169128717835614,train_acc:0.9844897985458374
node5 epoch1:node_model train_loss:0.03220382675870645,train_acc:0.9906123280525208
node5_model on test-dataset: loss:0.08194994056724682,acc:0.977993905544281
node5 weight score:59023.83780291866
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04900518202848616,train_acc:0.9849998354911804
node10 epoch1:node_model train_loss:0.03236758041894063,train_acc:0.9861665964126587
node10_model on test-dataset: loss:0.0665504979758407,acc:0.9794988036155701
node10 weight score:28775.141557846604
node15: train data size:1376
node15 epoch0:node_model train_loss:0.042277487560308406,train_acc:0.9876691699028015
node15 epoch1:node_model train_loss:0.039011401224083135,train_acc:0.9854886531829834
node15_model on test-dataset: loss:0.10995732854697053,acc:0.96989905834198
node15 weight score:12513.94534755556
node16: train data size:920
node16 epoch0:node_model train_loss:0.053688715863972905,train_acc:0.9889999628067017
node16 epoch1:node_model train_loss:0.025808999477885664,train_acc:0.9909999966621399
node16_model on test-dataset: loss:0.09239858739238116,acc:0.9748928546905518
node16 weight score:9956.862176833016
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042265918567645715,acc:0.9875969475507737
total cost energy:8.259713578936527 | all_enery_cp：6.829 | all_enery_tp: 1.4307135789365266
ef: 31.35242992724957
reward: 23.092716348313044
step 221:loss:30.056352615356445|running q:70.90428924560547
episode3,iteration41 selected nodes:[12, 13, 11, 19, 14],center node:12
################################################## episode3,iteration41 ##################################################
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06663309648865834,train_acc:0.9791666269302368
node11 epoch1:node_model train_loss:0.02012818072398659,train_acc:0.9943749308586121
node11_model on test-dataset: loss:0.07101536495363689,acc:0.9783967137336731
node11 weight score:22178.299034698408
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0384273291255037,train_acc:0.9873332977294922
node12 epoch1:node_model train_loss:0.023189961158398848,train_acc:0.9886665940284729
node12_model on test-dataset: loss:0.10008389695241932,acc:0.9721930623054504
node12 weight score:14048.213976604284
node13: train data size:1056
node13 epoch0:node_model train_loss:0.060588104713877496,train_acc:0.9772726893424988
node13 epoch1:node_model train_loss:0.038715403421189294,train_acc:0.9890908598899841
node13_model on test-dataset: loss:0.07416101659211563,acc:0.9786950945854187
node13 weight score:14239.28700718846
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07618225691840053,train_acc:0.9765625
node14 epoch1:node_model train_loss:0.03817348851589486,train_acc:0.9878124594688416
node14_model on test-dataset: loss:0.0642288385345455,acc:0.9810001850128174
node14 weight score:23976.768615731864
node19: train data size:5781
node19 epoch0:node_model train_loss:0.048983965247692864,train_acc:0.986207127571106
node19 epoch1:node_model train_loss:0.03011387845517361,train_acc:0.99137943983078
node19_model on test-dataset: loss:0.06746489946492147,acc:0.9795969128608704
node19 weight score:85689.00340547969
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04625899910239241,acc:0.9865969491004943
total cost energy:6.888901951359279 | all_enery_cp：5.679 | all_enery_tp: 1.2099019513592786
ef: 31.860025827030757
reward: 24.971123875671477
step 222:loss:46.18564987182617|running q:72.31766510009766
episode3,iteration42 selected nodes:[8, 0, 12, 11, 3],center node:8
################################################## episode3,iteration42 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.050710370705928653,train_acc:0.9838142395019531
node0 epoch1:node_model train_loss:0.03707221469570262,train_acc:0.9875001907348633
node0_model on test-dataset: loss:0.07128500530881865,acc:0.9786979556083679
node0 weight score:100512.021693202
node3: train data size:3762
node3 epoch0:node_model train_loss:0.039790486293464995,train_acc:0.9860524535179138
node3 epoch1:node_model train_loss:0.015225521996494774,train_acc:0.9955263733863831
node3_model on test-dataset: loss:0.06898382504114124,acc:0.9813979268074036
node3 weight score:54534.52309663
node8: train data size:2290
node8 epoch0:node_model train_loss:0.038757604617706456,train_acc:0.9847824573516846
node8 epoch1:node_model train_loss:0.025861915686856144,train_acc:0.9913042783737183
node8_model on test-dataset: loss:0.07103510291692146,acc:0.9795968532562256
node8 weight score:32237.58263119927
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05683694570325315,train_acc:0.9841665625572205
node11 epoch1:node_model train_loss:0.02003220303595299,train_acc:0.9931249618530273
node11_model on test-dataset: loss:0.06122615055464849,acc:0.9830971360206604
node11 weight score:25724.302209628644
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04371478290267987,train_acc:0.9859998822212219
node12 epoch1:node_model train_loss:0.019701802553026938,train_acc:0.9926666617393494
node12_model on test-dataset: loss:0.06654986181536515,acc:0.98029625415802
node12 weight score:21127.01606955674
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.046525133807663224,acc:0.9847969448566437
total cost energy:9.399389691313216 | all_enery_cp：8.099 | all_enery_tp: 1.300389691313216
ef: 32.307587449023856
reward: 22.90819775771064
step 223:loss:42.255027770996094|running q:73.68014526367188
episode3,iteration43 selected nodes:[0, 12, 18, 17, 5],center node:12
################################################## episode3,iteration43 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03626205866587245,train_acc:0.9884722828865051
node0 epoch1:node_model train_loss:0.02629574689490255,train_acc:0.9906945824623108
node0_model on test-dataset: loss:0.08821860344589368,acc:0.9734928607940674
node0 weight score:81218.69673888508
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04136325005080779,train_acc:0.9848150610923767
node5 epoch1:node_model train_loss:0.025506939164039736,train_acc:0.9906124472618103
node5_model on test-dataset: loss:0.08775741046027634,acc:0.9773985147476196
node5 weight score:55117.85243696865
node12: train data size:1406
node12 epoch0:node_model train_loss:0.026517098769545555,train_acc:0.9906666874885559
node12 epoch1:node_model train_loss:0.03942902187506358,train_acc:0.9822221398353577
node12_model on test-dataset: loss:0.0764504705053696,acc:0.9781949520111084
node12 weight score:18390.992111699925
node17: train data size:719
node17 epoch0:node_model train_loss:0.05112617820122978,train_acc:0.9887499809265137
node17 epoch1:node_model train_loss:0.041294322109024506,train_acc:0.981249988079071
node17_model on test-dataset: loss:0.06256326494651149,acc:0.9794990420341492
node17 weight score:11492.366976287278
node18: train data size:801
node18 epoch0:node_model train_loss:0.024886814489339788,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.029086796993700165,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.0721573695304869,acc:0.9790917634963989
node18 weight score:11100.737252645731
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045187919261225035,acc:0.9869969499111175
total cost energy:8.836387412183747 | all_enery_cp：7.4639999999999995 | all_enery_tp: 1.3723874121837472
ef: 31.68594818814363
reward: 22.849560775959883
step 224:loss:35.61464309692383|running q:75.00826263427734
episode3,iteration44 selected nodes:[16, 11, 10, 19, 3],center node:11
################################################## episode3,iteration44 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03301102780173287,train_acc:0.9894735217094421
node3 epoch1:node_model train_loss:0.01691383208871182,train_acc:0.9947368502616882
node3_model on test-dataset: loss:0.061211883004871195,acc:0.9814949035644531
node3 weight score:61458.655008221576
node10: train data size:1915
node10 epoch0:node_model train_loss:0.045129166089463976,train_acc:0.9839999079704285
node10 epoch1:node_model train_loss:0.027562425116775556,train_acc:0.9919999241828918
node10_model on test-dataset: loss:0.06816442746305257,acc:0.9801970720291138
node10 weight score:28093.832388425693
node11: train data size:1575
node11 epoch0:node_model train_loss:0.03973015926021617,train_acc:0.9868749380111694
node11 epoch1:node_model train_loss:0.030631657486082986,train_acc:0.9910416007041931
node11_model on test-dataset: loss:0.08542202463402646,acc:0.9758949875831604
node11 weight score:18437.867830313928
node16: train data size:920
node16 epoch0:node_model train_loss:0.061731474567204715,train_acc:0.9850000739097595
node16 epoch1:node_model train_loss:0.022679303749464454,train_acc:0.9930000305175781
node16_model on test-dataset: loss:0.07073401069879764,acc:0.9782969355583191
node16 weight score:13006.472995255144
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0441345660445473,train_acc:0.9861666560173035
node19 epoch1:node_model train_loss:0.02787646088058707,train_acc:0.9925863742828369
node19_model on test-dataset: loss:0.0706025534183118,acc:0.9794958829879761
node19 weight score:81880.8912724198
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04398293856647797,acc:0.986397956609726
total cost energy:8.392050721518206 | all_enery_cp：6.9765 | all_enery_tp: 1.4155507215182062
ef: 32.06550425298956
reward: 23.673453531471356
step 225:loss:30.194782257080078|running q:76.3140869140625
episode3,iteration45 selected nodes:[11, 7, 0, 10, 13],center node:11
################################################## episode3,iteration45 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.036202068812498406,train_acc:0.9871477484703064
node0 epoch1:node_model train_loss:0.017618362418337103,train_acc:0.9943058490753174
node0_model on test-dataset: loss:0.06404766233255942,acc:0.9828988313674927
node0 weight score:111869.81287149311
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0486008641719063,train_acc:0.9851350784301758
node7 epoch1:node_model train_loss:0.035691440627734,train_acc:0.9881081581115723
node7_model on test-dataset: loss:0.056961354201703215,acc:0.9833951592445374
node7 weight score:63850.307826622025
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04921353947138414,train_acc:0.9816665649414062
node10 epoch1:node_model train_loss:0.030908108729636296,train_acc:0.9869999289512634
node10_model on test-dataset: loss:0.08059590441851469,acc:0.977497935295105
node10 weight score:23760.5125696695
node11: train data size:1575
node11 epoch0:node_model train_loss:0.03974748956898111,train_acc:0.9860416054725647
node11 epoch1:node_model train_loss:0.018132752054953016,train_acc:0.9956249594688416
node11_model on test-dataset: loss:0.06808986574385927,acc:0.9818957448005676
node11 weight score:23131.195557424675
node13: train data size:1056
node13 epoch0:node_model train_loss:0.058361749524589286,train_acc:0.9820130467414856
node13 epoch1:node_model train_loss:0.04410767923532562,train_acc:0.9849349856376648
node13_model on test-dataset: loss:0.07478695931727998,acc:0.9777970910072327
node13 weight score:14120.108768160664
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04353715839459255,acc:0.9866979551315308
total cost energy:9.095359193188068 | all_enery_cp：7.6739999999999995 | all_enery_tp: 1.4213591931880694
ef: 32.2749762174714
reward: 23.17961702428333
step 226:loss:31.199779510498047|running q:77.6001968383789
episode3,iteration46 selected nodes:[15, 2, 0, 10, 11],center node:11
################################################## episode3,iteration46 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.024739274124537285,train_acc:0.9915279746055603
node0 epoch1:node_model train_loss:0.022899307707525116,train_acc:0.9926283955574036
node0_model on test-dataset: loss:0.0788803269057189,acc:0.978297233581543
node0 weight score:90833.80205261966
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04396517185415042,train_acc:0.9870213270187378
node2 epoch1:node_model train_loss:0.03115454863538926,train_acc:0.987659752368927
node2_model on test-dataset: loss:0.06950585396970382,acc:0.9799931049346924
node2 weight score:66325.34868227653
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05591702921083197,train_acc:0.9826666116714478
node10 epoch1:node_model train_loss:0.03542089577531442,train_acc:0.9859998822212219
node10_model on test-dataset: loss:0.06873494034662145,acc:0.9807968735694885
node10 weight score:27860.64831573144
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04298721866143751,train_acc:0.98541659116745
node11 epoch1:node_model train_loss:0.022346405494317878,train_acc:0.9935415387153625
node11_model on test-dataset: loss:0.08005231099697994,acc:0.9780967235565186
node11 weight score:19674.635002847306
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04227496053291751,train_acc:0.9854887127876282
node15 epoch1:node_model train_loss:0.03233403536225004,train_acc:0.9876314997673035
node15_model on test-dataset: loss:0.09056507054522626,acc:0.975295901298523
node15 weight score:15193.495590696359
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04345957202357567,acc:0.9871979540586472
total cost energy:9.694933841025195 | all_enery_cp：8.3205 | all_enery_tp: 1.3744338410251953
ef: 31.98804629192616
reward: 22.293112450900963
step 227:loss:35.45853805541992|running q:78.96672058105469
episode3,iteration47 selected nodes:[13, 2, 8, 7, 10],center node:7
################################################## episode3,iteration47 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03257931493584344,train_acc:0.9887234568595886
node2 epoch1:node_model train_loss:0.019644331252648237,train_acc:0.9940425753593445
node2_model on test-dataset: loss:0.07550745850072417,acc:0.9804952144622803
node2 weight score:61053.57128336914
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04172258363797556,train_acc:0.9878377318382263
node7 epoch1:node_model train_loss:0.0286982251784286,train_acc:0.989729642868042
node7_model on test-dataset: loss:0.07578220628551208,acc:0.9777938723564148
node7 weight score:47992.79643954251
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04414527133390631,train_acc:0.9873911738395691
node8 epoch1:node_model train_loss:0.023226260653008587,train_acc:0.9912076592445374
node8_model on test-dataset: loss:0.07001768086809534,acc:0.9809971451759338
node8 weight score:32706.02470130476
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04809027564479038,train_acc:0.9836665987968445
node10 epoch1:node_model train_loss:0.021771298225212378,train_acc:0.9924999475479126
node10_model on test-dataset: loss:0.06401819359827642,acc:0.9816982746124268
node10 weight score:29913.371377157357
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05909233379431746,train_acc:0.9838311076164246
node13 epoch1:node_model train_loss:0.024922089799391953,train_acc:0.9954545497894287
node13_model on test-dataset: loss:0.07137328642245848,acc:0.9790969491004944
node13 weight score:14795.451532798084
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04558498072301518,acc:0.986296945810318
total cost energy:8.119973668286322 | all_enery_cp：6.754 | all_enery_tp: 1.3659736682863222
ef: 31.956827188274797
reward: 23.836853519988473
step 228:loss:26.344018936157227|running q:80.323486328125
episode3,iteration48 selected nodes:[16, 7, 5, 19, 15],center node:15
################################################## episode3,iteration48 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04061020105811102,train_acc:0.9861224889755249
node5 epoch1:node_model train_loss:0.019961908545193016,train_acc:0.9929178953170776
node5_model on test-dataset: loss:0.060177846175333796,acc:0.9830949306488037
node5 weight score:80378.41676664444
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03761019351155925,train_acc:0.9905405044555664
node7 epoch1:node_model train_loss:0.025354590540676302,train_acc:0.9902702569961548
node7_model on test-dataset: loss:0.06772018026422302,acc:0.9815977811813354
node7 weight score:53706.29531418198
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04618763494571405,train_acc:0.9854886531829834
node15 epoch1:node_model train_loss:0.018492844754031727,train_acc:0.993571400642395
node15_model on test-dataset: loss:0.06637342224239547,acc:0.979295015335083
node15 weight score:20731.189586320463
node16: train data size:920
node16 epoch0:node_model train_loss:0.10644322382286191,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.026847415091469883,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.07389928372867871,acc:0.979297399520874
node16 weight score:12449.376415849723
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04132515265092511,train_acc:0.9876778721809387
node19 epoch1:node_model train_loss:0.02253081778602289,train_acc:0.9932355284690857
node19_model on test-dataset: loss:0.05213960288352609,acc:0.9838960766792297
node19 weight score:110875.41293542439
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03608885355162784,acc:0.9878969472646714
total cost energy:9.323370866461909 | all_enery_cp：8.275500000000001 | all_enery_tp: 1.0478708664619076
ef: 32.74193770468243
reward: 23.41856683822052
step 229:loss:46.299354553222656|running q:81.55396270751953
episode3,iteration49 selected nodes:[7, 16, 18, 12, 11],center node:12
################################################## episode3,iteration49 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03193630061011661,train_acc:0.9911614656448364
node7 epoch1:node_model train_loss:0.015231086611370178,train_acc:0.9948649406433105
node7_model on test-dataset: loss:0.054262406766356436,acc:0.9832971096038818
node7 weight score:67026.1460325604
node11: train data size:1575
node11 epoch0:node_model train_loss:0.040657785004441394,train_acc:0.9862499237060547
node11 epoch1:node_model train_loss:0.02506391417773557,train_acc:0.9924998879432678
node11_model on test-dataset: loss:0.05569961878894901,acc:0.983596682548523
node11 weight score:28276.67467470146
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04070820948885133,train_acc:0.9906666278839111
node12 epoch1:node_model train_loss:0.024180323746986688,train_acc:0.9906666278839111
node12_model on test-dataset: loss:0.08334337135821897,acc:0.9779972434043884
node12 weight score:16869.967906108064
node16: train data size:920
node16 epoch0:node_model train_loss:0.07057435638271273,train_acc:0.9809999465942383
node16 epoch1:node_model train_loss:0.018692505126819015,train_acc:0.9939999580383301
node16_model on test-dataset: loss:0.0652028484101902,acc:0.9819948077201843
node16 weight score:14109.813028601036
node18: train data size:801
node18 epoch0:node_model train_loss:0.03156336985476729,train_acc:0.9933332800865173
node18 epoch1:node_model train_loss:0.017237207929206813,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.11282522135584144,acc:0.9738959074020386
node18 weight score:7099.476432434482
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04351983488668339,acc:0.9873969477415084
total cost energy:4.9995563079745775 | all_enery_cp：4.1695 | all_enery_tp: 0.8300563079745771
ef: 32.19681906579236
reward: 27.197262757817782
step 230:loss:42.290767669677734|running q:82.93476104736328
episode3,iteration50 selected nodes:[2, 11, 4, 17, 6],center node:11
################################################## episode3,iteration50 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.032759758960232734,train_acc:0.9891490340232849
node2 epoch1:node_model train_loss:0.02149471972122828,train_acc:0.9923404455184937
node2_model on test-dataset: loss:0.05484917095567653,acc:0.9845969676971436
node2 weight score:84048.67238057853
node4: train data size:4298
node4 epoch0:node_model train_loss:0.0448438917952698,train_acc:0.9888277053833008
node4 epoch1:node_model train_loss:0.022301017488136368,train_acc:0.9923256039619446
node4_model on test-dataset: loss:0.08661243987480702,acc:0.9759979844093323
node4 weight score:49623.35671656977
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03208748765367394,train_acc:0.9897221326828003
node6 epoch1:node_model train_loss:0.020734450682842482,train_acc:0.9933331608772278
node6_model on test-dataset: loss:0.08627774661668809,acc:0.9765971899032593
node6 weight score:40902.7836074408
node11: train data size:1575
node11 epoch0:node_model train_loss:0.041142649308312684,train_acc:0.9881249070167542
node11 epoch1:node_model train_loss:0.020795063755940646,train_acc:0.9929165840148926
node11_model on test-dataset: loss:0.07374531227086663,acc:0.9809969067573547
node11 weight score:21357.289724600025
node17: train data size:719
node17 epoch0:node_model train_loss:0.051658702024724334,train_acc:0.982499897480011
node17 epoch1:node_model train_loss:0.02696195953467395,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.07414168122677438,acc:0.9783968925476074
node17 weight score:9697.648989113446
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038527952549229666,acc:0.9877969479560852
total cost energy:8.739933841025195 | all_enery_cp：7.3655 | all_enery_tp: 1.3744338410251953
ef: 32.0516923461001
reward: 23.311758505074902
step 231:loss:25.26996421813965|running q:84.1048583984375
episode3,iteration51 selected nodes:[14, 5, 11, 18, 6],center node:11
################################################## episode3,iteration51 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03491689488101674,train_acc:0.9867953658103943
node5 epoch1:node_model train_loss:0.022642132985923553,train_acc:0.9932653903961182
node5_model on test-dataset: loss:0.06314344589445682,acc:0.9825959801673889
node5 weight score:76603.35813925901
node6: train data size:3529
node6 epoch0:node_model train_loss:0.02791919907031115,train_acc:0.9899998903274536
node6 epoch1:node_model train_loss:0.018681866666560784,train_acc:0.9950000047683716
node6_model on test-dataset: loss:0.05188486959290458,acc:0.9856980443000793
node6 weight score:68015.97513280831
node11: train data size:1575
node11 epoch0:node_model train_loss:0.027930983385886066,train_acc:0.9924998879432678
node11 epoch1:node_model train_loss:0.011004761992808199,train_acc:0.9956248998641968
node11_model on test-dataset: loss:0.05245241549673665,acc:0.9849979281425476
node11 weight score:30027.215812358336
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05032845328969415,train_acc:0.9834374189376831
node14 epoch1:node_model train_loss:0.03202235144271981,train_acc:0.9890624284744263
node14_model on test-dataset: loss:0.0538960837029299,acc:0.9833990335464478
node14 weight score:28573.504681496597
node18: train data size:801
node18 epoch0:node_model train_loss:0.02116583047124247,train_acc:0.9933332800865173
node18 epoch1:node_model train_loss:0.010950141262608466,train_acc:0.9955554604530334
node18_model on test-dataset: loss:0.07916123645151857,acc:0.9786911010742188
node18 weight score:10118.588793020732
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03919443295188103,acc:0.9881969439983368
total cost energy:7.361079285608123 | all_enery_cp：6.141 | all_enery_tp: 1.220079285608123
ef: 32.67669798560826
reward: 25.315618700000133
step 232:loss:27.738943099975586|running q:85.42135620117188
episode3,iteration52 selected nodes:[6, 7, 16, 2, 14],center node:6
################################################## episode3,iteration52 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.034363586371368236,train_acc:0.988936185836792
node2 epoch1:node_model train_loss:0.018445116932970778,train_acc:0.9944681525230408
node2_model on test-dataset: loss:0.09414857624717116,acc:0.9772928357124329
node2 weight score:48965.15894087686
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0196234967164249,train_acc:0.9938888549804688
node6 epoch1:node_model train_loss:0.0170597706876126,train_acc:0.9936111569404602
node6_model on test-dataset: loss:0.06371734391002065,acc:0.9827962517738342
node6 weight score:55385.23396366815
node7: train data size:3637
node7 epoch0:node_model train_loss:0.027237384167902574,train_acc:0.9916216135025024
node7 epoch1:node_model train_loss:0.0108273195942889,train_acc:0.996486485004425
node7_model on test-dataset: loss:0.06519465324545308,acc:0.981196939945221
node7 weight score:55786.78340855595
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06090679208864458,train_acc:0.9837499260902405
node14 epoch1:node_model train_loss:0.028568169873324223,train_acc:0.9921875
node14_model on test-dataset: loss:0.061050781733283654,acc:0.9829980134963989
node14 weight score:25224.902225296537
node16: train data size:920
node16 epoch0:node_model train_loss:0.05522764567285776,train_acc:0.9809999465942383
node16 epoch1:node_model train_loss:0.03246098128147423,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.05890512635374762,acc:0.9827969074249268
node16 weight score:15618.335057547472
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04328646519643371,acc:0.9874969446659088
total cost energy:8.361650317030929 | all_enery_cp：7.118 | all_enery_tp: 1.2436503170309292
ef: 32.14208296484194
reward: 23.78043264781101
step 233:loss:33.819889068603516|running q:86.70121765136719
episode3,iteration53 selected nodes:[13, 11, 3, 8, 6],center node:11
################################################## episode3,iteration53 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03421357507192481,train_acc:0.9873683452606201
node3 epoch1:node_model train_loss:0.017634214894650013,train_acc:0.9934210777282715
node3_model on test-dataset: loss:0.05588596452478669,acc:0.9833968281745911
node3 weight score:67315.64950143194
node6: train data size:3529
node6 epoch0:node_model train_loss:0.021073598611918796,train_acc:0.9923756122589111
node6 epoch1:node_model train_loss:0.020732635698853603,train_acc:0.9916666150093079
node6_model on test-dataset: loss:0.07596827185589064,acc:0.9805968999862671
node6 weight score:46453.603771511334
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04887778019917238,train_acc:0.9843477606773376
node8 epoch1:node_model train_loss:0.017337221290369558,train_acc:0.9965216517448425
node8_model on test-dataset: loss:0.0692232104552022,acc:0.9801979660987854
node8 weight score:33081.38968044502
node11: train data size:1575
node11 epoch0:node_model train_loss:0.03644694867398357,train_acc:0.9866665601730347
node11 epoch1:node_model train_loss:0.016355425123038003,train_acc:0.9937499165534973
node11_model on test-dataset: loss:0.07569660436834966,acc:0.9780969023704529
node11 weight score:20806.74573374312
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05681928255679933,train_acc:0.9854544401168823
node13 epoch1:node_model train_loss:0.02086785737827251,train_acc:0.9918181300163269
node13_model on test-dataset: loss:0.0581409211299615,acc:0.9826969504356384
node13 weight score:18162.76693724098
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04142296568710663,acc:0.9873969453573227
total cost energy:7.536308784984488 | all_enery_cp：6.106 | all_enery_tp: 1.4303087849844882
ef: 32.22733247190154
reward: 24.691023686917053
step 234:loss:41.42080307006836|running q:87.9061279296875
episode3,iteration54 selected nodes:[8, 16, 7, 3, 14],center node:8
################################################## episode3,iteration54 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.02663929932475041,train_acc:0.9921051859855652
node3 epoch1:node_model train_loss:0.018850306121748872,train_acc:0.99289470911026
node3_model on test-dataset: loss:0.05916438136327997,acc:0.9841967821121216
node3 weight score:63585.55457380077
node7: train data size:3637
node7 epoch0:node_model train_loss:0.025926108890515123,train_acc:0.991351306438446
node7 epoch1:node_model train_loss:0.01827107836819581,train_acc:0.9930533766746521
node7_model on test-dataset: loss:0.0617169200729586,acc:0.9828968644142151
node7 weight score:58930.35484759324
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03987864774915025,train_acc:0.988695502281189
node8 epoch1:node_model train_loss:0.019668978385870225,train_acc:0.9934782385826111
node8_model on test-dataset: loss:0.06220866379572726,acc:0.9826988577842712
node8 weight score:36811.59279549236
node14: train data size:1540
node14 epoch0:node_model train_loss:0.038063742067606654,train_acc:0.9874999523162842
node14 epoch1:node_model train_loss:0.027120249256768147,train_acc:0.9912499785423279
node14_model on test-dataset: loss:0.05854750431327375,acc:0.9824958443641663
node14 weight score:26303.42690202177
node16: train data size:920
node16 epoch0:node_model train_loss:0.05783344185911119,train_acc:0.9849998354911804
node16 epoch1:node_model train_loss:0.01847742940299213,train_acc:0.9959999322891235
node16_model on test-dataset: loss:0.06460426686680876,acc:0.9822959899902344
node16 weight score:14240.545471968839
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04025732100355526,acc:0.9876989692449569
total cost energy:7.668724883296235 | all_enery_cp：6.0745000000000005 | all_enery_tp: 1.5942248832962345
ef: 32.36669797164899
reward: 24.697973088352754
step 235:loss:32.68461608886719|running q:89.26710510253906
episode3,iteration55 selected nodes:[8, 1, 9, 19, 12],center node:8
################################################## episode3,iteration55 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03711467735476291,train_acc:0.9879105687141418
node1 epoch1:node_model train_loss:0.022207471543797916,train_acc:0.9932836294174194
node1_model on test-dataset: loss:0.056640304288630434,acc:0.9833970665931702
node1 weight score:117531.14824519539
node8: train data size:2290
node8 epoch0:node_model train_loss:0.022713467600228993,train_acc:0.9929468035697937
node8 epoch1:node_model train_loss:0.02140862994786838,train_acc:0.9925119876861572
node8_model on test-dataset: loss:0.07832467510540937,acc:0.9781001806259155
node8 weight score:29237.274165748113
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04643766187787564,train_acc:0.9859089851379395
node9 epoch1:node_model train_loss:0.024793052405584604,train_acc:0.991363525390625
node9_model on test-dataset: loss:0.1097432215491699,acc:0.9655959010124207
node9 weight score:19363.382721983464
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03815900690387934,train_acc:0.9879999756813049
node12 epoch1:node_model train_loss:0.016452433679175253,train_acc:0.9946666359901428
node12_model on test-dataset: loss:0.07532164109593395,acc:0.9805989861488342
node12 weight score:18666.613997552682
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03753441435501686,train_acc:0.9892701506614685
node19 epoch1:node_model train_loss:0.028170753275405552,train_acc:0.9920690655708313
node19_model on test-dataset: loss:0.06345525711891241,acc:0.9807960391044617
node19 weight score:91103.56276969543
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03924889254289155,acc:0.987396948337555
total cost energy:10.761955532033674 | all_enery_cp：9.129499999999998 | all_enery_tp: 1.632455532033676
ef: 32.080728640466035
reward: 21.31877310843236
step 236:loss:30.526212692260742|running q:90.53520202636719
episode3,iteration56 selected nodes:[1, 17, 7, 15, 10],center node:7
################################################## episode3,iteration56 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03186083170989313,train_acc:0.9891412854194641
node1 epoch1:node_model train_loss:0.017727647864159,train_acc:0.993804931640625
node1_model on test-dataset: loss:0.06113031023457552,acc:0.9826947450637817
node1 weight score:108898.5149012834
node7: train data size:3637
node7 epoch0:node_model train_loss:0.018025348030622244,train_acc:0.9941344857215881
node7 epoch1:node_model train_loss:0.015928555703155597,train_acc:0.9944046139717102
node7_model on test-dataset: loss:0.08684364013621235,acc:0.9771931171417236
node7 weight score:41879.865863469626
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04906730740331113,train_acc:0.9821664690971375
node10 epoch1:node_model train_loss:0.03320322931976989,train_acc:0.9939999580383301
node10_model on test-dataset: loss:0.05671138167479512,acc:0.9824000597000122
node10 weight score:33767.47212722389
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05945262432630573,train_acc:0.9812405705451965
node15 epoch1:node_model train_loss:0.03138059817553897,train_acc:0.9904887080192566
node15_model on test-dataset: loss:0.058882451830922944,acc:0.9819990396499634
node15 weight score:23368.592122337108
node17: train data size:719
node17 epoch0:node_model train_loss:0.047631579102016985,train_acc:0.9837499260902405
node17 epoch1:node_model train_loss:0.02266036521177739,train_acc:0.9924999475479126
node17_model on test-dataset: loss:0.054576057796184616,acc:0.9834960103034973
node17 weight score:13174.275113184613
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04539696430450931,acc:0.9871979576349258
total cost energy:8.662654957016754 | all_enery_cp：7.151999999999999 | all_enery_tp: 1.510654957016754
ef: 32.12126627648748
reward: 23.458611319470727
step 237:loss:34.38904571533203|running q:91.79643249511719
episode3,iteration57 selected nodes:[0, 8, 16, 13, 2],center node:2
################################################## episode3,iteration57 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03712047571025323,train_acc:0.9877779483795166
node0 epoch1:node_model train_loss:0.01805653455610607,train_acc:0.994166910648346
node0_model on test-dataset: loss:0.059799259914107096,acc:0.9836958050727844
node0 weight score:119817.53637572564
node2: train data size:4610
node2 epoch0:node_model train_loss:0.02805999413311006,train_acc:0.9891490340232849
node2 epoch1:node_model train_loss:0.019498756410494606,train_acc:0.9942553043365479
node2_model on test-dataset: loss:0.06109677929976897,acc:0.9840960502624512
node2 weight score:75454.05916376074
node8: train data size:2290
node8 epoch0:node_model train_loss:0.02880911383798103,train_acc:0.9890820384025574
node8 epoch1:node_model train_loss:0.01265623908408958,train_acc:0.9943478107452393
node8_model on test-dataset: loss:0.06702412329766957,acc:0.9811971187591553
node8 weight score:34166.80274696892
node13: train data size:1056
node13 epoch0:node_model train_loss:0.03656507401981137,train_acc:0.9845454096794128
node13 epoch1:node_model train_loss:0.03021831539544192,train_acc:0.9918181896209717
node13_model on test-dataset: loss:0.07810174524289323,acc:0.9776960611343384
node13 weight score:13520.8246207032
node16: train data size:920
node16 epoch0:node_model train_loss:0.038820726762060075,train_acc:0.9889999628067017
node16 epoch1:node_model train_loss:0.028761475579813123,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.06919080650744945,acc:0.9808979630470276
node16 weight score:13296.56418878349
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04223442579193943,acc:0.9870979553461074
total cost energy:9.799508315442218 | all_enery_cp：8.0205 | all_enery_tp: 1.779008315442217
ef: 31.97486621262217
reward: 22.175357897179953
step 238:loss:34.531490325927734|running q:92.97112274169922
episode3,iteration58 selected nodes:[1, 11, 4, 2, 0],center node:2
################################################## episode3,iteration58 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.02073835244421692,train_acc:0.9926391243934631
node0 epoch1:node_model train_loss:0.019801477023672003,train_acc:0.9927781224250793
node0_model on test-dataset: loss:0.08085640739464907,acc:0.9778969287872314
node0 weight score:88613.88022137336
node1: train data size:6657
node1 epoch0:node_model train_loss:0.022546718486084312,train_acc:0.9928725361824036
node1 epoch1:node_model train_loss:0.018637913313986204,train_acc:0.993432879447937
node1_model on test-dataset: loss:0.08233317055128282,acc:0.9782960414886475
node1 weight score:80854.41087020397
node2: train data size:4610
node2 epoch0:node_model train_loss:0.020442829325552435,train_acc:0.992340624332428
node2 epoch1:node_model train_loss:0.017137331861820402,train_acc:0.9940425753593445
node2_model on test-dataset: loss:0.05878042829364858,acc:0.9833961129188538
node2 weight score:78427.46529456855
node4: train data size:4298
node4 epoch0:node_model train_loss:0.042087962409633015,train_acc:0.9885951280593872
node4 epoch1:node_model train_loss:0.019099499450909885,train_acc:0.993948757648468
node4_model on test-dataset: loss:0.05701586195951677,acc:0.9826960563659668
node4 weight score:75382.53132175267
node11: train data size:1575
node11 epoch0:node_model train_loss:0.032779065462818835,train_acc:0.9900000095367432
node11 epoch1:node_model train_loss:0.018003639328526333,train_acc:0.9922916293144226
node11_model on test-dataset: loss:0.08674312517585349,acc:0.975695013999939
node11 weight score:18157.058519704216
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04058932017729603,acc:0.9872969454526901
total cost energy:13.61349386061228 | all_enery_cp：12.1525 | all_enery_tp: 1.46099386061228
ef: 32.272983828925895
reward: 18.659489968313615
step 239:loss:43.85707473754883|running q:94.09188079833984
episode3,iteration59 selected nodes:[2, 16, 13, 3, 15],center node:15
################################################## episode3,iteration59 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.01990153936872338,train_acc:0.993617057800293
node2 epoch1:node_model train_loss:0.02148437603473406,train_acc:0.9929787516593933
node2_model on test-dataset: loss:0.08392214687322848,acc:0.9791968464851379
node2 weight score:54931.86449297819
node3: train data size:3762
node3 epoch0:node_model train_loss:0.025731025865656863,train_acc:0.9908913969993591
node3 epoch1:node_model train_loss:0.023051631183510547,train_acc:0.9915790557861328
node3_model on test-dataset: loss:0.057885855688946324,acc:0.9840958714485168
node3 weight score:64989.969574180075
node13: train data size:1056
node13 epoch0:node_model train_loss:0.041029947344213724,train_acc:0.9849349856376648
node13 epoch1:node_model train_loss:0.024231160959144207,train_acc:0.9938311576843262
node13_model on test-dataset: loss:0.06860287695242732,acc:0.980897068977356
node13 weight score:15392.940455431386
node15: train data size:1376
node15 epoch0:node_model train_loss:0.03956079549555268,train_acc:0.98883455991745
node15 epoch1:node_model train_loss:0.0235879953085844,train_acc:0.9924060106277466
node15_model on test-dataset: loss:0.06008484655736538,acc:0.9832990765571594
node15 weight score:22900.94888877312
node16: train data size:920
node16 epoch0:node_model train_loss:0.06014317120425403,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.03315677812788635,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.0549580819842231,acc:0.983497142791748
node16 weight score:16740.031070664107
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03858996238013788,acc:0.9881979560852051
total cost energy:7.501734523939112 | all_enery_cp：5.862 | all_enery_tp: 1.6397345239391126
ef: 32.396854836523
reward: 24.895120312583884
step 240:loss:32.13776779174805|running q:95.38648223876953
episode3_cost time: 13395.913061141968
episode4,iteration0 selected nodes:[4, 15, 12, 18, 9],center node:12
################################################## episode4,iteration0 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.9020024350909299,train_acc:0.7262125611305237
node4 epoch1:node_model train_loss:0.2982211709022522,train_acc:0.9062600135803223
node4_model on test-dataset: loss:0.2833945381827652,acc:0.9077919721603394
node4 weight score:15166.13561983385
node9: train data size:2125
node9 epoch0:node_model train_loss:1.3251339142972773,train_acc:0.6059091091156006
node9 epoch1:node_model train_loss:0.45040245896035974,train_acc:0.8554546236991882
node9_model on test-dataset: loss:0.5384198074042797,acc:0.8200886249542236
node9 weight score:3946.7344454592385
node12: train data size:1406
node12 epoch0:node_model train_loss:1.6291104714075724,train_acc:0.4893333613872528
node12 epoch1:node_model train_loss:0.6383695900440216,train_acc:0.8015556335449219
node12_model on test-dataset: loss:1.196147231105715,acc:0.5834444165229797
node12 weight score:1175.4405840995826
node15: train data size:1376
node15 epoch0:node_model train_loss:1.677394381591252,train_acc:0.4880450963973999
node15 epoch1:node_model train_loss:0.6480911735977445,train_acc:0.8065415024757385
node15_model on test-dataset: loss:0.6941621500998736,acc:0.7780866622924805
node15 weight score:1982.2457905577623
node18: train data size:801
node18 epoch0:node_model train_loss:2.1021637651655407,train_acc:0.2988888919353485
node18 epoch1:node_model train_loss:1.39399430486891,train_acc:0.5688889026641846
node18_model on test-dataset: loss:1.4279703132808208,acc:0.4581959843635559
node18 weight score:560.9360310577251
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.3159508741274476,acc:0.9129959386587143
total cost energy:6.659719600745605 | all_enery_cp：5.003 | all_enery_tp: 1.6567196007456046
ef: 25.728380723823282
reward: 19.068661123077675
step 241:loss:49.84029006958008|running q:1.92819344997406
episode4,iteration1 selected nodes:[12, 5, 15, 8, 7],center node:12
################################################## episode4,iteration1 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.3209069381867136,train_acc:0.8927742838859558
node5 epoch1:node_model train_loss:0.20591695203768964,train_acc:0.9324268102645874
node5_model on test-dataset: loss:0.1858086534263566,acc:0.9412970542907715
node5 weight score:26032.156795738774
node7: train data size:3637
node7 epoch0:node_model train_loss:0.366050726054488,train_acc:0.8836743235588074
node7 epoch1:node_model train_loss:0.21218312551846374,train_acc:0.9304309487342834
node7_model on test-dataset: loss:0.18925195549381896,acc:0.9373907446861267
node7 weight score:19217.76707939372
node8: train data size:2290
node8 epoch0:node_model train_loss:0.4365428239107132,train_acc:0.8583574295043945
node8 epoch1:node_model train_loss:0.23375423058219577,train_acc:0.9281158447265625
node8_model on test-dataset: loss:0.2641135470103472,acc:0.920495867729187
node8 weight score:8670.51321646248
node12: train data size:1406
node12 epoch0:node_model train_loss:0.4640379915634791,train_acc:0.8435556292533875
node12 epoch1:node_model train_loss:0.3708104997873306,train_acc:0.8997778296470642
node12_model on test-dataset: loss:0.39533395986072717,acc:0.8626706004142761
node12 weight score:3556.4867751187426
node15: train data size:1376
node15 epoch0:node_model train_loss:0.4543788071189608,train_acc:0.8538721799850464
node15 epoch1:node_model train_loss:0.26308864461524145,train_acc:0.9181579351425171
node15_model on test-dataset: loss:0.26155831586103884,acc:0.9184878468513489
node15 weight score:5260.777106131252
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.169083968331106,acc:0.9478979551792145
total cost energy:7.737098632478746 | all_enery_cp：6.773000000000001 | all_enery_tp: 0.9640986324787456
ef: 28.581813492553206
reward: 20.84471486007446
step 242:loss:26.674602508544922|running q:3.974574089050293
episode4,iteration2 selected nodes:[9, 14, 3, 11, 19],center node:14
################################################## episode4,iteration2 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.24333025985642484,train_acc:0.917877733707428
node3 epoch1:node_model train_loss:0.15254666577828557,train_acc:0.9525720477104187
node3_model on test-dataset: loss:0.3267868291307241,acc:0.8996850252151489
node3 weight score:11512.091873491916
node9: train data size:2125
node9 epoch0:node_model train_loss:0.27887550301172515,train_acc:0.9113636016845703
node9 epoch1:node_model train_loss:0.19377943941138007,train_acc:0.9422726631164551
node9_model on test-dataset: loss:0.1660834695282392,acc:0.9477962255477905
node9 weight score:12794.771243857509
node11: train data size:1575
node11 epoch0:node_model train_loss:0.31572333443909883,train_acc:0.8954167366027832
node11 epoch1:node_model train_loss:0.17718128813430667,train_acc:0.9472916126251221
node11_model on test-dataset: loss:0.1936472616204992,acc:0.9370962381362915
node11 weight score:8133.34506679785
node14: train data size:1540
node14 epoch0:node_model train_loss:0.3478365130722523,train_acc:0.8843749761581421
node14 epoch1:node_model train_loss:0.18278535408899188,train_acc:0.9415624737739563
node14_model on test-dataset: loss:0.19114983770996333,acc:0.9373908042907715
node14 weight score:8056.506970917143
node19: train data size:5781
node19 epoch0:node_model train_loss:0.2262642370729611,train_acc:0.9285098314285278
node19 epoch1:node_model train_loss:0.169866060260041,train_acc:0.9458320736885071
node19_model on test-dataset: loss:0.19186914085934403,acc:0.9386987686157227
node19 weight score:30129.91028212271
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.1377679251995869,acc:0.9567979574203491
total cost energy:8.89605896207429 | all_enery_cp：7.391499999999999 | all_enery_tp: 1.504558962074292
ef: 28.97267843905668
reward: 20.076619476982387
step 243:loss:37.69157028198242|running q:6.049243927001953
episode4,iteration3 selected nodes:[3, 19, 0, 11, 4],center node:4
################################################## episode4,iteration3 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.20029776176023814,train_acc:0.9367735981941223
node0 epoch1:node_model train_loss:0.1339167544307808,train_acc:0.9572650194168091
node0_model on test-dataset: loss:0.15889830619795248,acc:0.951496958732605
node0 weight score:45091.73301743053
node3: train data size:3762
node3 epoch0:node_model train_loss:0.20087523042763533,train_acc:0.9361968040466309
node3 epoch1:node_model train_loss:0.1437016997094217,train_acc:0.9522495269775391
node3_model on test-dataset: loss:0.1453942549508065,acc:0.9518977999687195
node3 weight score:25874.474897738262
node4: train data size:4298
node4 epoch0:node_model train_loss:0.20249683695823648,train_acc:0.94089674949646
node4 epoch1:node_model train_loss:0.13558319444919742,train_acc:0.9562411308288574
node4_model on test-dataset: loss:0.1741887329041492,acc:0.9397950768470764
node4 weight score:24674.38581326072
node11: train data size:1575
node11 epoch0:node_model train_loss:0.22372048953548074,train_acc:0.9295832514762878
node11 epoch1:node_model train_loss:0.13722186675295234,train_acc:0.9579166173934937
node11_model on test-dataset: loss:0.1953275621822104,acc:0.9390851855278015
node11 weight score:8063.3781653956685
node19: train data size:5781
node19 epoch0:node_model train_loss:0.17711472748939333,train_acc:0.9422520399093628
node19 epoch1:node_model train_loss:0.13012139730412384,train_acc:0.9578202962875366
node19_model on test-dataset: loss:0.1677102914606803,acc:0.9459999799728394
node19 weight score:34470.156539888645
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.10035829870263115,acc:0.9693999755382537
total cost energy:13.134305517016042 | all_enery_cp：11.290499999999998 | all_enery_tp: 1.8438055170160443
ef: 29.57898816460735
reward: 16.444682647591307
step 244:loss:57.56337356567383|running q:8.274479866027832
episode4,iteration4 selected nodes:[18, 15, 7, 3, 12],center node:12
################################################## episode4,iteration4 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.1629134014150814,train_acc:0.9460355043411255
node3 epoch1:node_model train_loss:0.10628420663507361,train_acc:0.9652036428451538
node3_model on test-dataset: loss:0.1533989422272134,acc:0.9498990178108215
node3 weight score:24524.289055577407
node7: train data size:3637
node7 epoch0:node_model train_loss:0.15929019853875442,train_acc:0.9488093852996826
node7 epoch1:node_model train_loss:0.10125454706517426,train_acc:0.9673775434494019
node7_model on test-dataset: loss:0.14313024373026564,acc:0.9544948935508728
node7 weight score:25410.42273954388
node12: train data size:1406
node12 epoch0:node_model train_loss:0.17241898632297914,train_acc:0.9486667513847351
node12 epoch1:node_model train_loss:0.13211420128742854,train_acc:0.949555516242981
node12_model on test-dataset: loss:0.3802230966661591,acc:0.8781778812408447
node12 weight score:3697.829017563566
node15: train data size:1376
node15 epoch0:node_model train_loss:0.20139652064868382,train_acc:0.9377443194389343
node15 epoch1:node_model train_loss:0.11094484903982707,train_acc:0.9664661288261414
node15_model on test-dataset: loss:0.1796547293663025,acc:0.9452928900718689
node15 weight score:7659.1359707232605
node18: train data size:801
node18 epoch0:node_model train_loss:0.13986843085634368,train_acc:0.95333331823349
node18 epoch1:node_model train_loss:0.08090458003183205,train_acc:0.98333340883255
node18_model on test-dataset: loss:0.15859702083165758,acc:0.9494959712028503
node18 weight score:5050.536232015477
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.09469252107664942,acc:0.9696979582309723
total cost energy:6.809691259711845 | all_enery_cp：5.4910000000000005 | all_enery_tp: 1.3186912597118445
ef: 29.126283474324175
reward: 22.31659221461233
step 245:loss:26.895822525024414|running q:10.368348121643066
episode4,iteration5 selected nodes:[11, 7, 2, 5, 9],center node:5
################################################## episode4,iteration5 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.16756000353934916,train_acc:0.9446806907653809
node2 epoch1:node_model train_loss:0.11353078496424442,train_acc:0.9627659320831299
node2_model on test-dataset: loss:0.1278243016079068,acc:0.9596999883651733
node2 weight score:36065.12957247278
node5: train data size:4837
node5 epoch0:node_model train_loss:0.15047158935696495,train_acc:0.9494263529777527
node5 epoch1:node_model train_loss:0.11411622520156052,train_acc:0.9626529812812805
node5_model on test-dataset: loss:0.1947132925130427,acc:0.9408979415893555
node5 weight score:24841.652758123833
node7: train data size:3637
node7 epoch0:node_model train_loss:0.1388361328155608,train_acc:0.9569173455238342
node7 epoch1:node_model train_loss:0.09293294833922708,train_acc:0.9713220596313477
node7_model on test-dataset: loss:0.11671480675693602,acc:0.9623979926109314
node7 weight score:31161.427594822828
node9: train data size:2125
node9 epoch0:node_model train_loss:0.15015006878159262,train_acc:0.9522727727890015
node9 epoch1:node_model train_loss:0.09047662449831312,train_acc:0.9699998497962952
node9_model on test-dataset: loss:0.15649092257128358,acc:0.9495818018913269
node9 weight score:13579.062383200124
node11: train data size:1575
node11 epoch0:node_model train_loss:0.184918113052845,train_acc:0.9389582872390747
node11 epoch1:node_model train_loss:0.12519648671150208,train_acc:0.9647916555404663
node11_model on test-dataset: loss:0.24083288454217835,acc:0.9277982711791992
node11 weight score:6539.804574421239
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07622279613511637,acc:0.9762999731302261
total cost energy:9.645663105724555 | all_enery_cp：8.392 | all_enery_tp: 1.253663105724556
ef: 29.753874299275108
reward: 20.108211193550552
step 246:loss:19.506885528564453|running q:12.481173515319824
episode4,iteration6 selected nodes:[11, 14, 12, 10, 1],center node:11
################################################## episode4,iteration6 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.1364144972567238,train_acc:0.9577219486236572
node1 epoch1:node_model train_loss:0.09647456314692747,train_acc:0.9697380661964417
node1_model on test-dataset: loss:0.10483939984551398,acc:0.9668918251991272
node1 weight score:63497.120450989016
node10: train data size:1915
node10 epoch0:node_model train_loss:0.16208958122879266,train_acc:0.9508334398269653
node10 epoch1:node_model train_loss:0.1452325158752501,train_acc:0.9505000114440918
node10_model on test-dataset: loss:0.14267981082055484,acc:0.9555941820144653
node10 weight score:13421.66063290098
node11: train data size:1575
node11 epoch0:node_model train_loss:0.1449334209319204,train_acc:0.9562499523162842
node11 epoch1:node_model train_loss:0.10060509038157761,train_acc:0.9687499403953552
node11_model on test-dataset: loss:0.1591298554162495,acc:0.9516000747680664
node11 weight score:9897.577018970693
node12: train data size:1406
node12 epoch0:node_model train_loss:0.12239881635953982,train_acc:0.9559999704360962
node12 epoch1:node_model train_loss:0.1028800973823915,train_acc:0.966666579246521
node12_model on test-dataset: loss:0.20134896677278447,acc:0.9438960552215576
node12 weight score:6982.90148956475
node14: train data size:1540
node14 epoch0:node_model train_loss:0.15432028868235648,train_acc:0.9531249403953552
node14 epoch1:node_model train_loss:0.11324776860419661,train_acc:0.9684374928474426
node14_model on test-dataset: loss:0.16182875755388523,acc:0.9507918953895569
node14 weight score:9516.231992865765
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07207240064512006,acc:0.977195935845375
total cost energy:7.768712513921045 | all_enery_cp：6.5465 | all_enery_tp: 1.2222125139210447
ef: 29.91781717773186
reward: 22.149104663810814
step 247:loss:30.775514602661133|running q:14.622957229614258
episode4,iteration7 selected nodes:[13, 17, 19, 6, 10],center node:17
################################################## episode4,iteration7 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.13582144869077536,train_acc:0.9605554938316345
node6 epoch1:node_model train_loss:0.07204674101538128,train_acc:0.9758619666099548
node6_model on test-dataset: loss:0.10595487904682159,acc:0.9651973247528076
node6 weight score:33306.630442572925
node10: train data size:1915
node10 epoch0:node_model train_loss:0.12253423649817705,train_acc:0.96399986743927
node10 epoch1:node_model train_loss:0.07186280358582735,train_acc:0.9706667065620422
node10_model on test-dataset: loss:0.1694507160752255,acc:0.9457938075065613
node10 weight score:11301.221053264007
node13: train data size:1056
node13 epoch0:node_model train_loss:0.12801629982211374,train_acc:0.9547402262687683
node13 epoch1:node_model train_loss:0.09012717533517968,train_acc:0.9705844521522522
node13_model on test-dataset: loss:0.11691672401619144,acc:0.9644908905029297
node13 weight score:9032.069696494043
node17: train data size:719
node17 epoch0:node_model train_loss:0.1370739022968337,train_acc:0.9599999785423279
node17 epoch1:node_model train_loss:0.1001692502759397,train_acc:0.9659209847450256
node17_model on test-dataset: loss:0.15937369583640248,acc:0.9477959275245667
node17 weight score:4511.409465825875
node19: train data size:5781
node19 epoch0:node_model train_loss:0.14255161947924003,train_acc:0.9571199417114258
node19 epoch1:node_model train_loss:0.09048183718375091,train_acc:0.9756088852882385
node19_model on test-dataset: loss:0.11105205764361017,acc:0.9634950160980225
node19 weight score:52056.6671403106
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07029161125821702,acc:0.9756969475746154
total cost energy:7.783788226773653 | all_enery_cp：6.5 | all_enery_tp: 1.2837882267736525
ef: 30.213682337641604
reward: 22.429894110867952
step 248:loss:52.008724212646484|running q:16.670204162597656
episode4,iteration8 selected nodes:[11, 3, 9, 13, 5],center node:5
################################################## episode4,iteration8 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.12323396853906543,train_acc:0.9647367596626282
node3 epoch1:node_model train_loss:0.08225124226392884,train_acc:0.9749404788017273
node3_model on test-dataset: loss:0.11769656280986965,acc:0.959294855594635
node3 weight score:31963.55025318149
node5: train data size:4837
node5 epoch0:node_model train_loss:0.11652279812462475,train_acc:0.9626531004905701
node5 epoch1:node_model train_loss:0.07559761895360995,train_acc:0.9783675074577332
node5_model on test-dataset: loss:0.08926431796426186,acc:0.9716991782188416
node5 weight score:54187.38540002688
node9: train data size:2125
node9 epoch0:node_model train_loss:0.10667941558428785,train_acc:0.9695453643798828
node9 epoch1:node_model train_loss:0.07614441529255021,train_acc:0.9749999046325684
node9_model on test-dataset: loss:0.11657240014174022,acc:0.9667959213256836
node9 weight score:18229.014736045712
node11: train data size:1575
node11 epoch0:node_model train_loss:0.13542912621051073,train_acc:0.9604165554046631
node11 epoch1:node_model train_loss:0.0903583902399987,train_acc:0.9683333039283752
node11_model on test-dataset: loss:0.167298711022886,acc:0.9497970342636108
node11 weight score:9414.298474687856
node13: train data size:1056
node13 epoch0:node_model train_loss:0.14817437055436047,train_acc:0.9560389518737793
node13 epoch1:node_model train_loss:0.07604939998550848,train_acc:0.9718180894851685
node13_model on test-dataset: loss:0.11623199170455337,acc:0.9603980183601379
node13 weight score:9085.278368835105
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05802069769590162,acc:0.980798966884613
total cost energy:8.299147436525153 | all_enery_cp：6.6775 | all_enery_tp: 1.6216474365251534
ef: 30.347065367261465
reward: 22.04791793073631
step 249:loss:34.64163589477539|running q:18.77065086364746
episode4,iteration9 selected nodes:[5, 11, 4, 9, 3],center node:4
################################################## episode4,iteration9 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.1112912007068333,train_acc:0.964049220085144
node3 epoch1:node_model train_loss:0.06437219941596452,train_acc:0.9774702191352844
node3_model on test-dataset: loss:0.16819463010440813,acc:0.9511968493461609
node3 weight score:22366.944757182253
node4: train data size:4298
node4 epoch0:node_model train_loss:0.1275508316060485,train_acc:0.965111494064331
node4 epoch1:node_model train_loss:0.07887229936327352,train_acc:0.9767345786094666
node4_model on test-dataset: loss:0.10243142577237449,acc:0.9655978083610535
node4 weight score:41959.779116529295
node5: train data size:4837
node5 epoch0:node_model train_loss:0.10166155233294988,train_acc:0.9679589867591858
node5 epoch1:node_model train_loss:0.07316468684572955,train_acc:0.9737340807914734
node5_model on test-dataset: loss:0.13509632056578993,acc:0.9608960747718811
node5 weight score:35804.08392872885
node9: train data size:2125
node9 epoch0:node_model train_loss:0.12848994279788298,train_acc:0.9618180394172668
node9 epoch1:node_model train_loss:0.06148557085543871,train_acc:0.9790908098220825
node9_model on test-dataset: loss:0.0830692864349112,acc:0.9715980291366577
node9 weight score:25581.05517934165
node11: train data size:1575
node11 epoch0:node_model train_loss:0.1164669746067375,train_acc:0.96916663646698
node11 epoch1:node_model train_loss:0.062191333854570985,train_acc:0.9814582467079163
node11_model on test-dataset: loss:0.11808895654321531,acc:0.9641962051391602
node11 weight score:13337.402972339924
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06349283200863283,acc:0.9789979517459869
total cost energy:9.53701648071345 | all_enery_cp：8.2985 | all_enery_tp: 1.2385164807134503
ef: 30.678137821320373
reward: 21.141121340606922
step 250:loss:24.697635650634766|running q:20.786806106567383
episode4,iteration10 selected nodes:[19, 1, 14, 4, 0],center node:4
################################################## episode4,iteration10 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.11398187859190835,train_acc:0.9649145603179932
node0 epoch1:node_model train_loss:0.08616244613141236,train_acc:0.9716562628746033
node0_model on test-dataset: loss:0.12573204469721533,acc:0.9627978801727295
node0 weight score:56986.268037353315
node1: train data size:6657
node1 epoch0:node_model train_loss:0.1043025643578661,train_acc:0.9677953720092773
node1 epoch1:node_model train_loss:0.07291279780442145,train_acc:0.9747003316879272
node1_model on test-dataset: loss:0.09906446065171622,acc:0.9682879447937012
node1 weight score:67198.66999936746
node4: train data size:4298
node4 epoch0:node_model train_loss:0.1052817615658738,train_acc:0.9679020047187805
node4 epoch1:node_model train_loss:0.06919592198764168,train_acc:0.9774371385574341
node4_model on test-dataset: loss:0.10902995776559692,acc:0.9679967761039734
node4 weight score:39420.3582949216
node14: train data size:1540
node14 epoch0:node_model train_loss:0.11153627024032176,train_acc:0.9653124213218689
node14 epoch1:node_model train_loss:0.08836851641535759,train_acc:0.9671874642372131
node14_model on test-dataset: loss:0.10408544663339853,acc:0.9695999026298523
node14 weight score:14795.536261895148
node19: train data size:5781
node19 epoch0:node_model train_loss:0.10216182979337614,train_acc:0.9664707779884338
node19 epoch1:node_model train_loss:0.0714836806412144,train_acc:0.978803813457489
node19_model on test-dataset: loss:0.09750848530966323,acc:0.9695978164672852
node19 weight score:59287.14800195029
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06209034314044402,acc:0.9796979546546936
total cost energy:14.912837195569367 | all_enery_cp：12.720499999999998 | all_enery_tp: 2.192337195569369
ef: 30.891444592481566
reward: 15.9786073969122
step 251:loss:21.0228214263916|running q:22.69017219543457
episode4,iteration11 selected nodes:[16, 1, 5, 2, 13],center node:2
################################################## episode4,iteration11 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.09265129170519536,train_acc:0.9719771146774292
node1 epoch1:node_model train_loss:0.06886972984604871,train_acc:0.9770125150680542
node1_model on test-dataset: loss:0.10615772312739864,acc:0.9672990441322327
node1 weight score:62708.579309025044
node2: train data size:4610
node2 epoch0:node_model train_loss:0.10883172831319748,train_acc:0.9657447338104248
node2 epoch1:node_model train_loss:0.08012838497203081,train_acc:0.9742552042007446
node2_model on test-dataset: loss:0.08339388514432358,acc:0.9714952111244202
node2 weight score:55279.832472390706
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08802583737640965,train_acc:0.9695916771888733
node5 epoch1:node_model train_loss:0.059264040960721215,train_acc:0.982448935508728
node5_model on test-dataset: loss:0.10517907196772285,acc:0.9669950604438782
node5 weight score:45988.23615295227
node13: train data size:1056
node13 epoch0:node_model train_loss:0.12731216509233823,train_acc:0.9576623439788818
node13 epoch1:node_model train_loss:0.09460606324401769,train_acc:0.9701947569847107
node13_model on test-dataset: loss:0.21073381676833378,acc:0.9372938275337219
node13 weight score:5011.0609497520445
node16: train data size:920
node16 epoch0:node_model train_loss:0.1099288858473301,train_acc:0.9660000205039978
node16 epoch1:node_model train_loss:0.09542069053277373,train_acc:0.9729999899864197
node16_model on test-dataset: loss:0.1559631482558325,acc:0.9536920189857483
node16 weight score:5898.8293727623895
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.058539043696946465,acc:0.9822979545593262
total cost energy:10.46135919318807 | all_enery_cp：9.040000000000001 | all_enery_tp: 1.4213591931880694
ef: 30.51820588869888
reward: 20.05684669551081
step 252:loss:30.00520133972168|running q:24.585317611694336
episode4,iteration12 selected nodes:[18, 8, 7, 15, 13],center node:15
################################################## episode4,iteration12 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.09673347641286012,train_acc:0.9689189195632935
node7 epoch1:node_model train_loss:0.06846227870649986,train_acc:0.9764864444732666
node7_model on test-dataset: loss:0.0802104257469182,acc:0.9755980372428894
node7 weight score:45343.232704879876
node8: train data size:2290
node8 epoch0:node_model train_loss:0.10488820999212888,train_acc:0.9693717956542969
node8 epoch1:node_model train_loss:0.07170213062478148,train_acc:0.9802896976470947
node8_model on test-dataset: loss:0.0968770630416111,acc:0.9702979922294617
node8 weight score:23638.206280225364
node13: train data size:1056
node13 epoch0:node_model train_loss:0.11008402112532746,train_acc:0.9663636684417725
node13 epoch1:node_model train_loss:0.0563563515686176,train_acc:0.9803895950317383
node13_model on test-dataset: loss:0.09781353047583252,acc:0.9684962034225464
node13 weight score:10796.052395439436
node15: train data size:1376
node15 epoch0:node_model train_loss:0.10397150101406234,train_acc:0.9695488810539246
node15 epoch1:node_model train_loss:0.06465993222913571,train_acc:0.9812029600143433
node15_model on test-dataset: loss:0.08568436309364187,acc:0.9729980230331421
node15 weight score:16058.939464791387
node18: train data size:801
node18 epoch0:node_model train_loss:0.056058446941379875,train_acc:0.9833332896232605
node18 epoch1:node_model train_loss:0.15072700629631677,train_acc:0.8744443655014038
node18_model on test-dataset: loss:0.25691986097794145,acc:0.9245665669441223
node18 weight score:3117.7036954288715
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05538093154435046,acc:0.9821979510784149
total cost energy:5.692241749487247 | all_enery_cp：4.58 | all_enery_tp: 1.1122417494872465
ef: 30.913646753542682
reward: 25.221405004055434
step 253:loss:49.31254959106445|running q:26.664548873901367
episode4,iteration13 selected nodes:[9, 16, 15, 12, 14],center node:12
################################################## episode4,iteration13 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:0.09531402524391358,train_acc:0.9713635444641113
node9 epoch1:node_model train_loss:0.05863949114626104,train_acc:0.9818180799484253
node9_model on test-dataset: loss:0.097192444721004,acc:0.9701938629150391
node9 weight score:21863.839376609198
node12: train data size:1406
node12 epoch0:node_model train_loss:0.09266271036273489,train_acc:0.971333384513855
node12 epoch1:node_model train_loss:0.06127701630660643,train_acc:0.9813332557678223
node12_model on test-dataset: loss:0.08604210667544976,acc:0.9731960892677307
node12 weight score:16340.836531390642
node14: train data size:1540
node14 epoch0:node_model train_loss:0.10965477605350316,train_acc:0.9618749022483826
node14 epoch1:node_model train_loss:0.06402019050437957,train_acc:0.9803124070167542
node14_model on test-dataset: loss:0.07749127915478311,acc:0.9738960862159729
node14 weight score:19873.20401466033
node15: train data size:1376
node15 epoch0:node_model train_loss:0.08034997413467083,train_acc:0.9769173860549927
node15 epoch1:node_model train_loss:0.06598124267267329,train_acc:0.9781203269958496
node15_model on test-dataset: loss:0.095454203480258,acc:0.9709956645965576
node15 weight score:14415.289739278864
node16: train data size:920
node16 epoch0:node_model train_loss:0.07642972227185965,train_acc:0.9739999771118164
node16 epoch1:node_model train_loss:0.0446227946667932,train_acc:0.9869999885559082
node16_model on test-dataset: loss:0.1177680570515804,acc:0.9631959199905396
node16 weight score:7811.965511132239
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.054013406810263405,acc:0.9819969451427459
total cost energy:5.158430105346567 | all_enery_cp：3.6835 | all_enery_tp: 1.474930105346567
ef: 30.953062932753678
reward: 25.79463282740711
step 254:loss:78.20083618164062|running q:28.686792373657227
episode4,iteration14 selected nodes:[7, 4, 1, 17, 19],center node:17
################################################## episode4,iteration14 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.07730374643718128,train_acc:0.9761195182800293
node1 epoch1:node_model train_loss:0.050784907310700685,train_acc:0.9846637845039368
node1_model on test-dataset: loss:0.10794910388918652,acc:0.9680930376052856
node1 weight score:61667.95054485714
node4: train data size:4298
node4 epoch0:node_model train_loss:0.08448447772236757,train_acc:0.9774370193481445
node4 epoch1:node_model train_loss:0.060813869847807775,train_acc:0.9804555177688599
node4_model on test-dataset: loss:0.11799388131068554,acc:0.9644991755485535
node4 weight score:36425.61760201012
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07767072798231163,train_acc:0.9744047522544861
node7 epoch1:node_model train_loss:0.05213956784054234,train_acc:0.9835938215255737
node7_model on test-dataset: loss:0.09438385594607097,acc:0.971497118473053
node7 weight score:38534.13238465388
node17: train data size:719
node17 epoch0:node_model train_loss:0.14533829712308943,train_acc:0.957763135433197
node17 epoch1:node_model train_loss:0.06066626636311412,train_acc:0.9837499260902405
node17_model on test-dataset: loss:0.09805750939587597,acc:0.9689950942993164
node17 weight score:7332.431798744413
node19: train data size:5781
node19 epoch0:node_model train_loss:0.09251850339230792,train_acc:0.972200870513916
node19 epoch1:node_model train_loss:0.06519408488710379,train_acc:0.980304479598999
node19_model on test-dataset: loss:0.11021599317526125,acc:0.9659937620162964
node19 weight score:52451.55293213458
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05475941398122813,acc:0.9824959361553192
total cost energy:12.482054156667099 | all_enery_cp：10.546 | all_enery_tp: 1.9360541566671
ef: 31.040629974217083
reward: 18.558575817549986
step 255:loss:29.33030891418457|running q:30.487274169921875
episode4,iteration15 selected nodes:[2, 17, 10, 16, 13],center node:16
################################################## episode4,iteration15 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.10406748191552594,train_acc:0.9678722620010376
node2 epoch1:node_model train_loss:0.052716848450376,train_acc:0.9844680428504944
node2_model on test-dataset: loss:0.08023101851940737,acc:0.9751917719841003
node2 weight score:57459.07362356208
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06447711950168014,train_acc:0.9809998869895935
node10 epoch1:node_model train_loss:0.060905977711081506,train_acc:0.9791664481163025
node10_model on test-dataset: loss:0.08813312454680272,acc:0.9743990898132324
node10 weight score:21728.49322938786
node13: train data size:1056
node13 epoch0:node_model train_loss:0.09697615693915974,train_acc:0.9678570628166199
node13 epoch1:node_model train_loss:0.0490349412641742,train_acc:0.9856492877006531
node13_model on test-dataset: loss:0.0750114664134162,acc:0.9784957766532898
node13 weight score:14077.847701043327
node16: train data size:920
node16 epoch0:node_model train_loss:0.14450034201145173,train_acc:0.9569999575614929
node16 epoch1:node_model train_loss:0.07420418551191688,train_acc:0.9710000157356262
node16_model on test-dataset: loss:0.08692548350605649,acc:0.9699920415878296
node16 weight score:10583.777770254213
node17: train data size:719
node17 epoch0:node_model train_loss:0.08553864015266299,train_acc:0.9709210991859436
node17 epoch1:node_model train_loss:0.0494948117993772,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.18480681104352698,acc:0.94499671459198
node17 weight score:3890.5492494573487
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.055250847923016405,acc:0.9829969465732574
total cost energy:6.116067554326878 | all_enery_cp：4.610000000000001 | all_enery_tp: 1.5060675543268764
ef: 31.10604456399028
reward: 24.989977009663402
step 256:loss:65.29622650146484|running q:32.55879592895508
episode4,iteration16 selected nodes:[13, 5, 14, 0, 15],center node:5
################################################## episode4,iteration16 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.08705263850667204,train_acc:0.9726392030715942
node0 epoch1:node_model train_loss:0.06199104554899451,train_acc:0.9782481789588928
node0_model on test-dataset: loss:0.11094954870655784,acc:0.9650906920433044
node0 weight score:64578.90170378405
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07032763667176573,train_acc:0.9775289297103882
node5 epoch1:node_model train_loss:0.06652243703375665,train_acc:0.9782845377922058
node5_model on test-dataset: loss:0.13755501587025265,acc:0.9595969319343567
node5 weight score:35164.112114693446
node13: train data size:1056
node13 epoch0:node_model train_loss:0.10855877653441647,train_acc:0.9673376083374023
node13 epoch1:node_model train_loss:0.05025956618853591,train_acc:0.981493353843689
node13_model on test-dataset: loss:0.08546896091545932,acc:0.9709991812705994
node13 weight score:12355.362563077499
node14: train data size:1540
node14 epoch0:node_model train_loss:0.10856102709658444,train_acc:0.9668750166893005
node14 epoch1:node_model train_loss:0.05014028665027581,train_acc:0.9818748831748962
node14_model on test-dataset: loss:0.07305303676825133,acc:0.977400004863739
node14 weight score:21080.57471841171
node15: train data size:1376
node15 epoch0:node_model train_loss:0.09360573693577733,train_acc:0.9721428155899048
node15 epoch1:node_model train_loss:0.050978731364011765,train_acc:0.9840601086616516
node15_model on test-dataset: loss:0.08208021559286863,acc:0.97409588098526
node15 weight score:16764.08852073667
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05188567091332516,acc:0.9843989676237106
total cost energy:9.690272384154145 | all_enery_cp：7.986999999999999 | all_enery_tp: 1.7032723841541455
ef: 30.742847403973446
reward: 21.0525750198193
step 257:loss:45.10990524291992|running q:34.42660140991211
episode4,iteration17 selected nodes:[15, 12, 16, 5, 4],center node:12
################################################## episode4,iteration17 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.0777866751128851,train_acc:0.9767345786094666
node4 epoch1:node_model train_loss:0.05358490289383849,train_acc:0.9837064743041992
node4_model on test-dataset: loss:0.0755450342898257,acc:0.9746989011764526
node4 weight score:56893.2166144883
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07480843557158903,train_acc:0.9766518473625183
node5 epoch1:node_model train_loss:0.05768058013779168,train_acc:0.9816325902938843
node5_model on test-dataset: loss:0.1026162600691896,acc:0.9682930707931519
node5 weight score:47136.77926615748
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06573193977431704,train_acc:0.9793332815170288
node12 epoch1:node_model train_loss:0.04810639433562756,train_acc:0.9768888354301453
node12_model on test-dataset: loss:0.09761481975991046,acc:0.9715960621833801
node12 weight score:14403.550643827872
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05757846465400819,train_acc:0.9833458065986633
node15 epoch1:node_model train_loss:0.04684718679969332,train_acc:0.9869171977043152
node15_model on test-dataset: loss:0.09444951297769875,acc:0.9737969636917114
node15 weight score:14568.629912627486
node16: train data size:920
node16 epoch0:node_model train_loss:0.05750563745386898,train_acc:0.9860000014305115
node16 epoch1:node_model train_loss:0.036746603716164825,train_acc:0.9859998822212219
node16_model on test-dataset: loss:0.10694961809320375,acc:0.9711951017379761
node16 weight score:8602.18125508634
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04637608450313564,acc:0.984996943473816
total cost energy:7.698826398495584 | all_enery_cp：6.4185 | all_enery_tp: 1.2803263984955835
ef: 31.240241962296967
reward: 23.541415563801383
step 258:loss:55.80921936035156|running q:36.33174133300781
episode4,iteration18 selected nodes:[17, 15, 7, 14, 6],center node:6
################################################## episode4,iteration18 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0850481626112014,train_acc:0.9715420603752136
node6 epoch1:node_model train_loss:0.03892539581283927,train_acc:0.9880554676055908
node6_model on test-dataset: loss:0.061145233114948495,acc:0.9802978038787842
node6 weight score:57715.046950033575
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07689537533332368,train_acc:0.9746748805046082
node7 epoch1:node_model train_loss:0.05060161069992024,train_acc:0.9832431674003601
node7_model on test-dataset: loss:0.09733022708707723,acc:0.9705990552902222
node7 weight score:37367.630887639156
node14: train data size:1540
node14 epoch0:node_model train_loss:0.1051375730894506,train_acc:0.9693748354911804
node14 epoch1:node_model train_loss:0.06916282954625785,train_acc:0.9765623807907104
node14_model on test-dataset: loss:0.0908247017802205,acc:0.9722959399223328
node14 weight score:16955.73968111147
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07706044414745909,train_acc:0.9769172668457031
node15 epoch1:node_model train_loss:0.04997834835999778,train_acc:0.9840600490570068
node15_model on test-dataset: loss:0.09924484462288091,acc:0.9688949584960938
node15 weight score:13864.700027781222
node17: train data size:719
node17 epoch0:node_model train_loss:0.0930512729100883,train_acc:0.9724999070167542
node17 epoch1:node_model train_loss:0.042337375780334696,train_acc:0.9924999475479126
node17_model on test-dataset: loss:0.08985984241764527,acc:0.9723979830741882
node17 weight score:8001.349442148743
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.047876629493257494,acc:0.9857989662885666
total cost energy:6.812133518063134 | all_enery_cp：5.4005 | all_enery_tp: 1.411633518063134
ef: 31.252570496899533
reward: 24.4404369788364
step 259:loss:52.22534942626953|running q:38.25170135498047
episode4,iteration19 selected nodes:[10, 4, 2, 16, 3],center node:10
################################################## episode4,iteration19 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07625843967964992,train_acc:0.9785105586051941
node2 epoch1:node_model train_loss:0.04999717316055234,train_acc:0.9836170673370361
node2_model on test-dataset: loss:0.0719975020000129,acc:0.978193998336792
node2 weight score:64029.999263018515
node3: train data size:3762
node3 epoch0:node_model train_loss:0.07736914930865169,train_acc:0.9737861156463623
node3 epoch1:node_model train_loss:0.04576766500739675,train_acc:0.9860524535179138
node3_model on test-dataset: loss:0.07229047317880032,acc:0.9752960205078125
node3 weight score:52040.05223060613
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07128034175785128,train_acc:0.9785951375961304
node4 epoch1:node_model train_loss:0.04533164678574648,train_acc:0.9865116477012634
node4_model on test-dataset: loss:0.11533151048526634,acc:0.964482843875885
node4 weight score:37266.48495208143
node10: train data size:1915
node10 epoch0:node_model train_loss:0.09048523176461458,train_acc:0.971500039100647
node10 epoch1:node_model train_loss:0.04647726705297828,train_acc:0.9854999780654907
node10_model on test-dataset: loss:0.07001155601465144,acc:0.9764960408210754
node10 weight score:27352.62732339851
node16: train data size:920
node16 epoch0:node_model train_loss:0.07323423624038697,train_acc:0.9729999899864197
node16 epoch1:node_model train_loss:0.05318780350498855,train_acc:0.9839999079704285
node16_model on test-dataset: loss:0.08764245722995838,acc:0.9706980586051941
node16 weight score:10497.195412790423
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04849822727468563,acc:0.9839979547262192
total cost energy:9.547221147617606 | all_enery_cp：7.7525 | all_enery_tp: 1.7947211476176061
ef: 31.611406653878827
reward: 22.06418550626122
step 260:loss:41.440189361572266|running q:40.05946731567383
episode4,iteration20 selected nodes:[5, 12, 0, 6, 17],center node:5
################################################## episode4,iteration20 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07288857710470135,train_acc:0.9765920639038086
node0 epoch1:node_model train_loss:0.05876615036848105,train_acc:0.9822225570678711
node0_model on test-dataset: loss:0.09154927991854493,acc:0.9724968671798706
node0 weight score:78263.8597089457
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06707419434144181,train_acc:0.9774682521820068
node5 epoch1:node_model train_loss:0.05040695061146909,train_acc:0.9848979115486145
node5_model on test-dataset: loss:0.08955293577630073,acc:0.9742972254753113
node5 weight score:54012.74629435502
node6: train data size:3529
node6 epoch0:node_model train_loss:0.062107330550336175,train_acc:0.9783332943916321
node6 epoch1:node_model train_loss:0.049458719464989066,train_acc:0.9841665625572205
node6_model on test-dataset: loss:0.0890536823234288,acc:0.9723939299583435
node6 weight score:39627.78301725058
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06354679564634959,train_acc:0.9766666889190674
node12 epoch1:node_model train_loss:0.06098372121341526,train_acc:0.9826666116714478
node12_model on test-dataset: loss:0.1445029490828165,acc:0.9583747386932373
node12 weight score:9729.905229783257
node17: train data size:719
node17 epoch0:node_model train_loss:0.07373222141177393,train_acc:0.9800000190734863
node17 epoch1:node_model train_loss:0.04754931849311106,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.07727830262039788,acc:0.975395917892456
node17 weight score:9304.034581761342
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.047246038203011265,acc:0.9835969400405884
total cost energy:10.039381041053224 | all_enery_cp：8.828000000000001 | all_enery_tp: 1.211381041053223
ef: 31.094788100723438
reward: 21.055407059670216
step 261:loss:44.63858413696289|running q:41.858673095703125
episode4,iteration21 selected nodes:[15, 17, 0, 9, 5],center node:5
################################################## episode4,iteration21 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.06316331363308968,train_acc:0.9805557727813721
node0 epoch1:node_model train_loss:0.04165747025722845,train_acc:0.9861754179000854
node0_model on test-dataset: loss:0.08355919880938018,acc:0.9749971032142639
node0 weight score:85747.59095459005
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05335518165149403,train_acc:0.9814286231994629
node5 epoch1:node_model train_loss:0.0433429220064106,train_acc:0.9857142567634583
node5_model on test-dataset: loss:0.07247936619154643,acc:0.9772961139678955
node5 weight score:66736.23479566463
node9: train data size:2125
node9 epoch0:node_model train_loss:0.07772442846643654,train_acc:0.9799998998641968
node9 epoch1:node_model train_loss:0.04168120835145766,train_acc:0.9872726202011108
node9_model on test-dataset: loss:0.10669477264978923,acc:0.9703898429870605
node9 weight score:19916.627096390348
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06565524830615946,train_acc:0.9764285087585449
node15 epoch1:node_model train_loss:0.037517011464972584,train_acc:0.98906010389328
node15_model on test-dataset: loss:0.09431598915085487,acc:0.9698960781097412
node15 weight score:14589.254827186724
node17: train data size:719
node17 epoch0:node_model train_loss:0.07087766041513532,train_acc:0.98499995470047
node17 epoch1:node_model train_loss:0.039410481447703205,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.08679767441237346,acc:0.9729949235916138
node17 weight score:8283.632077330207
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05270034368120832,acc:0.9840969449281692
total cost energy:9.711015992790491 | all_enery_cp：8.111 | all_enery_tp: 1.6000159927904904
ef: 31.172139620956564
reward: 21.46112362816607
step 262:loss:55.510498046875|running q:43.80171203613281
episode4,iteration22 selected nodes:[15, 6, 2, 9, 3],center node:6
################################################## episode4,iteration22 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.07010993914322015,train_acc:0.9778721332550049
node2 epoch1:node_model train_loss:0.046795003882738107,train_acc:0.9829786419868469
node2_model on test-dataset: loss:0.08792664980617701,acc:0.9721858501434326
node2 weight score:52430.065402948385
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06977128542616572,train_acc:0.9749403595924377
node3 epoch1:node_model train_loss:0.04926483657848286,train_acc:0.9839472770690918
node3_model on test-dataset: loss:0.08615864042949398,acc:0.9727001786231995
node3 weight score:43663.64164112536
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05552369433118858,train_acc:0.9813887476921082
node6 epoch1:node_model train_loss:0.050323083052515157,train_acc:0.9838887453079224
node6_model on test-dataset: loss:0.07214428044164378,acc:0.9775943756103516
node6 weight score:48915.866627217176
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06387559604958039,train_acc:0.9768180251121521
node9 epoch1:node_model train_loss:0.04429225338390097,train_acc:0.9854544997215271
node9_model on test-dataset: loss:0.07723468659038189,acc:0.9767979979515076
node9 weight score:27513.544675464876
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07951173545526606,train_acc:0.9759774804115295
node15 epoch1:node_model train_loss:0.053341975302568505,train_acc:0.9797743558883667
node15_model on test-dataset: loss:0.10922353452202514,acc:0.9672970175743103
node15 weight score:12598.017506222774
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045396537077322136,acc:0.9859959363937378
total cost energy:9.457959551080336 | all_enery_cp：7.7010000000000005 | all_enery_tp: 1.756959551080336
ef: 31.460016948077076
reward: 22.00205739699674
step 263:loss:45.395626068115234|running q:45.576255798339844
episode4,iteration23 selected nodes:[5, 7, 10, 4, 17],center node:10
################################################## episode4,iteration23 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.058838265027504326,train_acc:0.9795063138008118
node4 epoch1:node_model train_loss:0.036419980931853835,train_acc:0.98859041929245
node4_model on test-dataset: loss:0.08117718470690306,acc:0.9741918444633484
node4 weight score:52945.911040377716
node5: train data size:4837
node5 epoch0:node_model train_loss:0.053474062084391406,train_acc:0.9836735129356384
node5 epoch1:node_model train_loss:0.03456213730102291,train_acc:0.9888968467712402
node5_model on test-dataset: loss:0.0746767525890209,acc:0.9769949316978455
node5 weight score:64772.50057484615
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06375737275575867,train_acc:0.9794593453407288
node7 epoch1:node_model train_loss:0.0432758488113413,train_acc:0.9859459400177002
node7_model on test-dataset: loss:0.08691779046523151,acc:0.9732950329780579
node7 weight score:41844.13778275757
node10: train data size:1915
node10 epoch0:node_model train_loss:0.07958171698264778,train_acc:0.9741665124893188
node10 epoch1:node_model train_loss:0.037341166427358984,train_acc:0.9879999160766602
node10_model on test-dataset: loss:0.0811593968939269,acc:0.9768970012664795
node10 weight score:23595.542516202437
node17: train data size:719
node17 epoch0:node_model train_loss:0.12482494534924626,train_acc:0.9718420505523682
node17 epoch1:node_model train_loss:0.059024217538535595,train_acc:0.9799999594688416
node17_model on test-dataset: loss:0.08640493362399866,acc:0.9733000993728638
node17 weight score:8321.28409621624
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045812094674183755,acc:0.9842979562282562
total cost energy:9.017218233270954 | all_enery_cp：7.702999999999999 | all_enery_tp: 1.314218233270955
ef: 31.675476384706208
reward: 22.658258151435255
step 264:loss:39.76621627807617|running q:47.29141616821289
episode4,iteration24 selected nodes:[3, 4, 14, 10, 7],center node:10
################################################## episode4,iteration24 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06884861641906594,train_acc:0.977733314037323
node3 epoch1:node_model train_loss:0.04463809033806779,train_acc:0.9855261445045471
node3_model on test-dataset: loss:0.09850913199406933,acc:0.9715959429740906
node3 weight score:38189.35284321142
node4: train data size:4298
node4 epoch0:node_model train_loss:0.0557597310087362,train_acc:0.9825486540794373
node4 epoch1:node_model train_loss:0.024621063913760143,train_acc:0.9934790134429932
node4_model on test-dataset: loss:0.08328501620913813,acc:0.9733908772468567
node4 weight score:51605.921396559905
node7: train data size:3637
node7 epoch0:node_model train_loss:0.052589617908705734,train_acc:0.983593761920929
node7 epoch1:node_model train_loss:0.031475561790520676,train_acc:0.9902702569961548
node7_model on test-dataset: loss:0.10209239521384006,acc:0.9689950346946716
node7 weight score:35624.59272683372
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0589776641339995,train_acc:0.9829999208450317
node10 epoch1:node_model train_loss:0.040278658614261074,train_acc:0.9874998927116394
node10_model on test-dataset: loss:0.09703138323755411,acc:0.9715958833694458
node10 weight score:19735.88272272343
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07043746203999035,train_acc:0.9731249213218689
node14 epoch1:node_model train_loss:0.035314042455866,train_acc:0.989062488079071
node14_model on test-dataset: loss:0.07446250948578381,acc:0.9765990972518921
node14 weight score:20681.548481709615
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04820701154087146,acc:0.9843959391117096
total cost energy:9.031639589508265 | all_enery_cp：7.5760000000000005 | all_enery_tp: 1.4556395895082643
ef: 31.437054940024783
reward: 22.40541535051652
step 265:loss:34.918087005615234|running q:49.05876922607422
episode4,iteration25 selected nodes:[12, 19, 5, 13, 0],center node:12
################################################## episode4,iteration25 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.055126996856415644,train_acc:0.9820835590362549
node0 epoch1:node_model train_loss:0.04242927258989463,train_acc:0.98597252368927
node0_model on test-dataset: loss:0.08292255382606527,acc:0.9740999341011047
node0 weight score:86405.92540153781
node5: train data size:4837
node5 epoch0:node_model train_loss:0.050211053911824614,train_acc:0.9831218719482422
node5 epoch1:node_model train_loss:0.028016665931410934,train_acc:0.9918369650840759
node5_model on test-dataset: loss:0.07197710238484432,acc:0.977695882320404
node5 weight score:67201.92727595118
node12: train data size:1406
node12 epoch0:node_model train_loss:0.06239632264090081,train_acc:0.9826666116714478
node12 epoch1:node_model train_loss:0.050357886916026474,train_acc:0.9866666197776794
node12_model on test-dataset: loss:0.09166043314558919,acc:0.9735967516899109
node12 weight score:15339.224916892706
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07217790436168964,train_acc:0.9747403264045715
node13 epoch1:node_model train_loss:0.03954775610261343,train_acc:0.9918181896209717
node13_model on test-dataset: loss:0.1020627809092548,acc:0.969089925289154
node13 weight score:10346.572870073978
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07002256125019028,train_acc:0.9792698621749878
node19 epoch1:node_model train_loss:0.05073392195506664,train_acc:0.9833673238754272
node19_model on test-dataset: loss:0.06594998975488124,acc:0.9792980551719666
node19 weight score:87657.3297658795
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05103020774644392,acc:0.9825979572534561
total cost energy:11.588437901959148 | all_enery_cp：10.122499999999999 | all_enery_tp: 1.4659379019591492
ef: 31.849635714753564
reward: 20.261197812794414
step 266:loss:61.7541389465332|running q:50.63702392578125
episode4,iteration26 selected nodes:[14, 19, 1, 15, 6],center node:6
################################################## episode4,iteration26 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0626293536303426,train_acc:0.9797748923301697
node1 epoch1:node_model train_loss:0.04530322351909952,train_acc:0.9851480722427368
node1_model on test-dataset: loss:0.11368206743776682,acc:0.9657967686653137
node1 weight score:58558.04833637683
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05567036549715946,train_acc:0.981695294380188
node6 epoch1:node_model train_loss:0.03511166120491301,train_acc:0.9886110424995422
node6_model on test-dataset: loss:0.10228204496379475,acc:0.9702949523925781
node6 weight score:34502.63436998328
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06642411221400835,train_acc:0.9784374833106995
node14 epoch1:node_model train_loss:0.051942842081189156,train_acc:0.9843748807907104
node14_model on test-dataset: loss:0.0701218520963448,acc:0.9789958000183105
node14 weight score:21961.770175210113
node15: train data size:1376
node15 epoch0:node_model train_loss:0.057431926019489765,train_acc:0.9802631139755249
node15 epoch1:node_model train_loss:0.035630995740315745,train_acc:0.98906010389328
node15_model on test-dataset: loss:0.07375813016406027,acc:0.9780968427658081
node15 weight score:18655.5705376392
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0651226212423102,train_acc:0.9806492328643799
node19 epoch1:node_model train_loss:0.045195688043945824,train_acc:0.985132098197937
node19_model on test-dataset: loss:0.10044520466966787,acc:0.9699888229370117
node19 weight score:57553.76793757211
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04425651288984227,acc:0.9852979522943497
total cost energy:11.140822955501367 | all_enery_cp：9.4415 | all_enery_tp: 1.6993229555013682
ef: 31.202426313447045
reward: 20.06160335794568
step 267:loss:58.76837158203125|running q:52.25776290893555
episode4,iteration27 selected nodes:[5, 18, 16, 10, 4],center node:10
################################################## episode4,iteration27 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05312463757040542,train_acc:0.9825534224510193
node4 epoch1:node_model train_loss:0.03801156274519514,train_acc:0.9899858236312866
node4_model on test-dataset: loss:0.05566399673291016,acc:0.9821969270706177
node4 weight score:77213.28420995142
node5: train data size:4837
node5 epoch0:node_model train_loss:0.041653446203373294,train_acc:0.9865913391113281
node5 epoch1:node_model train_loss:0.034888234716478964,train_acc:0.9867348074913025
node5_model on test-dataset: loss:0.0726581571996212,acc:0.977597177028656
node5 weight score:66572.01595012676
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0633038831409067,train_acc:0.9824997782707214
node10 epoch1:node_model train_loss:0.034842729506635806,train_acc:0.9909999966621399
node10_model on test-dataset: loss:0.07313907902509982,acc:0.9794958829879761
node10 weight score:26182.99308011264
node16: train data size:920
node16 epoch0:node_model train_loss:0.10007778154686094,train_acc:0.9689998626708984
node16 epoch1:node_model train_loss:0.038761987909674646,train_acc:0.984000027179718
node16_model on test-dataset: loss:0.0917204197065439,acc:0.9712958335876465
node16 weight score:10030.48179395064
node18: train data size:801
node18 epoch0:node_model train_loss:0.05134357649108602,train_acc:0.9822222590446472
node18 epoch1:node_model train_loss:0.0434742576803223,train_acc:0.9844444990158081
node18_model on test-dataset: loss:0.07452196818483117,acc:0.9773929119110107
node18 weight score:10748.508386323621
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04257847744753235,acc:0.9856969451904297
total cost energy:8.018080357369106 | all_enery_cp：6.3854999999999995 | all_enery_tp: 1.6325803573691067
ef: 31.780398846987115
reward: 23.76231848961801
step 268:loss:54.768253326416016|running q:53.8804817199707
episode4,iteration28 selected nodes:[0, 10, 18, 14, 17],center node:10
################################################## episode4,iteration28 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.051250595980996475,train_acc:0.9842308163642883
node0 epoch1:node_model train_loss:0.037692487705498934,train_acc:0.9881947040557861
node0_model on test-dataset: loss:0.0641985761129763,acc:0.982299268245697
node0 weight score:111606.83669044424
node10: train data size:1915
node10 epoch0:node_model train_loss:0.056263240543194114,train_acc:0.9819999933242798
node10 epoch1:node_model train_loss:0.037507542128150814,train_acc:0.9879999160766602
node10_model on test-dataset: loss:0.08349931417460993,acc:0.9758971333503723
node10 weight score:22934.320107054293
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07299830735428259,train_acc:0.9781249165534973
node14 epoch1:node_model train_loss:0.06072742224205285,train_acc:0.97718745470047
node14_model on test-dataset: loss:0.10292128394765314,acc:0.9706920981407166
node14 weight score:14962.891453853807
node17: train data size:719
node17 epoch0:node_model train_loss:0.07251348084537312,train_acc:0.9759210348129272
node17 epoch1:node_model train_loss:0.060109211364760995,train_acc:0.9809209704399109
node17_model on test-dataset: loss:0.09380497511767316,acc:0.97079998254776
node17 weight score:7664.838662320993
node18: train data size:801
node18 epoch0:node_model train_loss:0.02403225392722182,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.016228734837366372,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.10536799008603338,acc:0.9707886576652527
node18 weight score:7601.929194492372
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051446367828848455,acc:0.9850979548692703
total cost energy:7.614965990938048 | all_enery_cp：6.069999999999999 | all_enery_tp: 1.5449659909380484
ef: 31.163060757770637
reward: 23.54809476683259
step 269:loss:30.33839225769043|running q:55.603553771972656
episode4,iteration29 selected nodes:[7, 13, 3, 17, 19],center node:17
################################################## episode4,iteration29 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06294583159155752,train_acc:0.977733314037323
node3 epoch1:node_model train_loss:0.03778853403453372,train_acc:0.9876314401626587
node3_model on test-dataset: loss:0.08431672661841731,acc:0.9742999076843262
node3 weight score:44617.48161815222
node7: train data size:3637
node7 epoch0:node_model train_loss:0.05564807198085898,train_acc:0.9813513159751892
node7 epoch1:node_model train_loss:0.029844970627989922,train_acc:0.9899999499320984
node7_model on test-dataset: loss:0.08570523988739297,acc:0.975097119808197
node7 weight score:42436.145150268625
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08327176777476614,train_acc:0.9763636589050293
node13 epoch1:node_model train_loss:0.03803589076481082,train_acc:0.9892855882644653
node13_model on test-dataset: loss:0.07096993196129915,acc:0.9768980741500854
node13 weight score:14879.540825484388
node17: train data size:719
node17 epoch0:node_model train_loss:0.05893272959656315,train_acc:0.9824999570846558
node17 epoch1:node_model train_loss:0.0411057845922187,train_acc:0.9900000095367432
node17_model on test-dataset: loss:0.08018996356957359,acc:0.9751979112625122
node17 weight score:8966.209335862693
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06159425640864105,train_acc:0.9809537529945374
node19 epoch1:node_model train_loss:0.044785487730505655,train_acc:0.9861260652542114
node19_model on test-dataset: loss:0.07452160152432043,acc:0.9761000871658325
node19 weight score:77574.82235688866
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04920830609109544,acc:0.9844989651441574
total cost energy:9.220455328423773 | all_enery_cp：7.477499999999999 | all_enery_tp: 1.742955328423773
ef: 31.677799426383256
reward: 22.457344097959485
step 270:loss:60.814453125|running q:57.268001556396484
episode4,iteration30 selected nodes:[7, 4, 14, 13, 9],center node:14
################################################## episode4,iteration30 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04764313160355181,train_acc:0.9844136834144592
node4 epoch1:node_model train_loss:0.03289612864014194,train_acc:0.9888325333595276
node4_model on test-dataset: loss:0.06138069763212115,acc:0.9805960655212402
node4 weight score:70022.01287707119
node7: train data size:3637
node7 epoch0:node_model train_loss:0.045224211122085516,train_acc:0.9841343760490417
node7 epoch1:node_model train_loss:0.03628653353331862,train_acc:0.9859457612037659
node7_model on test-dataset: loss:0.060651640744763424,acc:0.9813988208770752
node7 weight score:59965.40168311957
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04734306676651944,train_acc:0.9836362600326538
node9 epoch1:node_model train_loss:0.02989151389126412,train_acc:0.9899998903274536
node9_model on test-dataset: loss:0.1082222723258019,acc:0.9711907505989075
node9 weight score:19635.51452332023
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05870811594650149,train_acc:0.9774674773216248
node13 epoch1:node_model train_loss:0.046991385858167305,train_acc:0.9883766174316406
node13_model on test-dataset: loss:0.0808868725746288,acc:0.9761981964111328
node13 weight score:13055.270483176375
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06075945415068418,train_acc:0.9768749475479126
node14 epoch1:node_model train_loss:0.04601998580619693,train_acc:0.9865624308586121
node14_model on test-dataset: loss:0.06309191242973611,acc:0.9812991619110107
node14 weight score:24408.83372674841
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042149297206487975,acc:0.9867989635467529
total cost energy:8.26269529033394 | all_enery_cp：6.328000000000001 | all_enery_tp: 1.9346952903339394
ef: 31.933942092767346
reward: 23.671246802433405
step 271:loss:84.14846801757812|running q:58.774200439453125
episode4,iteration31 selected nodes:[11, 9, 16, 8, 7],center node:11
################################################## episode4,iteration31 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04088990993457972,train_acc:0.9865667223930359
node7 epoch1:node_model train_loss:0.03509326381734698,train_acc:0.9891891479492188
node7_model on test-dataset: loss:0.07784696936327236,acc:0.975095808506012
node7 weight score:46719.866293419385
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07541384102533692,train_acc:0.9769564270973206
node8 epoch1:node_model train_loss:0.050358016145132155,train_acc:0.9860868453979492
node8_model on test-dataset: loss:0.053780872692150296,acc:0.9818968772888184
node8 weight score:42580.19413534437
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04069729458371347,train_acc:0.9868180155754089
node9 epoch1:node_model train_loss:0.03296978725120425,train_acc:0.9895453453063965
node9_model on test-dataset: loss:0.06802479937199678,acc:0.980696976184845
node9 weight score:31238.60738462952
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06527242177980952,train_acc:0.978958249092102
node11 epoch1:node_model train_loss:0.04657050548121333,train_acc:0.9847915768623352
node11_model on test-dataset: loss:0.09152955119114267,acc:0.9730961918830872
node11 weight score:17207.557335345187
node16: train data size:920
node16 epoch0:node_model train_loss:0.06893847077153623,train_acc:0.9789999127388
node16 epoch1:node_model train_loss:0.027159707676037214,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.07017900681123138,acc:0.978996992111206
node16 weight score:13109.333428935961
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04229139073901024,acc:0.9859989619255066
total cost energy:6.596177276241436 | all_enery_cp：5.2735 | all_enery_tp: 1.322677276241436
ef: 31.98540425767662
reward: 25.389226981435186
step 272:loss:26.463117599487305|running q:60.29692077636719
episode4,iteration32 selected nodes:[15, 1, 7, 18, 16],center node:16
################################################## episode4,iteration32 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05634021353143365,train_acc:0.9809324145317078
node1 epoch1:node_model train_loss:0.04063432467858127,train_acc:0.9872376918792725
node1_model on test-dataset: loss:0.058008480823737045,acc:0.9824981689453125
node1 weight score:114759.08186990407
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03310764989362577,train_acc:0.9889188408851624
node7 epoch1:node_model train_loss:0.028113732386594387,train_acc:0.9899998307228088
node7_model on test-dataset: loss:0.08700306699105567,acc:0.9764910936355591
node7 weight score:41803.12402520133
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05598256417683193,train_acc:0.9816916584968567
node15 epoch1:node_model train_loss:0.04302472824097744,train_acc:0.9895488619804382
node15_model on test-dataset: loss:0.06429540924291359,acc:0.9797989726066589
node15 weight score:21401.216917390688
node16: train data size:920
node16 epoch0:node_model train_loss:0.06991246901452541,train_acc:0.9819999933242798
node16 epoch1:node_model train_loss:0.029725767066702248,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.0860382784504327,acc:0.974597156047821
node16 weight score:10692.915020725559
node18: train data size:801
node18 epoch0:node_model train_loss:0.036680556293504196,train_acc:0.98333340883255
node18 epoch1:node_model train_loss:0.015891081132293847,train_acc:0.9955554604530334
node18_model on test-dataset: loss:0.11423891353129875,acc:0.9682909846305847
node18 weight score:7011.621305209148
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.048340381060552315,acc:0.985096942782402
total cost energy:7.844183298050514 | all_enery_cp：6.6955 | all_enery_tp: 1.1486832980505142
ef: 32.01074433983986
reward: 24.166561041789347
step 273:loss:27.9249267578125|running q:61.83837890625
episode4,iteration33 selected nodes:[12, 8, 16, 3, 2],center node:12
################################################## episode4,iteration33 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.058494317226428935,train_acc:0.9827659130096436
node2 epoch1:node_model train_loss:0.03614831644982258,train_acc:0.9885106682777405
node2_model on test-dataset: loss:0.06550896934757475,acc:0.9798949956893921
node2 weight score:70372.04288073065
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06063423673377225,train_acc:0.9799999594688416
node3 epoch1:node_model train_loss:0.03518728933321606,train_acc:0.9894142150878906
node3_model on test-dataset: loss:0.06869938241958153,acc:0.9773949384689331
node3 weight score:54760.3175967956
node8: train data size:2290
node8 epoch0:node_model train_loss:0.062419680554581726,train_acc:0.9821255803108215
node8 epoch1:node_model train_loss:0.03376113906826662,train_acc:0.9894684553146362
node8_model on test-dataset: loss:0.057731236980616815,acc:0.9818990230560303
node8 weight score:39666.56735189763
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04837752975290641,train_acc:0.9853333234786987
node12 epoch1:node_model train_loss:0.03594286805722125,train_acc:0.9893332719802856
node12_model on test-dataset: loss:0.10133147970660503,acc:0.9726950526237488
node12 weight score:13875.25381126309
node16: train data size:920
node16 epoch0:node_model train_loss:0.05844624312594533,train_acc:0.9749999046325684
node16 epoch1:node_model train_loss:0.044234902248717844,train_acc:0.9819999933242798
node16_model on test-dataset: loss:0.0906818615715747,acc:0.9745898842811584
node16 weight score:10145.358554134322
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045340310799365395,acc:0.9855969452857971
total cost energy:8.104654957016754 | all_enery_cp：6.494 | all_enery_tp: 1.6106549570167539
ef: 31.90229176539779
reward: 23.797636808381036
step 274:loss:33.56578826904297|running q:63.4883918762207
episode4,iteration34 selected nodes:[15, 14, 2, 12, 16],center node:12
################################################## episode4,iteration34 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.062246805069314196,train_acc:0.9823404550552368
node2 epoch1:node_model train_loss:0.04180095023958587,train_acc:0.9861702919006348
node2_model on test-dataset: loss:0.07469032462220639,acc:0.9768907427787781
node2 weight score:61721.5151134768
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04782986763554315,train_acc:0.9860000014305115
node12 epoch1:node_model train_loss:0.024638537721087535,train_acc:0.9906666874885559
node12_model on test-dataset: loss:0.09117663658849778,acc:0.9737958312034607
node12 weight score:15420.617085774045
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05518508414388634,train_acc:0.9774999618530273
node14 epoch1:node_model train_loss:0.022123634596937336,train_acc:0.9937499761581421
node14_model on test-dataset: loss:0.0626161979175231,acc:0.9802988767623901
node14 weight score:24594.27514312606
node15: train data size:1376
node15 epoch0:node_model train_loss:0.047571396022768955,train_acc:0.9838346242904663
node15 epoch1:node_model train_loss:0.044734839749123366,train_acc:0.9853007197380066
node15_model on test-dataset: loss:0.07436224181044963,acc:0.9767950773239136
node15 weight score:18504.014490410907
node16: train data size:920
node16 epoch0:node_model train_loss:0.06140345702879131,train_acc:0.9769999384880066
node16 epoch1:node_model train_loss:0.04084939267486334,train_acc:0.9859998822212219
node16_model on test-dataset: loss:0.07901996056403732,acc:0.9762941002845764
node16 weight score:11642.627931387506
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04203945658882731,acc:0.9868949246406555
total cost energy:6.200930105346567 | all_enery_cp：4.926 | all_enery_tp: 1.274930105346567
ef: 31.72590492496824
reward: 25.524974819621676
step 275:loss:41.10416793823242|running q:65.09960174560547
episode4,iteration35 selected nodes:[5, 16, 1, 14, 0],center node:5
################################################## episode4,iteration35 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.054558963265865006,train_acc:0.9822863340377808
node0 epoch1:node_model train_loss:0.026035304203914065,train_acc:0.9917309880256653
node0_model on test-dataset: loss:0.06670620094635524,acc:0.9791968464851379
node0 weight score:107411.30357224292
node1: train data size:6657
node1 epoch0:node_model train_loss:0.046570175747845825,train_acc:0.9843285083770752
node1 epoch1:node_model train_loss:0.028678730112820197,train_acc:0.9900002479553223
node1_model on test-dataset: loss:0.06868550823724945,acc:0.9786950349807739
node1 weight score:96920.00788588157
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04887959575850745,train_acc:0.9837341904640198
node5 epoch1:node_model train_loss:0.02878863012361131,train_acc:0.9924489259719849
node5_model on test-dataset: loss:0.06444982336863177,acc:0.9816969037055969
node5 weight score:75050.63237076621
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05133477986964863,train_acc:0.9806248545646667
node14 epoch1:node_model train_loss:0.03301049525907729,train_acc:0.9893749356269836
node14_model on test-dataset: loss:0.06883856545242452,acc:0.9791979193687439
node14 weight score:22371.18089080923
node16: train data size:920
node16 epoch0:node_model train_loss:0.04078600686043501,train_acc:0.9869999289512634
node16 epoch1:node_model train_loss:0.043121706182137134,train_acc:0.9839999079704285
node16_model on test-dataset: loss:0.0856648480248623,acc:0.9751968383789062
node16 weight score:10739.527603352437
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04114444709732197,acc:0.9868979531526566
total cost energy:12.160546962275166 | all_enery_cp：10.5595 | all_enery_tp: 1.6010469622751655
ef: 32.061531002865244
reward: 19.900984040590078
step 276:loss:29.033239364624023|running q:66.7201919555664
episode4,iteration36 selected nodes:[13, 14, 2, 17, 5],center node:5
################################################## episode4,iteration36 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04003447231321417,train_acc:0.9863830208778381
node2 epoch1:node_model train_loss:0.02494039759039879,train_acc:0.9917020797729492
node2_model on test-dataset: loss:0.05633439275159617,acc:0.982396125793457
node2 weight score:81832.7805596055
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04264586038735448,train_acc:0.9847545623779297
node5 epoch1:node_model train_loss:0.025639492113200227,train_acc:0.990408182144165
node5_model on test-dataset: loss:0.05506789762621338,acc:0.9826958775520325
node5 weight score:87837.01954326098
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06537221219729293,train_acc:0.9769479632377625
node13 epoch1:node_model train_loss:0.03363514043898745,train_acc:0.9883764982223511
node13_model on test-dataset: loss:0.0725832172963419,acc:0.9781981706619263
node13 weight score:14548.817747890338
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05182924901600927,train_acc:0.9850000143051147
node14 epoch1:node_model train_loss:0.025180817086948082,train_acc:0.9918749332427979
node14_model on test-dataset: loss:0.06477130498973566,acc:0.9808939695358276
node14 weight score:23775.9606703006
node17: train data size:719
node17 epoch0:node_model train_loss:0.043089089886052534,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.03692680079257116,train_acc:0.9862499237060547
node17_model on test-dataset: loss:0.062244361722914615,acc:0.9824970960617065
node17 weight score:11551.247054322474
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04072503356983361,acc:0.9872969448566437
total cost energy:8.038933824461765 | all_enery_cp：6.381 | all_enery_tp: 1.657933824461764
ef: 32.10824809907828
reward: 24.069314274616517
step 277:loss:33.276729583740234|running q:68.19103240966797
episode4,iteration37 selected nodes:[19, 0, 12, 14, 6],center node:12
################################################## episode4,iteration37 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04514227872196999,train_acc:0.9848507046699524
node0 epoch1:node_model train_loss:0.02130799878068501,train_acc:0.9938890933990479
node0_model on test-dataset: loss:0.07443985435384093,acc:0.9785929918289185
node0 weight score:96252.20336866903
node6: train data size:3529
node6 epoch0:node_model train_loss:0.050616249998307064,train_acc:0.9845976233482361
node6 epoch1:node_model train_loss:0.03704771444447235,train_acc:0.9869730472564697
node6_model on test-dataset: loss:0.05932752466775128,acc:0.9815980195999146
node6 weight score:59483.351442071245
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0378786690455551,train_acc:0.9873332381248474
node12 epoch1:node_model train_loss:0.02648241810966283,train_acc:0.9879999756813049
node12_model on test-dataset: loss:0.06645426628088899,acc:0.9809960126876831
node12 weight score:21157.407623118088
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04979698156239465,train_acc:0.981874942779541
node14 epoch1:node_model train_loss:0.05011044784623664,train_acc:0.9843748807907104
node14_model on test-dataset: loss:0.07068512600664689,acc:0.9802951812744141
node14 weight score:21786.76175600487
node19: train data size:5781
node19 epoch0:node_model train_loss:0.056738704073660336,train_acc:0.9834079742431641
node19 epoch1:node_model train_loss:0.035996738935676245,train_acc:0.9868563413619995
node19_model on test-dataset: loss:0.0641767608076043,acc:0.9799998998641968
node19 weight score:90079.33599719807
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04162155186284508,acc:0.9867959356307984
total cost energy:11.303497140843808 | all_enery_cp：9.7105 | all_enery_tp: 1.5929971408438088
ef: 32.431754418104646
reward: 21.128257277260836
step 278:loss:33.638301849365234|running q:69.6873550415039
episode4,iteration38 selected nodes:[1, 8, 5, 16, 4],center node:5
################################################## episode4,iteration38 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04554251071983086,train_acc:0.9843651652336121
node1 epoch1:node_model train_loss:0.028252802691908916,train_acc:0.9911943078041077
node1_model on test-dataset: loss:0.07439060846868414,acc:0.9779969453811646
node1 weight score:89487.10243178566
node4: train data size:4298
node4 epoch0:node_model train_loss:0.042718627495517913,train_acc:0.9867441058158875
node4 epoch1:node_model train_loss:0.02307374073127501,train_acc:0.9918556809425354
node4_model on test-dataset: loss:0.07246505581890234,acc:0.979195773601532
node4 weight score:59311.34601953728
node5: train data size:4837
node5 epoch0:node_model train_loss:0.033232900323536325,train_acc:0.9893878698348999
node5 epoch1:node_model train_loss:0.02129245227041217,train_acc:0.99367356300354
node5_model on test-dataset: loss:0.06920450513824107,acc:0.9796992540359497
node5 weight score:69894.2935916923
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06531097577965778,train_acc:0.9833815097808838
node8 epoch1:node_model train_loss:0.034574366194884416,train_acc:0.9877293705940247
node8_model on test-dataset: loss:0.08253363032468769,acc:0.9749988913536072
node8 weight score:27746.265261701552
node16: train data size:920
node16 epoch0:node_model train_loss:0.06107415645383298,train_acc:0.9829999804496765
node16 epoch1:node_model train_loss:0.022797515738056974,train_acc:0.9939999580383301
node16_model on test-dataset: loss:0.07454262204075349,acc:0.9776949882507324
node16 weight score:12341.932371214729
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04285857496852259,acc:0.9855969446897507
total cost energy:10.902976483783707 | all_enery_cp：9.501 | all_enery_tp: 1.4019764837837085
ef: 31.950214563880614
reward: 21.047238080096907
step 279:loss:19.679683685302734|running q:71.24327087402344
episode4,iteration39 selected nodes:[0, 8, 4, 11, 5],center node:5
################################################## episode4,iteration39 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03987138668566735,train_acc:0.9879169464111328
node0 epoch1:node_model train_loss:0.020438729762746435,train_acc:0.9933333992958069
node0_model on test-dataset: loss:0.05934382064413512,acc:0.9821979999542236
node0 weight score:120737.08639297239
node4: train data size:4298
node4 epoch0:node_model train_loss:0.035077961085927346,train_acc:0.9881300926208496
node4 epoch1:node_model train_loss:0.02782736649076173,train_acc:0.9906882047653198
node4_model on test-dataset: loss:0.06964390228634329,acc:0.9794971346855164
node4 weight score:61713.945642055296
node5: train data size:4837
node5 epoch0:node_model train_loss:0.0267816268772419,train_acc:0.9920409917831421
node5 epoch1:node_model train_loss:0.02418799746225645,train_acc:0.9918369054794312
node5_model on test-dataset: loss:0.07849220344956848,acc:0.9773948192596436
node5 weight score:61623.95483148578
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05525243067709,train_acc:0.980772852897644
node8 epoch1:node_model train_loss:0.028136485964869676,train_acc:0.9929950833320618
node8_model on test-dataset: loss:0.08043440381650725,acc:0.9758960008621216
node8 weight score:28470.404346181425
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05596363259246573,train_acc:0.982916533946991
node11 epoch1:node_model train_loss:0.03624555066926405,train_acc:0.9887499213218689
node11_model on test-dataset: loss:0.07215584923207644,acc:0.977295994758606
node11 weight score:21827.75224409449
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04021863307018066,acc:0.9881979566812515
total cost energy:11.163755920004126 | all_enery_cp：10.0825 | all_enery_tp: 1.0812559200041265
ef: 32.32025128574949
reward: 21.156495365745364
step 280:loss:15.783416748046875|running q:72.71378326416016
episode4,iteration40 selected nodes:[16, 13, 12, 11, 0],center node:12
################################################## episode4,iteration40 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.023057880269738637,train_acc:0.9928420186042786
node0 epoch1:node_model train_loss:0.02507628403367966,train_acc:0.9914532899856567
node0_model on test-dataset: loss:0.09910877269652701,acc:0.9733930230140686
node0 weight score:72294.30659926917
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06129127866006456,train_acc:0.9797915816307068
node11 epoch1:node_model train_loss:0.025224193042959087,train_acc:0.9904166460037231
node11_model on test-dataset: loss:0.0784030877306941,acc:0.976394772529602
node11 weight score:20088.49454258677
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03786999351965884,train_acc:0.9879999160766602
node12 epoch1:node_model train_loss:0.022565847433482607,train_acc:0.9913333058357239
node12_model on test-dataset: loss:0.06535089830824291,acc:0.9814960360527039
node12 weight score:21514.623920979167
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06052277216010473,train_acc:0.9827271699905396
node13 epoch1:node_model train_loss:0.030752721708267927,train_acc:0.9899999499320984
node13_model on test-dataset: loss:0.0668402359893662,acc:0.9795941114425659
node13 weight score:15798.867020278056
node16: train data size:920
node16 epoch0:node_model train_loss:0.03740594231057912,train_acc:0.9899999499320984
node16 epoch1:node_model train_loss:0.02042861325899139,train_acc:0.9939999580383301
node16_model on test-dataset: loss:0.06728231239343586,acc:0.9822980761528015
node16 weight score:13673.72742215317
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04662038918988401,acc:0.9866969484090805
total cost energy:7.267701987234509 | all_enery_cp：6.061 | all_enery_tp: 1.206701987234509
ef: 31.91623136460245
reward: 24.64852937736794
step 281:loss:28.12677764892578|running q:74.2002182006836
episode4,iteration41 selected nodes:[13, 19, 14, 10, 1],center node:10
################################################## episode4,iteration41 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04016185485942067,train_acc:0.9855226874351501
node1 epoch1:node_model train_loss:0.02597164416011534,train_acc:0.9914928674697876
node1_model on test-dataset: loss:0.06487670234811958,acc:0.9799988865852356
node1 weight score:102610.0242314944
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06384478972759097,train_acc:0.9819998741149902
node10 epoch1:node_model train_loss:0.03026714501756942,train_acc:0.9904999732971191
node10_model on test-dataset: loss:0.0771242451897706,acc:0.9775989055633545
node10 weight score:24830.064725923523
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06265842389654029,train_acc:0.979480504989624
node13 epoch1:node_model train_loss:0.027680421629073946,train_acc:0.9863635897636414
node13_model on test-dataset: loss:0.06673961486740154,acc:0.9800981879234314
node13 weight score:15822.686452387592
node14: train data size:1540
node14 epoch0:node_model train_loss:0.056396941596176475,train_acc:0.97718745470047
node14 epoch1:node_model train_loss:0.04657843030872755,train_acc:0.9846874475479126
node14_model on test-dataset: loss:0.06645321853720816,acc:0.9822980165481567
node14 weight score:23174.197336096382
node19: train data size:5781
node19 epoch0:node_model train_loss:0.052535291815353234,train_acc:0.9844827651977539
node19 epoch1:node_model train_loss:0.04057554582511235,train_acc:0.9889253377914429
node19_model on test-dataset: loss:0.07779343724541832,acc:0.9752971529960632
node19 weight score:74312.18113891059
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03977592400256981,acc:0.9872999727725983
total cost energy:10.305466055946436 | all_enery_cp：8.474499999999999 | all_enery_tp: 1.8309660559464376
ef: 31.981292520051785
reward: 21.67582646410535
step 282:loss:41.57497024536133|running q:75.5923843383789
episode4,iteration42 selected nodes:[7, 18, 0, 6, 15],center node:6
################################################## episode4,iteration42 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.023297808289903008,train_acc:0.992361307144165
node0 epoch1:node_model train_loss:0.02154751735280216,train_acc:0.9919446110725403
node0_model on test-dataset: loss:0.06566088035579014,acc:0.9808928966522217
node0 weight score:109121.29050319949
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04324008081392902,train_acc:0.9874998927116394
node6 epoch1:node_model train_loss:0.031891244114376605,train_acc:0.9875286221504211
node6_model on test-dataset: loss:0.08918526558558369,acc:0.976195216178894
node6 weight score:39569.31648775001
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04258196565599458,train_acc:0.9860262870788574
node7 epoch1:node_model train_loss:0.024204033068255394,train_acc:0.9916214942932129
node7_model on test-dataset: loss:0.07776536068122368,acc:0.9770970940589905
node7 weight score:46768.89514997322
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05925168677432729,train_acc:0.9838345646858215
node15 epoch1:node_model train_loss:0.03615494489037831,train_acc:0.9900000095367432
node15_model on test-dataset: loss:0.058531435004988454,acc:0.9834958910942078
node15 weight score:23508.735090515514
node18: train data size:801
node18 epoch0:node_model train_loss:0.02595583305487202,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.015472801298276155,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.0870448473291981,acc:0.9757919907569885
node18 weight score:9202.152965708226
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04398480681644287,acc:0.9865969425439834
total cost energy:9.541705430228724 | all_enery_cp：8.254 | all_enery_tp: 1.2877054302287245
ef: 31.875998334607583
reward: 22.33429290437886
step 283:loss:20.336503982543945|running q:76.89025115966797
episode4,iteration43 selected nodes:[2, 0, 14, 19, 17],center node:17
################################################## episode4,iteration43 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.02061084023443982,train_acc:0.9938143491744995
node0 epoch1:node_model train_loss:0.01876607852763199,train_acc:0.9933333992958069
node0_model on test-dataset: loss:0.09069524126171018,acc:0.9737977981567383
node0 weight score:79000.83731322437
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04093943565013878,train_acc:0.9870213866233826
node2 epoch1:node_model train_loss:0.03188450230865125,train_acc:0.9897872805595398
node2_model on test-dataset: loss:0.06488157917972785,acc:0.9811939001083374
node2 weight score:71052.52458837173
node14: train data size:1540
node14 epoch0:node_model train_loss:0.056513182214985136,train_acc:0.9781248569488525
node14 epoch1:node_model train_loss:0.032356207695556805,train_acc:0.9881249070167542
node14_model on test-dataset: loss:0.06881644656779827,acc:0.9789960980415344
node14 weight score:22378.371404033267
node17: train data size:719
node17 epoch0:node_model train_loss:0.06786633974843426,train_acc:0.9809210300445557
node17 epoch1:node_model train_loss:0.018372523358266335,train_acc:0.9912499189376831
node17_model on test-dataset: loss:0.06465415286154894,acc:0.9802958369255066
node17 weight score:11120.708696619597
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04796637161554576,train_acc:0.9846148490905762
node19 epoch1:node_model train_loss:0.034514933874718205,train_acc:0.9889250993728638
node19_model on test-dataset: loss:0.07584078992134892,acc:0.9772980213165283
node19 weight score:76225.4718865035
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04617508367362461,acc:0.985798962712288
total cost energy:11.79751916075695 | all_enery_cp：9.9075 | all_enery_tp: 1.8900191607569492
ef: 32.14287480767545
reward: 20.3453556469185
step 284:loss:22.031307220458984|running q:78.26209259033203
episode4,iteration44 selected nodes:[16, 18, 12, 3, 14],center node:12
################################################## episode4,iteration44 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05280395315371846,train_acc:0.9829965233802795
node3 epoch1:node_model train_loss:0.03101341150039317,train_acc:0.9895754456520081
node3_model on test-dataset: loss:0.05928560555799777,acc:0.9812968969345093
node3 weight score:63455.538061759704
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05708291400223971,train_acc:0.9755555391311646
node12 epoch1:node_model train_loss:0.02622985311318189,train_acc:0.9886666536331177
node12_model on test-dataset: loss:0.07127527758180804,acc:0.9804979562759399
node12 weight score:19726.334960761495
node14: train data size:1540
node14 epoch0:node_model train_loss:0.044930559990461916,train_acc:0.9840624332427979
node14 epoch1:node_model train_loss:0.034638308330613654,train_acc:0.9881249666213989
node14_model on test-dataset: loss:0.06623882335166854,acc:0.9797899127006531
node14 weight score:23249.205255715155
node16: train data size:920
node16 epoch0:node_model train_loss:0.04562270157039165,train_acc:0.9829999804496765
node16 epoch1:node_model train_loss:0.05005077454261482,train_acc:0.9869999289512634
node16_model on test-dataset: loss:0.062153893973627417,acc:0.9828981757164001
node16 weight score:14801.968809715547
node18: train data size:801
node18 epoch0:node_model train_loss:0.025335772001802245,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.022793962594328657,train_acc:0.9922221302986145
node18_model on test-dataset: loss:0.0736749876209069,acc:0.979598879814148
node18 weight score:10872.075121634613
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04336659712680557,acc:0.985797957777977
total cost energy:5.901671854833814 | all_enery_cp：4.2145 | all_enery_tp: 1.6871718548338137
ef: 31.88635356838971
reward: 25.984681713555894
step 285:loss:17.493818283081055|running q:79.64701843261719
episode4,iteration45 selected nodes:[1, 6, 8, 15, 4],center node:6
################################################## episode4,iteration45 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0415594205764164,train_acc:0.9857085347175598
node1 epoch1:node_model train_loss:0.021417318565298372,train_acc:0.9934329986572266
node1_model on test-dataset: loss:0.06755112511465995,acc:0.9814980030059814
node1 weight score:98547.58138669842
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03448022486977712,train_acc:0.9876696467399597
node4 epoch1:node_model train_loss:0.02068554636816559,train_acc:0.9934883713722229
node4_model on test-dataset: loss:0.05898762350625475,acc:0.9827949404716492
node4 weight score:72862.74212325677
node6: train data size:3529
node6 epoch0:node_model train_loss:0.042258591484925195,train_acc:0.9868199229240417
node6 epoch1:node_model train_loss:0.030977270022655528,train_acc:0.9898754954338074
node6_model on test-dataset: loss:0.07496998784317839,acc:0.9786999821662903
node6 weight score:47072.16983123878
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05991571456315401,train_acc:0.9799515604972839
node8 epoch1:node_model train_loss:0.026466962921878567,train_acc:0.9920772314071655
node8_model on test-dataset: loss:0.07743442765786313,acc:0.9782969951629639
node8 weight score:29573.409002493747
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05528838946650337,train_acc:0.9812405109405518
node15 epoch1:node_model train_loss:0.04512111811033849,train_acc:0.9864284992218018
node15_model on test-dataset: loss:0.059581533043310625,acc:0.983494758605957
node15 weight score:23094.404083221005
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03954046145241591,acc:0.9868989628553391
total cost energy:10.607455532033676 | all_enery_cp：9.075 | all_enery_tp: 1.5324555320336761
ef: 32.29936846811762
reward: 21.691912936083945
step 286:loss:29.593990325927734|running q:80.91647338867188
episode4,iteration46 selected nodes:[5, 0, 10, 16, 2],center node:5
################################################## episode4,iteration46 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.020738103076130047,train_acc:0.9931946992874146
node0 epoch1:node_model train_loss:0.016587655369625684,train_acc:0.9951390027999878
node0_model on test-dataset: loss:0.068412337633672,acc:0.9809969663619995
node0 weight score:104732.57087583345
node2: train data size:4610
node2 epoch0:node_model train_loss:0.039180748442069015,train_acc:0.9857447743415833
node2 epoch1:node_model train_loss:0.030886844660036585,train_acc:0.9878722429275513
node2_model on test-dataset: loss:0.07106198396329944,acc:0.9809938073158264
node2 weight score:64872.94250581118
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03867038498675374,train_acc:0.987346887588501
node5 epoch1:node_model train_loss:0.023184333405248364,train_acc:0.9920408725738525
node5_model on test-dataset: loss:0.06262052587131621,acc:0.9816970825195312
node5 weight score:77243.04343818394
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04651909649837762,train_acc:0.9869999289512634
node10 epoch1:node_model train_loss:0.024386257646256126,train_acc:0.9944998621940613
node10_model on test-dataset: loss:0.06812781879700197,acc:0.9792001843452454
node10 weight score:28108.928684566537
node16: train data size:920
node16 epoch0:node_model train_loss:0.033681630808860066,train_acc:0.9899999499320984
node16 epoch1:node_model train_loss:0.01597305772593245,train_acc:0.9959999918937683
node16_model on test-dataset: loss:0.07194862752454355,acc:0.9794949889183044
node16 weight score:12786.901316306055
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04357158984959824,acc:0.9865969443321227
total cost energy:11.046177276241435 | all_enery_cp：9.7235 | all_enery_tp: 1.322677276241436
ef: 32.231047188782846
reward: 21.18486991254141
step 287:loss:27.868371963500977|running q:82.34458923339844
episode4,iteration47 selected nodes:[12, 3, 14, 1, 2],center node:2
################################################## episode4,iteration47 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03237044499012461,train_acc:0.9894396662712097
node1 epoch1:node_model train_loss:0.016411967173743464,train_acc:0.9938806295394897
node1_model on test-dataset: loss:0.05895324004194663,acc:0.9827969670295715
node1 weight score:112920.00228084812
node2: train data size:4610
node2 epoch0:node_model train_loss:0.030608717306328817,train_acc:0.988936185836792
node2 epoch1:node_model train_loss:0.019160129289648078,train_acc:0.9929788112640381
node2_model on test-dataset: loss:0.0924857182814958,acc:0.975695013999939
node2 weight score:49845.53383657238
node3: train data size:3762
node3 epoch0:node_model train_loss:0.043159020361551816,train_acc:0.9861543774604797
node3 epoch1:node_model train_loss:0.023417993267924573,train_acc:0.9919439554214478
node3_model on test-dataset: loss:0.060555021818399835,acc:0.9821999669075012
node3 weight score:62125.31821525831
node12: train data size:1406
node12 epoch0:node_model train_loss:0.038621525187045334,train_acc:0.9886667132377625
node12 epoch1:node_model train_loss:0.03201117527981599,train_acc:0.9906666278839111
node12_model on test-dataset: loss:0.0719055370409751,acc:0.9800971150398254
node12 weight score:19553.431597330204
node14: train data size:1540
node14 epoch0:node_model train_loss:0.039640054004848935,train_acc:0.9856249690055847
node14 epoch1:node_model train_loss:0.016898734131245874,train_acc:0.9956249594688416
node14_model on test-dataset: loss:0.07864145473900862,acc:0.9788958430290222
node14 weight score:19582.547208859196
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04449534153172863,acc:0.9870979565382004
total cost energy:10.80288303421637 | all_enery_cp：8.9875 | all_enery_tp: 1.8153830342163695
ef: 31.940174679823745
reward: 21.137291645607377
step 288:loss:17.342313766479492|running q:83.74124145507812
episode4,iteration48 selected nodes:[9, 19, 15, 16, 6],center node:15
################################################## episode4,iteration48 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03839695265439028,train_acc:0.987777590751648
node6 epoch1:node_model train_loss:0.02915530117590808,train_acc:0.9890421032905579
node6_model on test-dataset: loss:0.06409873771448474,acc:0.9816970825195312
node6 weight score:55055.686364983325
node9: train data size:2125
node9 epoch0:node_model train_loss:0.036626825500702995,train_acc:0.9899998903274536
node9 epoch1:node_model train_loss:0.02221098107375755,train_acc:0.9927272200584412
node9_model on test-dataset: loss:0.07527039720591347,acc:0.9786919355392456
node9 weight score:28231.55023596785
node15: train data size:1376
node15 epoch0:node_model train_loss:0.041384960491476316,train_acc:0.98906010389328
node15 epoch1:node_model train_loss:0.024263102644389228,train_acc:0.9892856478691101
node15_model on test-dataset: loss:0.06704109814676486,acc:0.9812939763069153
node15 weight score:20524.723461237045
node16: train data size:920
node16 epoch0:node_model train_loss:0.06473067603074015,train_acc:0.9829999208450317
node16 epoch1:node_model train_loss:0.02623580468352884,train_acc:0.9920000433921814
node16_model on test-dataset: loss:0.06754226109384036,acc:0.9795960783958435
node16 weight score:13621.101590333064
node19: train data size:5781
node19 epoch0:node_model train_loss:0.048246963033934345,train_acc:0.9874138832092285
node19 epoch1:node_model train_loss:0.030650784615788546,train_acc:0.991034746170044
node19_model on test-dataset: loss:0.05373818407730141,acc:0.9835980534553528
node19 weight score:107577.1371746417
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04676016838602663,acc:0.9856979548931122
total cost energy:8.212441344953364 | all_enery_cp：6.8655 | all_enery_tp: 1.3469413449533645
ef: 32.491145206407516
reward: 24.278703861454154
step 289:loss:23.528738021850586|running q:85.09373474121094
episode4,iteration49 selected nodes:[15, 3, 10, 13, 4],center node:10
################################################## episode4,iteration49 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.036377500504482,train_acc:0.9873683452606201
node3 epoch1:node_model train_loss:0.028669140563579276,train_acc:0.991417646408081
node3_model on test-dataset: loss:0.06296339778156834,acc:0.9816957116127014
node3 weight score:59748.999141550026
node4: train data size:4298
node4 epoch0:node_model train_loss:0.034703801591815645,train_acc:0.9892975687980652
node4 epoch1:node_model train_loss:0.021456714401422286,train_acc:0.9930233359336853
node4_model on test-dataset: loss:0.058577572773137944,acc:0.9823982119560242
node4 weight score:73372.79092538542
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04983395020390162,train_acc:0.9864999055862427
node10 epoch1:node_model train_loss:0.026852646243787602,train_acc:0.9925000071525574
node10_model on test-dataset: loss:0.0631290330831689,acc:0.9813990592956543
node10 weight score:30334.69556673071
node13: train data size:1056
node13 epoch0:node_model train_loss:0.040581132623959675,train_acc:0.9854544997215271
node13 epoch1:node_model train_loss:0.024251829388297418,train_acc:0.9918181300163269
node13_model on test-dataset: loss:0.05732701450289824,acc:0.9825970530509949
node13 weight score:18420.63482909288
node15: train data size:1376
node15 epoch0:node_model train_loss:0.047353780013509095,train_acc:0.9859773516654968
node15 epoch1:node_model train_loss:0.031095297674515417,train_acc:0.9902631640434265
node15_model on test-dataset: loss:0.05307250578858657,acc:0.982297956943512
node15 weight score:25926.795419859627
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03570316617977369,acc:0.9880979543924332
total cost energy:8.000629758820093 | all_enery_cp：6.2035 | all_enery_tp: 1.7971297588200936
ef: 32.38084109116842
reward: 24.380211332348328
step 290:loss:22.924915313720703|running q:86.28399658203125
episode4,iteration50 selected nodes:[9, 7, 15, 13, 0],center node:7
################################################## episode4,iteration50 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.019279636310077168,train_acc:0.9924253821372986
node0 epoch1:node_model train_loss:0.018788245928388077,train_acc:0.993333637714386
node0_model on test-dataset: loss:0.06864095105328033,acc:0.9817969799041748
node0 weight score:104383.75182824025
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03961673115594061,train_acc:0.9878378510475159
node7 epoch1:node_model train_loss:0.026138843713655463,train_acc:0.990810751914978
node7_model on test-dataset: loss:0.05872498111879395,acc:0.9829949736595154
node7 weight score:61932.757247597285
node9: train data size:2125
node9 epoch0:node_model train_loss:0.033858711781001395,train_acc:0.9881818294525146
node9 epoch1:node_model train_loss:0.022460956719111313,train_acc:0.9936363101005554
node9_model on test-dataset: loss:0.05324242940674594,acc:0.9842959642410278
node9 weight score:39911.777574348576
node13: train data size:1056
node13 epoch0:node_model train_loss:0.033312088598243215,train_acc:0.9890908598899841
node13 epoch1:node_model train_loss:0.009561958394690671,train_acc:0.9981818199157715
node13_model on test-dataset: loss:0.07003756762994272,acc:0.9773982167243958
node13 weight score:15077.622420863956
node15: train data size:1376
node15 epoch0:node_model train_loss:0.026523724664002657,train_acc:0.98883455991745
node15 epoch1:node_model train_loss:0.027373260741504573,train_acc:0.9928570985794067
node15_model on test-dataset: loss:0.056223395623164835,acc:0.9835990071296692
node15 weight score:24473.797513451296
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04102975437759596,acc:0.986697952747345
total cost energy:9.490353701402672 | all_enery_cp：7.679500000000001 | all_enery_tp: 1.8108537014026709
ef: 32.34364871051816
reward: 22.853295009115488
step 291:loss:28.048114776611328|running q:87.58604431152344
episode4,iteration51 selected nodes:[0, 11, 5, 13, 3],center node:5
################################################## episode4,iteration51 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.016726706476321043,train_acc:0.9950001835823059
node0 epoch1:node_model train_loss:0.015238161434050804,train_acc:0.9938142895698547
node0_model on test-dataset: loss:0.07563897389862177,acc:0.982096254825592
node0 weight score:94726.298238831
node3: train data size:3762
node3 epoch0:node_model train_loss:0.031264286779332906,train_acc:0.9896774291992188
node3 epoch1:node_model train_loss:0.024490243062916164,train_acc:0.9914175271987915
node3_model on test-dataset: loss:0.059882406490360154,acc:0.9835980534553528
node3 weight score:62823.126532257935
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03946820766503485,train_acc:0.9867347478866577
node5 epoch1:node_model train_loss:0.01630619342904538,train_acc:0.9942858815193176
node5_model on test-dataset: loss:0.05132197684215498,acc:0.9844969511032104
node5 weight score:94248.12327234776
node11: train data size:1575
node11 epoch0:node_model train_loss:0.056108044751454145,train_acc:0.9845831990242004
node11 epoch1:node_model train_loss:0.020116941384912934,train_acc:0.9962499141693115
node11_model on test-dataset: loss:0.06415346479334402,acc:0.9811971187591553
node11 weight score:24550.5056519318
node13: train data size:1056
node13 epoch0:node_model train_loss:0.03486423776485026,train_acc:0.9854545593261719
node13 epoch1:node_model train_loss:0.02769949757070704,train_acc:0.9911038279533386
node13_model on test-dataset: loss:0.07675256914459169,acc:0.9789971113204956
node13 weight score:13758.497100085804
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042424033249772036,acc:0.9871969449520112
total cost energy:10.688161607042034 | all_enery_cp：9.1975 | all_enery_tp: 1.4906616070420333
ef: 32.40912081965121
reward: 21.720959212609174
step 292:loss:17.27223777770996|running q:88.96439361572266
episode4,iteration52 selected nodes:[18, 6, 3, 16, 8],center node:6
################################################## episode4,iteration52 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.026937288464978337,train_acc:0.9903649687767029
node3 epoch1:node_model train_loss:0.026462148161188356,train_acc:0.9905261993408203
node3_model on test-dataset: loss:0.0728025787407023,acc:0.9799948930740356
node3 weight score:51673.99376605804
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03637321496272408,train_acc:0.9897221326828003
node6 epoch1:node_model train_loss:0.02313111054374733,train_acc:0.9919443130493164
node6_model on test-dataset: loss:0.04911510847394311,acc:0.9866979718208313
node6 weight score:71851.6177536741
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05954939703983457,train_acc:0.9820770621299744
node8 epoch1:node_model train_loss:0.039540997554507594,train_acc:0.9903380274772644
node8_model on test-dataset: loss:0.06651464148919331,acc:0.981893002986908
node8 weight score:34428.51000515515
node16: train data size:920
node16 epoch0:node_model train_loss:0.05218755843525287,train_acc:0.9869999289512634
node16 epoch1:node_model train_loss:0.024190892893238926,train_acc:0.9929999709129333
node16_model on test-dataset: loss:0.07167865475010331,acc:0.9808979034423828
node16 weight score:12835.062309796962
node18: train data size:801
node18 epoch0:node_model train_loss:0.02135840866685612,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.013116043901792156,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.09466297971357562,acc:0.9737982749938965
node18 weight score:8461.59715681471
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04356614720816651,acc:0.9876969438791275
total cost energy:7.383272742093016 | all_enery_cp：5.651 | all_enery_tp: 1.7322727420930168
ef: 31.94484162170658
reward: 24.561568879613564
step 293:loss:20.549579620361328|running q:90.2118911743164
episode4,iteration53 selected nodes:[10, 2, 0, 13, 11],center node:11
################################################## episode4,iteration53 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.017474422831987288,train_acc:0.9928421378135681
node0 epoch1:node_model train_loss:0.018094400351805637,train_acc:0.9936007261276245
node0_model on test-dataset: loss:0.07752922159976151,acc:0.9806939959526062
node0 weight score:92416.76689324636
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03650283977944166,train_acc:0.9880852699279785
node2 epoch1:node_model train_loss:0.027025436032612985,train_acc:0.9914893507957458
node2_model on test-dataset: loss:0.060478784796432594,acc:0.9826939702033997
node2 weight score:76225.0765374493
node10: train data size:1915
node10 epoch0:node_model train_loss:0.033253237928875023,train_acc:0.9884999394416809
node10 epoch1:node_model train_loss:0.03204009071778273,train_acc:0.9919999241828918
node10_model on test-dataset: loss:0.06269173950052391,acc:0.9818001389503479
node10 weight score:30546.28911651106
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04284095353796147,train_acc:0.9868749976158142
node11 epoch1:node_model train_loss:0.02653847137116827,train_acc:0.9908333420753479
node11_model on test-dataset: loss:0.09956379911192925,acc:0.9732959270477295
node11 weight score:15819.002629955803
node13: train data size:1056
node13 epoch0:node_model train_loss:0.0668824493715709,train_acc:0.9836362600326538
node13 epoch1:node_model train_loss:0.020927290495654397,train_acc:0.9927272200584412
node13_model on test-dataset: loss:0.06595997614957014,acc:0.9794970750808716
node13 weight score:16009.708639151502
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042689226703878376,acc:0.9877969485521316
total cost energy:9.711327043275217 | all_enery_cp：8.1605 | all_enery_tp: 1.5508270432752163
ef: 32.045656220531896
reward: 22.334329177256677
step 294:loss:22.62645721435547|running q:91.51407623291016
episode4,iteration54 selected nodes:[15, 9, 4, 8, 3],center node:4
################################################## episode4,iteration54 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.029014406131433423,train_acc:0.9918420910835266
node3 epoch1:node_model train_loss:0.016923403803957626,train_acc:0.9936841726303101
node3_model on test-dataset: loss:0.07274629599890431,acc:0.9792982339859009
node3 weight score:51713.97317681525
node4: train data size:4298
node4 epoch0:node_model train_loss:0.037545485541137846,train_acc:0.988832414150238
node4 epoch1:node_model train_loss:0.016001269613530725,train_acc:0.9946512579917908
node4_model on test-dataset: loss:0.05746999215349206,acc:0.983799159526825
node4 weight score:74786.85552141388
node8: train data size:2290
node8 epoch0:node_model train_loss:0.040529654796599694,train_acc:0.9864733219146729
node8 epoch1:node_model train_loss:0.021668503546844357,train_acc:0.9951691031455994
node8_model on test-dataset: loss:0.05825658818674128,acc:0.9831980466842651
node8 weight score:39308.858813691826
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0377959482615221,train_acc:0.9899998903274536
node9 epoch1:node_model train_loss:0.02073837824652649,train_acc:0.9918181300163269
node9_model on test-dataset: loss:0.07445355910494982,acc:0.9797969460487366
node9 weight score:28541.281646517364
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04272500793116966,train_acc:0.9859774112701416
node15 epoch1:node_model train_loss:0.021596919603845372,train_acc:0.9928571581840515
node15_model on test-dataset: loss:0.06394325175024278,acc:0.9809978008270264
node15 weight score:21519.08078391987
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0398011468013101,acc:0.9886979520320892
total cost energy:8.30330507660323 | all_enery_cp：6.9255 | all_enery_tp: 1.377805076603229
ef: 32.34304512660195
reward: 24.03974004999872
step 295:loss:30.79938316345215|running q:92.78407287597656
episode4,iteration55 selected nodes:[2, 19, 13, 3, 14],center node:14
################################################## episode4,iteration55 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.030399456686474025,train_acc:0.9917021989822388
node2 epoch1:node_model train_loss:0.018337071757451532,train_acc:0.9951064586639404
node2_model on test-dataset: loss:0.0755487738744705,acc:0.981492817401886
node2 weight score:61020.18290409098
node3: train data size:3762
node3 epoch0:node_model train_loss:0.02370611886573514,train_acc:0.9929966330528259
node3 epoch1:node_model train_loss:0.014046264847581225,train_acc:0.9957894086837769
node3_model on test-dataset: loss:0.0802161460295065,acc:0.9796959757804871
node3 weight score:46898.28901298993
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04342504385435446,train_acc:0.9881817698478699
node13 epoch1:node_model train_loss:0.018412512822330675,train_acc:0.9927272796630859
node13_model on test-dataset: loss:0.054932043320268346,acc:0.9824969172477722
node13 weight score:19223.75240701025
node14: train data size:1540
node14 epoch0:node_model train_loss:0.054730987088987604,train_acc:0.9787499308586121
node14 epoch1:node_model train_loss:0.03453243554395158,train_acc:0.9906249642372131
node14_model on test-dataset: loss:0.08228538285955438,acc:0.9764979481697083
node14 weight score:18715.353158512848
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04793595280377837,train_acc:0.9868966937065125
node19 epoch1:node_model train_loss:0.0204780329021842,train_acc:0.9944829940795898
node19_model on test-dataset: loss:0.05673872532997848,acc:0.9822990298271179
node19 weight score:101888.08378015413
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042985678308596106,acc:0.9864989638328552
total cost energy:10.838659599291619 | all_enery_cp：8.3745 | all_enery_tp: 2.464159599291619
ef: 31.900439890087892
reward: 21.06178029079627
step 296:loss:14.405289649963379|running q:94.15393829345703
episode4,iteration56 selected nodes:[19, 5, 1, 3, 8],center node:5
################################################## episode4,iteration56 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03643919095360507,train_acc:0.9886936545372009
node1 epoch1:node_model train_loss:0.013675459416465028,train_acc:0.9946635961532593
node1_model on test-dataset: loss:0.0723616855974933,acc:0.980396032333374
node1 weight score:91996.19861025744
node3: train data size:3762
node3 epoch0:node_model train_loss:0.020172372575248836,train_acc:0.9922071099281311
node3 epoch1:node_model train_loss:0.018071098418563213,train_acc:0.9944736957550049
node3_model on test-dataset: loss:0.08548319521704797,acc:0.9767959117889404
node3 weight score:44008.649775526195
node5: train data size:4837
node5 epoch0:node_model train_loss:0.027085416250605593,train_acc:0.9902041554450989
node5 epoch1:node_model train_loss:0.01919884474743728,train_acc:0.993061363697052
node5_model on test-dataset: loss:0.07444554854671878,acc:0.9802980422973633
node5 weight score:64973.663226680495
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03519333981524181,train_acc:0.9899998903274536
node8 epoch1:node_model train_loss:0.02588365525614632,train_acc:0.990869402885437
node8_model on test-dataset: loss:0.06589186727993365,acc:0.9806970953941345
node8 weight score:34753.909617877594
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0309916887259721,train_acc:0.9905174374580383
node19 epoch1:node_model train_loss:0.024852462981000606,train_acc:0.9929312467575073
node19_model on test-dataset: loss:0.06699679169452338,acc:0.9806979894638062
node19 weight score:86287.71399022924
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.044735885063528255,acc:0.987695935368538
total cost energy:13.116303527058925 | all_enery_cp：11.663499999999999 | all_enery_tp: 1.4528035270589248
ef: 32.28876933613955
reward: 19.172465809080627
step 297:loss:28.08389663696289|running q:95.31024932861328
episode4,iteration57 selected nodes:[9, 19, 10, 15, 7],center node:10
################################################## episode4,iteration57 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.032157562308114125,train_acc:0.988728940486908
node7 epoch1:node_model train_loss:0.01688701067925305,train_acc:0.994864821434021
node7_model on test-dataset: loss:0.057538950529124125,acc:0.982996940612793
node7 weight score:63209.35586336568
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03967557198749008,train_acc:0.9872726202011108
node9 epoch1:node_model train_loss:0.021919323528312485,train_acc:0.9909089803695679
node9_model on test-dataset: loss:0.053068911025184205,acc:0.9843981862068176
node9 weight score:40042.276333719514
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04056901912699686,train_acc:0.9894999861717224
node10 epoch1:node_model train_loss:0.026191054121591152,train_acc:0.9914999008178711
node10_model on test-dataset: loss:0.060822118792439143,acc:0.981898307800293
node10 weight score:31485.2563182665
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04498757359812901,train_acc:0.9850375652313232
node15 epoch1:node_model train_loss:0.028284248214081993,train_acc:0.9899999499320984
node15_model on test-dataset: loss:0.07288563795818845,acc:0.9802936911582947
node15 weight score:18878.89080135864
node19: train data size:5781
node19 epoch0:node_model train_loss:0.029660226759633274,train_acc:0.9903044700622559
node19 epoch1:node_model train_loss:0.012303076158598837,train_acc:0.9968966245651245
node19_model on test-dataset: loss:0.07272106332396105,acc:0.9798998832702637
node19 weight score:79495.53727297059
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041131798077431087,acc:0.9883979570865631
total cost energy:9.000788226773652 | all_enery_cp：7.417 | all_enery_tp: 1.5837882267736527
ef: 32.495987032879064
reward: 23.495198806105414
step 298:loss:17.456104278564453|running q:96.5291748046875
episode4,iteration58 selected nodes:[7, 6, 18, 13, 1],center node:7
################################################## episode4,iteration58 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.025051234299560257,train_acc:0.9924246668815613
node1 epoch1:node_model train_loss:0.016341558833512713,train_acc:0.9931343197822571
node1_model on test-dataset: loss:0.08030333564154717,acc:0.9808969497680664
node1 weight score:82898.17535992635
node6: train data size:3529
node6 epoch0:node_model train_loss:0.02845699598320708,train_acc:0.9919443130493164
node6 epoch1:node_model train_loss:0.019326340504449036,train_acc:0.9927776455879211
node6_model on test-dataset: loss:0.062317213816604634,acc:0.9830973744392395
node6 weight score:56629.61778723949
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03502485544792049,train_acc:0.9898903369903564
node7 epoch1:node_model train_loss:0.01765824924380443,train_acc:0.9945945143699646
node7_model on test-dataset: loss:0.060716463581484276,acc:0.9830969572067261
node7 weight score:59901.38070408167
node13: train data size:1056
node13 epoch0:node_model train_loss:0.02569927207448266,train_acc:0.9881817698478699
node13 epoch1:node_model train_loss:0.03271059719422324,train_acc:0.9856492877006531
node13_model on test-dataset: loss:0.0656872549523905,acc:0.9796980619430542
node13 weight score:16076.177955150946
node18: train data size:801
node18 epoch0:node_model train_loss:0.02261249093701028,train_acc:0.9944444298744202
node18 epoch1:node_model train_loss:0.020107271207962185,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.07456837326700225,acc:0.9797961711883545
node18 weight score:10741.819418963454
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.046205686773828344,acc:0.9868969440460205
total cost energy:8.951381041053223 | all_enery_cp：7.84 | all_enery_tp: 1.111381041053223
ef: 32.18036389064423
reward: 23.228982849591006
step 299:loss:12.434661865234375|running q:97.765625
episode4,iteration59 selected nodes:[14, 7, 18, 1, 3],center node:7
################################################## episode4,iteration59 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.02326361312871493,train_acc:0.9928361177444458
node1 epoch1:node_model train_loss:0.017383740771798165,train_acc:0.9945143461227417
node1_model on test-dataset: loss:0.07514753214593839,acc:0.9804949164390564
node1 weight score:88585.74340235071
node3: train data size:3762
node3 epoch0:node_model train_loss:0.022689541385165955,train_acc:0.9903649687767029
node3 epoch1:node_model train_loss:0.014923099188691643,train_acc:0.9956280589103699
node3_model on test-dataset: loss:0.06818786520949288,acc:0.9804952144622803
node3 weight score:55171.10688891706
node7: train data size:3637
node7 epoch0:node_model train_loss:0.028690647079005233,train_acc:0.9883491396903992
node7 epoch1:node_model train_loss:0.019023721841025493,train_acc:0.9940540194511414
node7_model on test-dataset: loss:0.06570131767402927,acc:0.9818980097770691
node7 weight score:55356.576226440746
node14: train data size:1540
node14 epoch0:node_model train_loss:0.037076651395182125,train_acc:0.9868748784065247
node14 epoch1:node_model train_loss:0.037295547052053735,train_acc:0.9862499237060547
node14_model on test-dataset: loss:0.08756702468673666,acc:0.9778980612754822
node14 weight score:17586.528781915506
node18: train data size:801
node18 epoch0:node_model train_loss:0.028875112843984932,train_acc:0.9922221302986145
node18 epoch1:node_model train_loss:0.005256488764037688,train_acc:0.9988888502120972
node18_model on test-dataset: loss:0.0863111160682081,acc:0.9790973663330078
node18 weight score:9280.380517464318
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04228303509866691,acc:0.9877979546785355
total cost energy:10.304965503015088 | all_enery_cp：8.1985 | all_enery_tp: 2.1064655030150887
ef: 31.61285480509448
reward: 21.30788930207939
step 300:loss:16.028104782104492|running q:99.04119873046875
episode4_cost time: 13578.221037626266
episode5,iteration0 selected nodes:[10, 7, 8, 11, 18],center node:11
################################################## episode5,iteration0 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:1.0866727530956268,train_acc:0.6715631484985352
node7 epoch1:node_model train_loss:0.3371390055965733,train_acc:0.887458086013794
node7_model on test-dataset: loss:0.33178191270679236,acc:0.8891899585723877
node7 weight score:10962.020112332488
node8: train data size:2290
node8 epoch0:node_model train_loss:1.302373652872832,train_acc:0.5997101664543152
node8 epoch1:node_model train_loss:0.4405870022981063,train_acc:0.8625121116638184
node8_model on test-dataset: loss:0.9250923262536526,acc:0.6904323697090149
node8 weight score:2475.428597785278
node10: train data size:1915
node10 epoch0:node_model train_loss:1.4072273224592209,train_acc:0.5589999556541443
node10 epoch1:node_model train_loss:0.5553780987858772,train_acc:0.8056667447090149
node10_model on test-dataset: loss:0.3895599580928683,acc:0.8823877573013306
node10 weight score:4915.802972602943
node11: train data size:1575
node11 epoch0:node_model train_loss:1.8009797036647797,train_acc:0.4477083086967468
node11 epoch1:node_model train_loss:0.6088766008615494,train_acc:0.8212500214576721
node11_model on test-dataset: loss:0.578602227633819,acc:0.8094969987869263
node11 weight score:2722.077318023693
node18: train data size:801
node18 epoch0:node_model train_loss:1.928573813703325,train_acc:0.4422222375869751
node18 epoch1:node_model train_loss:1.053443365626865,train_acc:0.6388888955116272
node18_model on test-dataset: loss:1.1020001585036516,acc:0.6369817852973938
node18 weight score:726.860149537216
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.48565353903919456,acc:0.8602929046750069
total cost energy:6.176004637770997 | all_enery_cp：5.109 | all_enery_tp: 1.067004637770997
ef: 26.275176448894044
reward: 20.099171811123046
step 301:loss:13.118794441223145|running q:2.0389719009399414
episode5,iteration1 selected nodes:[5, 3, 0, 9, 16],center node:5
################################################## episode5,iteration1 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.3537011444568634,train_acc:0.8853949904441833
node0 epoch1:node_model train_loss:0.19698900543153286,train_acc:0.9363780617713928
node0_model on test-dataset: loss:0.15806568526662887,acc:0.9466949105262756
node0 weight score:45329.25655504489
node3: train data size:3762
node3 epoch0:node_model train_loss:0.42255936249306325,train_acc:0.8641341328620911
node3 epoch1:node_model train_loss:0.24429909649648165,train_acc:0.921969473361969
node3_model on test-dataset: loss:0.4220816000085324,acc:0.8692948222160339
node3 weight score:8912.968487429802
node5: train data size:4837
node5 epoch0:node_model train_loss:0.4405562524892846,train_acc:0.8553224802017212
node5 epoch1:node_model train_loss:0.2213801591067898,train_acc:0.925223171710968
node5_model on test-dataset: loss:0.2893257326562889,acc:0.9053937196731567
node5 weight score:16718.181115767617
node9: train data size:2125
node9 epoch0:node_model train_loss:0.6341426873748953,train_acc:0.8172727227210999
node9 epoch1:node_model train_loss:0.28554252602837304,train_acc:0.9100000262260437
node9_model on test-dataset: loss:0.30786184661090377,acc:0.8949982523918152
node9 weight score:6902.446741592231
node16: train data size:920
node16 epoch0:node_model train_loss:0.7792580187320709,train_acc:0.7409999966621399
node16 epoch1:node_model train_loss:0.3612833932042122,train_acc:0.8630000352859497
node16_model on test-dataset: loss:0.33186753872781993,acc:0.8904978632926941
node16 weight score:2772.1903851359652
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.14317844526609405,acc:0.9559979522228241
total cost energy:11.080251924078564 | all_enery_cp：9.4045 | all_enery_tp: 1.675751924078562
ef: 28.025030907236548
reward: 16.944778983157985
step 302:loss:29.534015655517578|running q:4.129123687744141
episode5,iteration2 selected nodes:[13, 10, 0, 12, 7],center node:12
################################################## episode5,iteration2 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.20029945474945837,train_acc:0.9359400272369385
node0 epoch1:node_model train_loss:0.13171396414852804,train_acc:0.9579060077667236
node0_model on test-dataset: loss:0.15949812760867643,acc:0.9495940804481506
node0 weight score:44922.15744111491
node7: train data size:3637
node7 epoch0:node_model train_loss:0.2432945616341926,train_acc:0.9218625426292419
node7 epoch1:node_model train_loss:0.1532850158778397,train_acc:0.9495400190353394
node7_model on test-dataset: loss:0.19099323909264057,acc:0.9384928941726685
node7 weight score:19042.558874222173
node10: train data size:1915
node10 epoch0:node_model train_loss:0.22535104993730784,train_acc:0.9254999160766602
node10 epoch1:node_model train_loss:0.13766368571668863,train_acc:0.9599999785423279
node10_model on test-dataset: loss:0.2119282901076076,acc:0.9276973009109497
node10 weight score:9036.075358450962
node12: train data size:1406
node12 epoch0:node_model train_loss:0.2329116369287173,train_acc:0.9222222566604614
node12 epoch1:node_model train_loss:0.21062579254309335,train_acc:0.9204444885253906
node12_model on test-dataset: loss:0.19710534340701996,acc:0.9368910193443298
node12 weight score:7133.241421551055
node13: train data size:1056
node13 epoch0:node_model train_loss:0.30023076182061975,train_acc:0.9100649952888489
node13 epoch1:node_model train_loss:0.18915116786956787,train_acc:0.9433116316795349
node13_model on test-dataset: loss:0.3177884302753955,acc:0.8973989486694336
node13 weight score:3322.9655311392876
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.11214899353275541,acc:0.9630969494581223
total cost energy:8.99620198723451 | all_enery_cp：7.5895 | all_enery_tp: 1.4067019872345092
ef: 28.957267823714506
reward: 19.961065836479996
step 303:loss:28.906824111938477|running q:6.337813377380371
episode5,iteration3 selected nodes:[15, 19, 6, 18, 7],center node:15
################################################## episode5,iteration3 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.17951685655862093,train_acc:0.9391953349113464
node6 epoch1:node_model train_loss:0.12546628579083416,train_acc:0.961264431476593
node6_model on test-dataset: loss:0.1892876039982366,acc:0.9407907724380493
node6 weight score:18643.587458758662
node7: train data size:3637
node7 epoch0:node_model train_loss:0.17642895585379084,train_acc:0.9415923953056335
node7 epoch1:node_model train_loss:0.10971375011109016,train_acc:0.963483989238739
node7_model on test-dataset: loss:0.2581088626733981,acc:0.9171992540359497
node7 weight score:14090.953570246566
node15: train data size:1376
node15 epoch0:node_model train_loss:0.1898008015538965,train_acc:0.9353007078170776
node15 epoch1:node_model train_loss:0.14039764446871622,train_acc:0.9531954526901245
node15_model on test-dataset: loss:0.16949961408157832,acc:0.9450957179069519
node15 weight score:8118.012583425388
node18: train data size:801
node18 epoch0:node_model train_loss:0.15412144549190998,train_acc:0.9455555081367493
node18 epoch1:node_model train_loss:0.14223533868789673,train_acc:0.9599999189376831
node18_model on test-dataset: loss:0.20830153988557867,acc:0.9322850704193115
node18 weight score:3845.386838906684
node19: train data size:5781
node19 epoch0:node_model train_loss:0.17103272618661666,train_acc:0.9446658492088318
node19 epoch1:node_model train_loss:0.12593838257779336,train_acc:0.9581140279769897
node19_model on test-dataset: loss:0.133652641098015,acc:0.9562958478927612
node19 weight score:43253.915167755404
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.09751045586017426,acc:0.9677969479560852
total cost energy:8.543255920004126 | all_enery_cp：7.562 | all_enery_tp: 0.9812559200041266
ef: 29.409074428208235
reward: 20.865818508204107
step 304:loss:33.88798522949219|running q:8.360245704650879
episode5,iteration4 selected nodes:[11, 12, 16, 7, 5],center node:12
################################################## episode5,iteration4 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.15852845969552898,train_acc:0.9515498280525208
node5 epoch1:node_model train_loss:0.10852823695357965,train_acc:0.9646938443183899
node5_model on test-dataset: loss:0.11153457013773732,acc:0.9651989340782166
node5 weight score:43367.71992779141
node7: train data size:3637
node7 epoch0:node_model train_loss:0.15981212207997167,train_acc:0.9485097527503967
node7 epoch1:node_model train_loss:0.11126462994395075,train_acc:0.9671875238418579
node7_model on test-dataset: loss:0.1457568957610056,acc:0.9543929696083069
node7 weight score:24952.507262253373
node11: train data size:1575
node11 epoch0:node_model train_loss:0.2019291240721941,train_acc:0.93729168176651
node11 epoch1:node_model train_loss:0.12330311839468777,train_acc:0.9631249308586121
node11_model on test-dataset: loss:0.1305247310944833,acc:0.9584951400756836
node11 weight score:12066.678757299263
node12: train data size:1406
node12 epoch0:node_model train_loss:0.1435827208062013,train_acc:0.9546666741371155
node12 epoch1:node_model train_loss:0.0816519357264042,train_acc:0.9759999513626099
node12_model on test-dataset: loss:0.1492679103813134,acc:0.9538961052894592
node12 weight score:9419.305170202308
node16: train data size:920
node16 epoch0:node_model train_loss:0.2196216970682144,train_acc:0.925000011920929
node16 epoch1:node_model train_loss:0.12948880791664125,train_acc:0.9639999270439148
node16_model on test-dataset: loss:0.15280876209202687,acc:0.952695906162262
node16 weight score:6020.597166057423
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.08594289785891306,acc:0.9715979540348053
total cost energy:7.017556307974577 | all_enery_cp：6.1875 | all_enery_tp: 0.8300563079745771
ef: 30.246074356539513
reward: 23.228518048564936
step 305:loss:26.2372989654541|running q:10.555909156799316
episode5,iteration5 selected nodes:[0, 9, 3, 19, 13],center node:9
################################################## episode5,iteration5 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.13669127613926926,train_acc:0.954636812210083
node0 epoch1:node_model train_loss:0.09374559885408315,train_acc:0.9702779054641724
node0_model on test-dataset: loss:0.11156198534794384,acc:0.9629966616630554
node0 weight score:64224.385911146346
node3: train data size:3762
node3 epoch0:node_model train_loss:0.16830210897483325,train_acc:0.9473090171813965
node3 epoch1:node_model train_loss:0.10093272134269539,train_acc:0.9694734811782837
node3_model on test-dataset: loss:0.21399345639627426,acc:0.9358978271484375
node3 weight score:17579.976805615534
node9: train data size:2125
node9 epoch0:node_model train_loss:0.1823042927479202,train_acc:0.9431816935539246
node9 epoch1:node_model train_loss:0.0965359230441126,train_acc:0.965908944606781
node9_model on test-dataset: loss:0.11108677134499885,acc:0.9626991748809814
node9 weight score:19129.190400182317
node13: train data size:1056
node13 epoch0:node_model train_loss:0.1987708234651522,train_acc:0.9460389018058777
node13 epoch1:node_model train_loss:0.11960806088014082,train_acc:0.9583766460418701
node13_model on test-dataset: loss:0.17055315851190245,acc:0.9455981850624084
node13 weight score:6191.617963652691
node19: train data size:5781
node19 epoch0:node_model train_loss:0.13844957106329245,train_acc:0.9571604132652283
node19 epoch1:node_model train_loss:0.0913936014865236,train_acc:0.9711259603500366
node19_model on test-dataset: loss:0.129952893787995,acc:0.9582879543304443
node19 weight score:44485.350279549115
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0688924919947749,acc:0.9780969476699829
total cost energy:12.398415582734467 | all_enery_cp：9.9445 | all_enery_tp: 2.453915582734467
ef: 29.9622105429463
reward: 17.56379496021183
step 306:loss:25.78502655029297|running q:12.800909996032715
episode5,iteration6 selected nodes:[7, 18, 3, 6, 9],center node:6
################################################## episode5,iteration6 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.14174555903790811,train_acc:0.9584040641784668
node3 epoch1:node_model train_loss:0.09255477548331807,train_acc:0.9682002663612366
node3_model on test-dataset: loss:0.1184791147713986,acc:0.9651936888694763
node3 weight score:31752.431702909413
node6: train data size:3529
node6 epoch0:node_model train_loss:0.11934658026115762,train_acc:0.9602775573730469
node6 epoch1:node_model train_loss:0.06990330443821019,train_acc:0.9769442677497864
node6_model on test-dataset: loss:0.1180336555937538,acc:0.9614969491958618
node6 weight score:29898.252174329424
node7: train data size:3637
node7 epoch0:node_model train_loss:0.11452816069327496,train_acc:0.9642950892448425
node7 epoch1:node_model train_loss:0.08860405826488056,train_acc:0.9713220596313477
node7_model on test-dataset: loss:0.1469000100332778,acc:0.9510940909385681
node7 weight score:24758.337315130862
node9: train data size:2125
node9 epoch0:node_model train_loss:0.12669045376506718,train_acc:0.9559089541435242
node9 epoch1:node_model train_loss:0.07419484193352135,train_acc:0.9777271151542664
node9_model on test-dataset: loss:0.1885311648651259,acc:0.9390950798988342
node9 weight score:11271.34604785481
node18: train data size:801
node18 epoch0:node_model train_loss:0.083367227056341,train_acc:0.9699999690055847
node18 epoch1:node_model train_loss:0.07743496664352278,train_acc:0.9744443893432617
node18_model on test-dataset: loss:0.12911881122505292,acc:0.9575939178466797
node18 weight score:6203.58871337395
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06663144864374772,acc:0.9794959384202957
total cost energy:8.714945380563456 | all_enery_cp：6.927 | all_enery_tp: 1.787945380563456
ef: 29.871768479151786
reward: 21.15682309858833
step 307:loss:51.259361267089844|running q:14.807106971740723
episode5,iteration7 selected nodes:[5, 10, 15, 2, 8],center node:5
################################################## episode5,iteration7 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.1276041301403274,train_acc:0.9638295769691467
node2 epoch1:node_model train_loss:0.1014972569698349,train_acc:0.9689361453056335
node2_model on test-dataset: loss:0.11915201358962804,acc:0.9622920751571655
node2 weight score:38690.07212817503
node5: train data size:4837
node5 epoch0:node_model train_loss:0.11995579534191259,train_acc:0.9631219506263733
node5 epoch1:node_model train_loss:0.08421408274800193,train_acc:0.9714064598083496
node5_model on test-dataset: loss:0.1357944416468672,acc:0.9569939374923706
node5 weight score:35620.014643740695
node8: train data size:2290
node8 epoch0:node_model train_loss:0.14182802727041038,train_acc:0.959468424320221
node8 epoch1:node_model train_loss:0.09513318506271942,train_acc:0.9698551297187805
node8_model on test-dataset: loss:0.14885984892789567,acc:0.9534980058670044
node8 weight score:15383.597501225628
node10: train data size:1915
node10 epoch0:node_model train_loss:0.10623768460936844,train_acc:0.9704999327659607
node10 epoch1:node_model train_loss:0.07625354052870534,train_acc:0.9779998660087585
node10_model on test-dataset: loss:0.1362579162658949,acc:0.9575940370559692
node10 weight score:14054.229306303585
node15: train data size:1376
node15 epoch0:node_model train_loss:0.12182025983929634,train_acc:0.9616917371749878
node15 epoch1:node_model train_loss:0.08832470167960439,train_acc:0.9724059700965881
node15_model on test-dataset: loss:0.14707455584772106,acc:0.9542927145957947
node15 weight score:9355.799118813531
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07538006226997823,acc:0.9753979569673539
total cost energy:8.586134935173835 | all_enery_cp：7.513999999999999 | all_enery_tp: 1.072134935173836
ef: 30.144127553902184
reward: 21.55799261872835
step 308:loss:23.344404220581055|running q:16.782604217529297
episode5,iteration8 selected nodes:[10, 13, 0, 18, 19],center node:18
################################################## episode5,iteration8 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.10790062147296137,train_acc:0.9653207063674927
node0 epoch1:node_model train_loss:0.07475916371266875,train_acc:0.975480854511261
node0_model on test-dataset: loss:0.1127900858240173,acc:0.9625978469848633
node0 weight score:63525.086869597006
node10: train data size:1915
node10 epoch0:node_model train_loss:0.11796470172703266,train_acc:0.9641666412353516
node10 epoch1:node_model train_loss:0.0780206154100597,train_acc:0.9719999432563782
node10_model on test-dataset: loss:0.09407655248593073,acc:0.9684931039810181
node10 weight score:20355.76293345136
node13: train data size:1056
node13 epoch0:node_model train_loss:0.12001457945867018,train_acc:0.9596753120422363
node13 epoch1:node_model train_loss:0.10241249135949394,train_acc:0.9705842733383179
node13_model on test-dataset: loss:0.10993626536102966,acc:0.9666958451271057
node13 weight score:9605.565520459568
node18: train data size:801
node18 epoch0:node_model train_loss:0.22522063387764824,train_acc:0.861111044883728
node18 epoch1:node_model train_loss:0.1481931322875122,train_acc:0.9577776789665222
node18_model on test-dataset: loss:0.21566584119049367,acc:0.931181788444519
node18 weight score:3714.079130836911
node19: train data size:5781
node19 epoch0:node_model train_loss:0.11808156825858972,train_acc:0.9633270502090454
node19 epoch1:node_model train_loss:0.08080989390547419,train_acc:0.9761260151863098
node19_model on test-dataset: loss:0.11108549599186518,acc:0.9664936661720276
node19 weight score:52040.99732716992
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.062421816598944135,acc:0.9800979578495026
total cost energy:10.204962602917671 | all_enery_cp：8.359 | all_enery_tp: 1.845962602917671
ef: 30.36639592938342
reward: 20.161433326465747
step 309:loss:17.601511001586914|running q:18.785320281982422
episode5,iteration9 selected nodes:[17, 9, 13, 8, 0],center node:8
################################################## episode5,iteration9 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.08908039996296996,train_acc:0.971656084060669
node0 epoch1:node_model train_loss:0.062267152527864605,train_acc:0.9801177978515625
node0_model on test-dataset: loss:0.11093109467881732,acc:0.9668918251991272
node0 weight score:64589.6447767425
node8: train data size:2290
node8 epoch0:node_model train_loss:0.12493718107757361,train_acc:0.961642324924469
node8 epoch1:node_model train_loss:0.0769186680731566,train_acc:0.9751206636428833
node8_model on test-dataset: loss:0.12853029517224057,acc:0.9597989916801453
node8 weight score:17816.811180050758
node9: train data size:2125
node9 epoch0:node_model train_loss:0.12305651960725134,train_acc:0.9636362791061401
node9 epoch1:node_model train_loss:0.06442925312810323,train_acc:0.9790908098220825
node9_model on test-dataset: loss:0.11405464717987343,acc:0.9653908610343933
node9 weight score:18631.419696987028
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08789723027836192,train_acc:0.9683765172958374
node13 epoch1:node_model train_loss:0.06552207469940186,train_acc:0.975844144821167
node13_model on test-dataset: loss:0.12689812142634765,acc:0.9602929949760437
node13 weight score:8321.636192328568
node17: train data size:719
node17 epoch0:node_model train_loss:0.1165083764353767,train_acc:0.9643420577049255
node17 epoch1:node_model train_loss:0.1313451719470322,train_acc:0.9596710801124573
node17_model on test-dataset: loss:0.09203612847661134,acc:0.9720979332923889
node17 weight score:7812.1495536691955
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06579615530383308,acc:0.9811979556083679
total cost energy:8.362559146593059 | all_enery_cp：6.6775 | all_enery_tp: 1.6850591465930589
ef: 30.217162478935734
reward: 21.854603332342677
step 310:loss:31.113149642944336|running q:20.733482360839844
episode5,iteration10 selected nodes:[2, 18, 12, 7, 8],center node:12
################################################## episode5,iteration10 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.10819064726696369,train_acc:0.9651063680648804
node2 epoch1:node_model train_loss:0.08699789302463228,train_acc:0.9734042286872864
node2_model on test-dataset: loss:0.06487130943278316,acc:0.9788972735404968
node2 weight score:71063.77288062425
node7: train data size:3637
node7 epoch0:node_model train_loss:0.093308663851506,train_acc:0.96872878074646
node7 epoch1:node_model train_loss:0.07722815080873065,train_acc:0.9742145538330078
node7_model on test-dataset: loss:0.10783379951608367,acc:0.9662949442863464
node7 weight score:33727.82945905131
node8: train data size:2290
node8 epoch0:node_model train_loss:0.13201254671034607,train_acc:0.9583573937416077
node8 epoch1:node_model train_loss:0.07972416998413594,train_acc:0.9734297394752502
node8_model on test-dataset: loss:0.11184516024484764,acc:0.967595636844635
node8 weight score:20474.73484759474
node12: train data size:1406
node12 epoch0:node_model train_loss:0.09196726631295557,train_acc:0.9686665534973145
node12 epoch1:node_model train_loss:0.08286961279809475,train_acc:0.9739999771118164
node12_model on test-dataset: loss:0.19010140841102838,acc:0.9438868761062622
node12 weight score:7396.052516139242
node18: train data size:801
node18 epoch0:node_model train_loss:0.07947118950283362,train_acc:0.9744443893432617
node18 epoch1:node_model train_loss:0.0509997440320957,train_acc:0.9822222590446472
node18_model on test-dataset: loss:0.18675936175510288,acc:0.9406929016113281
node18 weight score:4288.941622376872
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.059160213815630414,acc:0.9810989665985107
total cost energy:7.594677276241436 | all_enery_cp：6.372 | all_enery_tp: 1.2226772762414362
ef: 30.668021815102183
reward: 23.073344538860745
step 311:loss:18.831451416015625|running q:22.660078048706055
episode5,iteration11 selected nodes:[14, 0, 19, 9, 3],center node:9
################################################## episode5,iteration11 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0800186312602212,train_acc:0.9738143086433411
node0 epoch1:node_model train_loss:0.05103264328661478,train_acc:0.9833977222442627
node0_model on test-dataset: loss:0.09106951448207838,acc:0.9711949229240417
node0 weight score:78676.16337638436
node3: train data size:3762
node3 epoch0:node_model train_loss:0.09688631317725308,train_acc:0.9698385000228882
node3 epoch1:node_model train_loss:0.05783984565029019,train_acc:0.980729877948761
node3_model on test-dataset: loss:0.08151415977918078,acc:0.9735971093177795
node3 weight score:46151.49085988417
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0827495481141589,train_acc:0.9731816649436951
node9 epoch1:node_model train_loss:0.05393707216717303,train_acc:0.9822726249694824
node9_model on test-dataset: loss:0.11180081265400077,acc:0.9655000567436218
node9 weight score:19007.017476486628
node14: train data size:1540
node14 epoch0:node_model train_loss:0.12135719403158873,train_acc:0.9640623927116394
node14 epoch1:node_model train_loss:0.08066269650589675,train_acc:0.9759374260902405
node14_model on test-dataset: loss:0.10511633267684374,acc:0.9669930338859558
node14 weight score:14650.435006464502
node19: train data size:5781
node19 epoch0:node_model train_loss:0.10080441840571062,train_acc:0.9706491231918335
node19 epoch1:node_model train_loss:0.0717186289919733,train_acc:0.9768561720848083
node19_model on test-dataset: loss:0.0906035882919059,acc:0.9690930843353271
node19 weight score:63805.4199506406
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05618716515324195,acc:0.9814979541301727
total cost energy:11.881836938971777 | all_enery_cp：10.1865 | all_enery_tp: 1.6953369389717767
ef: 31.35771780164158
reward: 19.475880862669804
step 312:loss:22.089948654174805|running q:24.648117065429688
episode5,iteration12 selected nodes:[18, 5, 12, 14, 11],center node:11
################################################## episode5,iteration12 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.09221736759859689,train_acc:0.9710808992385864
node5 epoch1:node_model train_loss:0.05681459548674068,train_acc:0.9814286231994629
node5_model on test-dataset: loss:0.08018444479675964,acc:0.9754961729049683
node5 weight score:60323.42073653005
node11: train data size:1575
node11 epoch0:node_model train_loss:0.12495237123221159,train_acc:0.9624998569488525
node11 epoch1:node_model train_loss:0.08433075819630176,train_acc:0.9695833325386047
node11_model on test-dataset: loss:0.08671002500806935,acc:0.9737977981567383
node11 weight score:18163.98968692984
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0716650438378565,train_acc:0.9760000705718994
node12 epoch1:node_model train_loss:0.04205302658180396,train_acc:0.9880000948905945
node12_model on test-dataset: loss:0.09529618896805914,acc:0.9706951975822449
node12 weight score:14754.00029345618
node14: train data size:1540
node14 epoch0:node_model train_loss:0.10536141018383205,train_acc:0.9671874642372131
node14 epoch1:node_model train_loss:0.06543192459503189,train_acc:0.9749999046325684
node14_model on test-dataset: loss:0.08056181422318331,acc:0.9739998579025269
node14 weight score:19115.75620347479
node18: train data size:801
node18 epoch0:node_model train_loss:0.06541384735363762,train_acc:0.9811111092567444
node18 epoch1:node_model train_loss:0.054795581452910684,train_acc:0.9811111092567444
node18_model on test-dataset: loss:0.12719763677279844,acc:0.9608958959579468
node18 weight score:6297.286807543079
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05444781522499397,acc:0.9824989646673202
total cost energy:6.175972487858143 | all_enery_cp：5.0794999999999995 | all_enery_tp: 1.096472487858144
ef: 31.246312552415848
reward: 25.070340064557705
step 313:loss:40.17886734008789|running q:26.652976989746094
episode5,iteration13 selected nodes:[4, 7, 9, 11, 5],center node:5
################################################## episode5,iteration13 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.10207136061995528,train_acc:0.9709159731864929
node4 epoch1:node_model train_loss:0.06221406233258719,train_acc:0.9823209047317505
node4_model on test-dataset: loss:0.10771970418281854,acc:0.9666959047317505
node4 weight score:39899.849638517095
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07595173591671854,train_acc:0.9736514091491699
node5 epoch1:node_model train_loss:0.061460339141135314,train_acc:0.9807336330413818
node5_model on test-dataset: loss:0.09203987006563694,acc:0.9722982048988342
node5 weight score:52553.311913093334
node7: train data size:3637
node7 epoch0:node_model train_loss:0.08199445238789997,train_acc:0.9722423553466797
node7 epoch1:node_model train_loss:0.053663224309078744,train_acc:0.9822424054145813
node7_model on test-dataset: loss:0.16588103172289265,acc:0.9505921006202698
node7 weight score:21925.3519358119
node9: train data size:2125
node9 epoch0:node_model train_loss:0.07199639433317563,train_acc:0.978181779384613
node9 epoch1:node_model train_loss:0.04369665978645736,train_acc:0.9863635301589966
node9_model on test-dataset: loss:0.1304551809860277,acc:0.9564896821975708
node9 weight score:16289.119251059843
node11: train data size:1575
node11 epoch0:node_model train_loss:0.10129955969750881,train_acc:0.9683332443237305
node11 epoch1:node_model train_loss:0.06771982798818499,train_acc:0.9799998998641968
node11_model on test-dataset: loss:0.10226252434891649,acc:0.9680917859077454
node11 weight score:15401.536486877147
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.049239675985299984,acc:0.9847979515790939
total cost energy:9.606820393249938 | all_enery_cp：8.236 | all_enery_tp: 1.370820393249937
ef: 30.67458433028329
reward: 21.067763937033355
step 314:loss:20.214872360229492|running q:28.52256202697754
episode5,iteration14 selected nodes:[15, 18, 4, 14, 16],center node:15
################################################## episode5,iteration14 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.08297043954303791,train_acc:0.9732416868209839
node4 epoch1:node_model train_loss:0.052861384928313104,train_acc:0.9839439988136292
node4_model on test-dataset: loss:0.10746268129121744,acc:0.9660908579826355
node4 weight score:39995.27974137066
node14: train data size:1540
node14 epoch0:node_model train_loss:0.11467752914177254,train_acc:0.9671873450279236
node14 epoch1:node_model train_loss:0.05391513730864972,train_acc:0.9834374189376831
node14_model on test-dataset: loss:0.07662830308254343,acc:0.9760980606079102
node14 weight score:20097.012958007483
node15: train data size:1376
node15 epoch0:node_model train_loss:0.08821342539574419,train_acc:0.969548761844635
node15 epoch1:node_model train_loss:0.063645876811019,train_acc:0.9783457517623901
node15_model on test-dataset: loss:0.08030983222648501,acc:0.9737991094589233
node15 weight score:17133.643065266115
node16: train data size:920
node16 epoch0:node_model train_loss:0.10033784555271268,train_acc:0.9749999046325684
node16 epoch1:node_model train_loss:0.07671068292111158,train_acc:0.9779999852180481
node16_model on test-dataset: loss:0.22198087010503514,acc:0.9268930554389954
node16 weight score:4144.501278712358
node18: train data size:801
node18 epoch0:node_model train_loss:0.06270211706992591,train_acc:0.9766666889190674
node18 epoch1:node_model train_loss:0.024511559706297703,train_acc:0.9900000095367432
node18_model on test-dataset: loss:0.09435759276966564,acc:0.9719960689544678
node18 weight score:8488.982990010189
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.056023762375116345,acc:0.9826969438791275
total cost energy:6.070498666823701 | all_enery_cp：4.4675 | all_enery_tp: 1.6029986668237004
ef: 30.78207045217957
reward: 24.711571785355872
step 315:loss:15.998005867004395|running q:30.368070602416992
episode5,iteration15 selected nodes:[1, 17, 7, 15, 9],center node:7
################################################## episode5,iteration15 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.08670740952687477,train_acc:0.9722753167152405
node1 epoch1:node_model train_loss:0.06460053748937684,train_acc:0.9792144298553467
node1_model on test-dataset: loss:0.10976644764712545,acc:0.9676907658576965
node1 weight score:60646.947611903815
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07228076729822804,train_acc:0.9798099398612976
node7 epoch1:node_model train_loss:0.04722267442156334,train_acc:0.9835134148597717
node7_model on test-dataset: loss:0.1054023811629304,acc:0.9682968854904175
node7 weight score:34505.86182088189
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0884628943379291,train_acc:0.9681817293167114
node9 epoch1:node_model train_loss:0.061374275500632146,train_acc:0.9840907454490662
node9_model on test-dataset: loss:0.1244257193109297,acc:0.9635972380638123
node9 weight score:17078.46265039303
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07957799343525299,train_acc:0.9741352796554565
node15 epoch1:node_model train_loss:0.04187785378391189,train_acc:0.9847744107246399
node15_model on test-dataset: loss:0.09044234639586649,acc:0.9716939926147461
node15 weight score:15214.112136998778
node17: train data size:719
node17 epoch0:node_model train_loss:0.07166648446582258,train_acc:0.9799999594688416
node17 epoch1:node_model train_loss:0.04682290245546028,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.1784133192580339,acc:0.9419646263122559
node17 weight score:4029.9681828133675
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051064674485533035,acc:0.9827979546785355
total cost energy:9.048452350444848 | all_enery_cp：7.257 | all_enery_tp: 1.7914523504448479
ef: 30.314853746023335
reward: 21.266401395578487
step 316:loss:36.963504791259766|running q:32.18026351928711
episode5,iteration16 selected nodes:[0, 5, 18, 8, 1],center node:5
################################################## episode5,iteration16 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07007238982866208,train_acc:0.97756427526474
node0 epoch1:node_model train_loss:0.0509508841464089,train_acc:0.9820086359977722
node0_model on test-dataset: loss:0.10392040035658283,acc:0.9700948596000671
node0 weight score:68947.0015070639
node1: train data size:6657
node1 epoch0:node_model train_loss:0.07704421221070103,train_acc:0.9740665555000305
node1 epoch1:node_model train_loss:0.052569951177505195,train_acc:0.9827233552932739
node1_model on test-dataset: loss:0.07075364933814854,acc:0.9773980975151062
node1 weight score:94087.01971236299
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07200765071854907,train_acc:0.9737948775291443
node5 epoch1:node_model train_loss:0.0529172629193992,train_acc:0.9831218123435974
node5_model on test-dataset: loss:0.14865264938875045,acc:0.9550937414169312
node5 weight score:32538.94242645129
node8: train data size:2290
node8 epoch0:node_model train_loss:0.10093700488948304,train_acc:0.9738646149635315
node8 epoch1:node_model train_loss:0.048430123184200216,train_acc:0.9843476414680481
node8_model on test-dataset: loss:0.0893616727006156,acc:0.9716971516609192
node8 weight score:25626.1989149653
node18: train data size:801
node18 epoch0:node_model train_loss:0.04311965142389656,train_acc:0.9855555295944214
node18 epoch1:node_model train_loss:0.05467874473995633,train_acc:0.9811111092567444
node18_model on test-dataset: loss:0.08754309642012231,acc:0.9737970232963562
node18 weight score:9149.779168833298
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05281420576371602,acc:0.9827979588508606
total cost energy:12.258889674749785 | all_enery_cp：10.875 | all_enery_tp: 1.3838896747497846
ef: 31.150627310227296
reward: 18.89173763547751
step 317:loss:14.22073745727539|running q:34.00330352783203
episode5,iteration17 selected nodes:[5, 4, 6, 9, 7],center node:5
################################################## episode5,iteration17 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07824903765563355,train_acc:0.9758093357086182
node4 epoch1:node_model train_loss:0.04082315997762043,train_acc:0.9876649379730225
node4_model on test-dataset: loss:0.08242326484156365,acc:0.9730968475341797
node4 weight score:52145.47140618293
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06833178698256308,train_acc:0.9763871431350708
node5 epoch1:node_model train_loss:0.041645906661276,train_acc:0.9846113324165344
node5_model on test-dataset: loss:0.07389930165212717,acc:0.9766958951950073
node5 weight score:65453.933824295724
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07497817792722748,train_acc:0.977499783039093
node6 epoch1:node_model train_loss:0.050334639510967664,train_acc:0.9839175343513489
node6_model on test-dataset: loss:0.070374908602098,acc:0.9778972268104553
node6 weight score:50145.71343819542
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06716816497312204,train_acc:0.9814315438270569
node7 epoch1:node_model train_loss:0.04786081676610519,train_acc:0.9827025532722473
node7_model on test-dataset: loss:0.10392117664156103,acc:0.9700928926467896
node7 weight score:34997.67917894667
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06477591169955717,train_acc:0.9759089946746826
node9 epoch1:node_model train_loss:0.04085558297281915,train_acc:0.9890908598899841
node9_model on test-dataset: loss:0.08841626290231944,acc:0.9739980101585388
node9 weight score:24034.04000854072
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.049368585915799484,acc:0.9834979557991028
total cost energy:10.560213595499958 | all_enery_cp：9.213 | all_enery_tp: 1.3472135954999582
ef: 31.792139244453196
reward: 21.231925648953236
step 318:loss:10.63484001159668|running q:35.807918548583984
episode5,iteration18 selected nodes:[0, 8, 7, 4, 15],center node:8
################################################## episode5,iteration18 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.05756325071464138,train_acc:0.9817951917648315
node0 epoch1:node_model train_loss:0.03801795083563775,train_acc:0.9874894022941589
node0_model on test-dataset: loss:0.11787492925403058,acc:0.9638000130653381
node0 weight score:60784.76606809927
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06737214749743946,train_acc:0.978827714920044
node4 epoch1:node_model train_loss:0.04955332066733823,train_acc:0.9834787845611572
node4_model on test-dataset: loss:0.11969456060865923,acc:0.9623899459838867
node4 weight score:35908.06447798651
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0648224623622121,train_acc:0.9789991974830627
node7 epoch1:node_model train_loss:0.03882710126581023,train_acc:0.9875676035881042
node7_model on test-dataset: loss:0.11197099316166714,acc:0.9662988185882568
node7 weight score:32481.626690126686
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08495615232411934,train_acc:0.973236620426178
node8 epoch1:node_model train_loss:0.048352472990265356,train_acc:0.9843476414680481
node8_model on test-dataset: loss:0.08096186718365062,acc:0.9748989939689636
node8 weight score:28284.92078629384
node15: train data size:1376
node15 epoch0:node_model train_loss:0.08216481432983917,train_acc:0.9762030243873596
node15 epoch1:node_model train_loss:0.037532102749017734,train_acc:0.9850000143051147
node15_model on test-dataset: loss:0.08693874738308295,acc:0.9737950563430786
node15 weight score:15827.235167500816
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.046185848488530606,acc:0.9837969422340394
total cost energy:10.958751924078562 | all_enery_cp：9.383000000000001 | all_enery_tp: 1.575751924078562
ef: 31.040289184376192
reward: 20.08153726029763
step 319:loss:26.492082595825195|running q:37.600318908691406
episode5,iteration19 selected nodes:[6, 0, 10, 7, 8],center node:6
################################################## episode5,iteration19 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0604198265937157,train_acc:0.9769338965415955
node0 epoch1:node_model train_loss:0.032118880244929135,train_acc:0.9897865653038025
node0_model on test-dataset: loss:0.0646787174171186,acc:0.9804960489273071
node0 weight score:110778.3253306076
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07081727150620686,train_acc:0.9755553007125854
node6 epoch1:node_model train_loss:0.05000836418993357,train_acc:0.9824040532112122
node6_model on test-dataset: loss:0.08998439027025597,acc:0.9722949862480164
node6 weight score:39217.91312249964
node7: train data size:3637
node7 epoch0:node_model train_loss:0.05605745162009387,train_acc:0.9829728603363037
node7 epoch1:node_model train_loss:0.03832112012959614,train_acc:0.9875674843788147
node7_model on test-dataset: loss:0.09532020111415478,acc:0.9716981053352356
node7 weight score:38155.60560603891
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08039181092349083,train_acc:0.9739129543304443
node8 epoch1:node_model train_loss:0.041007837341369494,train_acc:0.9850240349769592
node8_model on test-dataset: loss:0.06593264488852583,acc:0.979498028755188
node8 weight score:34732.41523788052
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0834303090814501,train_acc:0.9734998941421509
node10 epoch1:node_model train_loss:0.06204781223204918,train_acc:0.9819998741149902
node10_model on test-dataset: loss:0.07708673797780648,acc:0.9766988754272461
node10 weight score:24842.146006377057
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04420402776886476,acc:0.9851979541778565
total cost energy:10.469046962275165 | all_enery_cp：9.267999999999999 | all_enery_tp: 1.2010469622751656
ef: 31.9940001819564
reward: 21.524953219681237
step 320:loss:20.581787109375|running q:39.320037841796875
episode5,iteration20 selected nodes:[19, 18, 13, 12, 14],center node:12
################################################## episode5,iteration20 ##################################################
node12: train data size:1406
node12 epoch0:node_model train_loss:0.11010433062911033,train_acc:0.9635555148124695
node12 epoch1:node_model train_loss:0.07674409900015841,train_acc:0.9733332395553589
node12_model on test-dataset: loss:0.10558074596134247,acc:0.9688982367515564
node12 weight score:13316.82199437003
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07083612434904683,train_acc:0.9790908694267273
node13 epoch1:node_model train_loss:0.05323047944429246,train_acc:0.9845454692840576
node13_model on test-dataset: loss:0.07881696098396787,acc:0.9743960499763489
node13 weight score:13398.131402386862
node14: train data size:1540
node14 epoch0:node_model train_loss:0.09629478460556129,train_acc:0.9774999022483826
node14 epoch1:node_model train_loss:0.05539913778193295,train_acc:0.9799998998641968
node14_model on test-dataset: loss:0.08282816597202328,acc:0.9739977121353149
node14 weight score:18592.709157898815
node18: train data size:801
node18 epoch0:node_model train_loss:0.09593046746320194,train_acc:0.9833332896232605
node18 epoch1:node_model train_loss:0.3070631628442142,train_acc:0.8788888454437256
node18_model on test-dataset: loss:0.1744326161724166,acc:0.953191876411438
node18 weight score:4592.031109642119
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0804561813490401,train_acc:0.9778904914855957
node19 epoch1:node_model train_loss:0.05601822111980412,train_acc:0.9839251637458801
node19_model on test-dataset: loss:0.0904869897669414,acc:0.9700949788093567
node19 weight score:63887.63749230208
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.049761351149354593,acc:0.9838959354162217
total cost energy:6.684744663833897 | all_enery_cp：5.292 | all_enery_tp: 1.3927446638338976
ef: 30.782689500737938
reward: 24.09794483690404
step 321:loss:17.820331573486328|running q:41.03081512451172
episode5,iteration21 selected nodes:[10, 3, 8, 13, 17],center node:10
################################################## episode5,iteration21 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08711106597298854,train_acc:0.9708911776542664
node3 epoch1:node_model train_loss:0.04808860995169533,train_acc:0.9851018190383911
node3_model on test-dataset: loss:0.07234788182307966,acc:0.9767938256263733
node3 weight score:51998.75801754138
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07720229491267515,train_acc:0.9729467034339905
node8 epoch1:node_model train_loss:0.04478768935508054,train_acc:0.9860868453979492
node8_model on test-dataset: loss:0.10857129467534833,acc:0.9665939807891846
node8 weight score:21092.13127509988
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06691667545819655,train_acc:0.9766664505004883
node10 epoch1:node_model train_loss:0.04899234337208327,train_acc:0.9839998483657837
node10_model on test-dataset: loss:0.0696186607942218,acc:0.976897120475769
node10 weight score:27506.99278258655
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07070330297574401,train_acc:0.9731168746948242
node13 epoch1:node_model train_loss:0.03611756789244034,train_acc:0.9918181896209717
node13_model on test-dataset: loss:0.07681898105365689,acc:0.9755947589874268
node13 weight score:13746.602539057372
node17: train data size:719
node17 epoch0:node_model train_loss:0.08104727556928992,train_acc:0.9746710062026978
node17 epoch1:node_model train_loss:0.05660281493328512,train_acc:0.975921094417572
node17_model on test-dataset: loss:0.058797012115246614,acc:0.9800990223884583
node17 weight score:12228.512540581236
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04738794761353347,acc:0.9841969460248947
total cost energy:6.278106781186548 | all_enery_cp：4.8709999999999996 | all_enery_tp: 1.4071067811865479
ef: 31.605340872096935
reward: 25.327234090910387
step 322:loss:14.030628204345703|running q:42.77089309692383
episode5,iteration22 selected nodes:[3, 8, 10, 1, 17],center node:8
################################################## episode5,iteration22 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.06494515816639386,train_acc:0.9811940789222717
node1 epoch1:node_model train_loss:0.04381670125190224,train_acc:0.9855589866638184
node1_model on test-dataset: loss:0.09286844976868451,acc:0.9718981981277466
node1 weight score:71682.04074237448
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06348303661338593,train_acc:0.9790490865707397
node3 epoch1:node_model train_loss:0.047002165725356655,train_acc:0.9841510653495789
node3_model on test-dataset: loss:0.0855665494492132,acc:0.9726947546005249
node3 weight score:43965.7789663808
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06964626452764092,train_acc:0.9782125353813171
node8 epoch1:node_model train_loss:0.034695795048838074,train_acc:0.9895169138908386
node8_model on test-dataset: loss:0.07927662409529149,acc:0.9744970202445984
node8 weight score:28886.194715448422
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06399401221424342,train_acc:0.9829998016357422
node10 epoch1:node_model train_loss:0.03075794525211677,train_acc:0.9924999475479126
node10_model on test-dataset: loss:0.08048418948281323,acc:0.9743970036506653
node10 weight score:23793.493011555187
node17: train data size:719
node17 epoch0:node_model train_loss:0.07596041669603437,train_acc:0.9771710634231567
node17 epoch1:node_model train_loss:0.029677236278075725,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.06869271289440804,acc:0.9782979488372803
node17 weight score:10466.903543396531
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05316958811688892,acc:0.9827979570627212
total cost energy:8.992610255092798 | all_enery_cp：7.671499999999999 | all_enery_tp: 1.321110255092798
ef: 31.64174203760791
reward: 22.649131782515113
step 323:loss:16.070640563964844|running q:44.46409225463867
episode5,iteration23 selected nodes:[16, 13, 9, 5, 18],center node:16
################################################## episode5,iteration23 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05935901774055496,train_acc:0.9814891219139099
node5 epoch1:node_model train_loss:0.04037707714763071,train_acc:0.9880198836326599
node5_model on test-dataset: loss:0.06659448675811291,acc:0.9787982106208801
node5 weight score:72633.640342768
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06310787619176236,train_acc:0.9772725105285645
node9 epoch1:node_model train_loss:0.03295837998898192,train_acc:0.9890908002853394
node9_model on test-dataset: loss:0.0887781649095632,acc:0.9728947877883911
node9 weight score:23936.065835160043
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06153302080929279,train_acc:0.9801948070526123
node13 epoch1:node_model train_loss:0.051829772874374284,train_acc:0.9854544997215271
node13_model on test-dataset: loss:0.07866609745513414,acc:0.9746969938278198
node13 weight score:13423.825944871252
node16: train data size:920
node16 epoch0:node_model train_loss:0.0667902061715722,train_acc:0.9789999127388
node16 epoch1:node_model train_loss:0.046486358251422645,train_acc:0.9760000109672546
node16_model on test-dataset: loss:0.09203791993204505,acc:0.97049480676651
node16 weight score:9995.87996642329
node18: train data size:801
node18 epoch0:node_model train_loss:0.035179267120030194,train_acc:0.9899998903274536
node18 epoch1:node_model train_loss:0.02258273816874458,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.11685305630505355,acc:0.9680910110473633
node18 weight score:6854.76294183466
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0476097494843998,acc:0.9838979542255402
total cost energy:6.417147131067164 | all_enery_cp：4.8694999999999995 | all_enery_tp: 1.5476471310671644
ef: 31.400019961555138
reward: 24.982872830487974
step 324:loss:21.478025436401367|running q:46.0716438293457
episode5,iteration24 selected nodes:[14, 5, 19, 13, 7],center node:7
################################################## episode5,iteration24 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05276068069078788,train_acc:0.9829785823822021
node5 epoch1:node_model train_loss:0.029151986875780384,train_acc:0.9902040958404541
node5_model on test-dataset: loss:0.05906228274892783,acc:0.9816969633102417
node5 weight score:81896.59753860101
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0484495354828903,train_acc:0.9837837219238281
node7 epoch1:node_model train_loss:0.03954231636472852,train_acc:0.9862161874771118
node7_model on test-dataset: loss:0.12445874352273677,acc:0.9659969806671143
node7 weight score:29222.535091201316
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07317322412167084,train_acc:0.9753246307373047
node13 epoch1:node_model train_loss:0.045471912673251194,train_acc:0.9838311076164246
node13_model on test-dataset: loss:0.08237510423059576,acc:0.9720980525016785
node13 weight score:12819.407148109933
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08424674643902108,train_acc:0.9728124141693115
node14 epoch1:node_model train_loss:0.051713835273403674,train_acc:0.9862498641014099
node14_model on test-dataset: loss:0.06605004444878432,acc:0.978597104549408
node14 weight score:23315.654256586113
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06813451708776169,train_acc:0.979878842830658
node19 epoch1:node_model train_loss:0.04170704767075849,train_acc:0.9877184629440308
node19_model on test-dataset: loss:0.12082227653474548,acc:0.9624961018562317
node19 weight score:47847.13685093931
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045301231725970864,acc:0.9866979539394378
total cost energy:10.189065057083834 | all_enery_cp：8.4255 | all_enery_tp: 1.7635650570838348
ef: 31.368995749355854
reward: 21.17993069227202
step 325:loss:30.438194274902344|running q:47.82058334350586
episode5,iteration25 selected nodes:[13, 9, 8, 2, 15],center node:8
################################################## episode5,iteration25 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06601336087655038,train_acc:0.979787290096283
node2 epoch1:node_model train_loss:0.035558283532582016,train_acc:0.989787220954895
node2_model on test-dataset: loss:0.061214052538271065,acc:0.9816949367523193
node2 weight score:75309.50507022592
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06450196443145852,train_acc:0.9794202446937561
node8 epoch1:node_model train_loss:0.038508381431355425,train_acc:0.9895651340484619
node8_model on test-dataset: loss:0.07918865748681128,acc:0.9763970971107483
node8 weight score:28918.2829041065
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05138398880477656,train_acc:0.9840907454490662
node9 epoch1:node_model train_loss:0.03905947959389199,train_acc:0.9899998903274536
node9_model on test-dataset: loss:0.07492582632905397,acc:0.9788960218429565
node9 weight score:28361.3822377824
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06600651703774929,train_acc:0.9787662029266357
node13 epoch1:node_model train_loss:0.040551843206313526,train_acc:0.9856492877006531
node13_model on test-dataset: loss:0.09343448184110457,acc:0.9703938961029053
node13 weight score:11302.037312047623
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05181221946674798,train_acc:0.9804885983467102
node15 epoch1:node_model train_loss:0.055468672254521935,train_acc:0.9826315641403198
node15_model on test-dataset: loss:0.0842667048207295,acc:0.9748950004577637
node15 weight score:16329.106530596244
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04609866858809255,acc:0.9846969449520111
total cost energy:7.524481683258546 | all_enery_cp：5.7284999999999995 | all_enery_tp: 1.7959816832585465
ef: 31.55092353702265
reward: 24.026441853764105
step 326:loss:32.009700775146484|running q:49.49809265136719
episode5,iteration26 selected nodes:[17, 19, 1, 9, 5],center node:5
################################################## episode5,iteration26 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.055987992490739075,train_acc:0.9839174151420593
node1 epoch1:node_model train_loss:0.03928986978161135,train_acc:0.9883584380149841
node1_model on test-dataset: loss:0.07771742052307673,acc:0.9751980900764465
node1 weight score:85656.47129298802
node5: train data size:4837
node5 epoch0:node_model train_loss:0.045602466718635846,train_acc:0.9857749938964844
node5 epoch1:node_model train_loss:0.040161471861433616,train_acc:0.9871429204940796
node5_model on test-dataset: loss:0.06689166848242166,acc:0.978596031665802
node5 weight score:72310.94857906118
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03935331256467511,train_acc:0.9872726202011108
node9 epoch1:node_model train_loss:0.02229768729938025,train_acc:0.9945453405380249
node9_model on test-dataset: loss:0.0775735552351398,acc:0.9749979972839355
node9 weight score:27393.355809962966
node17: train data size:719
node17 epoch0:node_model train_loss:0.10684037965256721,train_acc:0.9730920791625977
node17 epoch1:node_model train_loss:0.05410998209845275,train_acc:0.9846710562705994
node17_model on test-dataset: loss:0.13643592980617542,acc:0.9572959542274475
node17 weight score:5269.872833508232
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06663272891129399,train_acc:0.9786719679832458
node19 epoch1:node_model train_loss:0.038039896147454094,train_acc:0.9890975952148438
node19_model on test-dataset: loss:0.06332112973788753,acc:0.9785991311073303
node19 weight score:91296.53914783202
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04300760362948495,acc:0.985298964381218
total cost energy:11.818095766321573 | all_enery_cp：10.0595 | all_enery_tp: 1.7585957663215734
ef: 31.768822315416955
reward: 19.950726549095382
step 327:loss:13.932427406311035|running q:51.15826416015625
episode5,iteration27 selected nodes:[16, 5, 8, 6, 14],center node:5
################################################## episode5,iteration27 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.033831695710042756,train_acc:0.9867347478866577
node5 epoch1:node_model train_loss:0.024228205115591386,train_acc:0.9916934370994568
node5_model on test-dataset: loss:0.11513912244343373,acc:0.9661948084831238
node5 weight score:42010.04747431831
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05480623466428369,train_acc:0.9802775979042053
node6 epoch1:node_model train_loss:0.03688255222449596,train_acc:0.9872221350669861
node6_model on test-dataset: loss:0.061079940192066716,acc:0.979095995426178
node6 weight score:57776.74288650269
node8: train data size:2290
node8 epoch0:node_model train_loss:0.06406369509742312,train_acc:0.9817389845848083
node8 epoch1:node_model train_loss:0.037310833556820515,train_acc:0.9895651340484619
node8_model on test-dataset: loss:0.08370199199591298,acc:0.9738991260528564
node8 weight score:27358.96655974229
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07467068993719295,train_acc:0.9724999070167542
node14 epoch1:node_model train_loss:0.04254636517725885,train_acc:0.9862498641014099
node14_model on test-dataset: loss:0.07815220268872508,acc:0.9765908718109131
node14 weight score:19705.138780716334
node16: train data size:920
node16 epoch0:node_model train_loss:0.0726239737123251,train_acc:0.9779998660087585
node16 epoch1:node_model train_loss:0.03164817733340897,train_acc:0.9869999289512634
node16_model on test-dataset: loss:0.08241878702130634,acc:0.9735968708992004
node16 weight score:11162.503517094567
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04579473238831269,acc:0.9852979570627213
total cost energy:7.823685424949238 | all_enery_cp：6.558 | all_enery_tp: 1.265685424949238
ef: 31.49970599760046
reward: 23.676020572651225
step 328:loss:29.4178524017334|running q:52.80552673339844
episode5,iteration28 selected nodes:[10, 1, 6, 4, 0],center node:0
################################################## episode5,iteration28 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04811452031006209,train_acc:0.9836114048957825
node0 epoch1:node_model train_loss:0.033199754736011125,train_acc:0.9893057942390442
node0_model on test-dataset: loss:0.09558906482652674,acc:0.9690971374511719
node0 weight score:74956.2725925074
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05089159300481317,train_acc:0.9829093217849731
node1 epoch1:node_model train_loss:0.039570482383454356,train_acc:0.985821008682251
node1_model on test-dataset: loss:0.0598399476511986,acc:0.9821971654891968
node1 weight score:111246.75507410239
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05330849643038629,train_acc:0.9823207259178162
node4 epoch1:node_model train_loss:0.03538161244914802,train_acc:0.9883671998977661
node4_model on test-dataset: loss:0.067304637801426,acc:0.9795939326286316
node4 weight score:63858.897995717874
node6: train data size:3529
node6 epoch0:node_model train_loss:0.044221183382129915,train_acc:0.9847221374511719
node6 epoch1:node_model train_loss:0.03891732132372757,train_acc:0.9872220158576965
node6_model on test-dataset: loss:0.06747426624351646,acc:0.9768968820571899
node6 weight score:52301.420918958094
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05445711216889322,train_acc:0.9859998822212219
node10 epoch1:node_model train_loss:0.03398937625461258,train_acc:0.9889999628067017
node10_model on test-dataset: loss:0.058624880738789215,acc:0.9806990027427673
node10 weight score:32665.311653810124
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04296399598530115,acc:0.9856979525089264
total cost energy:13.430528137423858 | all_enery_cp：11.782 | all_enery_tp: 1.6485281374238572
ef: 32.276501483657235
reward: 18.84597334623338
step 329:loss:14.04483413696289|running q:54.42308807373047
episode5,iteration29 selected nodes:[12, 3, 17, 1, 0],center node:0
################################################## episode5,iteration29 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.047738624974347964,train_acc:0.9840281009674072
node0 epoch1:node_model train_loss:0.028144796613358065,train_acc:0.990000307559967
node0_model on test-dataset: loss:0.08023010980636172,acc:0.9746971130371094
node0 weight score:89305.62375264084
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04454128149507651,train_acc:0.9855225682258606
node1 epoch1:node_model train_loss:0.03438680322576704,train_acc:0.98955237865448
node1_model on test-dataset: loss:0.09186594429571414,acc:0.9732950329780579
node1 weight score:72464.2853348493
node3: train data size:3762
node3 epoch0:node_model train_loss:0.0721090606683375,train_acc:0.979414165019989
node3 epoch1:node_model train_loss:0.031444635303494964,train_acc:0.9884210824966431
node3_model on test-dataset: loss:0.07294698121542752,acc:0.9774969220161438
node3 weight score:51571.702314726856
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04982179955113679,train_acc:0.9819998741149902
node12 epoch1:node_model train_loss:0.029739550842593113,train_acc:0.9919999837875366
node12_model on test-dataset: loss:0.08722329379350413,acc:0.9762969613075256
node12 weight score:16119.547185739395
node17: train data size:719
node17 epoch0:node_model train_loss:0.06424962705932558,train_acc:0.9862499833106995
node17 epoch1:node_model train_loss:0.02562197283259593,train_acc:0.9950000047683716
node17_model on test-dataset: loss:0.06632519144252001,acc:0.9791982173919678
node17 weight score:10840.526568598198
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04385859871537832,acc:0.9855979549884796
total cost energy:11.82616130595898 | all_enery_cp：9.8545 | all_enery_tp: 1.971661305958981
ef: 31.305509335571365
reward: 19.479348029612385
step 330:loss:12.270633697509766|running q:56.104286193847656
episode5,iteration30 selected nodes:[18, 6, 9, 11, 7],center node:11
################################################## episode5,iteration30 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04800676406982044,train_acc:0.9844443202018738
node6 epoch1:node_model train_loss:0.025563089784959123,train_acc:0.9908331632614136
node6_model on test-dataset: loss:0.08345952475961894,acc:0.9764938950538635
node6 weight score:42283.969506946814
node7: train data size:3637
node7 epoch0:node_model train_loss:0.05387833760422025,train_acc:0.9825127124786377
node7 epoch1:node_model train_loss:0.031670666595518183,train_acc:0.9888092279434204
node7_model on test-dataset: loss:0.0661608415824594,acc:0.9791960716247559
node7 weight score:54972.09396085197
node9: train data size:2125
node9 epoch0:node_model train_loss:0.052493583326312626,train_acc:0.9854544997215271
node9 epoch1:node_model train_loss:0.03229362011717802,train_acc:0.987727165222168
node9_model on test-dataset: loss:0.05870874334977998,acc:0.9819962382316589
node9 weight score:36195.630816682504
node11: train data size:1575
node11 epoch0:node_model train_loss:0.07282822037814185,train_acc:0.9760417342185974
node11 epoch1:node_model train_loss:0.0443329659756273,train_acc:0.9835416078567505
node11_model on test-dataset: loss:0.07999107079482201,acc:0.9761928915977478
node11 weight score:19689.697666879503
node18: train data size:801
node18 epoch0:node_model train_loss:0.0345582388755348,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.017237014344169035,train_acc:0.9944443106651306
node18_model on test-dataset: loss:0.11490609422820854,acc:0.9694950580596924
node18 weight score:6970.909640433682
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03949288543204602,acc:0.9872969412803649
total cost energy:7.200504637770997 | all_enery_cp：5.8335 | all_enery_tp: 1.367004637770997
ef: 31.77006303699597
reward: 24.569558399224974
step 331:loss:19.44539451599121|running q:57.7215461730957
episode5,iteration31 selected nodes:[2, 16, 15, 8, 19],center node:15
################################################## episode5,iteration31 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0648946934172195,train_acc:0.9808509945869446
node2 epoch1:node_model train_loss:0.040101947827621345,train_acc:0.9863831400871277
node2_model on test-dataset: loss:0.05344727258838247,acc:0.9834969639778137
node2 weight score:86253.23195634961
node8: train data size:2290
node8 epoch0:node_model train_loss:0.058963936186679035,train_acc:0.9815458655357361
node8 epoch1:node_model train_loss:0.04390749568119645,train_acc:0.9860868453979492
node8_model on test-dataset: loss:0.078951023666923,acc:0.9738970994949341
node8 weight score:29005.32372652958
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06866293041301626,train_acc:0.9812029004096985
node15 epoch1:node_model train_loss:0.02510545768642,train_acc:0.991428554058075
node15_model on test-dataset: loss:0.06732331932704255,acc:0.9784979224205017
node15 weight score:20438.683263902672
node16: train data size:920
node16 epoch0:node_model train_loss:0.05829285173676908,train_acc:0.9819999933242798
node16 epoch1:node_model train_loss:0.02637376746861264,train_acc:0.9879999160766602
node16_model on test-dataset: loss:0.06591362981649582,acc:0.9801950454711914
node16 weight score:13957.65947894675
node19: train data size:5781
node19 epoch0:node_model train_loss:0.058254796040148056,train_acc:0.9832355976104736
node19 epoch1:node_model train_loss:0.036436780590708526,train_acc:0.9897874593734741
node19_model on test-dataset: loss:0.05795688872964092,acc:0.9818001985549927
node19 weight score:99746.55518461985
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040461416929174446,acc:0.9865979546308518
total cost energy:8.769222344609215 | all_enery_cp：7.4885 | all_enery_tp: 1.2807223446092155
ef: 32.598727409029934
reward: 23.82950506442072
step 332:loss:23.331214904785156|running q:59.29616928100586
episode5,iteration32 selected nodes:[15, 16, 12, 18, 13],center node:16
################################################## episode5,iteration32 ##################################################
node12: train data size:1406
node12 epoch0:node_model train_loss:0.050490345891254645,train_acc:0.9819999933242798
node12 epoch1:node_model train_loss:0.039532552294743555,train_acc:0.9886666536331177
node12_model on test-dataset: loss:0.06716507514560362,acc:0.9803979396820068
node12 weight score:20933.498502785962
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05394093992865898,train_acc:0.9854544997215271
node13 epoch1:node_model train_loss:0.042178969978439534,train_acc:0.9842208027839661
node13_model on test-dataset: loss:0.0810432943052001,acc:0.9758960604667664
node13 weight score:13030.072494625163
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05116589038516395,train_acc:0.9797743558883667
node15 epoch1:node_model train_loss:0.0373035232935633,train_acc:0.9869171977043152
node15_model on test-dataset: loss:0.06701432932750322,acc:0.9799976348876953
node15 weight score:20532.9220453047
node16: train data size:920
node16 epoch0:node_model train_loss:0.06535522071644664,train_acc:0.9829999804496765
node16 epoch1:node_model train_loss:0.057611734885722396,train_acc:0.9819998741149902
node16_model on test-dataset: loss:0.06425852187036071,acc:0.9810971617698669
node16 weight score:14317.167174434348
node18: train data size:801
node18 epoch0:node_model train_loss:0.02815314122603417,train_acc:0.9877777099609375
node18 epoch1:node_model train_loss:0.02239873333504268,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.0781784935406904,acc:0.9762960076332092
node18 weight score:10245.784533864098
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04281966944923624,acc:0.9863979554176331
total cost energy:3.3445281539872886 | all_enery_cp：2.7795 | all_enery_tp: 0.5650281539872885
ef: 32.19678780286834
reward: 28.852259648881056
step 333:loss:13.300670623779297|running q:60.91440963745117
episode5,iteration33 selected nodes:[3, 18, 14, 8, 13],center node:8
################################################## episode5,iteration33 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06092954779926099,train_acc:0.9783615469932556
node3 epoch1:node_model train_loss:0.033265583168127034,train_acc:0.9887266755104065
node3_model on test-dataset: loss:0.06453735186310951,acc:0.9815971255302429
node3 weight score:58291.8246782049
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05045074923976284,train_acc:0.9860868453979492
node8 epoch1:node_model train_loss:0.02705273852157204,train_acc:0.9925603866577148
node8_model on test-dataset: loss:0.056619341565565265,acc:0.9834991097450256
node8 weight score:40445.542754116585
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04671599191020836,train_acc:0.9856492877006531
node13 epoch1:node_model train_loss:0.037079696712846104,train_acc:0.9899999499320984
node13_model on test-dataset: loss:0.0724894927139394,acc:0.9781968593597412
node13 weight score:14567.628499860313
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07097414537565783,train_acc:0.9806248545646667
node14 epoch1:node_model train_loss:0.03969701626920141,train_acc:0.9837498664855957
node14_model on test-dataset: loss:0.06260010520898504,acc:0.9793991446495056
node14 weight score:24600.597632525427
node18: train data size:801
node18 epoch0:node_model train_loss:0.02284367006541288,train_acc:0.9922221302986145
node18 epoch1:node_model train_loss:0.01322138534225006,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.07484054392221878,acc:0.9780982136726379
node18 weight score:10702.754924289076
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04237039910076419,acc:0.9854979544878006
total cost energy:6.559269282535372 | all_enery_cp：4.724500000000001 | all_enery_tp: 1.8347692825353703
ef: 31.87876306173702
reward: 25.319493779201647
step 334:loss:17.816444396972656|running q:62.577266693115234
episode5,iteration34 selected nodes:[4, 10, 9, 6, 3],center node:4
################################################## episode5,iteration34 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04394985844479188,train_acc:0.9851018190383911
node3 epoch1:node_model train_loss:0.028866201415535454,train_acc:0.9884210228919983
node3_model on test-dataset: loss:0.06072474974538636,acc:0.9809950590133667
node3 weight score:61951.67564747062
node4: train data size:4298
node4 epoch0:node_model train_loss:0.050875717576398236,train_acc:0.9853394031524658
node4 epoch1:node_model train_loss:0.028026607569826896,train_acc:0.9909256100654602
node4_model on test-dataset: loss:0.07070315149408998,acc:0.9790970683097839
node4 weight score:60789.369486015996
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0421689133056336,train_acc:0.9847220182418823
node6 epoch1:node_model train_loss:0.029584899835754186,train_acc:0.9908332824707031
node6_model on test-dataset: loss:0.05359882741351612,acc:0.9844979643821716
node6 weight score:65840.99261675424
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04867649181026288,train_acc:0.9845454096794128
node9 epoch1:node_model train_loss:0.025673134696923873,train_acc:0.9895454049110413
node9_model on test-dataset: loss:0.05684698518307414,acc:0.9830971956253052
node9 weight score:37381.050079551205
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05981473562424071,train_acc:0.9794999361038208
node10 epoch1:node_model train_loss:0.036973988522368016,train_acc:0.9879998564720154
node10_model on test-dataset: loss:0.06597991453083524,acc:0.9797958731651306
node10 weight score:29023.984247585508
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040605235421680846,acc:0.988098965883255
total cost energy:9.075055127546399 | all_enery_cp：7.814500000000001 | all_enery_tp: 1.260555127546399
ef: 32.68228290753456
reward: 23.607227779988158
step 335:loss:32.326412200927734|running q:64.29822540283203
episode5,iteration35 selected nodes:[5, 1, 0, 6, 9],center node:5
################################################## episode5,iteration35 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.039687288773065016,train_acc:0.9865280985832214
node0 epoch1:node_model train_loss:0.027204555144029047,train_acc:0.9900000691413879
node0_model on test-dataset: loss:0.06805241468391614,acc:0.9798938632011414
node0 weight score:105286.49179135467
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04571088224844034,train_acc:0.9849254488945007
node1 epoch1:node_model train_loss:0.025748370329974526,train_acc:0.9907830953598022
node1_model on test-dataset: loss:0.07594305113932932,acc:0.9773917198181152
node1 weight score:87657.78962168243
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03756104589782047,train_acc:0.98836749792099
node5 epoch1:node_model train_loss:0.018851161630330036,train_acc:0.9934695363044739
node5_model on test-dataset: loss:0.08888279809601954,acc:0.9741970896720886
node5 weight score:54419.97893422098
node6: train data size:3529
node6 epoch0:node_model train_loss:0.038652343009339854,train_acc:0.9868198037147522
node6 epoch1:node_model train_loss:0.0367744751015885,train_acc:0.9890421032905579
node6_model on test-dataset: loss:0.05682396795895329,acc:0.9832959771156311
node6 weight score:62104.07556454291
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04198648399324156,train_acc:0.9881817698478699
node9 epoch1:node_model train_loss:0.024136488664556633,train_acc:0.9927271604537964
node9_model on test-dataset: loss:0.05393735589197604,acc:0.9820960164070129
node9 weight score:39397.55601397814
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040151780186060936,acc:0.9871959346532821
total cost energy:13.480496489063194 | all_enery_cp：12.1565 | all_enery_tp: 1.323996489063195
ef: 32.48775731016055
reward: 19.00726082109736
step 336:loss:10.573858261108398|running q:65.85832977294922
episode5,iteration36 selected nodes:[5, 18, 17, 8, 0],center node:5
################################################## episode5,iteration36 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03491801606853389,train_acc:0.9884724020957947
node0 epoch1:node_model train_loss:0.02060510362369112,train_acc:0.9936113357543945
node0_model on test-dataset: loss:0.07899950956976681,acc:0.9770941138267517
node0 weight score:90696.76557513785
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03550564354209571,train_acc:0.9888362884521484
node5 epoch1:node_model train_loss:0.03288683387907032,train_acc:0.9890402555465698
node5_model on test-dataset: loss:0.08252016171361902,acc:0.9773960709571838
node5 weight score:58615.978199200596
node8: train data size:2290
node8 epoch0:node_model train_loss:0.058048772220702274,train_acc:0.9825602769851685
node8 epoch1:node_model train_loss:0.037844775568531906,train_acc:0.988260805606842
node8_model on test-dataset: loss:0.07554104454175103,acc:0.9766980409622192
node8 weight score:30314.645685556177
node17: train data size:719
node17 epoch0:node_model train_loss:0.06349316949490458,train_acc:0.9837499260902405
node17 epoch1:node_model train_loss:0.02138891612412408,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.10553168781640125,acc:0.9689970016479492
node17 weight score:6813.119498769699
node18: train data size:801
node18 epoch0:node_model train_loss:0.030088579385417487,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.010188184028568989,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.08783100704182288,acc:0.9735919237136841
node18 weight score:9119.786132232142
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04835612342600143,acc:0.9861979550123214
total cost energy:9.341645109765153 | all_enery_cp：7.906000000000001 | all_enery_tp: 1.4356451097651517
ef: 31.268014679441258
reward: 21.926369569676105
step 337:loss:8.328907012939453|running q:67.40760040283203
episode5,iteration37 selected nodes:[3, 16, 4, 2, 8],center node:8
################################################## episode5,iteration37 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.051933403894741166,train_acc:0.983404278755188
node2 epoch1:node_model train_loss:0.03176670132373321,train_acc:0.9897873997688293
node2_model on test-dataset: loss:0.070600108262297,acc:0.9789982438087463
node2 weight score:65297.35029403498
node3: train data size:3762
node3 epoch0:node_model train_loss:0.047368530574634575,train_acc:0.9836841225624084
node3 epoch1:node_model train_loss:0.026326380743596115,train_acc:0.9913157224655151
node3_model on test-dataset: loss:0.06364426576437836,acc:0.9806970953941345
node3 weight score:59109.80282069006
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05088265136501557,train_acc:0.985339343547821
node4 epoch1:node_model train_loss:0.025678375346022984,train_acc:0.9911580681800842
node4_model on test-dataset: loss:0.054684965703490886,acc:0.982296884059906
node4 weight score:78595.64223381475
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05284291060398454,train_acc:0.9847341775894165
node8 epoch1:node_model train_loss:0.02179125957595913,train_acc:0.9938647150993347
node8_model on test-dataset: loss:0.0822774155967636,acc:0.9758960008621216
node8 weight score:27832.668094767883
node16: train data size:920
node16 epoch0:node_model train_loss:0.05455195007962175,train_acc:0.98499995470047
node16 epoch1:node_model train_loss:0.02824632809497416,train_acc:0.9919999241828918
node16_model on test-dataset: loss:0.08908077457075705,acc:0.9755919575691223
node16 weight score:10327.705438497755
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04122040145834035,acc:0.987997955083847
total cost energy:9.579563442988615 | all_enery_cp：7.9399999999999995 | all_enery_tp: 1.639563442988616
ef: 32.11844873285905
reward: 22.538885289870436
step 338:loss:12.663019180297852|running q:68.8969955444336
episode5,iteration38 selected nodes:[18, 14, 2, 12, 6],center node:12
################################################## episode5,iteration38 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04078075895085931,train_acc:0.9876593947410583
node2 epoch1:node_model train_loss:0.02831706297682955,train_acc:0.9895744919776917
node2_model on test-dataset: loss:0.07434921650696197,acc:0.9765979051589966
node2 weight score:62004.68836908759
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04240634588899815,train_acc:0.9847221374511719
node6 epoch1:node_model train_loss:0.023543168247366946,train_acc:0.9918199181556702
node6_model on test-dataset: loss:0.06484077517408877,acc:0.98139488697052
node6 weight score:54425.62940564959
node12: train data size:1406
node12 epoch0:node_model train_loss:0.049901518815507494,train_acc:0.9795554876327515
node12 epoch1:node_model train_loss:0.041532250214368106,train_acc:0.977555513381958
node12_model on test-dataset: loss:0.08654494213638828,acc:0.974291980266571
node12 weight score:16245.894506281493
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05083374696550891,train_acc:0.9831249117851257
node14 epoch1:node_model train_loss:0.03595099107042188,train_acc:0.9874998331069946
node14_model on test-dataset: loss:0.07135697607034672,acc:0.978298008441925
node14 weight score:21581.632025463117
node18: train data size:801
node18 epoch0:node_model train_loss:0.021230757883194604,train_acc:0.9922221302986145
node18 epoch1:node_model train_loss:0.020733042316149093,train_acc:0.9944443106651306
node18_model on test-dataset: loss:0.07754315914819017,acc:0.9776918292045593
node18 weight score:10329.731323806855
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042159479544770874,acc:0.9874979555606842
total cost energy:7.335744663833898 | all_enery_cp：5.9430000000000005 | all_enery_tp: 1.3927446638338978
ef: 31.799003007171923
reward: 24.463258343338026
step 339:loss:22.246723175048828|running q:70.45803833007812
episode5,iteration39 selected nodes:[4, 11, 9, 1, 8],center node:8
################################################## episode5,iteration39 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.043531173567371025,train_acc:0.9864546060562134
node1 epoch1:node_model train_loss:0.027502236668882307,train_acc:0.9919403791427612
node1_model on test-dataset: loss:0.07081188893364015,acc:0.9802948832511902
node1 weight score:94009.63736807622
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05081732194805735,train_acc:0.9846464395523071
node4 epoch1:node_model train_loss:0.02157655910068993,train_acc:0.9939535856246948
node4_model on test-dataset: loss:0.06387158827424173,acc:0.9813949465751648
node4 weight score:67291.26543003638
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03183250550342643,train_acc:0.9921739101409912
node8 epoch1:node_model train_loss:0.029940900271353516,train_acc:0.9899516701698303
node8_model on test-dataset: loss:0.07149026344821323,acc:0.9781977534294128
node8 weight score:32032.33404866176
node9: train data size:2125
node9 epoch0:node_model train_loss:0.044421048053085214,train_acc:0.9877271056175232
node9 epoch1:node_model train_loss:0.023156523831527342,train_acc:0.9909089207649231
node9_model on test-dataset: loss:0.05895771297335159,acc:0.9823989868164062
node9 weight score:36042.78206925161
node11: train data size:1575
node11 epoch0:node_model train_loss:0.08358717814553529,train_acc:0.9781249165534973
node11 epoch1:node_model train_loss:0.03969849395798519,train_acc:0.9874999523162842
node11_model on test-dataset: loss:0.06685197307466297,acc:0.981195867061615
node11 weight score:23559.514066114054
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04091611595213181,acc:0.9867969423532486
total cost energy:9.828562329783654 | all_enery_cp：8.4725 | all_enery_tp: 1.356062329783655
ef: 32.44514898698255
reward: 22.616586657198898
step 340:loss:13.480205535888672|running q:72.02144622802734
episode5,iteration40 selected nodes:[1, 17, 8, 13, 15],center node:15
################################################## episode5,iteration40 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03442656233691291,train_acc:0.9879472255706787
node1 epoch1:node_model train_loss:0.025912749683553938,train_acc:0.9902986884117126
node1_model on test-dataset: loss:0.07626116685765737,acc:0.9773960113525391
node1 weight score:87292.13404805872
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04614828578601389,train_acc:0.984106183052063
node8 epoch1:node_model train_loss:0.035449294459439167,train_acc:0.988260805606842
node8_model on test-dataset: loss:0.0807941543601919,acc:0.9762939810752869
node8 weight score:28343.63473613267
node13: train data size:1056
node13 epoch0:node_model train_loss:0.055999927603724325,train_acc:0.9811038970947266
node13 epoch1:node_model train_loss:0.039159047565507615,train_acc:0.9883766174316406
node13_model on test-dataset: loss:0.08707934950041818,acc:0.9728971123695374
node13 weight score:12126.870561830838
node15: train data size:1376
node15 epoch0:node_model train_loss:0.042488405747073035,train_acc:0.98499995470047
node15 epoch1:node_model train_loss:0.027720951533410698,train_acc:0.9916917681694031
node15_model on test-dataset: loss:0.07261396657058868,acc:0.978897213935852
node15 weight score:18949.52259166807
node17: train data size:719
node17 epoch0:node_model train_loss:0.05331138249312062,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.022574582806555554,train_acc:0.9925000071525574
node17_model on test-dataset: loss:0.06394796379528998,acc:0.9801961779594421
node17 weight score:11243.51671777479
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04640316501629968,acc:0.9846969455480575
total cost energy:7.551703444029738 | all_enery_cp：6.049 | all_enery_tp: 1.5027034440297378
ef: 31.714370589204744
reward: 24.162667145175007
step 341:loss:22.82887840270996|running q:73.5083236694336
episode5,iteration41 selected nodes:[0, 10, 18, 16, 1],center node:16
################################################## episode5,iteration41 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03752298769541085,train_acc:0.987286388874054
node0 epoch1:node_model train_loss:0.023568818918849703,train_acc:0.9928420186042786
node0_model on test-dataset: loss:0.06808367371156691,acc:0.9815972447395325
node0 weight score:105238.1519592225
node1: train data size:6657
node1 epoch0:node_model train_loss:0.028609207157156806,train_acc:0.989439845085144
node1 epoch1:node_model train_loss:0.0212431780893161,train_acc:0.9925374984741211
node1_model on test-dataset: loss:0.08421106331288115,acc:0.9748896360397339
node1 weight score:79051.37090201932
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05305559886619449,train_acc:0.9839999079704285
node10 epoch1:node_model train_loss:0.03308028858155012,train_acc:0.9884999394416809
node10_model on test-dataset: loss:0.09087189015031981,acc:0.9709978103637695
node10 weight score:21073.6235026279
node16: train data size:920
node16 epoch0:node_model train_loss:0.07778449840843678,train_acc:0.9779998660087585
node16 epoch1:node_model train_loss:0.03488608137704432,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.05952835667256295,acc:0.981195867061615
node16 weight score:15454.819373907474
node18: train data size:801
node18 epoch0:node_model train_loss:0.02971252053976059,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.021277295799437625,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.07973128167606774,acc:0.9771928787231445
node18 weight score:10046.245126903928
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04425497783508945,acc:0.9863969433307648
total cost energy:10.75238245098362 | all_enery_cp：8.729 | all_enery_tp: 2.02338245098362
ef: 31.986228177409693
reward: 21.233845726426075
step 342:loss:19.923044204711914|running q:75.01376342773438
episode5,iteration42 selected nodes:[2, 14, 4, 10, 9],center node:9
################################################## episode5,iteration42 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.04360362567859603,train_acc:0.9876593947410583
node2 epoch1:node_model train_loss:0.027717000298311024,train_acc:0.9910640716552734
node2_model on test-dataset: loss:0.06019227925797168,acc:0.981797993183136
node2 weight score:76587.8956043929
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03768907621676146,train_acc:0.9893023371696472
node4 epoch1:node_model train_loss:0.013829537779688402,train_acc:0.9955718517303467
node4_model on test-dataset: loss:0.055286001528729686,acc:0.982798159122467
node4 weight score:77741.19815422932
node9: train data size:2125
node9 epoch0:node_model train_loss:0.048357194480062884,train_acc:0.9836363792419434
node9 epoch1:node_model train_loss:0.023750741749112916,train_acc:0.992272675037384
node9_model on test-dataset: loss:0.06607975147609978,acc:0.9801979064941406
node9 weight score:32158.111259976304
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05338591041509062,train_acc:0.983166515827179
node10 epoch1:node_model train_loss:0.030941156763583423,train_acc:0.9879999160766602
node10_model on test-dataset: loss:0.05930127958105004,acc:0.9815988540649414
node10 weight score:32292.72645597256
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05926623426785227,train_acc:0.9821874499320984
node14 epoch1:node_model train_loss:0.031684756082540844,train_acc:0.9868749380111694
node14_model on test-dataset: loss:0.07215673100567073,acc:0.977893054485321
node14 weight score:21342.430270004512
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039990021004450683,acc:0.9881979531049728
total cost energy:8.606531611330107 | all_enery_cp：7.244 | all_enery_tp: 1.3625316113301074
ef: 32.65237169301494
reward: 24.045840081684837
step 343:loss:15.659466743469238|running q:76.42340087890625
episode5,iteration43 selected nodes:[12, 5, 14, 17, 18],center node:12
################################################## episode5,iteration43 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04304741784379038,train_acc:0.9849585294723511
node5 epoch1:node_model train_loss:0.023451965974586805,train_acc:0.9932653903961182
node5_model on test-dataset: loss:0.06161951488576506,acc:0.9823968410491943
node5 weight score:78497.85914360407
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04912431278535223,train_acc:0.9833333492279053
node12 epoch1:node_model train_loss:0.023084993363590912,train_acc:0.9906666874885559
node12_model on test-dataset: loss:0.05523334150442679,acc:0.9830970764160156
node12 weight score:25455.63896197215
node14: train data size:1540
node14 epoch0:node_model train_loss:0.045743287933873944,train_acc:0.9837498664855957
node14 epoch1:node_model train_loss:0.04275473648158368,train_acc:0.9868748784065247
node14_model on test-dataset: loss:0.05612598386818718,acc:0.9822978973388672
node14 weight score:27438.27179255719
node17: train data size:719
node17 epoch0:node_model train_loss:0.041440194712777156,train_acc:0.987500011920929
node17 epoch1:node_model train_loss:0.01748206796037266,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.07231349108657013,acc:0.9788979291915894
node17 weight score:9942.819648124148
node18: train data size:801
node18 epoch0:node_model train_loss:0.049841611607310675,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.01738367064131631,train_acc:0.995555579662323
node18_model on test-dataset: loss:0.16285212844342337,acc:0.9602959752082825
node18 weight score:4918.5724967560145
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0420420418806043,acc:0.9857969444990158
total cost energy:5.950694174058496 | all_enery_cp：4.6515 | all_enery_tp: 1.2991941740584956
ef: 31.953250277896
reward: 26.002556103837506
step 344:loss:12.823511123657227|running q:77.8891372680664
episode5,iteration44 selected nodes:[13, 1, 0, 7, 14],center node:7
################################################## episode5,iteration44 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03261750688802244,train_acc:0.9890921115875244
node0 epoch1:node_model train_loss:0.018514555009864528,train_acc:0.9939534068107605
node0_model on test-dataset: loss:0.07021128974985913,acc:0.9791911840438843
node0 weight score:102049.11525662972
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03467756796475333,train_acc:0.9889553189277649
node1 epoch1:node_model train_loss:0.02737242883460513,train_acc:0.9900002479553223
node1_model on test-dataset: loss:0.08521641657876898,acc:0.9749961495399475
node1 weight score:78118.75067342998
node7: train data size:3637
node7 epoch0:node_model train_loss:0.05249112376603424,train_acc:0.9860261678695679
node7 epoch1:node_model train_loss:0.03526337643632212,train_acc:0.9898101687431335
node7_model on test-dataset: loss:0.08240473909201683,acc:0.9757950901985168
node7 weight score:44135.81112050804
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06781370734626596,train_acc:0.9809090495109558
node13 epoch1:node_model train_loss:0.03455489243126728,train_acc:0.9901946783065796
node13_model on test-dataset: loss:0.05307286800612929,acc:0.9822958111763
node13 weight score:19897.172315580243
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04648351621290203,train_acc:0.9853124022483826
node14 epoch1:node_model train_loss:0.026751730387331918,train_acc:0.9918749332427979
node14_model on test-dataset: loss:0.06812404571985098,acc:0.9791979193687439
node14 weight score:22605.821244571976
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04324379590143508,acc:0.9863959324359893
total cost energy:11.797390871741394 | all_enery_cp：10.0275 | all_enery_tp: 1.7698908717413941
ef: 31.952474172389827
reward: 20.155083300648435
step 345:loss:12.131636619567871|running q:79.31939697265625
episode5,iteration45 selected nodes:[16, 17, 12, 4, 15],center node:12
################################################## episode5,iteration45 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.0405361196351086,train_acc:0.9872092008590698
node4 epoch1:node_model train_loss:0.018618468257468625,train_acc:0.9939439296722412
node4_model on test-dataset: loss:0.05225061368830211,acc:0.983597993850708
node4 weight score:82257.40707352186
node12: train data size:1406
node12 epoch0:node_model train_loss:0.0452051877665023,train_acc:0.9893332719802856
node12 epoch1:node_model train_loss:0.06692173552388946,train_acc:0.9782221913337708
node12_model on test-dataset: loss:0.06417298523764202,acc:0.9816979169845581
node12 weight score:21909.530853105476
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04453712113068572,train_acc:0.9897744059562683
node15 epoch1:node_model train_loss:0.019217352499254048,train_acc:0.9928570985794067
node15_model on test-dataset: loss:0.06638818054198055,acc:0.9798958897590637
node15 weight score:20726.58097821926
node16: train data size:920
node16 epoch0:node_model train_loss:0.04881044500507414,train_acc:0.9860000014305115
node16 epoch1:node_model train_loss:0.024181179582956246,train_acc:0.9929999709129333
node16_model on test-dataset: loss:0.07613581645637169,acc:0.9801939725875854
node16 weight score:12083.668932967837
node17: train data size:719
node17 epoch0:node_model train_loss:0.05904464769992046,train_acc:0.9809210300445557
node17 epoch1:node_model train_loss:0.02549519605236128,train_acc:0.9924999475479126
node17_model on test-dataset: loss:0.08159756081360683,acc:0.9749981760978699
node17 weight score:8811.53790420783
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03776303377802833,acc:0.987997955083847
total cost energy:5.580590483770943 | all_enery_cp：4.3595 | all_enery_tp: 1.2210904837709435
ef: 32.16457123819423
reward: 26.583980754423283
step 346:loss:15.013065338134766|running q:80.93397521972656
episode5,iteration46 selected nodes:[4, 16, 15, 0, 14],center node:15
################################################## episode5,iteration46 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.027404761669054603,train_acc:0.9916667938232422
node0 epoch1:node_model train_loss:0.016850878002717055,train_acc:0.9936113357543945
node0_model on test-dataset: loss:0.06724870302074124,acc:0.9788939952850342
node0 weight score:106544.8057457722
node4: train data size:4298
node4 epoch0:node_model train_loss:0.0234736877149729,train_acc:0.9927811026573181
node4 epoch1:node_model train_loss:0.015156008781009722,train_acc:0.9953441023826599
node4_model on test-dataset: loss:0.0856878928137121,acc:0.9769991040229797
node4 weight score:50158.77808249963
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06569000877789222,train_acc:0.981874942779541
node14 epoch1:node_model train_loss:0.023145042432588525,train_acc:0.9909374713897705
node14_model on test-dataset: loss:0.05920507002971135,acc:0.9812958240509033
node14 weight score:26011.285844728664
node15: train data size:1376
node15 epoch0:node_model train_loss:0.039890149708038995,train_acc:0.9852631092071533
node15 epoch1:node_model train_loss:0.024852433591149747,train_acc:0.9926316142082214
node15_model on test-dataset: loss:0.05940364920070351,acc:0.982196033000946
node15 weight score:23163.560126600845
node16: train data size:920
node16 epoch0:node_model train_loss:0.08422475224360823,train_acc:0.9750000238418579
node16 epoch1:node_model train_loss:0.029525130265392364,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.06574623092310503,acc:0.9808949828147888
node16 weight score:13993.197588406345
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042830716347016275,acc:0.9864959335327148
total cost energy:9.832187565679188 | all_enery_cp：7.6495 | all_enery_tp: 2.182687565679189
ef: 32.194739914968224
reward: 22.362552349289036
step 347:loss:26.98650360107422|running q:82.3783187866211
episode5,iteration47 selected nodes:[13, 19, 17, 5, 0],center node:17
################################################## episode5,iteration47 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.027054629037997477,train_acc:0.9909723997116089
node0 epoch1:node_model train_loss:0.011905270657532836,train_acc:0.996314287185669
node0_model on test-dataset: loss:0.057578676833636565,acc:0.983495831489563
node0 weight score:124438.4274529615
node5: train data size:4837
node5 epoch0:node_model train_loss:0.031542174285277724,train_acc:0.9897960424423218
node5 epoch1:node_model train_loss:0.022037201919189026,train_acc:0.9918368458747864
node5_model on test-dataset: loss:0.05968413950380636,acc:0.9839950203895569
node5 weight score:81043.30631576786
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05947578376667066,train_acc:0.9829221367835999
node13 epoch1:node_model train_loss:0.02478606156496839,train_acc:0.9901946783065796
node13_model on test-dataset: loss:0.05808727240677399,acc:0.9806982278823853
node13 weight score:18179.54185565876
node17: train data size:719
node17 epoch0:node_model train_loss:0.037809621237101965,train_acc:0.9925000071525574
node17 epoch1:node_model train_loss:0.02387197274947539,train_acc:0.9925000071525574
node17_model on test-dataset: loss:0.06364741357945604,acc:0.9792980551719666
node17 weight score:11296.60986306091
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06404160234097649,train_acc:0.9847874045372009
node19 epoch1:node_model train_loss:0.035498749998655044,train_acc:0.989310622215271
node19_model on test-dataset: loss:0.06551222437541583,acc:0.9791958332061768
node19 weight score:88243.07303125832
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03916639789211331,acc:0.9885979551076889
total cost energy:11.507956503227085 | all_enery_cp：9.779 | all_enery_tp: 1.7289565032270853
ef: 32.626190364960614
reward: 21.11823386173353
step 348:loss:23.891324996948242|running q:83.86235046386719
episode5,iteration48 selected nodes:[7, 4, 6, 5, 1],center node:6
################################################## episode5,iteration48 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.028915359024410205,train_acc:0.9898875951766968
node1 epoch1:node_model train_loss:0.014767980532296724,train_acc:0.9948495626449585
node1_model on test-dataset: loss:0.06694550685977446,acc:0.9819971919059753
node1 weight score:99439.08579173057
node4: train data size:4298
node4 epoch0:node_model train_loss:0.02720122475740175,train_acc:0.9913811087608337
node4 epoch1:node_model train_loss:0.021789301706607953,train_acc:0.9927907586097717
node4_model on test-dataset: loss:0.06895803909748793,acc:0.9796980619430542
node4 weight score:62327.75839121231
node5: train data size:4837
node5 epoch0:node_model train_loss:0.028682171794756944,train_acc:0.9902042150497437
node5 epoch1:node_model train_loss:0.02364911095533824,train_acc:0.9920408725738525
node5_model on test-dataset: loss:0.06643918640402262,acc:0.9806990027427673
node5 weight score:72803.42011694382
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04090009044416042,train_acc:0.9862642288208008
node6 epoch1:node_model train_loss:0.027108390611829236,train_acc:0.9925000071525574
node6_model on test-dataset: loss:0.05525162932648527,acc:0.9836979508399963
node6 weight score:63871.419594649815
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0523091486176929,train_acc:0.9852154850959778
node7 epoch1:node_model train_loss:0.03311512180654382,train_acc:0.9900802373886108
node7_model on test-dataset: loss:0.05995666521892417,acc:0.9812958836555481
node7 weight score:60660.47847591182
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03831391768824687,acc:0.987897954583168
total cost energy:12.679000000000002 | all_enery_cp：11.479000000000001 | all_enery_tp: 1.2000000000000002
ef: 32.8356328644192
reward: 20.1566328644192
step 349:loss:13.816082954406738|running q:85.23908996582031
episode5,iteration49 selected nodes:[4, 0, 17, 13, 12],center node:12
################################################## episode5,iteration49 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.020313126891172335,train_acc:0.992361307144165
node0 epoch1:node_model train_loss:0.010528180684761738,train_acc:0.9966667890548706
node0_model on test-dataset: loss:0.07252602877300887,acc:0.980696976184845
node0 weight score:98792.11810183257
node4: train data size:4298
node4 epoch0:node_model train_loss:0.024545675089452847,train_acc:0.993948757648468
node4 epoch1:node_model train_loss:0.02183412055912709,train_acc:0.9932557940483093
node4_model on test-dataset: loss:0.0682658005044459,acc:0.9794939756393433
node4 weight score:62959.78320389119
node12: train data size:1406
node12 epoch0:node_model train_loss:0.038170881833260256,train_acc:0.9788888692855835
node12 epoch1:node_model train_loss:0.03209005702350017,train_acc:0.9900000095367432
node12_model on test-dataset: loss:0.08638789458025713,acc:0.9776960611343384
node12 weight score:16275.428482561072
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04974196599373086,train_acc:0.9858441948890686
node13 epoch1:node_model train_loss:0.02832121591729281,train_acc:0.9872727394104004
node13_model on test-dataset: loss:0.05500413279656641,acc:0.9849990606307983
node13 weight score:19198.55738669004
node17: train data size:719
node17 epoch0:node_model train_loss:0.06707452679984272,train_acc:0.9743421077728271
node17 epoch1:node_model train_loss:0.010213244240731001,train_acc:0.997499942779541
node17_model on test-dataset: loss:0.07444883662858956,acc:0.9775998592376709
node17 weight score:9657.639159453196
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04182048109867537,acc:0.9881969469785691
total cost energy:9.061157519268185 | all_enery_cp：7.322 | all_enery_tp: 1.739157519268185
ef: 31.903844441096528
reward: 22.842686921828342
step 350:loss:12.824700355529785|running q:86.59102630615234
episode5,iteration50 selected nodes:[14, 6, 17, 19, 5],center node:17
################################################## episode5,iteration50 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.024480700672233516,train_acc:0.9912245273590088
node5 epoch1:node_model train_loss:0.01634126592175655,train_acc:0.9946939945220947
node5_model on test-dataset: loss:0.06593120617049862,acc:0.9826979637145996
node5 weight score:73364.34870448873
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03676672147897383,train_acc:0.9908331632614136
node6 epoch1:node_model train_loss:0.02647053983592842,train_acc:0.9916952848434448
node6_model on test-dataset: loss:0.05898965614925146,acc:0.9830989837646484
node6 weight score:59824.0476444069
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04959431549650617,train_acc:0.981249988079071
node14 epoch1:node_model train_loss:0.02216831398254726,train_acc:0.994999885559082
node14_model on test-dataset: loss:0.07284588387723488,acc:0.9784001111984253
node14 weight score:21140.52185289314
node17: train data size:719
node17 epoch0:node_model train_loss:0.03853954916121438,train_acc:0.9887499809265137
node17 epoch1:node_model train_loss:0.01355508211418055,train_acc:0.9962499737739563
node17_model on test-dataset: loss:0.06309045381945907,acc:0.9819958209991455
node17 weight score:11396.335839610616
node19: train data size:5781
node19 epoch0:node_model train_loss:0.054881135375110496,train_acc:0.984442412853241
node19 epoch1:node_model train_loss:0.025840125012548704,train_acc:0.9924139380455017
node19_model on test-dataset: loss:0.06091862033179495,acc:0.9811959862709045
node19 weight score:94897.09334376949
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04196236760831198,acc:0.9877989619970322
total cost energy:9.581353043922608 | all_enery_cp：8.203 | all_enery_tp: 1.3783530439226077
ef: 32.56873704240999
reward: 22.987383998487385
step 351:loss:17.686716079711914|running q:88.00695037841797
episode5,iteration51 selected nodes:[12, 14, 17, 6, 5],center node:12
################################################## episode5,iteration51 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.018881122287355688,train_acc:0.9932653307914734
node5 epoch1:node_model train_loss:0.022132568092237473,train_acc:0.9926531314849854
node5_model on test-dataset: loss:0.05415849574493677,acc:0.9848971366882324
node5 weight score:89311.93404595632
node6: train data size:3529
node6 epoch0:node_model train_loss:0.028505042793565534,train_acc:0.9878065586090088
node6 epoch1:node_model train_loss:0.01991319056590631,train_acc:0.9919443130493164
node6_model on test-dataset: loss:0.062099975435894524,acc:0.984096884727478
node6 weight score:56827.71974109665
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04700549067929387,train_acc:0.9839998483657837
node12 epoch1:node_model train_loss:0.03316835177441438,train_acc:0.9886666536331177
node12_model on test-dataset: loss:0.08409587594338518,acc:0.977095901966095
node12 weight score:16719.012486968375
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04856244691472966,train_acc:0.9837499856948853
node14 epoch1:node_model train_loss:0.029610893434437457,train_acc:0.9884374141693115
node14_model on test-dataset: loss:0.06089000408013817,acc:0.9819968938827515
node14 weight score:25291.50758428567
node17: train data size:719
node17 epoch0:node_model train_loss:0.033450870017986745,train_acc:0.9912499785423279
node17 epoch1:node_model train_loss:0.024861114798113704,train_acc:0.9949999451637268
node17_model on test-dataset: loss:0.055129633319884304,acc:0.9839988350868225
node17 weight score:13041.987706104861
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04167800124625501,acc:0.9871979522705078
total cost energy:7.231851461583876 | all_enery_cp：6.015499999999999 | all_enery_tp: 1.2163514615838766
ef: 32.41056415830133
reward: 25.17871269671745
step 352:loss:14.616448402404785|running q:89.38764953613281
episode5,iteration52 selected nodes:[1, 6, 3, 12, 17],center node:6
################################################## episode5,iteration52 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03247779082861354,train_acc:0.9888060688972473
node1 epoch1:node_model train_loss:0.0168320006984564,train_acc:0.9938807487487793
node1_model on test-dataset: loss:0.06108164672788916,acc:0.9822940826416016
node1 weight score:108985.2739179753
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04624900352638705,train_acc:0.9831578135490417
node3 epoch1:node_model train_loss:0.02588271522453349,train_acc:0.9903649091720581
node3_model on test-dataset: loss:0.06339070003213237,acc:0.9816969633102417
node3 weight score:59346.244766078686
node6: train data size:3529
node6 epoch0:node_model train_loss:0.026439015427766006,train_acc:0.9891666769981384
node6 epoch1:node_model train_loss:0.017380989711253077,train_acc:0.9943199157714844
node6_model on test-dataset: loss:0.05405385440237296,acc:0.9832970499992371
node6 weight score:65286.74114024101
node12: train data size:1406
node12 epoch0:node_model train_loss:0.021283874267505117,train_acc:0.9939999580383301
node12 epoch1:node_model train_loss:0.02777060279234623,train_acc:0.9913333058357239
node12_model on test-dataset: loss:0.10163508003286552,acc:0.9737960696220398
node12 weight score:13833.806197086133
node17: train data size:719
node17 epoch0:node_model train_loss:0.023391666733004968,train_acc:0.9925000071525574
node17 epoch1:node_model train_loss:0.026915807320619933,train_acc:0.9912499189376831
node17_model on test-dataset: loss:0.06458692340547714,acc:0.9806990623474121
node17 weight score:11132.28440200059
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03940949478004768,acc:0.9881969457864761
total cost energy:9.557086815591589 | all_enery_cp：8.0365 | all_enery_tp: 1.520586815591588
ef: 32.22122257206946
reward: 22.66413575647787
step 353:loss:12.581363677978516|running q:90.69202423095703
episode5,iteration53 selected nodes:[8, 13, 4, 16, 15],center node:15
################################################## episode5,iteration53 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.029553550280641418,train_acc:0.9916278719902039
node4 epoch1:node_model train_loss:0.014884234118061465,train_acc:0.9941861033439636
node4_model on test-dataset: loss:0.06343093382609367,acc:0.9813939929008484
node4 weight score:67758.73758667456
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04045528810694004,train_acc:0.9869081377983093
node8 epoch1:node_model train_loss:0.030761867640135082,train_acc:0.9913042783737183
node8_model on test-dataset: loss:0.06922564683503879,acc:0.9816970825195312
node8 weight score:33080.225388965366
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04273715725337917,train_acc:0.9858441352844238
node13 epoch1:node_model train_loss:0.027712373401631008,train_acc:0.9899999499320984
node13_model on test-dataset: loss:0.06105010720214523,acc:0.9815949201583862
node13 weight score:17297.266923765426
node15: train data size:1376
node15 epoch0:node_model train_loss:0.032827639154025486,train_acc:0.99097740650177
node15 epoch1:node_model train_loss:0.021120619496546818,train_acc:0.9950000047683716
node15_model on test-dataset: loss:0.06064367932547612,acc:0.9819989800453186
node15 weight score:22689.916167767034
node16: train data size:920
node16 epoch0:node_model train_loss:0.04227008931338787,train_acc:0.9889999628067017
node16 epoch1:node_model train_loss:0.039105590048711746,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.0807610885817121,acc:0.9771930575370789
node16 weight score:11391.624557774088
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04307558979824535,acc:0.9868979543447495
total cost energy:6.502397703836328 | all_enery_cp：4.97 | all_enery_tp: 1.532397703836328
ef: 32.215116788584176
reward: 25.712719084747846
step 354:loss:16.195621490478516|running q:92.033935546875
episode5,iteration54 selected nodes:[0, 4, 9, 5, 3],center node:4
################################################## episode5,iteration54 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.023215307308419142,train_acc:0.9911004304885864
node0 epoch1:node_model train_loss:0.013819475463808177,train_acc:0.9951390027999878
node0_model on test-dataset: loss:0.09893114870550562,acc:0.9744979739189148
node0 weight score:72424.10599444763
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03586534156745888,train_acc:0.9889472126960754
node3 epoch1:node_model train_loss:0.027022112493317475,train_acc:0.9908913373947144
node3_model on test-dataset: loss:0.05991660556501301,acc:0.9834968447685242
node3 weight score:62787.268479653954
node4: train data size:4298
node4 epoch0:node_model train_loss:0.017381343035552606,train_acc:0.9941860437393188
node4 epoch1:node_model train_loss:0.011993684967015979,train_acc:0.9965068697929382
node4_model on test-dataset: loss:0.07328519171696826,acc:0.9796981811523438
node4 weight score:58647.59167990076
node5: train data size:4837
node5 epoch0:node_model train_loss:0.019425977855963558,train_acc:0.9934695363044739
node5 epoch1:node_model train_loss:0.019677233830871232,train_acc:0.9932653903961182
node5_model on test-dataset: loss:0.08093330272630737,acc:0.9772961139678955
node5 weight score:59765.26148151042
node9: train data size:2125
node9 epoch0:node_model train_loss:0.028504520876925777,train_acc:0.9913636445999146
node9 epoch1:node_model train_loss:0.01834051678253507,train_acc:0.9940909147262573
node9_model on test-dataset: loss:0.06821354660802172,acc:0.9792948961257935
node9 weight score:31152.169996540044
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04358115002991326,acc:0.9870989644527435
total cost energy:12.21776406871193 | all_enery_cp：11.0935 | all_enery_tp: 1.1242640687119285
ef: 32.2826726762844
reward: 20.06490860757247
step 355:loss:8.967809677124023|running q:93.3003921508789
episode5,iteration55 selected nodes:[1, 9, 2, 17, 8],center node:8
################################################## episode5,iteration55 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.023270784482818597,train_acc:0.9928726553916931
node1 epoch1:node_model train_loss:0.023549252147994824,train_acc:0.9920897483825684
node1_model on test-dataset: loss:0.0800126955481437,acc:0.9796990752220154
node1 weight score:83199.29674153369
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0495042798088918,train_acc:0.98617023229599
node2 epoch1:node_model train_loss:0.024283675377831816,train_acc:0.9923403859138489
node2_model on test-dataset: loss:0.0735774744817354,acc:0.9789971709251404
node2 weight score:62655.045344677725
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03672954846052048,train_acc:0.9902898073196411
node8 epoch1:node_model train_loss:0.026575122580296644,train_acc:0.9929468035697937
node8_model on test-dataset: loss:0.07842862000090463,acc:0.9769971370697021
node8 weight score:29198.52472188834
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0326282592816576,train_acc:0.9909089207649231
node9 epoch1:node_model train_loss:0.013535153427669271,train_acc:0.9954544901847839
node9_model on test-dataset: loss:0.06946792263199314,acc:0.9808958768844604
node9 weight score:30589.65806789997
node17: train data size:719
node17 epoch0:node_model train_loss:0.04800134856486693,train_acc:0.9834210276603699
node17 epoch1:node_model train_loss:0.03767651176895015,train_acc:0.9887499809265137
node17_model on test-dataset: loss:0.06297352636433061,acc:0.9813950657844543
node17 weight score:11417.496232309695
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.048289048214555805,acc:0.9858979564905167
total cost energy:9.801546962275165 | all_enery_cp：8.2005 | all_enery_tp: 1.6010469622751655
ef: 31.873941552667485
reward: 22.07239459039232
step 356:loss:10.983247756958008|running q:94.58712768554688
episode5,iteration56 selected nodes:[6, 7, 14, 5, 3],center node:5
################################################## episode5,iteration56 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.038385277825383174,train_acc:0.9889472126960754
node3 epoch1:node_model train_loss:0.020343025815428087,train_acc:0.9931579232215881
node3_model on test-dataset: loss:0.0666550960137738,acc:0.9816979169845581
node3 weight score:56439.79567927724
node5: train data size:4837
node5 epoch0:node_model train_loss:0.014647550005715207,train_acc:0.995102047920227
node5 epoch1:node_model train_loss:0.016361808392447323,train_acc:0.9946940541267395
node5_model on test-dataset: loss:0.0718177977385767,acc:0.9802950620651245
node5 weight score:67350.993100723
node6: train data size:3529
node6 epoch0:node_model train_loss:0.026236867808620445,train_acc:0.9899998903274536
node6 epoch1:node_model train_loss:0.018337987016922044,train_acc:0.9933331608772278
node6_model on test-dataset: loss:0.0667162522190938,acc:0.9818970561027527
node6 weight score:52895.65709432972
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04604404326528311,train_acc:0.9862964153289795
node7 epoch1:node_model train_loss:0.027409822267882928,train_acc:0.990971565246582
node7_model on test-dataset: loss:0.08883716634554731,acc:0.9762970805168152
node7 weight score:40940.07215238348
node14: train data size:1540
node14 epoch0:node_model train_loss:0.03999025310622528,train_acc:0.9846873879432678
node14 epoch1:node_model train_loss:0.03494794301514048,train_acc:0.9881249070167542
node14_model on test-dataset: loss:0.06684995862295182,acc:0.9804991483688354
node14 weight score:23036.663473285484
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04525342688579258,acc:0.9866979551315308
total cost energy:9.989074631273695 | all_enery_cp：8.6525 | all_enery_tp: 1.3365746312736946
ef: 32.150509716770316
reward: 22.16143508549662
step 357:loss:12.008331298828125|running q:95.88223266601562
episode5,iteration57 selected nodes:[16, 17, 19, 7, 13],center node:16
################################################## episode5,iteration57 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.035877731008883064,train_acc:0.9897298216819763
node7 epoch1:node_model train_loss:0.016043399674566212,train_acc:0.9951351284980774
node7_model on test-dataset: loss:0.0722019471648673,acc:0.9789968729019165
node7 weight score:50372.60271797387
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04137621905697002,train_acc:0.9845454096794128
node13 epoch1:node_model train_loss:0.0376647706143558,train_acc:0.9883766174316406
node13_model on test-dataset: loss:0.08003801214610576,acc:0.9781949520111084
node13 weight score:13193.730974631402
node16: train data size:920
node16 epoch0:node_model train_loss:0.04209136734716594,train_acc:0.9889999628067017
node16 epoch1:node_model train_loss:0.018991588958306237,train_acc:0.9950000047683716
node16_model on test-dataset: loss:0.0820086845595506,acc:0.9780939221382141
node16 weight score:11218.324070689636
node17: train data size:719
node17 epoch0:node_model train_loss:0.03082529931998579,train_acc:0.9887499213218689
node17 epoch1:node_model train_loss:0.008786228758253856,train_acc:0.997499942779541
node17_model on test-dataset: loss:0.08385449283778143,acc:0.9781962633132935
node17 weight score:8574.376585771297
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04743822795290757,train_acc:0.9866838455200195
node19 epoch1:node_model train_loss:0.03635438277329928,train_acc:0.9891380667686462
node19_model on test-dataset: loss:0.06353884609772649,acc:0.9808980822563171
node19 weight score:90983.71083271613
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04195143372959137,acc:0.9867979544401169
total cost energy:7.113219600745604 | all_enery_cp：6.0565 | all_enery_tp: 1.0567196007456046
ef: 31.89804906910062
reward: 24.784829468355014
step 358:loss:15.6388578414917|running q:97.15321350097656
episode5,iteration58 selected nodes:[9, 1, 4, 3, 14],center node:4
################################################## episode5,iteration58 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0235282615237216,train_acc:0.9920138120651245
node1 epoch1:node_model train_loss:0.014428043595529091,train_acc:0.9952239990234375
node1_model on test-dataset: loss:0.0580086675941493,acc:0.9855961203575134
node1 weight score:114758.71238027573
node3: train data size:3762
node3 epoch0:node_model train_loss:0.035811406235504696,train_acc:0.9890493154525757
node3 epoch1:node_model train_loss:0.019777108360645605,train_acc:0.9927334189414978
node3_model on test-dataset: loss:0.08847326584860297,acc:0.9766950011253357
node3 weight score:42521.319450754774
node4: train data size:4298
node4 epoch0:node_model train_loss:0.026116423643204968,train_acc:0.9916280508041382
node4 epoch1:node_model train_loss:0.015110943205167301,train_acc:0.9941860437393188
node4_model on test-dataset: loss:0.0725481042660249,acc:0.9771909713745117
node4 weight score:59243.45016983169
node9: train data size:2125
node9 epoch0:node_model train_loss:0.033530839025677946,train_acc:0.9890908598899841
node9 epoch1:node_model train_loss:0.023810385514728048,train_acc:0.9936362504959106
node9_model on test-dataset: loss:0.07813183256425874,acc:0.9773969054222107
node9 weight score:27197.62138245401
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06200338708003983,train_acc:0.9809373617172241
node14 epoch1:node_model train_loss:0.019638621131889522,train_acc:0.9937499165534973
node14_model on test-dataset: loss:0.05573744001594605,acc:0.982799232006073
node14 weight score:27629.54307839431
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04350926050852649,acc:0.9874959373474121
total cost energy:10.478048159266773 | all_enery_cp：9.190999999999999 | all_enery_tp: 1.287048159266775
ef: 32.32873535245169
reward: 21.850687193184914
step 359:loss:15.7119779586792|running q:98.4792709350586
episode5,iteration59 selected nodes:[18, 6, 11, 3, 19],center node:11
################################################## episode5,iteration59 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.026742838471699015,train_acc:0.9896773099899292
node3 epoch1:node_model train_loss:0.021665627451607418,train_acc:0.9918420910835266
node3_model on test-dataset: loss:0.058031919095737974,acc:0.983098030090332
node3 weight score:64826.39310607068
node6: train data size:3529
node6 epoch0:node_model train_loss:0.027410966790436458,train_acc:0.9908331632614136
node6 epoch1:node_model train_loss:0.020399054762189433,train_acc:0.9926531314849854
node6_model on test-dataset: loss:0.07733621124294586,acc:0.9809958338737488
node6 weight score:45631.92252739035
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0719410844903905,train_acc:0.978958249092102
node11 epoch1:node_model train_loss:0.018864150425542903,train_acc:0.9962499141693115
node11_model on test-dataset: loss:0.05621737444511382,acc:0.9846969246864319
node11 weight score:28016.249701197
node18: train data size:801
node18 epoch0:node_model train_loss:0.03707658853899273,train_acc:0.9933332800865173
node18 epoch1:node_model train_loss:0.018423491183461413,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.08675150275808846,acc:0.9758949875831604
node18 weight score:9233.269448180446
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03985062577753681,train_acc:0.9882760047912598
node19 epoch1:node_model train_loss:0.020957088539505314,train_acc:0.9939252734184265
node19_model on test-dataset: loss:0.06979172739402656,acc:0.9803973436355591
node19 weight score:82832.16673176645
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03931238301836856,acc:0.9880979514122009
total cost energy:9.207484880797747 | all_enery_cp：7.724 | all_enery_tp: 1.483484880797746
ef: 32.27898191625225
reward: 23.071497035454502
step 360:loss:7.868788719177246|running q:99.76516723632812
episode5_cost time: 13676.332619905472
episode6,iteration0 selected nodes:[2, 9, 4, 19, 5],center node:5
################################################## episode6,iteration0 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.8709681690373319,train_acc:0.7265957593917847
node2 epoch1:node_model train_loss:0.29116541147232056,train_acc:0.8997872471809387
node2_model on test-dataset: loss:0.24129238152876498,acc:0.9245860576629639
node2 weight score:19105.451945031396
node4: train data size:4298
node4 epoch0:node_model train_loss:0.8632771989633871,train_acc:0.7294541597366333
node4 epoch1:node_model train_loss:0.27478804491287057,train_acc:0.9167108535766602
node4_model on test-dataset: loss:0.3370190108381212,acc:0.8907911777496338
node4 weight score:12752.989777376204
node5: train data size:4837
node5 epoch0:node_model train_loss:0.907016711271539,train_acc:0.716039776802063
node5 epoch1:node_model train_loss:0.30445176568262433,train_acc:0.90163254737854
node5_model on test-dataset: loss:0.3327888555126265,acc:0.8810555338859558
node5 weight score:14534.741533183575
node9: train data size:2125
node9 epoch0:node_model train_loss:1.3722071593458003,train_acc:0.5872727036476135
node9 epoch1:node_model train_loss:0.4966034415093335,train_acc:0.845000147819519
node9_model on test-dataset: loss:0.4459317205753177,acc:0.8446827530860901
node9 weight score:4765.30352507428
node19: train data size:5781
node19 epoch0:node_model train_loss:0.7662939290548193,train_acc:0.766330361366272
node19 epoch1:node_model train_loss:0.25644262725936956,train_acc:0.9209641218185425
node19_model on test-dataset: loss:0.26030538336373865,acc:0.9176989793777466
node19 weight score:22208.530324253414
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.325730915106833,acc:0.9223908871412277
total cost energy:12.494072788688028 | all_enery_cp：10.8255 | all_enery_tp: 1.6685727886880275
ef: 27.845848043095184
reward: 15.351775254407157
step 361:loss:15.44417953491211|running q:2.0723869800567627
episode6,iteration1 selected nodes:[17, 3, 1, 19, 7],center node:7
################################################## episode6,iteration1 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.31031850325082666,train_acc:0.9026864767074585
node1 epoch1:node_model train_loss:0.18499316152796816,train_acc:0.9439146518707275
node1_model on test-dataset: loss:0.19841298368293792,acc:0.937689483165741
node1 weight score:33551.23176131368
node3: train data size:3762
node3 epoch0:node_model train_loss:0.3270478750530042,train_acc:0.890407383441925
node3 epoch1:node_model train_loss:0.21662576300533196,train_acc:0.9284633994102478
node3_model on test-dataset: loss:0.45667837854474785,acc:0.8483971953392029
node3 weight score:8237.74493547953
node7: train data size:3637
node7 epoch0:node_model train_loss:0.38817066236122233,train_acc:0.8807012438774109
node7 epoch1:node_model train_loss:0.20171243516174522,train_acc:0.9370781183242798
node7_model on test-dataset: loss:0.20453040600754321,acc:0.9353940486907959
node7 weight score:17782.197136331237
node17: train data size:719
node17 epoch0:node_model train_loss:0.560677245259285,train_acc:0.820921003818512
node17 epoch1:node_model train_loss:0.34709029272198677,train_acc:0.890592098236084
node17_model on test-dataset: loss:0.35463687879033384,acc:0.8894878029823303
node17 weight score:2027.4259193023258
node19: train data size:5781
node19 epoch0:node_model train_loss:0.30458472848966206,train_acc:0.9003149271011353
node19 epoch1:node_model train_loss:0.1790043734784784,train_acc:0.9440165758132935
node19_model on test-dataset: loss:0.22272526990098412,acc:0.9263968467712402
node19 weight score:25955.743605429376
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.14433630717452617,acc:0.9567959392070771
total cost energy:12.258450094062622 | all_enery_cp：10.278 | all_enery_tp: 1.980450094062622
ef: 28.24476413311879
reward: 15.986314039056166
step 362:loss:10.377400398254395|running q:4.250302314758301
episode6,iteration2 selected nodes:[18, 7, 8, 11, 19],center node:11
################################################## episode6,iteration2 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.2176319995039218,train_acc:0.9298903346061707
node7 epoch1:node_model train_loss:0.134247546965206,train_acc:0.9598100185394287
node7_model on test-dataset: loss:0.1351593201525975,acc:0.9583981037139893
node7 weight score:26908.984122543356
node8: train data size:2290
node8 epoch0:node_model train_loss:0.2691055921756703,train_acc:0.91280198097229
node8 epoch1:node_model train_loss:0.1736517645742582,train_acc:0.9474879503250122
node8_model on test-dataset: loss:0.213520053839311,acc:0.9275970458984375
node8 weight score:10724.987928878043
node11: train data size:1575
node11 epoch0:node_model train_loss:0.26291241124272346,train_acc:0.9168750047683716
node11 epoch1:node_model train_loss:0.18465840490534902,train_acc:0.9362499713897705
node11_model on test-dataset: loss:0.2209575699083507,acc:0.92600017786026
node11 weight score:7128.065359576874
node18: train data size:801
node18 epoch0:node_model train_loss:0.263221557950601,train_acc:0.9166666865348816
node18 epoch1:node_model train_loss:0.15596268926229742,train_acc:0.9488887786865234
node18_model on test-dataset: loss:0.437648562816903,acc:0.8608726859092712
node18 weight score:1830.235645798546
node19: train data size:5781
node19 epoch0:node_model train_loss:0.20866531005193448,train_acc:0.9356085658073425
node19 epoch1:node_model train_loss:0.14790047210609092,train_acc:0.9538547992706299
node19_model on test-dataset: loss:0.1833858307194896,acc:0.9423919320106506
node19 weight score:31523.700480669773
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.10919174011796713,acc:0.9653979557752609
total cost energy:8.225232403787835 | all_enery_cp：7.042 | all_enery_tp: 1.183232403787835
ef: 28.990209579642762
reward: 20.764977175854927
step 363:loss:14.713593482971191|running q:6.450896263122559
episode6,iteration3 selected nodes:[8, 14, 15, 17, 6],center node:8
################################################## episode6,iteration3 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.19324792311009434,train_acc:0.9380555748939514
node6 epoch1:node_model train_loss:0.1298487623118692,train_acc:0.9541953206062317
node6_model on test-dataset: loss:0.14996250697644428,acc:0.9496938586235046
node6 weight score:23532.548709353905
node8: train data size:2290
node8 epoch0:node_model train_loss:0.23665679570125497,train_acc:0.9273914098739624
node8 epoch1:node_model train_loss:0.13693962926449982,train_acc:0.9549757242202759
node8_model on test-dataset: loss:0.20116652586730197,acc:0.9349960088729858
node8 weight score:11383.603659341325
node14: train data size:1540
node14 epoch0:node_model train_loss:0.2687014895491302,train_acc:0.9215624928474426
node14 epoch1:node_model train_loss:0.1279073494952172,train_acc:0.9606248736381531
node14_model on test-dataset: loss:0.14227284913460608,acc:0.9555981159210205
node14 weight score:10824.271878768572
node15: train data size:1376
node15 epoch0:node_model train_loss:0.20857749400394304,train_acc:0.9312406182289124
node15 epoch1:node_model train_loss:0.15145431432340825,train_acc:0.9476315975189209
node15_model on test-dataset: loss:0.2918006408493966,acc:0.9048981070518494
node15 weight score:4715.548245523482
node17: train data size:719
node17 epoch0:node_model train_loss:0.32189018093049526,train_acc:0.8992762565612793
node17 epoch1:node_model train_loss:0.21110716927796602,train_acc:0.9324999451637268
node17_model on test-dataset: loss:0.30081498372368515,acc:0.9108896255493164
node17 weight score:2390.1734916916253
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.1217036973964423,acc:0.9610969448089599
total cost energy:6.133839201537814 | all_enery_cp：4.727 | all_enery_tp: 1.4068392015378142
ef: 28.781488198310743
reward: 22.64764899677293
step 364:loss:13.186677932739258|running q:8.531423568725586
episode6,iteration4 selected nodes:[17, 1, 5, 0, 13],center node:5
################################################## episode6,iteration4 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.16855207297744024,train_acc:0.9474039077758789
node0 epoch1:node_model train_loss:0.12088541210525566,train_acc:0.9627031683921814
node0_model on test-dataset: loss:0.1249272971926257,acc:0.9616978168487549
node0 weight score:57353.358001112196
node1: train data size:6657
node1 epoch0:node_model train_loss:0.16660226736940556,train_acc:0.9511544704437256
node1 epoch1:node_model train_loss:0.11923718708219813,train_acc:0.9616783261299133
node1_model on test-dataset: loss:0.11619554459350184,acc:0.96219801902771
node1 weight score:57291.35332416428
node5: train data size:4837
node5 epoch0:node_model train_loss:0.1847176107825065,train_acc:0.9445503950119019
node5 epoch1:node_model train_loss:0.11117426701346222,train_acc:0.9636732935905457
node5_model on test-dataset: loss:0.22683305568061768,acc:0.9244939088821411
node5 weight score:21324.05255259853
node13: train data size:1056
node13 epoch0:node_model train_loss:0.2307909307154742,train_acc:0.9255193471908569
node13 epoch1:node_model train_loss:0.11700277572328394,train_acc:0.9627272486686707
node13_model on test-dataset: loss:0.2818198705313262,acc:0.9072877168655396
node13 weight score:3747.074320945082
node17: train data size:719
node17 epoch0:node_model train_loss:0.16913473140448332,train_acc:0.9330921173095703
node17 epoch1:node_model train_loss:0.12803759053349495,train_acc:0.9562499523162842
node17_model on test-dataset: loss:0.19973794570192693,acc:0.9346908926963806
node17 weight score:3599.7166060422915
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0732017798628658,acc:0.9747989636659622
total cost energy:11.844609936838454 | all_enery_cp：10.217 | all_enery_tp: 1.6276099368384533
ef: 29.337740870979363
reward: 17.493130934140908
step 365:loss:16.470699310302734|running q:10.669661521911621
episode6,iteration5 selected nodes:[7, 0, 9, 18, 5],center node:5
################################################## episode6,iteration5 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.12611052748333249,train_acc:0.9617305397987366
node0 epoch1:node_model train_loss:0.10187661363225844,train_acc:0.9682585597038269
node0_model on test-dataset: loss:0.14065332723999746,acc:0.9548970460891724
node0 weight score:50940.849680536354
node5: train data size:4837
node5 epoch0:node_model train_loss:0.1346452520514021,train_acc:0.9585713744163513
node5 epoch1:node_model train_loss:0.10420716002735557,train_acc:0.9679369926452637
node5_model on test-dataset: loss:0.1353793117992609,acc:0.959290087223053
node5 weight score:35729.24057386446
node7: train data size:3637
node7 epoch0:node_model train_loss:0.13071157974568573,train_acc:0.955215573310852
node7 epoch1:node_model train_loss:0.11116996050082348,train_acc:0.9645946025848389
node7_model on test-dataset: loss:0.140588521376485,acc:0.9561948776245117
node7 weight score:25869.821834603412
node9: train data size:2125
node9 epoch0:node_model train_loss:0.15101559527895667,train_acc:0.9549999237060547
node9 epoch1:node_model train_loss:0.08110024754635313,train_acc:0.9754545092582703
node9_model on test-dataset: loss:0.14843428557971491,acc:0.9538000822067261
node9 weight score:14316.099489418792
node18: train data size:801
node18 epoch0:node_model train_loss:0.12070279325497621,train_acc:0.9599999189376831
node18 epoch1:node_model train_loss:0.1044738996360037,train_acc:0.9699999690055847
node18_model on test-dataset: loss:0.209177036305191,acc:0.9342958331108093
node18 weight score:3829.2922308705747
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06988955257285852,acc:0.9780989688634872
total cost energy:10.911626786466034 | all_enery_cp：9.282499999999999 | all_enery_tp: 1.6291267864660344
ef: 29.705213836429774
reward: 18.79358704996374
step 366:loss:29.847562789916992|running q:12.832511901855469
episode6,iteration6 selected nodes:[13, 9, 11, 8, 5],center node:8
################################################## episode6,iteration6 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.11793716671895615,train_acc:0.966039776802063
node5 epoch1:node_model train_loss:0.08920268368508134,train_acc:0.968753457069397
node5_model on test-dataset: loss:0.11282766639022157,acc:0.96359783411026
node5 weight score:42870.690804424965
node8: train data size:2290
node8 epoch0:node_model train_loss:0.1645111342811066,train_acc:0.9474879503250122
node8 epoch1:node_model train_loss:0.09699239889564722,train_acc:0.9650722146034241
node8_model on test-dataset: loss:0.12078090112889185,acc:0.9607958793640137
node8 weight score:18959.951272065908
node9: train data size:2125
node9 epoch0:node_model train_loss:0.11723406803370877,train_acc:0.9631816148757935
node9 epoch1:node_model train_loss:0.05945752713490616,train_acc:0.9795452952384949
node9_model on test-dataset: loss:0.11736244442639872,acc:0.9615959525108337
node9 weight score:18106.303173777596
node11: train data size:1575
node11 epoch0:node_model train_loss:0.1467558559961617,train_acc:0.9587499499320984
node11 epoch1:node_model train_loss:0.09735141054261476,train_acc:0.9687499403953552
node11_model on test-dataset: loss:0.126910726132337,acc:0.960294246673584
node11 weight score:12410.29854606346
node13: train data size:1056
node13 epoch0:node_model train_loss:0.15283019942316142,train_acc:0.9596753120422363
node13 epoch1:node_model train_loss:0.09244084832343188,train_acc:0.9745454788208008
node13_model on test-dataset: loss:0.20611774172022707,acc:0.9363960027694702
node13 weight score:5123.285318317511
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06564143614959903,acc:0.9792969405651093
total cost energy:7.231032173033948 | all_enery_cp：5.9415 | all_enery_tp: 1.2895321730339486
ef: 30.205517755171282
reward: 22.974485582137333
step 367:loss:18.61524200439453|running q:14.869701385498047
episode6,iteration7 selected nodes:[9, 6, 0, 13, 5],center node:5
################################################## episode6,iteration7 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.10741403740313318,train_acc:0.9645621180534363
node0 epoch1:node_model train_loss:0.0757344877994102,train_acc:0.9751390814781189
node0_model on test-dataset: loss:0.15532089148444356,acc:0.9545980095863342
node0 weight score:46130.30437516915
node5: train data size:4837
node5 epoch0:node_model train_loss:0.10080408859921962,train_acc:0.9667953848838806
node5 epoch1:node_model train_loss:0.0745625134405433,train_acc:0.9753060340881348
node5_model on test-dataset: loss:0.11631705950101605,acc:0.9599968791007996
node5 weight score:41584.61381976173
node6: train data size:3529
node6 epoch0:node_model train_loss:0.1166230503262745,train_acc:0.9612643122673035
node6 epoch1:node_model train_loss:0.07730478669206302,train_acc:0.9752776026725769
node6_model on test-dataset: loss:0.17599750785426294,acc:0.9446921944618225
node6 weight score:20051.420290122715
node9: train data size:2125
node9 epoch0:node_model train_loss:0.11590779225595972,train_acc:0.9672726392745972
node9 epoch1:node_model train_loss:0.074740444987335,train_acc:0.9763633608818054
node9_model on test-dataset: loss:0.1436210580309853,acc:0.9564000964164734
node9 weight score:14795.880417073276
node13: train data size:1056
node13 epoch0:node_model train_loss:0.11587737043472854,train_acc:0.9620779752731323
node13 epoch1:node_model train_loss:0.08109770579771562,train_acc:0.9712986946105957
node13_model on test-dataset: loss:0.19782693734217902,acc:0.9350998401641846
node13 weight score:5337.999031817638
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.06945315508390194,acc:0.9775989663600921
total cost energy:10.857957842230245 | all_enery_cp：9.356 | all_enery_tp: 1.5019578422302464
ef: 29.73518764580662
reward: 18.877229803576373
step 368:loss:15.030343055725098|running q:17.021764755249023
episode6,iteration8 selected nodes:[11, 6, 0, 12, 2],center node:6
################################################## episode6,iteration8 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.09308921788922614,train_acc:0.9686539173126221
node0 epoch1:node_model train_loss:0.07456311433472568,train_acc:0.9754059910774231
node0_model on test-dataset: loss:0.16463970684882953,acc:0.9468958973884583
node0 weight score:43519.27088025508
node2: train data size:4610
node2 epoch0:node_model train_loss:0.11912704930898357,train_acc:0.9619150161743164
node2 epoch1:node_model train_loss:0.09450639454410115,train_acc:0.9687233567237854
node2_model on test-dataset: loss:0.12269161007367074,acc:0.9624879956245422
node2 weight score:37573.8813536794
node6: train data size:3529
node6 epoch0:node_model train_loss:0.10831888961709207,train_acc:0.9657087326049805
node6 epoch1:node_model train_loss:0.07991060345537132,train_acc:0.9709863662719727
node6_model on test-dataset: loss:0.11091333660442615,acc:0.9670939445495605
node6 weight score:31817.634452619746
node11: train data size:1575
node11 epoch0:node_model train_loss:0.12415628519374877,train_acc:0.9618749618530273
node11 epoch1:node_model train_loss:0.07984309736639261,train_acc:0.9758332371711731
node11_model on test-dataset: loss:0.10372890472994185,acc:0.9677991271018982
node11 weight score:15183.810183868341
node12: train data size:1406
node12 epoch0:node_model train_loss:0.11978274658322334,train_acc:0.9548889398574829
node12 epoch1:node_model train_loss:0.09688402190804482,train_acc:0.9575555324554443
node12_model on test-dataset: loss:0.19695495221065357,acc:0.9375941157341003
node12 weight score:7138.688234130868
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0592848154422245,acc:0.9816959363222122
total cost energy:10.190370866461908 | all_enery_cp：9.1425 | all_enery_tp: 1.0478708664619074
ef: 30.373008797990014
reward: 20.182637931528106
step 369:loss:14.990419387817383|running q:19.024627685546875
episode6,iteration9 selected nodes:[7, 6, 16, 9, 8],center node:6
################################################## episode6,iteration9 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.09412687597796321,train_acc:0.9687643051147461
node6 epoch1:node_model train_loss:0.0627290032312481,train_acc:0.9805553555488586
node6_model on test-dataset: loss:0.09245489914843347,acc:0.9722941517829895
node6 weight score:38169.96213834272
node7: train data size:3637
node7 epoch0:node_model train_loss:0.10960550820203246,train_acc:0.9661869406700134
node7 epoch1:node_model train_loss:0.06951602168280531,train_acc:0.9784586429595947
node7_model on test-dataset: loss:0.18652389579219744,acc:0.9418980479240417
node7 weight score:19498.842143271067
node8: train data size:2290
node8 epoch0:node_model train_loss:0.11458098766920359,train_acc:0.9625603556632996
node8 epoch1:node_model train_loss:0.07636411917274413,train_acc:0.9752172827720642
node8_model on test-dataset: loss:0.08708987683625309,acc:0.9720991849899292
node8 weight score:26294.67491733479
node9: train data size:2125
node9 epoch0:node_model train_loss:0.0946626457047056,train_acc:0.9713634252548218
node9 epoch1:node_model train_loss:0.057045124005526304,train_acc:0.9827271699905396
node9_model on test-dataset: loss:0.10573081330127025,acc:0.9649967551231384
node9 weight score:20098.20915635074
node16: train data size:920
node16 epoch0:node_model train_loss:0.12110420130193233,train_acc:0.9549999237060547
node16 epoch1:node_model train_loss:0.08555759405717253,train_acc:0.9729999899864197
node16_model on test-dataset: loss:0.15446928700897844,acc:0.953082799911499
node16 weight score:5955.876522861956
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0579614460535231,acc:0.9810979562997818
total cost energy:7.659738425596913 | all_enery_cp：6.250500000000001 | all_enery_tp: 1.409238425596913
ef: 30.466750558822568
reward: 22.807012133225655
step 370:loss:14.15829849243164|running q:21.02324676513672
episode6,iteration10 selected nodes:[8, 4, 11, 5, 16],center node:8
################################################## episode6,iteration10 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.13568901773108993,train_acc:0.9616087675094604
node4 epoch1:node_model train_loss:0.06933153113133686,train_acc:0.9792880415916443
node4_model on test-dataset: loss:0.12046166521395207,acc:0.9632990956306458
node4 weight score:35679.40051605893
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08197134641018144,train_acc:0.972449004650116
node5 epoch1:node_model train_loss:0.08022885461698989,train_acc:0.9733260869979858
node5_model on test-dataset: loss:0.07343787127640099,acc:0.976898193359375
node5 weight score:65865.19892161354
node8: train data size:2290
node8 epoch0:node_model train_loss:0.10251278478814208,train_acc:0.9671979546546936
node8 epoch1:node_model train_loss:0.06198866487197254,train_acc:0.980724573135376
node8_model on test-dataset: loss:0.21937447755248285,acc:0.9334979057312012
node8 weight score:10438.771298963635
node11: train data size:1575
node11 epoch0:node_model train_loss:0.10416851390618831,train_acc:0.966249942779541
node11 epoch1:node_model train_loss:0.0740090124309063,train_acc:0.9747917056083679
node11_model on test-dataset: loss:0.11235841964662541,acc:0.9662918448448181
node11 weight score:14017.641089590601
node16: train data size:920
node16 epoch0:node_model train_loss:0.09276822060346604,train_acc:0.968999981880188
node16 epoch1:node_model train_loss:0.0630662209354341,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.21513923737627919,acc:0.9332929849624634
node16 weight score:4276.30036817002
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.057627463004319,acc:0.9817999738454819
total cost energy:8.179772400717576 | all_enery_cp：6.96 | all_enery_tp: 1.219772400717577
ef: 30.40242559280618
reward: 22.2226531920886
step 371:loss:24.99533462524414|running q:23.063705444335938
episode6,iteration11 selected nodes:[1, 16, 7, 0, 5],center node:7
################################################## episode6,iteration11 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.08810971763968053,train_acc:0.9716667532920837
node0 epoch1:node_model train_loss:0.058659120268809296,train_acc:0.981592059135437
node0_model on test-dataset: loss:0.1141324452997469,acc:0.9636931419372559
node0 weight score:62777.94172535694
node1: train data size:6657
node1 epoch0:node_model train_loss:0.09731074756205972,train_acc:0.9691411256790161
node1 epoch1:node_model train_loss:0.06637042644781185,train_acc:0.9779104590415955
node1_model on test-dataset: loss:0.10654091834767314,acc:0.9672878980636597
node1 weight score:62483.03565655711
node5: train data size:4837
node5 epoch0:node_model train_loss:0.08583560164029501,train_acc:0.9711804389953613
node5 epoch1:node_model train_loss:0.05434541227486061,train_acc:0.9810811877250671
node5_model on test-dataset: loss:0.07749885494005866,acc:0.9756993651390076
node5 weight score:62413.82538801597
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0994288088451769,train_acc:0.9681883454322815
node7 epoch1:node_model train_loss:0.06957434482771803,train_acc:0.9767565727233887
node7_model on test-dataset: loss:0.10296428651665337,acc:0.9671983122825623
node7 weight score:35322.92723080982
node16: train data size:920
node16 epoch0:node_model train_loss:0.10361158996820449,train_acc:0.9749999046325684
node16 epoch1:node_model train_loss:0.06471805162727833,train_acc:0.9759999513626099
node16_model on test-dataset: loss:0.14196847282437375,acc:0.9556958675384521
node16 weight score:6480.312013626525
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05342586212122114,acc:0.9820969474315643
total cost energy:13.040455532033677 | all_enery_cp：11.608 | all_enery_tp: 1.432455532033676
ef: 31.048429626196064
reward: 18.007974094162385
step 372:loss:17.246807098388672|running q:25.04692840576172
episode6,iteration12 selected nodes:[17, 4, 8, 11, 0],center node:8
################################################## episode6,iteration12 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07222751969109392,train_acc:0.9741562604904175
node0 epoch1:node_model train_loss:0.05739544186184907,train_acc:0.9801390171051025
node0_model on test-dataset: loss:0.07886268567031948,acc:0.9753920435905457
node0 weight score:90854.12117402181
node4: train data size:4298
node4 epoch0:node_model train_loss:0.09511460831692052,train_acc:0.9713810682296753
node4 epoch1:node_model train_loss:0.06316495461519374,train_acc:0.977655291557312
node4_model on test-dataset: loss:0.07888530045456718,acc:0.9744980931282043
node4 weight score:54484.16847287498
node8: train data size:2290
node8 epoch0:node_model train_loss:0.1006512445114229,train_acc:0.9739129543304443
node8 epoch1:node_model train_loss:0.06993417107783582,train_acc:0.9771496653556824
node8_model on test-dataset: loss:0.11365808069356717,acc:0.9642002582550049
node8 weight score:20148.14948506877
node11: train data size:1575
node11 epoch0:node_model train_loss:0.09104055457282811,train_acc:0.9708333015441895
node11 epoch1:node_model train_loss:0.052424916852032766,train_acc:0.9824999570846558
node11_model on test-dataset: loss:0.10054092715057777,acc:0.968697190284729
node11 weight score:15665.262342778675
node17: train data size:719
node17 epoch0:node_model train_loss:0.10722001898102462,train_acc:0.965592086315155
node17 epoch1:node_model train_loss:0.06739393225871027,train_acc:0.9812499284744263
node17_model on test-dataset: loss:0.15094961028822582,acc:0.9561000466346741
node17 weight score:4763.1789086909785
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051415302054956556,acc:0.9825979554653168
total cost energy:9.323889691313216 | all_enery_cp：8.0235 | all_enery_tp: 1.300389691313216
ef: 31.0990351812556
reward: 21.775145489942386
step 373:loss:11.973431587219238|running q:26.977474212646484
episode6,iteration13 selected nodes:[13, 4, 6, 2, 12],center node:6
################################################## episode6,iteration13 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.09039432670366257,train_acc:0.9736171364784241
node2 epoch1:node_model train_loss:0.06341660803337483,train_acc:0.9808509349822998
node2_model on test-dataset: loss:0.14139340992551297,acc:0.9573960900306702
node2 weight score:32604.065510751738
node4: train data size:4298
node4 epoch0:node_model train_loss:0.08952113940532125,train_acc:0.9739391207695007
node4 epoch1:node_model train_loss:0.07006231045653653,train_acc:0.9776695370674133
node4_model on test-dataset: loss:0.08487264277006033,acc:0.9722928404808044
node4 weight score:50640.58169655773
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07699783257622686,train_acc:0.9757087826728821
node6 epoch1:node_model train_loss:0.04445467924233526,train_acc:0.9852775931358337
node6_model on test-dataset: loss:0.08751615372253582,acc:0.9723960757255554
node6 weight score:40323.98420054498
node12: train data size:1406
node12 epoch0:node_model train_loss:0.11649422670404116,train_acc:0.9575555920600891
node12 epoch1:node_model train_loss:0.05359656363337611,train_acc:0.982666552066803
node12_model on test-dataset: loss:0.15131260804948396,acc:0.9532950520515442
node12 weight score:9292.021452304847
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08529216592962091,train_acc:0.9703895449638367
node13 epoch1:node_model train_loss:0.05461539395830848,train_acc:0.9811038970947266
node13_model on test-dataset: loss:0.08467276578041492,acc:0.972296953201294
node13 weight score:12471.542535158998
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04972150248533581,acc:0.9840979552268982
total cost energy:8.810055127546399 | all_enery_cp：7.4495 | all_enery_tp: 1.360555127546399
ef: 30.87102417642428
reward: 22.060969048877883
step 374:loss:15.130536079406738|running q:28.913087844848633
episode6,iteration14 selected nodes:[5, 14, 7, 12, 2],center node:5
################################################## episode6,iteration14 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.08878604138388912,train_acc:0.973404049873352
node2 epoch1:node_model train_loss:0.058488119472848606,train_acc:0.981489360332489
node2_model on test-dataset: loss:0.0645303812646307,acc:0.9803981781005859
node2 weight score:71439.21839071413
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07781946617273652,train_acc:0.9772036075592041
node5 epoch1:node_model train_loss:0.05641225371414758,train_acc:0.9814285635948181
node5_model on test-dataset: loss:0.15073684427945408,acc:0.9566939473152161
node5 weight score:32089.03585000485
node7: train data size:3637
node7 epoch0:node_model train_loss:0.09306012182118925,train_acc:0.9720523953437805
node7 epoch1:node_model train_loss:0.05258378847124609,train_acc:0.9827026128768921
node7_model on test-dataset: loss:0.08857359773261124,acc:0.9715949296951294
node7 weight score:41061.89759819274
node12: train data size:1406
node12 epoch0:node_model train_loss:0.11454462228963773,train_acc:0.9511111378669739
node12 epoch1:node_model train_loss:0.07357475341608127,train_acc:0.9719999432563782
node12_model on test-dataset: loss:0.13440679687279045,acc:0.9583977460861206
node12 weight score:10460.780501530076
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08518195454962552,train_acc:0.9724999070167542
node14 epoch1:node_model train_loss:0.06293440709123388,train_acc:0.9809375405311584
node14_model on test-dataset: loss:0.09435466180220828,acc:0.9718990921974182
node14 weight score:16321.39812263052
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04351450371061219,acc:0.9853989630937576
total cost energy:9.304949493661168 | all_enery_cp：8.015 | all_enery_tp: 1.2899494936611668
ef: 31.09589662743066
reward: 21.790947133769492
step 375:loss:10.781510353088379|running q:30.756711959838867
episode6,iteration15 selected nodes:[7, 3, 9, 8, 14],center node:8
################################################## episode6,iteration15 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08709060235292111,train_acc:0.9702629446983337
node3 epoch1:node_model train_loss:0.0646597532261359,train_acc:0.9789472818374634
node3_model on test-dataset: loss:0.09284652209906198,acc:0.9696999192237854
node3 weight score:40518.4805520896
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06995143343669337,train_acc:0.9803504943847656
node7 epoch1:node_model train_loss:0.05295286286068526,train_acc:0.9808910489082336
node7_model on test-dataset: loss:0.09753767006652198,acc:0.9700971245765686
node7 weight score:37288.157462850184
node8: train data size:2290
node8 epoch0:node_model train_loss:0.10844714737132839,train_acc:0.9686955213546753
node8 epoch1:node_model train_loss:0.051922383677700294,train_acc:0.9820771813392639
node8_model on test-dataset: loss:0.09474999520152778,acc:0.9708941578865051
node8 weight score:24168.866659352352
node9: train data size:2125
node9 epoch0:node_model train_loss:0.07716951857913625,train_acc:0.9736363291740417
node9 epoch1:node_model train_loss:0.05902287145991894,train_acc:0.9781817197799683
node9_model on test-dataset: loss:0.09408740814513294,acc:0.9725960493087769
node9 weight score:22585.381422369686
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08302670414559543,train_acc:0.971562385559082
node14 epoch1:node_model train_loss:0.07142249296884984,train_acc:0.9818748831748962
node14_model on test-dataset: loss:0.11160322077921592,acc:0.9645000100135803
node14 weight score:13798.884917905498
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04505653642627294,acc:0.9841989630460739
total cost energy:8.04893616859962 | all_enery_cp：6.677 | all_enery_tp: 1.371936168599622
ef: 31.127134773157078
reward: 23.078198604557457
step 376:loss:13.381532669067383|running q:32.71031188964844
episode6,iteration16 selected nodes:[5, 7, 2, 3, 9],center node:5
################################################## episode6,iteration16 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0764638505915695,train_acc:0.9765956401824951
node2 epoch1:node_model train_loss:0.055521419896011025,train_acc:0.9817021489143372
node2_model on test-dataset: loss:0.07380066586600151,acc:0.9757969379425049
node2 weight score:62465.561061065375
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08701705373823643,train_acc:0.9739472270011902
node3 epoch1:node_model train_loss:0.05188786559493134,train_acc:0.9819439649581909
node3_model on test-dataset: loss:0.06884209000709234,acc:0.9776980876922607
node3 weight score:54646.80110107677
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06085665358648616,train_acc:0.9793877601623535
node5 epoch1:node_model train_loss:0.04511107140154179,train_acc:0.985102117061615
node5_model on test-dataset: loss:0.07760733264032751,acc:0.975199282169342
node5 weight score:62326.58481405562
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06482135343390542,train_acc:0.9805403351783752
node7 epoch1:node_model train_loss:0.04234862007905502,train_acc:0.9852153658866882
node7_model on test-dataset: loss:0.10078996408556122,acc:0.9682959318161011
node7 weight score:36084.94191854785
node9: train data size:2125
node9 epoch0:node_model train_loss:0.07094086010263047,train_acc:0.9790908098220825
node9 epoch1:node_model train_loss:0.04892631912265311,train_acc:0.9840907454490662
node9_model on test-dataset: loss:0.08100808123926981,acc:0.975295901298523
node9 weight score:26231.9507818422
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.046347155853000005,acc:0.9846999788284302
total cost energy:10.927866870536343 | all_enery_cp：9.4855 | all_enery_tp: 1.4423668705363433
ef: 31.87140265168712
reward: 20.94353578115078
step 377:loss:14.751005172729492|running q:34.50504684448242
episode6,iteration17 selected nodes:[9, 13, 14, 17, 12],center node:17
################################################## episode6,iteration17 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05946678340181031,train_acc:0.9777271151542664
node9 epoch1:node_model train_loss:0.047914772696623746,train_acc:0.9868180751800537
node9_model on test-dataset: loss:0.08087915631942451,acc:0.9757969379425049
node9 weight score:26273.765661050114
node12: train data size:1406
node12 epoch0:node_model train_loss:0.08139520065791052,train_acc:0.9793333411216736
node12 epoch1:node_model train_loss:0.03829209773490826,train_acc:0.9860000014305115
node12_model on test-dataset: loss:0.10142758372800018,acc:0.9691988825798035
node12 weight score:13862.106818698261
node13: train data size:1056
node13 epoch0:node_model train_loss:0.08799286601556973,train_acc:0.9751298427581787
node13 epoch1:node_model train_loss:0.057289828207682476,train_acc:0.9812987446784973
node13_model on test-dataset: loss:0.08352768714306875,acc:0.9737980365753174
node13 weight score:12642.514549591815
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08601725473999977,train_acc:0.9706249833106995
node14 epoch1:node_model train_loss:0.04064553140779026,train_acc:0.9868748784065247
node14_model on test-dataset: loss:0.08354123863769929,acc:0.9752998352050781
node14 weight score:18434.009659333096
node17: train data size:719
node17 epoch0:node_model train_loss:0.07264918343207682,train_acc:0.9812499284744263
node17 epoch1:node_model train_loss:0.037243065919028595,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.09641808858898003,acc:0.9691993594169617
node17 weight score:7457.106965322864
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.047272926052464755,acc:0.9851999753713607
total cost energy:5.044647436525153 | all_enery_cp：3.423 | all_enery_tp: 1.6216474365251534
ef: 30.950717568227507
reward: 25.906070131702354
step 378:loss:10.196203231811523|running q:36.41192626953125
episode6,iteration18 selected nodes:[7, 3, 12, 14, 19],center node:12
################################################## episode6,iteration18 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.0787690719589591,train_acc:0.9712560772895813
node3 epoch1:node_model train_loss:0.04716028191002184,train_acc:0.9842103719711304
node3_model on test-dataset: loss:0.08215306982165202,acc:0.9742969870567322
node3 weight score:45792.56755915527
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0697121385459763,train_acc:0.9789186716079712
node7 epoch1:node_model train_loss:0.03456923139055033,train_acc:0.9881079792976379
node7_model on test-dataset: loss:0.10416910125059076,acc:0.9691888689994812
node7 weight score:34914.38398081959
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07127640854256849,train_acc:0.9688888788223267
node12 epoch1:node_model train_loss:0.05740577660423393,train_acc:0.9773333072662354
node12_model on test-dataset: loss:0.11305756557936547,acc:0.9673980474472046
node12 weight score:12436.14253318589
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06833112600725144,train_acc:0.9712499380111694
node14 epoch1:node_model train_loss:0.03702726055053063,train_acc:0.9831249117851257
node14_model on test-dataset: loss:0.10773692658156506,acc:0.9663999676704407
node14 weight score:14294.077702635248
node19: train data size:5781
node19 epoch0:node_model train_loss:0.09047277740620334,train_acc:0.9750914573669434
node19 epoch1:node_model train_loss:0.060390929201746296,train_acc:0.9816434383392334
node19_model on test-dataset: loss:0.08516104989212181,acc:0.973798930644989
node19 weight score:67883.14619562712
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.050244356851326305,acc:0.983699975013733
total cost energy:9.767329142359193 | all_enery_cp：8.062999999999999 | all_enery_tp: 1.7043291423591944
ef: 31.119478541396745
reward: 21.35214939903755
step 379:loss:9.392902374267578|running q:38.279319763183594
episode6,iteration19 selected nodes:[4, 1, 2, 12, 10],center node:2
################################################## episode6,iteration19 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0804319264742532,train_acc:0.9734329581260681
node1 epoch1:node_model train_loss:0.048084393525913136,train_acc:0.9837681651115417
node1_model on test-dataset: loss:0.0814436890318757,acc:0.9758938550949097
node1 weight score:81737.45662962984
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0793337211845086,train_acc:0.9759573936462402
node2 epoch1:node_model train_loss:0.054868638931595264,train_acc:0.9821277260780334
node2_model on test-dataset: loss:0.12408033613148291,acc:0.9626929759979248
node2 weight score:37153.34873944063
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07451405747642004,train_acc:0.9751160144805908
node4 epoch1:node_model train_loss:0.04498663257677541,train_acc:0.9865022897720337
node4_model on test-dataset: loss:0.07719096474349499,acc:0.9743990898132324
node4 weight score:55680.09176050879
node10: train data size:1915
node10 epoch0:node_model train_loss:0.08442050041630864,train_acc:0.9741665124893188
node10 epoch1:node_model train_loss:0.05631780814146623,train_acc:0.981166660785675
node10_model on test-dataset: loss:0.09238202088075922,acc:0.9729980230331421
node10 weight score:20729.141685174425
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05492719952793171,train_acc:0.9773333072662354
node12 epoch1:node_model train_loss:0.050323147947589554,train_acc:0.9822221994400024
node12_model on test-dataset: loss:0.09796546122030123,acc:0.969893753528595
node12 weight score:14351.996943476206
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04295534933859017,acc:0.9866979563236237
total cost energy:11.075455532033676 | all_enery_cp：9.443 | all_enery_tp: 1.632455532033676
ef: 31.19302318472538
reward: 20.117567652691704
step 380:loss:12.288225173950195|running q:40.16581726074219
episode6,iteration20 selected nodes:[7, 4, 13, 8, 6],center node:6
################################################## episode6,iteration20 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06496974977469722,train_acc:0.9818462133407593
node4 epoch1:node_model train_loss:0.03531725191359603,train_acc:0.9890602231025696
node4_model on test-dataset: loss:0.07988100380454853,acc:0.9758919477462769
node4 weight score:53805.03242693686
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05966051740364896,train_acc:0.9797220230102539
node6 epoch1:node_model train_loss:0.04137865293564068,train_acc:0.9854309558868408
node6_model on test-dataset: loss:0.0842784277903047,acc:0.9752897620201111
node6 weight score:41873.11145362838
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06093335281302397,train_acc:0.9779983758926392
node7 epoch1:node_model train_loss:0.04738701779294659,train_acc:0.9854053258895874
node7_model on test-dataset: loss:0.06947730913612758,acc:0.9780969619750977
node7 weight score:52348.02621491845
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08116255141794682,train_acc:0.9743477702140808
node8 epoch1:node_model train_loss:0.05095847436915273,train_acc:0.9873429536819458
node8_model on test-dataset: loss:0.06359717191357049,acc:0.9795980453491211
node8 weight score:36007.89046267882
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06673390388658101,train_acc:0.9818181395530701
node13 epoch1:node_model train_loss:0.034185918725349686,train_acc:0.9872727394104004
node13_model on test-dataset: loss:0.1225034310366027,acc:0.9630991220474243
node13 weight score:8620.166725652596
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04733957625343464,acc:0.9853979587554932
total cost energy:8.781782893563236 | all_enery_cp：7.404999999999999 | all_enery_tp: 1.3767828935632371
ef: 31.775410613175982
reward: 22.993627719612746
step 381:loss:9.014481544494629|running q:41.98674392700195
episode6,iteration21 selected nodes:[9, 3, 15, 19, 2],center node:15
################################################## episode6,iteration21 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06364411977615128,train_acc:0.9785107374191284
node2 epoch1:node_model train_loss:0.054853203602412595,train_acc:0.9838298559188843
node2_model on test-dataset: loss:0.10730792199385178,acc:0.9676849246025085
node2 weight score:42960.48152217625
node3: train data size:3762
node3 epoch0:node_model train_loss:0.06792908561366953,train_acc:0.9770456552505493
node3 epoch1:node_model train_loss:0.043865133609965835,train_acc:0.9838878512382507
node3_model on test-dataset: loss:0.05967691795143765,acc:0.9804962873458862
node3 weight score:63039.44856973586
node9: train data size:2125
node9 epoch0:node_model train_loss:0.05938911953919821,train_acc:0.9827271699905396
node9 epoch1:node_model train_loss:0.039779470725492996,train_acc:0.9859089851379395
node9_model on test-dataset: loss:0.07925100921842386,acc:0.9756960868835449
node9 weight score:26813.538666028635
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06606309674680233,train_acc:0.9809774160385132
node15 epoch1:node_model train_loss:0.05944025150633284,train_acc:0.9826314449310303
node15_model on test-dataset: loss:0.09596734421895235,acc:0.971799910068512
node15 weight score:14338.210681965056
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07407647491721757,train_acc:0.978367269039154
node19 epoch1:node_model train_loss:0.05197407006575116,train_acc:0.9842702150344849
node19_model on test-dataset: loss:0.06186182401375845,acc:0.9815981984138489
node19 weight score:93450.20280543732
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0418803484684031,acc:0.986299975514412
total cost energy:11.07384130512566 | all_enery_cp：8.827 | all_enery_tp: 2.24684130512566
ef: 31.769020790785635
reward: 20.695179485659978
step 382:loss:11.31320571899414|running q:43.74443435668945
episode6,iteration22 selected nodes:[5, 12, 10, 1, 7],center node:5
################################################## episode6,iteration22 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.06729903793372492,train_acc:0.9786934852600098
node1 epoch1:node_model train_loss:0.04145638852505319,train_acc:0.9891046285629272
node1_model on test-dataset: loss:0.06519342037383467,acc:0.9803979992866516
node1 weight score:102111.53152921214
node5: train data size:4837
node5 epoch0:node_model train_loss:0.056776131009112815,train_acc:0.9813459515571594
node5 epoch1:node_model train_loss:0.04098554821304825,train_acc:0.9861831665039062
node5_model on test-dataset: loss:0.10756598710839171,acc:0.970596969127655
node5 weight score:44967.74612523073
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06243158852983568,train_acc:0.9772679209709167
node7 epoch1:node_model train_loss:0.0473410585844839,train_acc:0.9833233952522278
node7_model on test-dataset: loss:0.10364996142758173,acc:0.9689990282058716
node7 weight score:35089.25570166375
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0797646639868617,train_acc:0.9771665930747986
node10 epoch1:node_model train_loss:0.053882133401930335,train_acc:0.98499995470047
node10_model on test-dataset: loss:0.07130618322335067,acc:0.9779980182647705
node10 weight score:26856.01603442567
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05563979066597919,train_acc:0.9826666116714478
node12 epoch1:node_model train_loss:0.03811529939994216,train_acc:0.9899998903274536
node12_model on test-dataset: loss:0.1219477332464885,acc:0.9624972939491272
node12 weight score:11529.52959902997
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04401460068067536,acc:0.9862989640235901
total cost energy:10.393004637770998 | all_enery_cp：9.226 | all_enery_tp: 1.167004637770997
ef: 31.535340924668272
reward: 21.142336286897276
step 383:loss:8.630708694458008|running q:45.51974105834961
episode6,iteration23 selected nodes:[12, 18, 16, 2, 5],center node:12
################################################## episode6,iteration23 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06369101815421055,train_acc:0.97957444190979
node2 epoch1:node_model train_loss:0.029895488068738832,train_acc:0.9912765026092529
node2_model on test-dataset: loss:0.07564040354929602,acc:0.9749000072479248
node2 weight score:60946.263950001165
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05472999664821795,train_acc:0.9830613732337952
node5 epoch1:node_model train_loss:0.03459519249558145,train_acc:0.9889796376228333
node5_model on test-dataset: loss:0.0624080845413846,acc:0.9788973927497864
node5 weight score:77505.98396899116
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05632685060845688,train_acc:0.9826666116714478
node12 epoch1:node_model train_loss:0.018824290146585553,train_acc:0.9953332543373108
node12_model on test-dataset: loss:0.07826035856516683,acc:0.9775981903076172
node12 weight score:17965.67286143002
node16: train data size:920
node16 epoch0:node_model train_loss:0.08444490972906352,train_acc:0.9719999432563782
node16 epoch1:node_model train_loss:0.03962726379686501,train_acc:0.9860000014305115
node16_model on test-dataset: loss:0.12691605418731342,acc:0.9649940133094788
node16 weight score:7248.885934022078
node18: train data size:801
node18 epoch0:node_model train_loss:0.05046037560128348,train_acc:0.9822222590446472
node18 epoch1:node_model train_loss:0.022525278870792437,train_acc:0.992222249507904
node18_model on test-dataset: loss:0.0947873340422484,acc:0.9710931181907654
node18 weight score:8450.496135306223
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041784247391624375,acc:0.9858989608287811
total cost energy:7.476292222699217 | all_enery_cp：6.287 | all_enery_tp: 1.189292222699217
ef: 31.614818657843205
reward: 24.138526435143987
step 384:loss:6.429056167602539|running q:47.2773551940918
episode6,iteration24 selected nodes:[6, 15, 9, 3, 17],center node:17
################################################## episode6,iteration24 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.05923362712890498,train_acc:0.9781578779220581
node3 epoch1:node_model train_loss:0.03811450058128685,train_acc:0.9869439005851746
node3_model on test-dataset: loss:0.07281517440176685,acc:0.9785971641540527
node3 weight score:51665.05513318822
node6: train data size:3529
node6 epoch0:node_model train_loss:0.08059489324740651,train_acc:0.9726530909538269
node6 epoch1:node_model train_loss:0.043483129162470706,train_acc:0.9852778315544128
node6_model on test-dataset: loss:0.07730259324012877,acc:0.9765980243682861
node6 weight score:45651.767322186686
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04921286351005123,train_acc:0.9840907454490662
node9 epoch1:node_model train_loss:0.03515397079966285,train_acc:0.9872725605964661
node9_model on test-dataset: loss:0.1540637589863036,acc:0.9570983052253723
node9 weight score:13792.9907330699
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07339753263763019,train_acc:0.979060173034668
node15 epoch1:node_model train_loss:0.045446806121617556,train_acc:0.9883458018302917
node15_model on test-dataset: loss:0.07658124992245575,acc:0.9768980145454407
node15 weight score:17967.84462767718
node17: train data size:719
node17 epoch0:node_model train_loss:0.13530716343666427,train_acc:0.9784210324287415
node17 epoch1:node_model train_loss:0.03791635885136202,train_acc:0.9887499809265137
node17_model on test-dataset: loss:0.09162063203446451,acc:0.971199095249176
node17 weight score:7847.577385512219
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.045452192638913404,acc:0.9859989666938782
total cost energy:7.637040622211742 | all_enery_cp：5.7555 | all_enery_tp: 1.881540622211743
ef: 31.328994832037292
reward: 23.69195420982555
step 385:loss:6.300192356109619|running q:48.99834060668945
episode6,iteration25 selected nodes:[8, 4, 6, 11, 17],center node:11
################################################## episode6,iteration25 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.055303812189417524,train_acc:0.9820930361747742
node4 epoch1:node_model train_loss:0.02984357113018632,train_acc:0.9899953007698059
node4_model on test-dataset: loss:0.07337246805327595,acc:0.9750947952270508
node4 weight score:58577.83054100361
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04857840494110456,train_acc:0.9832087755203247
node6 epoch1:node_model train_loss:0.041005850394463375,train_acc:0.9870977401733398
node6_model on test-dataset: loss:0.07049621630532783,acc:0.9774991273880005
node6 weight score:50059.424249316646
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07159604796248933,train_acc:0.9785022735595703
node8 epoch1:node_model train_loss:0.04620262680818205,train_acc:0.9858936071395874
node8_model on test-dataset: loss:0.06202989912533667,acc:0.981200098991394
node8 weight score:36917.68054261802
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06700518261641264,train_acc:0.9791666865348816
node11 epoch1:node_model train_loss:0.043197827209951356,train_acc:0.9885416030883789
node11_model on test-dataset: loss:0.07024942193267635,acc:0.9785948991775513
node11 weight score:22420.113314375794
node17: train data size:719
node17 epoch0:node_model train_loss:0.04218071259674616,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.039849759778007865,train_acc:0.9887499213218689
node17_model on test-dataset: loss:0.07830884888055152,acc:0.9772980213165283
node17 weight score:9181.593271747965
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04136237059632549,acc:0.9870989626646042
total cost energy:7.391230076213408 | all_enery_cp：6.2055 | all_enery_tp: 1.1857300762134082
ef: 32.17309667729147
reward: 24.78186660107806
step 386:loss:10.476529121398926|running q:50.622764587402344
episode6,iteration26 selected nodes:[11, 14, 0, 18, 7],center node:11
################################################## episode6,iteration26 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0641339977349465,train_acc:0.9798505902290344
node0 epoch1:node_model train_loss:0.04471390418216793,train_acc:0.9863141179084778
node0_model on test-dataset: loss:0.06245319521898637,acc:0.9804958701133728
node0 weight score:114725.91554165626
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04802299540873768,train_acc:0.9829728603363037
node7 epoch1:node_model train_loss:0.02992042400748343,train_acc:0.989729642868042
node7_model on test-dataset: loss:0.09616076113015878,acc:0.9714950323104858
node7 weight score:37822.07999661238
node11: train data size:1575
node11 epoch0:node_model train_loss:0.06476988428039476,train_acc:0.9779166579246521
node11 epoch1:node_model train_loss:0.04235949640860781,train_acc:0.9860415458679199
node11_model on test-dataset: loss:0.09606323093539686,acc:0.9717958569526672
node11 weight score:16395.451044731126
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08249584736768156,train_acc:0.9737498760223389
node14 epoch1:node_model train_loss:0.03862724994542077,train_acc:0.9884374737739563
node14_model on test-dataset: loss:0.07902436844931798,acc:0.974699079990387
node14 weight score:19487.659695599767
node18: train data size:801
node18 epoch0:node_model train_loss:0.03612202023052507,train_acc:0.9844443202018738
node18 epoch1:node_model train_loss:0.03457390105662247,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.11989248902050349,acc:0.9649989008903503
node18 weight score:6680.985660936745
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.044001011400687275,acc:0.9861979526281357
total cost energy:8.953224883296235 | all_enery_cp：7.359000000000001 | all_enery_tp: 1.5942248832962345
ef: 31.357439249539897
reward: 22.404214366243664
step 387:loss:16.445932388305664|running q:52.21849060058594
episode6,iteration27 selected nodes:[18, 10, 5, 16, 7],center node:16
################################################## episode6,iteration27 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05426997562148133,train_acc:0.9835299849510193
node5 epoch1:node_model train_loss:0.03484456568756806,train_acc:0.9853062033653259
node5_model on test-dataset: loss:0.09739692596209353,acc:0.9701971411705017
node5 weight score:49662.75836962801
node7: train data size:3637
node7 epoch0:node_model train_loss:0.05184636640714834,train_acc:0.9839443564414978
node7 epoch1:node_model train_loss:0.031697865896480715,train_acc:0.989810049533844
node7_model on test-dataset: loss:0.10153356934402837,acc:0.9689959287643433
node7 weight score:35820.665258764566
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06233635256066918,train_acc:0.9819998741149902
node10 epoch1:node_model train_loss:0.03521364904008806,train_acc:0.9899998903274536
node10_model on test-dataset: loss:0.08521473369110026,acc:0.9751889705657959
node10 weight score:22472.63961349445
node16: train data size:920
node16 epoch0:node_model train_loss:0.06210999768227339,train_acc:0.9759999513626099
node16 epoch1:node_model train_loss:0.0383481073891744,train_acc:0.9880000352859497
node16_model on test-dataset: loss:0.08554006527173443,acc:0.9744982123374939
node16 weight score:10755.194037758138
node18: train data size:801
node18 epoch0:node_model train_loss:0.04461463287265764,train_acc:0.992222249507904
node18 epoch1:node_model train_loss:0.027333466257762968,train_acc:0.9877777099609375
node18_model on test-dataset: loss:0.14072453163698811,acc:0.9599969983100891
node18 weight score:5691.971333514566
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04604176194610773,acc:0.9849969446659088
total cost energy:7.481129717376117 | all_enery_cp：6.055000000000001 | all_enery_tp: 1.4261297173761167
ef: 31.071185233141467
reward: 23.59005551576535
step 388:loss:6.812249660491943|running q:53.86672592163086
episode6,iteration28 selected nodes:[4, 16, 6, 0, 1],center node:6
################################################## episode6,iteration28 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04664976966644948,train_acc:0.9836113452911377
node0 epoch1:node_model train_loss:0.037030645600882255,train_acc:0.9862396717071533
node0_model on test-dataset: loss:0.0949606242912705,acc:0.9733906984329224
node0 weight score:75452.32619809831
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0603148326861547,train_acc:0.9818275570869446
node1 epoch1:node_model train_loss:0.038942635309562754,train_acc:0.987761378288269
node1_model on test-dataset: loss:0.07950996365281754,acc:0.9758960008621216
node1 weight score:83725.35584430619
node4: train data size:4298
node4 epoch0:node_model train_loss:0.060523224615513585,train_acc:0.9818462133407593
node4 epoch1:node_model train_loss:0.03218273371241467,train_acc:0.9888325333595276
node4_model on test-dataset: loss:0.06067590328369988,acc:0.9804967045783997
node4 weight score:70835.36902457
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04974164172179169,train_acc:0.9845976233482361
node6 epoch1:node_model train_loss:0.029328134884255834,train_acc:0.9907088875770569
node6_model on test-dataset: loss:0.06658821187393187,acc:0.980798065662384
node6 weight score:52997.36846337426
node16: train data size:920
node16 epoch0:node_model train_loss:0.05304603106342256,train_acc:0.9779998660087585
node16 epoch1:node_model train_loss:0.030801736935973167,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.08506088496586017,acc:0.9735928773880005
node16 weight score:10815.782134987769
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04454990373098553,acc:0.9857969444990158
total cost energy:12.969319196258327 | all_enery_cp：11.2845 | all_enery_tp: 1.6848191962583277
ef: 31.95134313532336
reward: 18.98202393906503
step 389:loss:14.62016487121582|running q:55.46815872192383
episode6,iteration29 selected nodes:[8, 12, 13, 6, 1],center node:6
################################################## episode6,iteration29 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05078203326313576,train_acc:0.9832838773727417
node1 epoch1:node_model train_loss:0.03503026396258554,train_acc:0.9876120090484619
node1_model on test-dataset: loss:0.06596492464742369,acc:0.9784982204437256
node1 weight score:100917.26831465417
node6: train data size:3529
node6 epoch0:node_model train_loss:0.045382007633128926,train_acc:0.9840420484542847
node6 epoch1:node_model train_loss:0.02605924057489675,train_acc:0.9897220730781555
node6_model on test-dataset: loss:0.09498911843431415,acc:0.9709948301315308
node6 weight score:37151.623871952615
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05886373160730885,train_acc:0.9803863167762756
node8 epoch1:node_model train_loss:0.040989454430730446,train_acc:0.9863767623901367
node8_model on test-dataset: loss:0.054618310883815864,acc:0.982897937297821
node8 weight score:41927.331016722404
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05820583146996796,train_acc:0.9833333492279053
node12 epoch1:node_model train_loss:0.020457288252267367,train_acc:0.9919999241828918
node12_model on test-dataset: loss:0.08865570019741426,acc:0.97469562292099
node12 weight score:15859.104342633205
node13: train data size:1056
node13 epoch0:node_model train_loss:0.045699602077630436,train_acc:0.9881818294525146
node13 epoch1:node_model train_loss:0.0426293041726405,train_acc:0.9865584373474121
node13_model on test-dataset: loss:0.07939209192511043,acc:0.9755961894989014
node13 weight score:13301.07287002982
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042133936819809606,acc:0.9858989638090133
total cost energy:8.645782893563236 | all_enery_cp：7.469 | all_enery_tp: 1.176782893563237
ef: 31.928303298946037
reward: 23.2825204053828
step 390:loss:17.182140350341797|running q:57.122222900390625
episode6,iteration30 selected nodes:[1, 3, 2, 15, 7],center node:2
################################################## episode6,iteration30 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.041116395369241596,train_acc:0.9862688779830933
node1 epoch1:node_model train_loss:0.033188078785315156,train_acc:0.9879106879234314
node1_model on test-dataset: loss:0.07262275522720302,acc:0.9796947836875916
node1 weight score:91665.48389927269
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06312907306687036,train_acc:0.9789360165596008
node2 epoch1:node_model train_loss:0.028094324357299867,train_acc:0.9921277165412903
node2_model on test-dataset: loss:0.059621201088593805,acc:0.9829970598220825
node2 weight score:77321.4882596846
node3: train data size:3762
node3 epoch0:node_model train_loss:0.0478309110836371,train_acc:0.9852630496025085
node3 epoch1:node_model train_loss:0.03388821281566236,train_acc:0.9873683452606201
node3_model on test-dataset: loss:0.0686027869569807,acc:0.978796660900116
node3 weight score:54837.422309957576
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04658163627106193,train_acc:0.9824323058128357
node7 epoch1:node_model train_loss:0.03387104302512582,train_acc:0.9886484146118164
node7_model on test-dataset: loss:0.06644364937470527,acc:0.9786971211433411
node7 weight score:54738.113186549104
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06581311911993128,train_acc:0.9819172620773315
node15 epoch1:node_model train_loss:0.025912345220733966,train_acc:0.9928571581840515
node15_model on test-dataset: loss:0.059073552269255744,acc:0.9826979041099548
node15 weight score:23292.995717071946
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04052271936508987,acc:0.9867979562282563
total cost energy:11.46278500213908 | all_enery_cp：10.021 | all_enery_tp: 1.4417850021390795
ef: 32.42729629525
reward: 20.964511293110924
step 391:loss:10.352316856384277|running q:58.787437438964844
episode6,iteration31 selected nodes:[18, 2, 13, 11, 7],center node:7
################################################## episode6,iteration31 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.039793810678368555,train_acc:0.9876595735549927
node2 epoch1:node_model train_loss:0.02717958625029535,train_acc:0.9923404455184937
node2_model on test-dataset: loss:0.07447310694376938,acc:0.9779959917068481
node2 weight score:61901.53988715365
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04607616056927253,train_acc:0.984214723110199
node7 epoch1:node_model train_loss:0.02210713972060664,train_acc:0.9937838315963745
node7_model on test-dataset: loss:0.07793562669685343,acc:0.9774969816207886
node7 weight score:46666.719113542975
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05865256179822609,train_acc:0.9845831990242004
node11 epoch1:node_model train_loss:0.04191924523911439,train_acc:0.9870832562446594
node11_model on test-dataset: loss:0.08259904125909089,acc:0.9763981699943542
node11 weight score:19068.017933279036
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07292403035204519,train_acc:0.973831057548523
node13 epoch1:node_model train_loss:0.03828531406311826,train_acc:0.9874675273895264
node13_model on test-dataset: loss:0.09338046879973262,acc:0.9721948504447937
node13 weight score:11308.574625650452
node18: train data size:801
node18 epoch0:node_model train_loss:0.028609157772671603,train_acc:0.992222249507904
node18 epoch1:node_model train_loss:0.03209748056391012,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.06575429322903802,acc:0.9790971279144287
node18 weight score:12181.714085343818
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04133226348501921,acc:0.9862959337234497
total cost energy:7.041102785260984 | all_enery_cp：5.8395 | all_enery_tp: 1.2016027852609832
ef: 31.52307161659555
reward: 24.48196883133457
step 392:loss:9.919659614562988|running q:60.386959075927734
episode6,iteration32 selected nodes:[14, 0, 3, 17, 9],center node:9
################################################## episode6,iteration32 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.048786289501650676,train_acc:0.9838142395019531
node0 epoch1:node_model train_loss:0.03097711369143023,train_acc:0.9895835518836975
node0_model on test-dataset: loss:0.06471297137795773,acc:0.9797971248626709
node0 weight score:110719.68799195818
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04916126460547706,train_acc:0.9836841225624084
node3 epoch1:node_model train_loss:0.030408416519333657,train_acc:0.9884634613990784
node3_model on test-dataset: loss:0.08642968634194403,acc:0.9740990996360779
node3 weight score:43526.71124035208
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04702133863148364,train_acc:0.9849998354911804
node9 epoch1:node_model train_loss:0.03064611681144346,train_acc:0.9895454049110413
node9_model on test-dataset: loss:0.06821399562977604,acc:0.980695903301239
node9 weight score:31151.964935952496
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06126002504606731,train_acc:0.9837499260902405
node14 epoch1:node_model train_loss:0.034360442848992534,train_acc:0.9924998879432678
node14_model on test-dataset: loss:0.07319846367943683,acc:0.9784971475601196
node14 weight score:21038.69292590935
node17: train data size:719
node17 epoch0:node_model train_loss:0.06434749672189355,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.025970443042751867,train_acc:0.9887499809265137
node17_model on test-dataset: loss:0.06952912349937833,acc:0.9788979887962341
node17 weight score:10340.990419740136
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04253627932146628,acc:0.9853969436883926
total cost energy:9.21853302643529 | all_enery_cp：7.6555 | all_enery_tp: 1.56303302643529
ef: 31.93879696143281
reward: 22.72026393499752
step 393:loss:12.467182159423828|running q:61.984107971191406
episode6,iteration33 selected nodes:[3, 11, 17, 6, 2],center node:11
################################################## episode6,iteration33 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.044651800858233365,train_acc:0.9863830208778381
node2 epoch1:node_model train_loss:0.02406379498016248,train_acc:0.9921275973320007
node2_model on test-dataset: loss:0.07804480400649481,acc:0.9767919182777405
node2 weight score:59068.634468174976
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04425297472871056,train_acc:0.9866806864738464
node3 epoch1:node_model train_loss:0.029856030743471103,train_acc:0.9897367358207703
node3_model on test-dataset: loss:0.0823379791413754,acc:0.9770950078964233
node3 weight score:45689.72956623815
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0424106645061531,train_acc:0.9872221350669861
node6 epoch1:node_model train_loss:0.03005499840623492,train_acc:0.9908332824707031
node6_model on test-dataset: loss:0.06449238485329488,acc:0.9809950590133667
node6 weight score:54719.638729869446
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05953507934464142,train_acc:0.9804166555404663
node11 epoch1:node_model train_loss:0.03248907298257109,train_acc:0.9885415434837341
node11_model on test-dataset: loss:0.07170538554579252,acc:0.9789949059486389
node11 weight score:21964.87736606859
node17: train data size:719
node17 epoch0:node_model train_loss:0.050371858378639445,train_acc:0.9887499809265137
node17 epoch1:node_model train_loss:0.015337548655224964,train_acc:0.9962499737739563
node17_model on test-dataset: loss:0.07302325365017168,acc:0.9771998524665833
node17 weight score:9846.178635705171
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04167565798088617,acc:0.9857959353923798
total cost energy:8.516512549796275 | all_enery_cp：7.0975 | all_enery_tp: 1.4190125497962751
ef: 32.01994373185865
reward: 23.50343118206238
step 394:loss:13.688596725463867|running q:63.58359909057617
episode6,iteration34 selected nodes:[0, 12, 9, 18, 15],center node:12
################################################## episode6,iteration34 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04310693828705957,train_acc:0.98375004529953
node0 epoch1:node_model train_loss:0.022685212057290807,train_acc:0.9927780628204346
node0_model on test-dataset: loss:0.06219513481555623,acc:0.981395423412323
node0 weight score:115201.93695613458
node9: train data size:2125
node9 epoch0:node_model train_loss:0.036178591169035906,train_acc:0.9890909194946289
node9 epoch1:node_model train_loss:0.025830135254760866,train_acc:0.9936363101005554
node9_model on test-dataset: loss:0.05753035212692339,acc:0.9824979901313782
node9 weight score:36937.024047963896
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04040607619099319,train_acc:0.9873332977294922
node12 epoch1:node_model train_loss:0.022375655888269345,train_acc:0.9933332800865173
node12_model on test-dataset: loss:0.07145544871504171,acc:0.9800939559936523
node12 weight score:19676.596050876527
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0670681626402906,train_acc:0.9826314449310303
node15 epoch1:node_model train_loss:0.038020458504823704,train_acc:0.9888721704483032
node15_model on test-dataset: loss:0.05649284910010465,acc:0.9830960631370544
node15 weight score:24357.0650430773
node18: train data size:801
node18 epoch0:node_model train_loss:0.03860732634550206,train_acc:0.9877777099609375
node18 epoch1:node_model train_loss:0.03190608547690014,train_acc:0.9933333992958069
node18_model on test-dataset: loss:0.09667072300844666,acc:0.9720889329910278
node18 weight score:8285.859204032355
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04336167465891776,acc:0.9860969436168671
total cost energy:8.04385925819646 | all_enery_cp：6.4365 | all_enery_tp: 1.6073592581964586
ef: 32.18725370311114
reward: 24.14339444491468
step 395:loss:16.939430236816406|running q:65.16193389892578
episode6,iteration35 selected nodes:[0, 1, 17, 13, 19],center node:17
################################################## episode6,iteration35 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.023157782400731877,train_acc:0.9909725189208984
node0 epoch1:node_model train_loss:0.02850705869948595,train_acc:0.990416944026947
node0_model on test-dataset: loss:0.09202567954373081,acc:0.9746919870376587
node0 weight score:77858.7024352825
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04469387430815833,train_acc:0.9858576059341431
node1 epoch1:node_model train_loss:0.0322018545676968,train_acc:0.9885441660881042
node1_model on test-dataset: loss:0.06205281557602575,acc:0.9821969866752625
node1 weight score:107279.58011581263
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04754730144684965,train_acc:0.9867532253265381
node13 epoch1:node_model train_loss:0.023864927130158652,train_acc:0.9927272200584412
node13_model on test-dataset: loss:0.06435345428821165,acc:0.9805960655212402
node13 weight score:16409.375560022414
node17: train data size:719
node17 epoch0:node_model train_loss:0.03915422651334666,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.02808843442471698,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.059485631894349356,acc:0.9832979440689087
node17 weight score:12086.95237997966
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06646555649309323,train_acc:0.9805172681808472
node19 epoch1:node_model train_loss:0.045211716651402674,train_acc:0.9870692491531372
node19_model on test-dataset: loss:0.06169643136949162,acc:0.9795929789543152
node19 weight score:93700.71933947637
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038940413303171224,acc:0.9872979545593261
total cost energy:12.712752721851865 | all_enery_cp：10.688999999999998 | all_enery_tp: 2.023752721851867
ef: 32.36208877059134
reward: 19.649336048739478
step 396:loss:10.804917335510254|running q:66.7087631225586
episode6,iteration36 selected nodes:[18, 13, 5, 15, 3],center node:15
################################################## episode6,iteration36 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.053496334977497007,train_acc:0.9823682904243469
node3 epoch1:node_model train_loss:0.020970732733411223,train_acc:0.9923684000968933
node3_model on test-dataset: loss:0.05970902000983187,acc:0.9828946590423584
node3 weight score:63005.55593410405
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04767930209731721,train_acc:0.9844897985458374
node5 epoch1:node_model train_loss:0.028269054535633827,train_acc:0.9910813570022583
node5_model on test-dataset: loss:0.06680763879863662,acc:0.9808971285820007
node5 weight score:72401.90024645372
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04376546924256466,train_acc:0.9845454096794128
node13 epoch1:node_model train_loss:0.0325140700918961,train_acc:0.9890909194946289
node13_model on test-dataset: loss:0.05704207164701074,acc:0.9817980527877808
node13 weight score:18512.65161852408
node15: train data size:1376
node15 epoch0:node_model train_loss:0.06857331040581423,train_acc:0.9771804809570312
node15 epoch1:node_model train_loss:0.02034822680538387,train_acc:0.9931203126907349
node15_model on test-dataset: loss:0.05941274944721954,acc:0.9827977418899536
node15 weight score:23160.012165779266
node18: train data size:801
node18 epoch0:node_model train_loss:0.03223145653545442,train_acc:0.9866665601730347
node18 epoch1:node_model train_loss:0.03553467388774657,train_acc:0.9888888597488403
node18_model on test-dataset: loss:0.08902960808431089,acc:0.9744898676872253
node18 weight score:8997.006919781725
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042124593731423375,acc:0.9873969465494156
total cost energy:7.511517997529072 | all_enery_cp：5.916 | all_enery_tp: 1.5955179975290719
ef: 32.30545397390187
reward: 24.793935976372797
step 397:loss:11.57789134979248|running q:68.35987854003906
episode6,iteration37 selected nodes:[13, 10, 8, 6, 19],center node:10
################################################## episode6,iteration37 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04755254006401325,train_acc:0.985153079032898
node6 epoch1:node_model train_loss:0.02457286744740688,train_acc:0.9908331632614136
node6_model on test-dataset: loss:0.06115099341055611,acc:0.9823969006538391
node6 weight score:57709.61031339208
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05412832631365112,train_acc:0.9825602769851685
node8 epoch1:node_model train_loss:0.027236526923092162,train_acc:0.99173903465271
node8_model on test-dataset: loss:0.07640328445548221,acc:0.9793003797531128
node8 weight score:29972.533462672156
node10: train data size:1915
node10 epoch0:node_model train_loss:0.057941384136211124,train_acc:0.9849998354911804
node10 epoch1:node_model train_loss:0.026610736298607662,train_acc:0.9929999709129333
node10_model on test-dataset: loss:0.056742124176744256,acc:0.9829971194267273
node10 weight score:33749.17713751827
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05303834848613902,train_acc:0.983311653137207
node13 epoch1:node_model train_loss:0.03492572969249026,train_acc:0.9867532253265381
node13_model on test-dataset: loss:0.06264846987993224,acc:0.9826958775520325
node13 weight score:16855.958366163726
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0589686873095945,train_acc:0.9816836714744568
node19 epoch1:node_model train_loss:0.03284479053048738,train_acc:0.9910345077514648
node19_model on test-dataset: loss:0.07176195349762565,acc:0.9782979488372803
node19 weight score:80558.00766615519
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03725358864217924,acc:0.9876989662647248
total cost energy:8.770319196258328 | all_enery_cp：7.285499999999999 | all_enery_tp: 1.4848191962583277
ef: 32.355038082755954
reward: 23.584718886497626
step 398:loss:28.705692291259766|running q:69.84347534179688
episode6,iteration38 selected nodes:[15, 5, 17, 10, 4],center node:10
################################################## episode6,iteration38 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05531554204563415,train_acc:0.9837114810943604
node4 epoch1:node_model train_loss:0.023852525582147198,train_acc:0.9925581216812134
node4_model on test-dataset: loss:0.059504500476759856,acc:0.98139888048172
node4 weight score:72229.83077857501
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03578301750085488,train_acc:0.9863266944885254
node5 epoch1:node_model train_loss:0.019770414259626855,train_acc:0.9931221008300781
node5_model on test-dataset: loss:0.0702240288356188,acc:0.9796957969665527
node5 weight score:68879.5570433947
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04514810970867984,train_acc:0.9860000014305115
node10 epoch1:node_model train_loss:0.02850289022317156,train_acc:0.9915000200271606
node10_model on test-dataset: loss:0.052944011076397145,acc:0.9839968681335449
node10 weight score:36170.285572747656
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04828700293520732,train_acc:0.9840601086616516
node15 epoch1:node_model train_loss:0.020614438382576088,train_acc:0.9921428561210632
node15_model on test-dataset: loss:0.06910200674254156,acc:0.9805980324745178
node15 weight score:19912.591035549292
node17: train data size:719
node17 epoch0:node_model train_loss:0.05843542201910168,train_acc:0.979671061038971
node17 epoch1:node_model train_loss:0.026974353211699054,train_acc:0.9899999499320984
node17_model on test-dataset: loss:0.07032908132649027,acc:0.9775999784469604
node17 weight score:10223.366869562396
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.037427894256543366,acc:0.9879979562759399
total cost energy:7.851815200332764 | all_enery_cp：6.572500000000001 | all_enery_tp: 1.279315200332763
ef: 32.378542943220516
reward: 24.52672774288775
step 399:loss:19.24491310119629|running q:71.38088989257812
episode6,iteration39 selected nodes:[5, 3, 10, 1, 9],center node:5
################################################## episode6,iteration39 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03855431551259678,train_acc:0.9873135685920715
node1 epoch1:node_model train_loss:0.025769797819249554,train_acc:0.9913434386253357
node1_model on test-dataset: loss:0.06485219591544591,acc:0.9796971678733826
node1 weight score:102648.79864175108
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03737745489802604,train_acc:0.9865788817405701
node3 epoch1:node_model train_loss:0.022550536511086027,train_acc:0.9931578636169434
node3_model on test-dataset: loss:0.0706845799171424,acc:0.9798969626426697
node3 weight score:53222.357753414915
node5: train data size:4837
node5 epoch0:node_model train_loss:0.029861088455488374,train_acc:0.990612268447876
node5 epoch1:node_model train_loss:0.02047706777419971,train_acc:0.9940818548202515
node5_model on test-dataset: loss:0.09287221022762879,acc:0.9747897386550903
node5 weight score:52082.3181460263
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04102265821050175,train_acc:0.987727165222168
node9 epoch1:node_model train_loss:0.02283304841363464,train_acc:0.9927272200584412
node9_model on test-dataset: loss:0.07728037543070969,acc:0.9770960211753845
node9 weight score:27497.27842491261
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04231974338181317,train_acc:0.9884998202323914
node10 epoch1:node_model train_loss:0.03132458984619006,train_acc:0.9904999136924744
node10_model on test-dataset: loss:0.06495255749017816,acc:0.9806002378463745
node10 weight score:29483.057696220476
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03749076110208989,acc:0.9887979543209076
total cost energy:11.091686083358102 | all_enery_cp：9.648 | all_enery_tp: 1.443686083358102
ef: 32.08852225286382
reward: 20.996836169505716
step 400:loss:12.579328536987305|running q:72.85273742675781
episode6,iteration40 selected nodes:[7, 13, 8, 17, 2],center node:7
################################################## episode6,iteration40 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.052154966471518606,train_acc:0.9836170077323914
node2 epoch1:node_model train_loss:0.035061321872722435,train_acc:0.9885106682777405
node2_model on test-dataset: loss:0.09082343987702188,acc:0.9747917056083679
node2 weight score:50757.82205829356
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03876123415633432,train_acc:0.9867566227912903
node7 epoch1:node_model train_loss:0.02038581625910828,train_acc:0.9924324154853821
node7_model on test-dataset: loss:0.07842553581338507,acc:0.9800949692726135
node7 weight score:46375.20116731246
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04629739106673261,train_acc:0.9852172136306763
node8 epoch1:node_model train_loss:0.03081927991107754,train_acc:0.9895169138908386
node8_model on test-dataset: loss:0.06983792876606458,acc:0.9803990125656128
node8 weight score:32790.2049854713
node13: train data size:1056
node13 epoch0:node_model train_loss:0.048266165805133904,train_acc:0.9874675273895264
node13 epoch1:node_model train_loss:0.037289683537727054,train_acc:0.9901948571205139
node13_model on test-dataset: loss:0.07882868094617151,acc:0.9755960702896118
node13 weight score:13396.1394168335
node17: train data size:719
node17 epoch0:node_model train_loss:0.046392185817239806,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.020741858912515454,train_acc:0.9912499189376831
node17_model on test-dataset: loss:0.07498837142076809,acc:0.9787977337837219
node17 weight score:9588.15328800263
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03939158855704591,acc:0.987097954750061
total cost energy:7.521973668286323 | all_enery_cp：6.156000000000001 | all_enery_tp: 1.3659736682863222
ef: 31.462236475506874
reward: 23.94026280722055
step 401:loss:25.663105010986328|running q:74.44486236572266
episode6,iteration41 selected nodes:[9, 2, 13, 12, 19],center node:12
################################################## episode6,iteration41 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03489603457459685,train_acc:0.9891490936279297
node2 epoch1:node_model train_loss:0.02053868678953261,train_acc:0.9934044480323792
node2_model on test-dataset: loss:0.06327167523399112,acc:0.9813960790634155
node2 weight score:72860.40685585314
node9: train data size:2125
node9 epoch0:node_model train_loss:0.038869997137226164,train_acc:0.9859091639518738
node9 epoch1:node_model train_loss:0.04087743143794465,train_acc:0.9859089851379395
node9_model on test-dataset: loss:0.08366618774954987,acc:0.9762970805168152
node9 weight score:25398.55175857983
node12: train data size:1406
node12 epoch0:node_model train_loss:0.051272384356707335,train_acc:0.982666552066803
node12 epoch1:node_model train_loss:0.028473032320228717,train_acc:0.9900000095367432
node12_model on test-dataset: loss:0.06810500417610456,acc:0.9804959893226624
node12 weight score:20644.591642111838
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06057842293838886,train_acc:0.9856492877006531
node13 epoch1:node_model train_loss:0.023289461387321353,train_acc:0.9920130968093872
node13_model on test-dataset: loss:0.056595877784930054,acc:0.9809969663619995
node13 weight score:18658.60273451195
node19: train data size:5781
node19 epoch0:node_model train_loss:0.053281022874028264,train_acc:0.9852639436721802
node19 epoch1:node_model train_loss:0.036408291948991346,train_acc:0.989097535610199
node19_model on test-dataset: loss:0.07233175328852667,acc:0.9764991402626038
node19 weight score:79923.40482802852
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040946189752576176,acc:0.9861979550123214
total cost energy:9.089 | all_enery_cp：7.489000000000001 | all_enery_tp: 1.6
ef: 32.182114139835335
reward: 23.093114139835336
step 402:loss:19.389177322387695|running q:75.9714584350586
episode6,iteration42 selected nodes:[10, 11, 19, 8, 17],center node:11
################################################## episode6,iteration42 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04383454784629461,train_acc:0.9869564175605774
node8 epoch1:node_model train_loss:0.025475671484499522,train_acc:0.9902898669242859
node8_model on test-dataset: loss:0.07842456526806928,acc:0.9767990708351135
node8 weight score:29200.03435367946
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04108790970058181,train_acc:0.9854998588562012
node10 epoch1:node_model train_loss:0.027105480519821867,train_acc:0.9909998774528503
node10_model on test-dataset: loss:0.05730708996503381,acc:0.9814969897270203
node10 weight score:33416.45861216206
node11: train data size:1575
node11 epoch0:node_model train_loss:0.048309222285752185,train_acc:0.9847915172576904
node11 epoch1:node_model train_loss:0.028968134487513453,train_acc:0.9912498593330383
node11_model on test-dataset: loss:0.07268391890116618,acc:0.9787940979003906
node11 weight score:21669.167318036973
node17: train data size:719
node17 epoch0:node_model train_loss:0.042796607594937086,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.017300049599725753,train_acc:0.9962499737739563
node17_model on test-dataset: loss:0.05966723765195638,acc:0.9816001653671265
node17 weight score:12050.16401452976
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04570905546505197,train_acc:0.9871606230735779
node19 epoch1:node_model train_loss:0.030207995453785205,train_acc:0.990689754486084
node19_model on test-dataset: loss:0.0629429934925065,acc:0.9801979660987854
node19 weight score:91845.01211700776
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03978724076630897,acc:0.9875989669561386
total cost energy:7.079834563766816 | all_enery_cp：6.14 | all_enery_tp: 0.9398345637668171
ef: 32.41060469725937
reward: 25.330770133492557
step 403:loss:21.7071475982666|running q:77.45247650146484
episode6,iteration43 selected nodes:[5, 12, 10, 1, 19],center node:12
################################################## episode6,iteration43 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03513555302946662,train_acc:0.9872010350227356
node1 epoch1:node_model train_loss:0.02288975779995767,train_acc:0.9917911887168884
node1_model on test-dataset: loss:0.07830095818178961,acc:0.978297233581543
node1 weight score:85018.11669462064
node5: train data size:4837
node5 epoch0:node_model train_loss:0.037080546972586545,train_acc:0.98699951171875
node5 epoch1:node_model train_loss:0.018013996613801132,train_acc:0.9937341213226318
node5_model on test-dataset: loss:0.07177382490936907,acc:0.9794952273368835
node5 weight score:67392.25624533488
node10: train data size:1915
node10 epoch0:node_model train_loss:0.04298455601092428,train_acc:0.9856666922569275
node10 epoch1:node_model train_loss:0.026744822337059306,train_acc:0.9909998774528503
node10_model on test-dataset: loss:0.06955023045040434,acc:0.9775990843772888
node10 weight score:27534.056862192134
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04749480600779255,train_acc:0.9860000014305115
node12 epoch1:node_model train_loss:0.04428840234953289,train_acc:0.9886666536331177
node12_model on test-dataset: loss:0.06886133473919472,acc:0.9802980422973633
node12 weight score:20417.844140330442
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0362151199028474,train_acc:0.9892699122428894
node19 epoch1:node_model train_loss:0.02346345867785015,train_acc:0.9927586913108826
node19_model on test-dataset: loss:0.06741317974901903,acc:0.9793949127197266
node19 weight score:85754.74442123645
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03992318099582917,acc:0.9874979573488235
total cost energy:11.68084271247462 | all_enery_cp：10.298 | all_enery_tp: 1.3828427124746192
ef: 32.33563173414713
reward: 20.654789021672507
step 404:loss:22.12760353088379|running q:78.82315063476562
episode6,iteration44 selected nodes:[12, 11, 2, 19, 10],center node:11
################################################## episode6,iteration44 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.044668166843698696,train_acc:0.986595869064331
node2 epoch1:node_model train_loss:0.0211162189562914,train_acc:0.9929787516593933
node2_model on test-dataset: loss:0.05725480435718055,acc:0.9848958253860474
node2 weight score:80517.26054709402
node10: train data size:1915
node10 epoch0:node_model train_loss:0.03434714357135817,train_acc:0.9864999651908875
node10 epoch1:node_model train_loss:0.03068949510998209,train_acc:0.9919999241828918
node10_model on test-dataset: loss:0.057898747261206154,acc:0.9826982021331787
node10 weight score:33074.98159434454
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05415210494538769,train_acc:0.981458306312561
node11 epoch1:node_model train_loss:0.025191759545123205,train_acc:0.9924998879432678
node11_model on test-dataset: loss:0.0826498925991109,acc:0.975989818572998
node11 weight score:19056.286106014166
node12: train data size:1406
node12 epoch0:node_model train_loss:0.01972824988576273,train_acc:0.9919999837875366
node12 epoch1:node_model train_loss:0.02600266052116543,train_acc:0.9933332800865173
node12_model on test-dataset: loss:0.07430397513395291,acc:0.9790960550308228
node12 weight score:18922.271620936925
node19: train data size:5781
node19 epoch0:node_model train_loss:0.033982061159772925,train_acc:0.9910346865653992
node19 epoch1:node_model train_loss:0.02020540934634106,train_acc:0.9937123656272888
node19_model on test-dataset: loss:0.06893109346939127,acc:0.977091908454895
node19 weight score:83866.36144930797
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03927246355066018,acc:0.9875989681482316
total cost energy:8.672038328578603 | all_enery_cp：7.6434999999999995 | all_enery_tp: 1.028538328578604
ef: 32.51602776991073
reward: 23.843989441332127
step 405:loss:15.416094779968262|running q:80.22661590576172
episode6,iteration45 selected nodes:[14, 4, 18, 12, 19],center node:12
################################################## episode6,iteration45 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05050290641266593,train_acc:0.9867395162582397
node4 epoch1:node_model train_loss:0.027196573066460186,train_acc:0.9902278184890747
node4_model on test-dataset: loss:0.08040925135615225,acc:0.9771000742912292
node4 weight score:53451.560952397216
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07150582092193265,train_acc:0.9748888611793518
node12 epoch1:node_model train_loss:0.04151155048360427,train_acc:0.9866666793823242
node12_model on test-dataset: loss:0.10839652886366821,acc:0.968794047832489
node12 weight score:12970.89505299884
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05598182955873199,train_acc:0.9799999594688416
node14 epoch1:node_model train_loss:0.0289788561349269,train_acc:0.9896875023841858
node14_model on test-dataset: loss:0.06476342099170324,acc:0.9803981781005859
node14 weight score:23778.855045308486
node18: train data size:801
node18 epoch0:node_model train_loss:0.018819403730958584,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.010203143342550902,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.10148605162427884,acc:0.9712870717048645
node18 weight score:7892.710251113702
node19: train data size:5781
node19 epoch0:node_model train_loss:0.026893245947450913,train_acc:0.9915521144866943
node19 epoch1:node_model train_loss:0.019328242111629968,train_acc:0.9934080243110657
node19_model on test-dataset: loss:0.10244373922992964,acc:0.9706999063491821
node19 weight score:56430.974146939785
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043754847044474444,acc:0.9858999764919281
total cost energy:8.638200195867574 | all_enery_cp：6.913 | all_enery_tp: 1.7252001958675738
ef: 31.130666946303542
reward: 22.492466750435966
step 406:loss:10.516620635986328|running q:81.5584945678711
episode6,iteration46 selected nodes:[11, 2, 6, 13, 14],center node:6
################################################## episode6,iteration46 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03556096938963821,train_acc:0.9868085980415344
node2 epoch1:node_model train_loss:0.011995482823856056,train_acc:0.9959573745727539
node2_model on test-dataset: loss:0.05609235432122659,acc:0.9842000603675842
node2 weight score:82185.88889315835
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04766079129088515,train_acc:0.9852777123451233
node6 epoch1:node_model train_loss:0.027838902113014936,train_acc:0.9918197989463806
node6_model on test-dataset: loss:0.057875422705801614,acc:0.9826977849006653
node6 weight score:60975.796547335485
node11: train data size:1575
node11 epoch0:node_model train_loss:0.055969625478610396,train_acc:0.9831249713897705
node11 epoch1:node_model train_loss:0.02616393214702839,train_acc:0.9912499189376831
node11_model on test-dataset: loss:0.05766612169492873,acc:0.9815919399261475
node11 weight score:27312.39684076948
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04916805869222365,train_acc:0.9827272295951843
node13 epoch1:node_model train_loss:0.024528602159328082,train_acc:0.9899999499320984
node13_model on test-dataset: loss:0.07174871415147209,acc:0.9782971143722534
node13 weight score:14718.033800168581
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04929025611636462,train_acc:0.9849998950958252
node14 epoch1:node_model train_loss:0.021517612971365452,train_acc:0.9918749332427979
node14_model on test-dataset: loss:0.06037042039853986,acc:0.9809988141059875
node14 weight score:25509.1813148488
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03653217865161423,acc:0.9890999746322632
total cost energy:7.522257114780907 | all_enery_cp：6.154999999999999 | all_enery_tp: 1.367257114780908
ef: 32.45829122999692
reward: 24.93603411521601
step 407:loss:18.136634826660156|running q:82.8874282836914
episode6,iteration47 selected nodes:[3, 10, 19, 6, 7],center node:6
################################################## episode6,iteration47 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.042180991719329826,train_acc:0.9855687022209167
node3 epoch1:node_model train_loss:0.025388492896270595,train_acc:0.9907894134521484
node3_model on test-dataset: loss:0.07403544275355671,acc:0.977294921875
node3 weight score:50813.50040037778
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03437859989612156,train_acc:0.989750862121582
node6 epoch1:node_model train_loss:0.02295383001263771,train_acc:0.9911110997200012
node6_model on test-dataset: loss:0.06082642940316873,acc:0.9825978875160217
node6 weight score:58017.54327233546
node7: train data size:3637
node7 epoch0:node_model train_loss:0.03495368283443354,train_acc:0.9867567420005798
node7 epoch1:node_model train_loss:0.019548180324302333,train_acc:0.994324266910553
node7_model on test-dataset: loss:0.06565995960429064,acc:0.9801948070526123
node7 weight score:55391.444373693084
node10: train data size:1915
node10 epoch0:node_model train_loss:0.045015177900131674,train_acc:0.9859998822212219
node10 epoch1:node_model train_loss:0.029726562672294676,train_acc:0.9899998903274536
node10_model on test-dataset: loss:0.06584248874547484,acc:0.979498028755188
node10 weight score:29084.562817829577
node19: train data size:5781
node19 epoch0:node_model train_loss:0.028997610151518843,train_acc:0.9912070631980896
node19 epoch1:node_model train_loss:0.02398322931599225,train_acc:0.9925459623336792
node19_model on test-dataset: loss:0.06012004344192974,acc:0.9818000793457031
node19 weight score:96157.61514849697
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03775667802252428,acc:0.9877979546785355
total cost energy:10.88083138057622 | all_enery_cp：9.312 | all_enery_tp: 1.568831380576221
ef: 32.5275989853766
reward: 21.64676760480038
step 408:loss:18.671384811401367|running q:84.16136169433594
episode6,iteration48 selected nodes:[9, 10, 17, 2, 15],center node:10
################################################## episode6,iteration48 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.023147517458555547,train_acc:0.9927660226821899
node2 epoch1:node_model train_loss:0.017783767396356277,train_acc:0.9944682717323303
node2_model on test-dataset: loss:0.06747020420565604,acc:0.9802958369255066
node2 weight score:68326.45690456562
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04008663463702595,train_acc:0.987727165222168
node9 epoch1:node_model train_loss:0.013844972021285106,train_acc:0.996363639831543
node9_model on test-dataset: loss:0.06533233823588488,acc:0.9813968539237976
node9 weight score:32526.00561038558
node10: train data size:1915
node10 epoch0:node_model train_loss:0.05969648935133591,train_acc:0.9821667075157166
node10 epoch1:node_model train_loss:0.02063123310217634,train_acc:0.9934999346733093
node10_model on test-dataset: loss:0.05049568813377846,acc:0.985299289226532
node10 weight score:37924.03016524068
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05474544529403959,train_acc:0.9838345050811768
node15 epoch1:node_model train_loss:0.03676583809595156,train_acc:0.9885714054107666
node15_model on test-dataset: loss:0.06850033352276114,acc:0.980697751045227
node15 weight score:20087.493435966786
node17: train data size:719
node17 epoch0:node_model train_loss:0.046536812631529756,train_acc:0.9887499809265137
node17 epoch1:node_model train_loss:0.02019792192731984,train_acc:0.9925000071525574
node17_model on test-dataset: loss:0.06314314568291593,acc:0.9810000658035278
node17 weight score:11386.825794371745
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04014121655993222,acc:0.9882989674806595
total cost energy:6.867653275036385 | all_enery_cp：5.3725 | all_enery_tp: 1.4951532750363852
ef: 32.305738120660756
reward: 25.43808484562437
step 409:loss:7.956235885620117|running q:85.5428237915039
episode6,iteration49 selected nodes:[1, 8, 2, 19, 15],center node:2
################################################## episode6,iteration49 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.0407102672251136,train_acc:0.9856716394424438
node1 epoch1:node_model train_loss:0.016037669309950087,train_acc:0.994328498840332
node1_model on test-dataset: loss:0.06949897628801409,acc:0.9807910919189453
node1 weight score:95785.58355179797
node2: train data size:4610
node2 epoch0:node_model train_loss:0.021744056770756366,train_acc:0.9917021989822388
node2 epoch1:node_model train_loss:0.016858758503927828,train_acc:0.9942553639411926
node2_model on test-dataset: loss:0.07491748922067927,acc:0.9784960746765137
node2 weight score:61534.363310289824
node8: train data size:2290
node8 epoch0:node_model train_loss:0.0513860982432759,train_acc:0.9869080781936646
node8 epoch1:node_model train_loss:0.03024308662355432,train_acc:0.9925603866577148
node8_model on test-dataset: loss:0.05533810224980698,acc:0.9844999313354492
node8 weight score:41381.97565327581
node15: train data size:1376
node15 epoch0:node_model train_loss:0.049409253597592136,train_acc:0.9821803569793701
node15 epoch1:node_model train_loss:0.02995883147897465,train_acc:0.9904887080192566
node15_model on test-dataset: loss:0.05688805613695877,acc:0.9819991588592529
node15 weight score:24187.854067069216
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03347698638892074,train_acc:0.9894424080848694
node19 epoch1:node_model train_loss:0.019068659407664734,train_acc:0.9948276877403259
node19_model on test-dataset: loss:0.05030526140668371,acc:0.9842969179153442
node19 weight score:114918.39696974358
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038723991220840616,acc:0.9876979553699493
total cost energy:12.091166020071206 | all_enery_cp：10.357 | all_enery_tp: 1.734166020071207
ef: 32.54252362627269
reward: 20.451357606201483
step 410:loss:14.972652435302734|running q:86.8875503540039
episode6,iteration50 selected nodes:[9, 11, 4, 6, 7],center node:11
################################################## episode6,iteration50 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04018883349246692,train_acc:0.988604724407196
node4 epoch1:node_model train_loss:0.029429608162053925,train_acc:0.9911628365516663
node4_model on test-dataset: loss:0.08695315702985681,acc:0.973293662071228
node4 weight score:49428.912610087405
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03396644530585036,train_acc:0.987777590751648
node6 epoch1:node_model train_loss:0.021277082688660204,train_acc:0.9922221302986145
node6_model on test-dataset: loss:0.062475984846778375,acc:0.9823980331420898
node6 weight score:56485.70420546121
node7: train data size:3637
node7 epoch0:node_model train_loss:0.037328865448315,train_acc:0.988728940486908
node7 epoch1:node_model train_loss:0.031082790964786465,train_acc:0.9897297024726868
node7_model on test-dataset: loss:0.08835357992735225,acc:0.9746974110603333
node7 weight score:41164.1498057066
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03134025932839987,train_acc:0.9863635897636414
node9 epoch1:node_model train_loss:0.03021862933052365,train_acc:0.9881817698478699
node9_model on test-dataset: loss:0.06381489735686045,acc:0.9830971360206604
node9 weight score:33299.43458369523
node11: train data size:1575
node11 epoch0:node_model train_loss:0.043297286436427385,train_acc:0.9847915172576904
node11 epoch1:node_model train_loss:0.023311741766519845,train_acc:0.9931249618530273
node11_model on test-dataset: loss:0.07036493317427812,acc:0.9808951616287231
node11 weight score:22383.30840305183
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038112370609815115,acc:0.9882979518175126
total cost energy:9.12696599093805 | all_enery_cp：7.582000000000001 | all_enery_tp: 1.5449659909380484
ef: 32.09916709756289
reward: 22.97220110662484
step 411:loss:19.118427276611328|running q:88.24951934814453
episode6,iteration51 selected nodes:[11, 0, 6, 18, 12],center node:12
################################################## episode6,iteration51 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03656846202082104,train_acc:0.9883975982666016
node0 epoch1:node_model train_loss:0.023134617332188,train_acc:0.9918698072433472
node0_model on test-dataset: loss:0.05117171141428116,acc:0.9841969609260559
node0 weight score:140018.7682212319
node6: train data size:3529
node6 epoch0:node_model train_loss:0.02298114771555346,train_acc:0.9930554628372192
node6 epoch1:node_model train_loss:0.01801362308348568,train_acc:0.9941666126251221
node6_model on test-dataset: loss:0.07473352746443197,acc:0.9799001812934875
node6 weight score:47221.10837976385
node11: train data size:1575
node11 epoch0:node_model train_loss:0.041469729112577625,train_acc:0.9866666197776794
node11 epoch1:node_model train_loss:0.022982112197496463,train_acc:0.9918749332427979
node11_model on test-dataset: loss:0.08094930347418995,acc:0.9760939478874207
node11 weight score:19456.622014075467
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03668192634359002,train_acc:0.9853333234786987
node12 epoch1:node_model train_loss:0.02375743329175748,train_acc:0.9906666278839111
node12_model on test-dataset: loss:0.06574510373684461,acc:0.9805947542190552
node12 weight score:21385.622960270044
node18: train data size:801
node18 epoch0:node_model train_loss:0.045602968247193426,train_acc:0.9911110401153564
node18 epoch1:node_model train_loss:0.01731479074805975,train_acc:0.9955554604530334
node18_model on test-dataset: loss:0.08373907965149556,acc:0.9776991605758667
node18 weight score:9565.42636166523
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03989004752023902,acc:0.9875979560613632
total cost energy:8.40393790195915 | all_enery_cp：7.238 | all_enery_tp: 1.1659379019591491
ef: 32.310174660667684
reward: 23.906236758708534
step 412:loss:9.325923919677734|running q:89.5521469116211
episode6,iteration52 selected nodes:[4, 15, 8, 1, 16],center node:8
################################################## episode6,iteration52 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.032174262051373276,train_acc:0.9895522594451904
node1 epoch1:node_model train_loss:0.01809182585045042,train_acc:0.9936189651489258
node1_model on test-dataset: loss:0.05885397878013464,acc:0.9831992387771606
node1 weight score:113110.4495903169
node4: train data size:4298
node4 epoch0:node_model train_loss:0.041285753775344686,train_acc:0.9899953603744507
node4 epoch1:node_model train_loss:0.022547781123606444,train_acc:0.9941861033439636
node4_model on test-dataset: loss:0.060206473419184475,acc:0.9815938472747803
node4 weight score:71387.6723865787
node8: train data size:2290
node8 epoch0:node_model train_loss:0.052697915635729696,train_acc:0.983333170413971
node8 epoch1:node_model train_loss:0.0213120804603576,train_acc:0.9926086068153381
node8_model on test-dataset: loss:0.06187813877317239,acc:0.9826979041099548
node8 weight score:37008.223669986044
node15: train data size:1376
node15 epoch0:node_model train_loss:0.048654274424604536,train_acc:0.9828946590423584
node15 epoch1:node_model train_loss:0.02596476667427591,train_acc:0.9885714054107666
node15_model on test-dataset: loss:0.06838251912654414,acc:0.9816979765892029
node15 weight score:20122.101636145722
node16: train data size:920
node16 epoch0:node_model train_loss:0.0490416431799531,train_acc:0.9880000352859497
node16 epoch1:node_model train_loss:0.021345115546137095,train_acc:0.9909998774528503
node16_model on test-dataset: loss:0.06478029618345317,acc:0.9795928001403809
node16 weight score:14201.849238148368
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038804058271271064,acc:0.9875989639759064
total cost energy:9.572457842230246 | all_enery_cp：7.7705 | all_enery_tp: 1.8019578422302462
ef: 32.2782489650864
reward: 22.70579112285615
step 413:loss:7.116276741027832|running q:90.85481262207031
episode6,iteration53 selected nodes:[17, 9, 16, 19, 10],center node:17
################################################## episode6,iteration53 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04763688301318325,train_acc:0.9836362600326538
node9 epoch1:node_model train_loss:0.029695951432751663,train_acc:0.9895455241203308
node9_model on test-dataset: loss:0.06105704449857512,acc:0.9828980565071106
node9 weight score:34803.51886422525
node10: train data size:1915
node10 epoch0:node_model train_loss:0.028267692064400763,train_acc:0.9899998903274536
node10 epoch1:node_model train_loss:0.027146263829490636,train_acc:0.9899999499320984
node10_model on test-dataset: loss:0.09786351993809149,acc:0.9730939865112305
node10 weight score:19568.067868511473
node16: train data size:920
node16 epoch0:node_model train_loss:0.03977690794272348,train_acc:0.9899999499320984
node16 epoch1:node_model train_loss:0.01139535178663209,train_acc:0.9959999918937683
node16_model on test-dataset: loss:0.06485013147245126,acc:0.9828960299491882
node16 weight score:14186.555664745592
node17: train data size:719
node17 epoch0:node_model train_loss:0.05233766103629023,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.010598284803563729,train_acc:0.9950000047683716
node17_model on test-dataset: loss:0.06093815154868025,acc:0.9818989634513855
node17 weight score:11798.848204734748
node19: train data size:5781
node19 epoch0:node_model train_loss:0.030236721548637182,train_acc:0.9908217787742615
node19 epoch1:node_model train_loss:0.023018529527250614,train_acc:0.9934485554695129
node19_model on test-dataset: loss:0.06556970108120368,acc:0.9811998605728149
node19 weight score:88165.72143345016
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03907986936133966,acc:0.9885999786853791
total cost energy:7.009008315442218 | all_enery_cp：5.73 | all_enery_tp: 1.279008315442217
ef: 32.14208738413081
reward: 25.133079068688595
step 414:loss:15.387762069702148|running q:92.1349105834961
episode6,iteration54 selected nodes:[19, 9, 7, 17, 2],center node:17
################################################## episode6,iteration54 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.033509924831958686,train_acc:0.9887234568595886
node2 epoch1:node_model train_loss:0.02138294995405731,train_acc:0.993617057800293
node2_model on test-dataset: loss:0.06919265927699599,acc:0.9793981909751892
node2 weight score:66625.56473722142
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0329595161055221,train_acc:0.9905403852462769
node7 epoch1:node_model train_loss:0.02347245377589118,train_acc:0.9921620488166809
node7_model on test-dataset: loss:0.059922408319871466,acc:0.9831981658935547
node7 weight score:60695.1573205361
node9: train data size:2125
node9 epoch0:node_model train_loss:0.028149764429227533,train_acc:0.9904544353485107
node9 epoch1:node_model train_loss:0.018860985368850048,train_acc:0.991363525390625
node9_model on test-dataset: loss:0.0646937167935539,acc:0.9814969897270203
node9 weight score:32847.084776117474
node17: train data size:719
node17 epoch0:node_model train_loss:0.043327310850145295,train_acc:0.9912499785423279
node17 epoch1:node_model train_loss:0.023636082070879638,train_acc:0.9884210228919983
node17_model on test-dataset: loss:0.07956859226243979,acc:0.9759989976882935
node17 weight score:9036.228737446228
node19: train data size:5781
node19 epoch0:node_model train_loss:0.018226037122076377,train_acc:0.9948276877403259
node19 epoch1:node_model train_loss:0.012408867922454976,train_acc:0.9956493973731995
node19_model on test-dataset: loss:0.07787518241070301,acc:0.9781960844993591
node19 weight score:74234.17603713338
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0386027286072931,acc:0.987597953081131
total cost energy:10.17142768548054 | all_enery_cp：8.436 | all_enery_tp: 1.7354276854805397
ef: 32.199091731314134
reward: 22.027664045833596
step 415:loss:11.96865177154541|running q:93.39155578613281
episode6,iteration55 selected nodes:[11, 12, 8, 15, 10],center node:11
################################################## episode6,iteration55 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.0333912977543862,train_acc:0.9886955618858337
node8 epoch1:node_model train_loss:0.020248395552777725,train_acc:0.9938163757324219
node8_model on test-dataset: loss:0.06874791713698869,acc:0.9802999496459961
node8 weight score:33310.100078186995
node10: train data size:1915
node10 epoch0:node_model train_loss:0.03854370840126649,train_acc:0.9899998903274536
node10 epoch1:node_model train_loss:0.029970452601264696,train_acc:0.9884998202323914
node10_model on test-dataset: loss:0.0740496909799856,acc:0.9796971082687378
node10 weight score:25861.012715334527
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04152066589449532,train_acc:0.9841665625572205
node11 epoch1:node_model train_loss:0.017525805251352722,train_acc:0.9922915697097778
node11_model on test-dataset: loss:0.060405056830131795,acc:0.9817969799041748
node11 weight score:26073.975965772857
node12: train data size:1406
node12 epoch0:node_model train_loss:0.033423222684844704,train_acc:0.9893333315849304
node12 epoch1:node_model train_loss:0.031036645449542752,train_acc:0.9893333315849304
node12_model on test-dataset: loss:0.06924851334884806,acc:0.979593813419342
node12 weight score:20303.684974680957
node15: train data size:1376
node15 epoch0:node_model train_loss:0.047549968219495246,train_acc:0.98714280128479
node15 epoch1:node_model train_loss:0.03152950182889721,train_acc:0.9897744059562683
node15_model on test-dataset: loss:0.07136475151670311,acc:0.9801968932151794
node15 weight score:19281.22736723806
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039595630465410064,acc:0.9870999771356582
total cost energy:5.028213595499959 | all_enery_cp：4.281000000000001 | all_enery_tp: 0.7472135954999579
ef: 32.36793272812654
reward: 27.33971913262658
step 416:loss:5.226633071899414|running q:94.61083984375
episode6,iteration56 selected nodes:[18, 12, 13, 14, 9],center node:12
################################################## episode6,iteration56 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:0.022552761847195638,train_acc:0.9909090995788574
node9 epoch1:node_model train_loss:0.013068112587048248,train_acc:0.9959090352058411
node9_model on test-dataset: loss:0.06339315894380888,acc:0.98189777135849
node9 weight score:33520.9671738173
node12: train data size:1406
node12 epoch0:node_model train_loss:0.02795068499399349,train_acc:0.9906666278839111
node12 epoch1:node_model train_loss:0.02340248429682106,train_acc:0.9953333139419556
node12_model on test-dataset: loss:0.07901807036403624,acc:0.9790977835655212
node12 weight score:17793.398314114205
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05452784174121916,train_acc:0.9849349856376648
node13 epoch1:node_model train_loss:0.018703584728593178,train_acc:0.9945454597473145
node13_model on test-dataset: loss:0.05962499991790537,acc:0.9811959862709045
node13 weight score:17710.69184828432
node14: train data size:1540
node14 epoch0:node_model train_loss:0.043022810728871264,train_acc:0.9899998903274536
node14 epoch1:node_model train_loss:0.04019716416951269,train_acc:0.987187385559082
node14_model on test-dataset: loss:0.05072239149332745,acc:0.9845000505447388
node14 weight score:30361.344460711945
node18: train data size:801
node18 epoch0:node_model train_loss:0.03361137455026943,train_acc:0.9888888597488403
node18 epoch1:node_model train_loss:0.009381581469319321,train_acc:0.995555579662323
node18_model on test-dataset: loss:0.062185808033245846,acc:0.9823950529098511
node18 weight score:12880.752463194955
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04046345687902431,acc:0.9877959334850311
total cost energy:5.156744663833898 | all_enery_cp：3.4640000000000004 | all_enery_tp: 1.6927446638338977
ef: 31.909167441035294
reward: 26.752422777201396
step 417:loss:10.87547492980957|running q:95.84285736083984
episode6,iteration57 selected nodes:[8, 14, 5, 0, 15],center node:8
################################################## episode6,iteration57 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.028634201602673985,train_acc:0.9904810786247253
node0 epoch1:node_model train_loss:0.017182740591831842,train_acc:0.9929169416427612
node0_model on test-dataset: loss:0.06248357366364871,acc:0.9829971790313721
node0 weight score:114670.13776403778
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04276581301486918,train_acc:0.9879591464996338
node5 epoch1:node_model train_loss:0.015308126039822035,train_acc:0.9951019883155823
node5_model on test-dataset: loss:0.051220031419288714,acc:0.9844968318939209
node5 weight score:94435.70934980834
node8: train data size:2290
node8 epoch0:node_model train_loss:0.03663294087164104,train_acc:0.9899516105651855
node8 epoch1:node_model train_loss:0.01608480735803428,train_acc:0.9934781193733215
node8_model on test-dataset: loss:0.055639510294931825,acc:0.984097957611084
node8 weight score:41157.802932866485
node14: train data size:1540
node14 epoch0:node_model train_loss:0.03822201213915832,train_acc:0.9887498617172241
node14 epoch1:node_model train_loss:0.01529613183811307,train_acc:0.9943749308586121
node14_model on test-dataset: loss:0.06700301637858502,acc:0.9814980030059814
node14 weight score:22984.03987215421
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04875619019315179,train_acc:0.9788345098495483
node15 epoch1:node_model train_loss:0.022241502035675303,train_acc:0.9921428561210632
node15_model on test-dataset: loss:0.06730143092665458,acc:0.9796958565711975
node15 weight score:20445.330523500626
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039255594462847515,acc:0.9873969453573227
total cost energy:9.875477664211887 | all_enery_cp：8.604000000000001 | all_enery_tp: 1.2714776642118868
ef: 32.739073705756496
reward: 22.86359604154461
step 418:loss:11.359407424926758|running q:97.06500244140625
episode6,iteration58 selected nodes:[10, 15, 11, 9, 1],center node:11
################################################## episode6,iteration58 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03125885615139199,train_acc:0.9912307262420654
node1 epoch1:node_model train_loss:0.01633356647208027,train_acc:0.9950748085975647
node1_model on test-dataset: loss:0.060411531336612825,acc:0.9833977222442627
node1 weight score:110194.19393471788
node9: train data size:2125
node9 epoch0:node_model train_loss:0.022087980602836153,train_acc:0.991363525390625
node9 epoch1:node_model train_loss:0.015043036490996283,train_acc:0.9945454597473145
node9_model on test-dataset: loss:0.05616597050036944,acc:0.9829989671707153
node9 weight score:37834.29683612469
node10: train data size:1915
node10 epoch0:node_model train_loss:0.03223207371775061,train_acc:0.9899998903274536
node10 epoch1:node_model train_loss:0.029490986715245526,train_acc:0.9904999136924744
node10_model on test-dataset: loss:0.05830207240792788,acc:0.9832980632781982
node10 weight score:32846.17374492505
node11: train data size:1575
node11 epoch0:node_model train_loss:0.04558889628970064,train_acc:0.9847915768623352
node11 epoch1:node_model train_loss:0.022769740418880247,train_acc:0.9937499165534973
node11_model on test-dataset: loss:0.08151445740477356,acc:0.9789949059486389
node11 weight score:19321.725864886474
node15: train data size:1376
node15 epoch0:node_model train_loss:0.047950650831418376,train_acc:0.9826315641403198
node15 epoch1:node_model train_loss:0.023809864568257972,train_acc:0.9919173121452332
node15_model on test-dataset: loss:0.06097464456164062,acc:0.9830998778343201
node15 weight score:22566.757213467165
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04181104207304088,acc:0.987098963856697
total cost energy:8.257508749109258 | all_enery_cp：6.824 | all_enery_tp: 1.4335087491092575
ef: 32.4781176620114
reward: 24.220608912902144
step 419:loss:7.53949499130249|running q:98.31550598144531
episode6,iteration59 selected nodes:[14, 9, 16, 8, 11],center node:8
################################################## episode6,iteration59 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.04449491713919839,train_acc:0.9852172136306763
node8 epoch1:node_model train_loss:0.016833969232444044,train_acc:0.9960386157035828
node8_model on test-dataset: loss:0.05954041489472729,acc:0.9824990630149841
node8 weight score:38461.27045048178
node9: train data size:2125
node9 epoch0:node_model train_loss:0.012619274616802366,train_acc:0.9950000047683716
node9 epoch1:node_model train_loss:0.013355985159498894,train_acc:0.9954544901847839
node9_model on test-dataset: loss:0.06728211529352848,acc:0.9805969595909119
node9 weight score:31583.43031769087
node11: train data size:1575
node11 epoch0:node_model train_loss:0.03196492722054245,train_acc:0.9906249046325684
node11 epoch1:node_model train_loss:0.029267567580973264,train_acc:0.9899998903274536
node11_model on test-dataset: loss:0.06719899932752015,acc:0.9804969429969788
node11 weight score:23437.849012060913
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05013912799040554,train_acc:0.9831249117851257
node14 epoch1:node_model train_loss:0.015584600068450527,train_acc:0.9968749284744263
node14_model on test-dataset: loss:0.06726212256839062,acc:0.9809001684188843
node14 weight score:22895.501081372546
node16: train data size:920
node16 epoch0:node_model train_loss:0.06864745507482439,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.023063710937276483,train_acc:0.9920000433921814
node16_model on test-dataset: loss:0.07987615105983423,acc:0.9795980453491211
node16 weight score:11517.830889358194
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039434662496223606,acc:0.9885989636182785
total cost energy:5.586193756954886 | all_enery_cp：4.225 | all_enery_tp: 1.3611937569548864
ef: 32.010500443780664
reward: 26.42430668682578
step 420:loss:13.071649551391602|running q:99.50515747070312
episode6_cost time: 14313.873608589172
episode7,iteration0 selected nodes:[16, 10, 14, 12, 9],center node:10
################################################## episode7,iteration0 ##################################################
node9: train data size:2125
node9 epoch0:node_model train_loss:1.2830633399161426,train_acc:0.5972726941108704
node9 epoch1:node_model train_loss:0.442358981479298,train_acc:0.862727165222168
node9_model on test-dataset: loss:0.460900435494259,acc:0.8511000871658325
node9 weight score:4610.540230280318
node10: train data size:1915
node10 epoch0:node_model train_loss:1.4522328197956085,train_acc:0.5536666512489319
node10 epoch1:node_model train_loss:0.46293083801865575,train_acc:0.859666645526886
node10_model on test-dataset: loss:0.7406247253064067,acc:0.7400432229042053
node10 weight score:2585.654967443516
node12: train data size:1406
node12 epoch0:node_model train_loss:1.5966771602630616,train_acc:0.5173333287239075
node12 epoch1:node_model train_loss:0.5556799829006195,train_acc:0.8313333988189697
node12_model on test-dataset: loss:0.6894853514805436,acc:0.7752598524093628
node12 weight score:2039.2021338537106
node14: train data size:1540
node14 epoch0:node_model train_loss:1.6642089299857616,train_acc:0.49781250953674316
node14 epoch1:node_model train_loss:0.6062359362840652,train_acc:0.8243749737739563
node14_model on test-dataset: loss:0.543856155667454,acc:0.8271888494491577
node14 weight score:2831.631092067012
node16: train data size:920
node16 epoch0:node_model train_loss:1.8476004719734191,train_acc:0.4220000207424164
node16 epoch1:node_model train_loss:0.783870393037796,train_acc:0.7599999904632568
node16_model on test-dataset: loss:0.8301832545176149,acc:0.7359524965286255
node16 weight score:1108.1890594560039
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.7994289617240429,acc:0.7434908935427665
total cost energy:5.286508749109258 | all_enery_cp：3.9530000000000003 | all_enery_tp: 1.3335087491092574
ef: 25.604354547095962
reward: 20.317845797986706
step 421:loss:22.249839782714844|running q:2.0887584686279297
episode7,iteration1 selected nodes:[7, 17, 16, 3, 10],center node:10
################################################## episode7,iteration1 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.5429040460210097,train_acc:0.8289303779602051
node3 epoch1:node_model train_loss:0.2615835517644882,train_acc:0.9169864654541016
node3_model on test-dataset: loss:0.2521680949255824,acc:0.9256901144981384
node3 weight score:14918.620062185932
node7: train data size:3637
node7 epoch0:node_model train_loss:0.49094982404966614,train_acc:0.8365084528923035
node7 epoch1:node_model train_loss:0.2513923248326456,train_acc:0.9204310178756714
node7_model on test-dataset: loss:0.3343890890013427,acc:0.8797895908355713
node7 weight score:10876.551058714109
node10: train data size:1915
node10 epoch0:node_model train_loss:0.5820375047624111,train_acc:0.8171667456626892
node10 epoch1:node_model train_loss:0.297373590990901,train_acc:0.9099999666213989
node10_model on test-dataset: loss:0.4064689435076434,acc:0.8641970753669739
node10 weight score:4711.307052082294
node16: train data size:920
node16 epoch0:node_model train_loss:0.8296242833137513,train_acc:0.7279999852180481
node16 epoch1:node_model train_loss:0.4102677643299103,train_acc:0.8690000772476196
node16_model on test-dataset: loss:0.5900172215350904,acc:0.7916786670684814
node16 weight score:1559.2765201096497
node17: train data size:719
node17 epoch0:node_model train_loss:0.8314859569072723,train_acc:0.7152631878852844
node17 epoch1:node_model train_loss:0.48505908995866776,train_acc:0.8455920815467834
node17_model on test-dataset: loss:0.5369041743129492,acc:0.8188905119895935
node17 weight score:1339.1588935214932
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.21843594824895263,acc:0.9306969493627548
total cost energy:7.140722328045784 | all_enery_cp：5.4765 | all_enery_tp: 1.6642223280457842
ef: 26.906203244644058
reward: 19.765480916598275
step 422:loss:18.646772384643555|running q:4.351187705993652
episode7,iteration2 selected nodes:[7, 8, 14, 18, 11],center node:11
################################################## episode7,iteration2 ##################################################
node7: train data size:3637
node7 epoch0:node_model train_loss:0.30108030986141515,train_acc:0.8972680568695068
node7 epoch1:node_model train_loss:0.18293734881523493,train_acc:0.945186197757721
node7_model on test-dataset: loss:0.19839659434510395,acc:0.9365890026092529
node7 weight score:18331.967905021418
node8: train data size:2290
node8 epoch0:node_model train_loss:0.3902601508990578,train_acc:0.8706280589103699
node8 epoch1:node_model train_loss:0.2464910143095514,train_acc:0.9183575510978699
node8_model on test-dataset: loss:0.2518483516504057,acc:0.915693998336792
node8 weight score:9092.773428903683
node11: train data size:1575
node11 epoch0:node_model train_loss:0.413713944144547,train_acc:0.8737499713897705
node11 epoch1:node_model train_loss:0.2404756648465991,train_acc:0.92291659116745
node11_model on test-dataset: loss:0.35244206158444286,acc:0.8817899227142334
node11 weight score:4468.819620789331
node14: train data size:1540
node14 epoch0:node_model train_loss:0.4159134142100811,train_acc:0.8681249618530273
node14 epoch1:node_model train_loss:0.24803095636889338,train_acc:0.9199999570846558
node14_model on test-dataset: loss:0.2591916738450527,acc:0.9206799268722534
node14 weight score:5941.548882162885
node18: train data size:801
node18 epoch0:node_model train_loss:0.4063882976770401,train_acc:0.8733333349227905
node18 epoch1:node_model train_loss:0.29078539046976304,train_acc:0.9033333659172058
node18_model on test-dataset: loss:0.49717595897149297,acc:0.8322991728782654
node18 weight score:1611.0996228720055
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.14916685218922793,acc:0.9546969473361969
total cost energy:6.200815200332762 | all_enery_cp：4.921499999999999 | all_enery_tp: 1.279315200332763
ef: 27.861301615588992
reward: 21.66048641525623
step 423:loss:11.322994232177734|running q:6.519134521484375
episode7,iteration3 selected nodes:[19, 0, 8, 11, 1],center node:11
################################################## episode7,iteration3 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.22670068509048885,train_acc:0.9237394332885742
node0 epoch1:node_model train_loss:0.14589319455747804,train_acc:0.9538888931274414
node0_model on test-dataset: loss:0.19512584810494446,acc:0.93628990650177
node0 weight score:36719.89164729447
node1: train data size:6657
node1 epoch0:node_model train_loss:0.21368176278783313,train_acc:0.9315266609191895
node1 epoch1:node_model train_loss:0.135993414878178,train_acc:0.957910418510437
node1_model on test-dataset: loss:0.16240245389577468,acc:0.9470957517623901
node1 weight score:40990.75993194213
node8: train data size:2290
node8 epoch0:node_model train_loss:0.25471158649610437,train_acc:0.9179226756095886
node8 epoch1:node_model train_loss:0.15719726746496948,train_acc:0.9577293395996094
node8_model on test-dataset: loss:0.16684425115119667,acc:0.9458980560302734
node8 weight score:13725.375517582377
node11: train data size:1575
node11 epoch0:node_model train_loss:0.2923187594860792,train_acc:0.9006249308586121
node11 epoch1:node_model train_loss:0.17196155479177833,train_acc:0.9477083086967468
node11_model on test-dataset: loss:0.16911452368833124,acc:0.946293830871582
node11 weight score:9313.215480549963
node19: train data size:5781
node19 epoch0:node_model train_loss:0.22936347409568983,train_acc:0.9231542944908142
node19 epoch1:node_model train_loss:0.13768892870124044,train_acc:0.957383930683136
node19_model on test-dataset: loss:0.16196161009953358,acc:0.94819575548172
node19 weight score:35693.64367548139
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.09832280022383202,acc:0.9664949238300323
total cost energy:13.322252995839547 | all_enery_cp：11.734 | all_enery_tp: 1.588252995839546
ef: 29.653657252304775
reward: 16.331404256465227
step 424:loss:19.10559844970703|running q:8.591512680053711
episode7,iteration4 selected nodes:[8, 5, 10, 9, 4],center node:8
################################################## episode7,iteration4 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.17661271634143452,train_acc:0.9436970949172974
node4 epoch1:node_model train_loss:0.11412804287879966,train_acc:0.9653345346450806
node4_model on test-dataset: loss:0.12284876866382546,acc:0.9612980484962463
node4 weight score:34986.10565451769
node5: train data size:4837
node5 epoch0:node_model train_loss:0.171776216781261,train_acc:0.9459182620048523
node5 epoch1:node_model train_loss:0.1129881969976182,train_acc:0.9623662233352661
node5_model on test-dataset: loss:0.18358558473410086,acc:0.9397948980331421
node5 weight score:26347.384556395027
node8: train data size:2290
node8 epoch0:node_model train_loss:0.19391860424176507,train_acc:0.9424153566360474
node8 epoch1:node_model train_loss:0.12274529639145602,train_acc:0.9633334875106812
node8_model on test-dataset: loss:0.1366237167431973,acc:0.956591784954071
node8 weight score:16761.365117187994
node9: train data size:2125
node9 epoch0:node_model train_loss:0.19582554071464323,train_acc:0.9372727870941162
node9 epoch1:node_model train_loss:0.11358659900724888,train_acc:0.9604544639587402
node9_model on test-dataset: loss:0.17545819688588382,acc:0.9448972344398499
node9 weight score:12111.146915422127
node10: train data size:1915
node10 epoch0:node_model train_loss:0.17839633747935296,train_acc:0.9433333277702332
node10 epoch1:node_model train_loss:0.11123455315828323,train_acc:0.9648331999778748
node10_model on test-dataset: loss:0.14694962303619832,acc:0.9497967958450317
node10 weight score:13031.676845664824
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.08072779513429851,acc:0.9742979556322098
total cost energy:8.606376888270985 | all_enery_cp：7.7325 | all_enery_tp: 0.8738768882709855
ef: 30.059479493190274
reward: 21.453102604919287
step 425:loss:10.176473617553711|running q:10.736919403076172
episode7,iteration5 selected nodes:[14, 4, 2, 17, 12],center node:12
################################################## episode7,iteration5 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.16157449444042557,train_acc:0.9499998688697815
node2 epoch1:node_model train_loss:0.11536879409500893,train_acc:0.9657446146011353
node2_model on test-dataset: loss:0.21199972037225961,acc:0.9320826530456543
node2 weight score:21745.31170090743
node4: train data size:4298
node4 epoch0:node_model train_loss:0.1529061534810205,train_acc:0.9529851078987122
node4 epoch1:node_model train_loss:0.09395585553504007,train_acc:0.9711529612541199
node4_model on test-dataset: loss:0.14378816344134976,acc:0.9539982676506042
node4 weight score:29891.194776634908
node12: train data size:1406
node12 epoch0:node_model train_loss:0.15009398981928826,train_acc:0.9482222199440002
node12 epoch1:node_model train_loss:0.11719612504045168,train_acc:0.9595555663108826
node12_model on test-dataset: loss:0.4754974151024362,acc:0.8517796397209167
node12 weight score:2956.903561078468
node14: train data size:1540
node14 epoch0:node_model train_loss:0.1888434262946248,train_acc:0.942187488079071
node14 epoch1:node_model train_loss:0.11127306311391294,train_acc:0.9662498831748962
node14_model on test-dataset: loss:0.15717250548419542,acc:0.9484927654266357
node14 weight score:9798.151370405276
node17: train data size:719
node17 epoch0:node_model train_loss:0.152338030282408,train_acc:0.9546710252761841
node17 epoch1:node_model train_loss:0.15832138434052467,train_acc:0.9571710228919983
node17_model on test-dataset: loss:0.24318789227399976,acc:0.9220980405807495
node17 weight score:2956.5616662769658
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07738384585245513,acc:0.9743979573249817
total cost energy:8.052464281142933 | all_enery_cp：6.2865 | all_enery_tp: 1.7659642811429335
ef: 28.529094912158868
reward: 20.476630631015937
step 426:loss:9.744739532470703|running q:12.837227821350098
episode7,iteration6 selected nodes:[11, 14, 9, 3, 12],center node:14
################################################## episode7,iteration6 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.14006067498734123,train_acc:0.9547793865203857
node3 epoch1:node_model train_loss:0.09400833505941064,train_acc:0.9698809385299683
node3_model on test-dataset: loss:0.11748538030311466,acc:0.9628981351852417
node3 weight score:32021.005424623592
node9: train data size:2125
node9 epoch0:node_model train_loss:0.15307339136912065,train_acc:0.952727198600769
node9 epoch1:node_model train_loss:0.09859109754589471,train_acc:0.9659088850021362
node9_model on test-dataset: loss:0.17714147792197765,acc:0.9462946653366089
node9 weight score:11996.061142359673
node11: train data size:1575
node11 epoch0:node_model train_loss:0.13609247421845794,train_acc:0.9583333730697632
node11 epoch1:node_model train_loss:0.12045508669689298,train_acc:0.9612499475479126
node11_model on test-dataset: loss:0.3181786435097456,acc:0.9079947471618652
node11 weight score:4950.049389319742
node12: train data size:1406
node12 epoch0:node_model train_loss:0.13125102287546422,train_acc:0.9553332924842834
node12 epoch1:node_model train_loss:0.08614896262685458,train_acc:0.9679999351501465
node12_model on test-dataset: loss:0.11231950538349338,acc:0.9636932015419006
node12 weight score:12517.86139192372
node14: train data size:1540
node14 epoch0:node_model train_loss:0.16194068687036633,train_acc:0.9449998736381531
node14 epoch1:node_model train_loss:0.08378468872979283,train_acc:0.9718748927116394
node14_model on test-dataset: loss:0.14796802758413832,acc:0.9525942206382751
node14 weight score:10407.6537691517
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.08029724546766374,acc:0.9743979531526565
total cost energy:6.679944432720121 | all_enery_cp：5.204000000000001 | all_enery_tp: 1.4759444327201203
ef: 29.6147506338515
reward: 22.93480620113138
step 427:loss:12.143872261047363|running q:14.915019989013672
episode7,iteration7 selected nodes:[4, 13, 7, 5, 16],center node:7
################################################## episode7,iteration7 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.12386840075081171,train_acc:0.9616277813911438
node4 epoch1:node_model train_loss:0.09194421012301085,train_acc:0.9711484909057617
node4_model on test-dataset: loss:0.20306297447445104,acc:0.9343967437744141
node4 weight score:21165.847743162874
node5: train data size:4837
node5 epoch0:node_model train_loss:0.12689045942103377,train_acc:0.9588361978530884
node5 epoch1:node_model train_loss:0.08258230419715448,train_acc:0.9723659753799438
node5_model on test-dataset: loss:0.11241795423731674,acc:0.9631969928741455
node5 weight score:43026.934912807505
node7: train data size:3637
node7 epoch0:node_model train_loss:0.14446957223117352,train_acc:0.9560262560844421
node7 epoch1:node_model train_loss:0.08501370648878652,train_acc:0.972242534160614
node7_model on test-dataset: loss:0.10391835743328556,acc:0.9650981426239014
node7 weight score:34998.62863339534
node13: train data size:1056
node13 epoch0:node_model train_loss:0.1460102932019667,train_acc:0.9556493163108826
node13 epoch1:node_model train_loss:0.11745175820859996,train_acc:0.9601948857307434
node13_model on test-dataset: loss:0.14192474963376298,acc:0.9551980495452881
node13 weight score:7440.562711754007
node16: train data size:920
node16 epoch0:node_model train_loss:0.13859146181493998,train_acc:0.949999988079071
node16 epoch1:node_model train_loss:0.11441641412675381,train_acc:0.9669999480247498
node16_model on test-dataset: loss:0.1266274849549518,acc:0.9611871242523193
node16 weight score:7265.405297493617
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0683116433012765,acc:0.9766979545354844
total cost energy:8.973070478491456 | all_enery_cp：7.374 | all_enery_tp: 1.5990704784914571
ef: 30.099589998034183
reward: 21.126519519542725
step 428:loss:8.497779846191406|running q:16.931289672851562
episode7,iteration8 selected nodes:[4, 0, 10, 19, 18],center node:10
################################################## episode7,iteration8 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.12175286918257673,train_acc:0.961858868598938
node0 epoch1:node_model train_loss:0.0909662444351448,train_acc:0.9704062342643738
node0_model on test-dataset: loss:0.15429908854304814,acc:0.9522991180419922
node0 weight score:46435.789528342066
node4: train data size:4298
node4 epoch0:node_model train_loss:0.11458782067652358,train_acc:0.9664782285690308
node4 epoch1:node_model train_loss:0.07322364079571048,train_acc:0.9767439961433411
node4_model on test-dataset: loss:0.2154713605181314,acc:0.9353960156440735
node4 weight score:19946.96645375446
node10: train data size:1915
node10 epoch0:node_model train_loss:0.11775609077885747,train_acc:0.9626666903495789
node10 epoch1:node_model train_loss:0.0909079376142472,train_acc:0.9699999094009399
node10_model on test-dataset: loss:0.11308337360445876,acc:0.9641969799995422
node10 weight score:16934.408118193012
node18: train data size:801
node18 epoch0:node_model train_loss:0.1436634419692887,train_acc:0.9744443893432617
node18 epoch1:node_model train_loss:0.1321579913298289,train_acc:0.9633333683013916
node18_model on test-dataset: loss:0.16425939719891175,acc:0.9505950212478638
node18 weight score:4876.433334465608
node19: train data size:5781
node19 epoch0:node_model train_loss:0.11889082288112619,train_acc:0.9672923684120178
node19 epoch1:node_model train_loss:0.08747024301054149,train_acc:0.9718157052993774
node19_model on test-dataset: loss:0.09312811154872179,acc:0.9696980118751526
node19 weight score:62075.77823561425
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.07236067167308648,acc:0.9763989645242691
total cost energy:11.803335676971777 | all_enery_cp：9.979999999999999 | all_enery_tp: 1.823335676971778
ef: 30.094172976488235
reward: 18.290837299516458
step 429:loss:9.662630081176758|running q:19.03213119506836
episode7,iteration9 selected nodes:[15, 18, 0, 1, 16],center node:15
################################################## episode7,iteration9 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.09712052462984705,train_acc:0.968525767326355
node0 epoch1:node_model train_loss:0.0825836438452825,train_acc:0.9740172624588013
node0_model on test-dataset: loss:0.17806795465789038,acc:0.9439918398857117
node0 weight score:40237.44762928073
node1: train data size:6657
node1 epoch0:node_model train_loss:0.10874526358362455,train_acc:0.9665645360946655
node1 epoch1:node_model train_loss:0.0866931174784454,train_acc:0.9719769954681396
node1_model on test-dataset: loss:0.1404589034046512,acc:0.9540870189666748
node1 weight score:47394.645968591256
node15: train data size:1376
node15 epoch0:node_model train_loss:0.1352248904960496,train_acc:0.9631203413009644
node15 epoch1:node_model train_loss:0.08412150872339096,train_acc:0.9760149717330933
node15_model on test-dataset: loss:0.10018732037686277,acc:0.9665980935096741
node15 weight score:13734.272908228942
node16: train data size:920
node16 epoch0:node_model train_loss:0.10404037898406386,train_acc:0.9709998965263367
node16 epoch1:node_model train_loss:0.06471967175602913,train_acc:0.9799999594688416
node16_model on test-dataset: loss:0.10120942275447305,acc:0.9692949652671814
node16 weight score:9090.062713151277
node18: train data size:801
node18 epoch0:node_model train_loss:0.09687586293047287,train_acc:0.9655556082725525
node18 epoch1:node_model train_loss:0.15813461215131813,train_acc:0.9666666388511658
node18_model on test-dataset: loss:0.774862096837228,acc:0.7946192622184753
node18 weight score:1033.7323289775816
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.061692015327280386,acc:0.9796969479322434
total cost energy:10.03030786435993 | all_enery_cp：8.4595 | all_enery_tp: 1.5708078643599295
ef: 30.02238251883921
reward: 19.99207465447928
step 430:loss:9.392541885375977|running q:21.054306030273438
episode7,iteration10 selected nodes:[0, 4, 1, 17, 2],center node:2
################################################## episode7,iteration10 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0989072058453328,train_acc:0.9691453576087952
node0 epoch1:node_model train_loss:0.0672942126273281,train_acc:0.9789426922798157
node0_model on test-dataset: loss:0.09437520724080968,acc:0.971796989440918
node0 weight score:75920.36308558923
node1: train data size:6657
node1 epoch0:node_model train_loss:0.09242538419732851,train_acc:0.972387969493866
node1 epoch1:node_model train_loss:0.06879598858641155,train_acc:0.9800340533256531
node1_model on test-dataset: loss:0.13614749078114982,acc:0.9586886167526245
node1 weight score:48895.502677319186
node2: train data size:4610
node2 epoch0:node_model train_loss:0.10625385413778589,train_acc:0.9663829207420349
node2 epoch1:node_model train_loss:0.07998900777323449,train_acc:0.9714893102645874
node2_model on test-dataset: loss:0.09223346250830218,acc:0.9704980850219727
node2 weight score:49981.85988718619
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07943778727636781,train_acc:0.9769672155380249
node4 epoch1:node_model train_loss:0.06506760688065442,train_acc:0.9764971733093262
node4_model on test-dataset: loss:0.08391396408202127,acc:0.973397970199585
node4 weight score:51219.12719793504
node17: train data size:719
node17 epoch0:node_model train_loss:0.11861474812030792,train_acc:0.9646710157394409
node17 epoch1:node_model train_loss:0.06383108452428132,train_acc:0.981249988079071
node17_model on test-dataset: loss:0.10883298675937113,acc:0.964599072933197
node17 weight score:6606.452890884116
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05594178121187724,acc:0.9819969421625138
total cost energy:13.381459551080336 | all_enery_cp：11.7245 | all_enery_tp: 1.656959551080336
ef: 30.950354234688287
reward: 17.56889468360795
step 431:loss:6.983670234680176|running q:23.04043960571289
episode7,iteration11 selected nodes:[12, 19, 5, 17, 4],center node:17
################################################## episode7,iteration11 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.08426400871817456,train_acc:0.9739486575126648
node4 epoch1:node_model train_loss:0.058967664588762576,train_acc:0.9785950183868408
node4_model on test-dataset: loss:0.14628722534820554,acc:0.9562950134277344
node4 weight score:29380.555887703304
node5: train data size:4837
node5 epoch0:node_model train_loss:0.10229295910317071,train_acc:0.9678763151168823
node5 epoch1:node_model train_loss:0.059530616870948246,train_acc:0.9806728363037109
node5_model on test-dataset: loss:0.11276088048369275,acc:0.9653958082199097
node5 weight score:42896.08221620367
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07605768022476696,train_acc:0.9793332815170288
node12 epoch1:node_model train_loss:0.06781040988862515,train_acc:0.9715555310249329
node12_model on test-dataset: loss:0.14808302378078225,acc:0.9575999975204468
node12 weight score:9494.673758697694
node17: train data size:719
node17 epoch0:node_model train_loss:0.09382288088090718,train_acc:0.9774999618530273
node17 epoch1:node_model train_loss:0.05747624847572297,train_acc:0.977171003818512
node17_model on test-dataset: loss:0.11008769015636062,acc:0.9652920961380005
node17 weight score:6531.157107382162
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0917190598195483,train_acc:0.9715516567230225
node19 epoch1:node_model train_loss:0.08177545268473954,train_acc:0.9744019508361816
node19_model on test-dataset: loss:0.11383851509483066,acc:0.9637978076934814
node19 weight score:50782.461411977005
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05908321043796604,acc:0.9806969481706619
total cost energy:9.938151140292337 | all_enery_cp：8.520499999999998 | all_enery_tp: 1.4176511402923393
ef: 30.551595918513456
reward: 20.61344477822112
step 432:loss:13.643511772155762|running q:25.110549926757812
episode7,iteration12 selected nodes:[13, 15, 12, 14, 1],center node:12
################################################## episode7,iteration12 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.08774437179872349,train_acc:0.9735063910484314
node1 epoch1:node_model train_loss:0.06943361406951253,train_acc:0.9757056832313538
node1_model on test-dataset: loss:0.08469434595783241,acc:0.9735928773880005
node1 weight score:78600.28818587707
node12: train data size:1406
node12 epoch0:node_model train_loss:0.09342716026779575,train_acc:0.9713333249092102
node12 epoch1:node_model train_loss:0.05887984732301751,train_acc:0.9806666374206543
node12_model on test-dataset: loss:0.07483887667767704,acc:0.9761939644813538
node12 weight score:18787.027042849535
node13: train data size:1056
node13 epoch0:node_model train_loss:0.1081062602725896,train_acc:0.9674674272537231
node13 epoch1:node_model train_loss:0.07905996785583821,train_acc:0.9656493663787842
node13_model on test-dataset: loss:0.09989745350787416,acc:0.9694938659667969
node13 weight score:10570.840025634523
node14: train data size:1540
node14 epoch0:node_model train_loss:0.11480938433669508,train_acc:0.9678124785423279
node14 epoch1:node_model train_loss:0.05584503768477589,train_acc:0.9821874499320984
node14_model on test-dataset: loss:0.09337802405876573,acc:0.9699982404708862
node14 weight score:16492.10309944907
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0941744175340448,train_acc:0.9726316332817078
node15 epoch1:node_model train_loss:0.04670344931738717,train_acc:0.9885714054107666
node15_model on test-dataset: loss:0.09748135168978479,acc:0.9696959257125854
node15 weight score:14115.520313863199
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.055786383682279846,acc:0.9823969465494156
total cost energy:7.468823307596588 | all_enery_cp：6.0175 | all_enery_tp: 1.451323307596588
ef: 31.28873996178618
reward: 23.819916654189594
step 433:loss:15.678149223327637|running q:27.028060913085938
episode7,iteration13 selected nodes:[19, 15, 12, 7, 0],center node:12
################################################## episode7,iteration13 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.07667611632496119,train_acc:0.9758227467536926
node0 epoch1:node_model train_loss:0.06247990073946615,train_acc:0.9788782596588135
node0_model on test-dataset: loss:0.07527930516574997,acc:0.9758966565132141
node0 weight score:95178.88062627708
node7: train data size:3637
node7 epoch0:node_model train_loss:0.10078702283066672,train_acc:0.9686194062232971
node7 epoch1:node_model train_loss:0.05873744890748246,train_acc:0.9802700877189636
node7_model on test-dataset: loss:0.1764945735083893,acc:0.941493809223175
node7 weight score:20606.865852604373
node12: train data size:1406
node12 epoch0:node_model train_loss:0.09573775244255861,train_acc:0.9739999771118164
node12 epoch1:node_model train_loss:0.06072907272415857,train_acc:0.9786666035652161
node12_model on test-dataset: loss:0.1304486336687114,acc:0.9620928764343262
node12 weight score:10778.188781729144
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0981937859739576,train_acc:0.9702631235122681
node15 epoch1:node_model train_loss:0.050777191030127664,train_acc:0.9840600490570068
node15_model on test-dataset: loss:0.06742477247054922,acc:0.9786946773529053
node15 weight score:20407.929453540677
node19: train data size:5781
node19 epoch0:node_model train_loss:0.0957502951636782,train_acc:0.9704363346099854
node19 epoch1:node_model train_loss:0.055536014630041756,train_acc:0.9830631613731384
node19_model on test-dataset: loss:0.09686457779462217,acc:0.9703952074050903
node19 weight score:59681.25946160843
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05501450721101719,acc:0.981596947312355
total cost energy:10.930623343471819 | all_enery_cp：9.682500000000001 | all_enery_tp: 1.2481233434718186
ef: 31.345934547252043
reward: 20.415311203780224
step 434:loss:15.10424518585205|running q:29.00840950012207
episode7,iteration14 selected nodes:[13, 12, 4, 6, 17],center node:12
################################################## episode7,iteration14 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.07380535574846489,train_acc:0.9776601791381836
node4 epoch1:node_model train_loss:0.05118047921420183,train_acc:0.9839439988136292
node4_model on test-dataset: loss:0.11367302348102384,acc:0.9619970321655273
node4 weight score:37810.20217798194
node6: train data size:3529
node6 epoch0:node_model train_loss:0.07800834296115984,train_acc:0.9752777218818665
node6 epoch1:node_model train_loss:0.051960132847954005,train_acc:0.9819443821907043
node6_model on test-dataset: loss:0.08569580477283126,acc:0.9725971817970276
node6 weight score:41180.54564461974
node12: train data size:1406
node12 epoch0:node_model train_loss:0.1277859496573607,train_acc:0.9608888626098633
node12 epoch1:node_model train_loss:0.07466958890290698,train_acc:0.9739999771118164
node12_model on test-dataset: loss:0.19645968302334949,acc:0.9418880343437195
node12 weight score:7156.684661009533
node13: train data size:1056
node13 epoch0:node_model train_loss:0.0771618876606226,train_acc:0.9705843925476074
node13 epoch1:node_model train_loss:0.05452638254924254,train_acc:0.9824025630950928
node13_model on test-dataset: loss:0.08170479575986973,acc:0.9741981029510498
node13 weight score:12924.577929349243
node17: train data size:719
node17 epoch0:node_model train_loss:0.08095438982127234,train_acc:0.979671061038971
node17 epoch1:node_model train_loss:0.0461624221643433,train_acc:0.9837499856948853
node17_model on test-dataset: loss:0.10441804471061915,acc:0.9653959274291992
node17 weight score:6885.783027182837
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05209669495146954,acc:0.9835979562997818
total cost energy:6.860062329783655 | all_enery_cp：5.504 | all_enery_tp: 1.356062329783655
ef: 30.60738952117336
reward: 23.747327191389704
step 435:loss:10.727532386779785|running q:30.895305633544922
episode7,iteration15 selected nodes:[8, 13, 12, 16, 6],center node:12
################################################## episode7,iteration15 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.06534866784608716,train_acc:0.9797220230102539
node6 epoch1:node_model train_loss:0.05513529705866757,train_acc:0.9811108708381653
node6_model on test-dataset: loss:0.08448053848725977,acc:0.9744950532913208
node6 weight score:41772.93449108633
node8: train data size:2290
node8 epoch0:node_model train_loss:0.10708501789232959,train_acc:0.965796947479248
node8 epoch1:node_model train_loss:0.06554108242625775,train_acc:0.9771980047225952
node8_model on test-dataset: loss:0.11165296073348145,acc:0.9649991393089294
node8 weight score:20509.98007537203
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07562577190498511,train_acc:0.9695554375648499
node12 epoch1:node_model train_loss:0.05376760304789059,train_acc:0.9786666631698608
node12_model on test-dataset: loss:0.10760235539630231,acc:0.968193769454956
node12 weight score:13066.628465721451
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06876343116164207,train_acc:0.9776622653007507
node13 epoch1:node_model train_loss:0.06074043939059431,train_acc:0.979285717010498
node13_model on test-dataset: loss:0.09232992760982597,acc:0.9704989194869995
node13 weight score:11437.244968527604
node16: train data size:920
node16 epoch0:node_model train_loss:0.0697452362626791,train_acc:0.9769999384880066
node16 epoch1:node_model train_loss:0.04361364226788282,train_acc:0.9869999885559082
node16_model on test-dataset: loss:0.11135140883030545,acc:0.9656938910484314
node16 weight score:8262.131657463255
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05371171350474469,acc:0.9822969460487365
total cost energy:5.640334563766817 | all_enery_cp：4.6005 | all_enery_tp: 1.039834563766817
ef: 30.916673609607106
reward: 25.276339045840288
step 436:loss:9.843339920043945|running q:32.814815521240234
episode7,iteration16 selected nodes:[2, 17, 3, 1, 13],center node:2
################################################## episode7,iteration16 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.06978162928178573,train_acc:0.9773499965667725
node1 epoch1:node_model train_loss:0.06552061099392265,train_acc:0.9812307357788086
node1_model on test-dataset: loss:0.11657535251695662,acc:0.9649967551231384
node1 weight score:57104.6954289218
node2: train data size:4610
node2 epoch0:node_model train_loss:0.08605118023548672,train_acc:0.973616898059845
node2 epoch1:node_model train_loss:0.06280730956016069,train_acc:0.9772337675094604
node2_model on test-dataset: loss:0.08591211889695842,acc:0.9739981889724731
node2 weight score:53659.48435667334
node3: train data size:3762
node3 epoch0:node_model train_loss:0.08714652233021825,train_acc:0.9709930419921875
node3 epoch1:node_model train_loss:0.06131560430175772,train_acc:0.9788879752159119
node3_model on test-dataset: loss:0.07885264950455166,acc:0.9761970043182373
node3 weight score:47709.240255558485
node13: train data size:1056
node13 epoch0:node_model train_loss:0.07850720394741405,train_acc:0.9780519008636475
node13 epoch1:node_model train_loss:0.05526990147138184,train_acc:0.9800000190734863
node13_model on test-dataset: loss:0.15033782979560784,acc:0.953993022441864
node13 weight score:7024.180151035088
node17: train data size:719
node17 epoch0:node_model train_loss:0.10706697450950742,train_acc:0.9696710109710693
node17 epoch1:node_model train_loss:0.0627055752556771,train_acc:0.9837499260902405
node17_model on test-dataset: loss:0.08795791855984135,acc:0.9714989066123962
node17 weight score:8174.363511237877
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.053934780374693216,acc:0.9822979533672332
total cost energy:10.218552506059645 | all_enery_cp：8.402000000000001 | all_enery_tp: 1.8165525060596441
ef: 30.700544827605334
reward: 20.48199232154569
step 437:loss:13.15286922454834|running q:34.627288818359375
episode7,iteration17 selected nodes:[2, 16, 14, 13, 7],center node:7
################################################## episode7,iteration17 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.0756625821021326,train_acc:0.9757446050643921
node2 epoch1:node_model train_loss:0.05342840953195031,train_acc:0.982553243637085
node2_model on test-dataset: loss:0.08686932094104122,acc:0.9725961685180664
node2 weight score:53068.21729536527
node7: train data size:3637
node7 epoch0:node_model train_loss:0.09009599428925966,train_acc:0.970701277256012
node7 epoch1:node_model train_loss:0.05803946439315238,train_acc:0.9808107614517212
node7_model on test-dataset: loss:0.09675245762511622,acc:0.968393862247467
node7 weight score:37590.77639239069
node13: train data size:1056
node13 epoch0:node_model train_loss:0.0664718695492907,train_acc:0.9803895950317383
node13 epoch1:node_model train_loss:0.05175067162649198,train_acc:0.9831168055534363
node13_model on test-dataset: loss:0.09343205118639161,acc:0.9731980562210083
node13 weight score:11302.331336955669
node14: train data size:1540
node14 epoch0:node_model train_loss:0.10302073380444199,train_acc:0.9703125357627869
node14 epoch1:node_model train_loss:0.052684882015455514,train_acc:0.9821873307228088
node14_model on test-dataset: loss:0.0776087910705246,acc:0.9754979610443115
node14 weight score:19843.112858188608
node16: train data size:920
node16 epoch0:node_model train_loss:0.07915217941626906,train_acc:0.9709998965263367
node16 epoch1:node_model train_loss:0.060195501614362004,train_acc:0.9809999465942383
node16_model on test-dataset: loss:0.11354596568620763,acc:0.9649960994720459
node16 weight score:8102.445511296153
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.051627141819917594,acc:0.9824969446659089
total cost energy:7.374997669491374 | all_enery_cp：5.881500000000001 | all_enery_tp: 1.493497669491373
ef: 30.917510365975943
reward: 23.542512696484568
step 438:loss:10.622772216796875|running q:36.50259017944336
episode7,iteration18 selected nodes:[13, 17, 14, 9, 6],center node:17
################################################## episode7,iteration18 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.060305483277059264,train_acc:0.978888750076294
node6 epoch1:node_model train_loss:0.0413351217414149,train_acc:0.9863887429237366
node6_model on test-dataset: loss:0.09935468772891909,acc:0.9699980616569519
node6 weight score:35519.20981955657
node9: train data size:2125
node9 epoch0:node_model train_loss:0.08191884679465809,train_acc:0.9754544496536255
node9 epoch1:node_model train_loss:0.04913037769835104,train_acc:0.9854544997215271
node9_model on test-dataset: loss:0.10404363648005528,acc:0.969399094581604
node9 weight score:20424.12272284767
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06423284964297306,train_acc:0.9800000190734863
node13 epoch1:node_model train_loss:0.04178080593489788,train_acc:0.9901948571205139
node13_model on test-dataset: loss:0.082573214941076,acc:0.9743989109992981
node13 weight score:12788.650662972956
node14: train data size:1540
node14 epoch0:node_model train_loss:0.08837213646620512,train_acc:0.9721874594688416
node14 epoch1:node_model train_loss:0.053916061675408855,train_acc:0.9831249117851257
node14_model on test-dataset: loss:0.08087109530111775,acc:0.9748961925506592
node14 weight score:19042.650458311713
node17: train data size:719
node17 epoch0:node_model train_loss:0.11203979537822306,train_acc:0.9705920815467834
node17 epoch1:node_model train_loss:0.050336727988906205,train_acc:0.98499995470047
node17_model on test-dataset: loss:0.12091480219503864,acc:0.9597846269607544
node17 weight score:5946.335659055495
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05282707955891965,acc:0.9829989659786225
total cost energy:6.29485120133694 | all_enery_cp：4.4845 | all_enery_tp: 1.8103512013369405
ef: 30.719225303898078
reward: 24.42437410256114
step 439:loss:12.173856735229492|running q:38.3756217956543
episode7,iteration19 selected nodes:[0, 17, 6, 9, 3],center node:3
################################################## episode7,iteration19 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0751686559847763,train_acc:0.97714763879776
node0 epoch1:node_model train_loss:0.04891521631119152,train_acc:0.9838038682937622
node0_model on test-dataset: loss:0.10578449252276187,acc:0.9692948460578918
node0 weight score:67732.04492575594
node3: train data size:3762
node3 epoch0:node_model train_loss:0.07013629489627324,train_acc:0.9757893085479736
node3 epoch1:node_model train_loss:0.04289475405637763,train_acc:0.9853650331497192
node3_model on test-dataset: loss:0.07762837274080084,acc:0.9753960371017456
node3 weight score:48461.66249756674
node6: train data size:3529
node6 epoch0:node_model train_loss:0.050650280667468905,train_acc:0.9844443202018738
node6 epoch1:node_model train_loss:0.03922455321622288,train_acc:0.9844443202018738
node6_model on test-dataset: loss:0.11992788246167038,acc:0.9625970721244812
node6 weight score:29426.01776636795
node9: train data size:2125
node9 epoch0:node_model train_loss:0.050085859203880485,train_acc:0.9818180799484253
node9 epoch1:node_model train_loss:0.043290329800749365,train_acc:0.987727165222168
node9_model on test-dataset: loss:0.09252741830190643,acc:0.9730978012084961
node9 weight score:22966.165478284143
node17: train data size:719
node17 epoch0:node_model train_loss:0.06756790226791054,train_acc:0.9799998998641968
node17 epoch1:node_model train_loss:0.05603687226539478,train_acc:0.9824999570846558
node17_model on test-dataset: loss:0.13360907054686322,acc:0.9607990980148315
node17 weight score:5381.371167819117
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05027023346934584,acc:0.9840969449281692
total cost energy:10.625938161762768 | all_enery_cp：8.65 | all_enery_tp: 1.9759381617627687
ef: 30.81672705213777
reward: 20.190788890375003
step 440:loss:12.434064865112305|running q:40.1799430847168
episode7,iteration20 selected nodes:[2, 19, 14, 8, 7],center node:8
################################################## episode7,iteration20 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06650064420945784,train_acc:0.9808510541915894
node2 epoch1:node_model train_loss:0.04659257494309481,train_acc:0.9825531840324402
node2_model on test-dataset: loss:0.10603978070459562,acc:0.9701929092407227
node2 weight score:43474.25060074845
node7: train data size:3637
node7 epoch0:node_model train_loss:0.0628342133493641,train_acc:0.9816214442253113
node7 epoch1:node_model train_loss:0.05019113707129617,train_acc:0.9824321866035461
node7_model on test-dataset: loss:0.07836042265145807,acc:0.9764937162399292
node7 weight score:46413.736385485485
node8: train data size:2290
node8 epoch0:node_model train_loss:0.08192325314587873,train_acc:0.976425051689148
node8 epoch1:node_model train_loss:0.054603181860369186,train_acc:0.9847341775894165
node8_model on test-dataset: loss:0.06751395810540999,acc:0.9783960580825806
node8 weight score:33918.91194446944
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07069795450661331,train_acc:0.9709374904632568
node14 epoch1:node_model train_loss:0.045650154701434076,train_acc:0.9796874523162842
node14_model on test-dataset: loss:0.0717208444478456,acc:0.9777999520301819
node14 weight score:21472.139820103017
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07661115920877662,train_acc:0.9779311418533325
node19 epoch1:node_model train_loss:0.05769601540543653,train_acc:0.9834079146385193
node19_model on test-dataset: loss:0.07485775129345712,acc:0.9748961329460144
node19 weight score:77226.47154250389
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.049185498490260215,acc:0.9837969422340394
total cost energy:10.548417343748314 | all_enery_cp：8.929 | all_enery_tp: 1.6194173437483137
ef: 31.911946235989255
reward: 21.363528892240943
step 441:loss:12.679620742797852|running q:41.9874382019043
episode7,iteration21 selected nodes:[18, 7, 13, 4, 6],center node:7
################################################## episode7,iteration21 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.06955788167583388,train_acc:0.976506769657135
node4 epoch1:node_model train_loss:0.03791944273249354,train_acc:0.9886047840118408
node4_model on test-dataset: loss:0.06908300643841357,acc:0.9774938821792603
node4 weight score:62215.01092069002
node6: train data size:3529
node6 epoch0:node_model train_loss:0.05383292548746491,train_acc:0.9811108708381653
node6 epoch1:node_model train_loss:0.03595315458046065,train_acc:0.9844443202018738
node6_model on test-dataset: loss:0.0914698780834442,acc:0.9740960001945496
node6 weight score:38581.0069275553
node7: train data size:3637
node7 epoch0:node_model train_loss:0.07102352128414488,train_acc:0.9789187908172607
node7 epoch1:node_model train_loss:0.03990865051998077,train_acc:0.9875674247741699
node7_model on test-dataset: loss:0.0892821180939427,acc:0.971290111541748
node7 weight score:40736.04073968257
node13: train data size:1056
node13 epoch0:node_model train_loss:0.06884073618460786,train_acc:0.9811038970947266
node13 epoch1:node_model train_loss:0.030491289344023575,train_acc:0.993636429309845
node13_model on test-dataset: loss:0.09365776984050171,acc:0.9711990356445312
node13 weight score:11275.092304657242
node18: train data size:801
node18 epoch0:node_model train_loss:0.04959179927714609,train_acc:0.9822221398353577
node18 epoch1:node_model train_loss:0.025986887995208638,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.24008756975028972,acc:0.9344929456710815
node18 weight score:3336.2826773293764
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05105929545319668,acc:0.9839969503879548
total cost energy:8.155653275036386 | all_enery_cp：6.6605 | all_enery_tp: 1.4951532750363852
ef: 31.036792958015408
reward: 22.881139682979022
step 442:loss:14.96192455291748|running q:43.716552734375
episode7,iteration22 selected nodes:[19, 0, 2, 12, 8],center node:12
################################################## episode7,iteration22 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0643629103886067,train_acc:0.9795727133750916
node0 epoch1:node_model train_loss:0.03927534075854863,train_acc:0.9870835542678833
node0_model on test-dataset: loss:0.06261581920960452,acc:0.9795979261398315
node0 weight score:114427.95271935011
node2: train data size:4610
node2 epoch0:node_model train_loss:0.061114344517997604,train_acc:0.9814891815185547
node2 epoch1:node_model train_loss:0.047559040452432914,train_acc:0.9846807718276978
node2_model on test-dataset: loss:0.07960372947054566,acc:0.9769949316978455
node2 weight score:57911.85954052261
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07427874980899303,train_acc:0.9723671674728394
node8 epoch1:node_model train_loss:0.05255047495112471,train_acc:0.9834781289100647
node8_model on test-dataset: loss:0.11671879457566775,acc:0.9668969511985779
node8 weight score:19619.80509073381
node12: train data size:1406
node12 epoch0:node_model train_loss:0.07337963637886182,train_acc:0.9746666550636292
node12 epoch1:node_model train_loss:0.023453484165171783,train_acc:0.9913333654403687
node12_model on test-dataset: loss:0.06921056097533437,acc:0.9791971445083618
node12 weight score:20314.818723996148
node19: train data size:5781
node19 epoch0:node_model train_loss:0.07245147249115438,train_acc:0.9783270359039307
node19 epoch1:node_model train_loss:0.044151408754385495,train_acc:0.9875461459159851
node19_model on test-dataset: loss:0.056780568176764065,acc:0.9826958775520325
node19 weight score:101813.00021519898
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04508306081348565,acc:0.9844969439506531
total cost energy:12.225322955501367 | all_enery_cp：10.626 | all_enery_tp: 1.5993229555013682
ef: 32.287240356204244
reward: 20.061917400702875
step 443:loss:13.229425430297852|running q:45.5119743347168
episode7,iteration23 selected nodes:[12, 6, 4, 16, 19],center node:12
################################################## episode7,iteration23 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05388035970945777,train_acc:0.9837019443511963
node4 epoch1:node_model train_loss:0.0336951433334413,train_acc:0.9899905920028687
node4_model on test-dataset: loss:0.0675791226819274,acc:0.9773982167243958
node4 weight score:63599.523483447185
node6: train data size:3529
node6 epoch0:node_model train_loss:0.042725563016978815,train_acc:0.9872221350669861
node6 epoch1:node_model train_loss:0.03195494686951861,train_acc:0.989597737789154
node6_model on test-dataset: loss:0.10545020427658529,acc:0.9689950346946716
node6 weight score:33466.032846591625
node12: train data size:1406
node12 epoch0:node_model train_loss:0.10125608956441283,train_acc:0.9708888530731201
node12 epoch1:node_model train_loss:0.05887012149517735,train_acc:0.981999933719635
node12_model on test-dataset: loss:0.14652364567613405,acc:0.9542969465255737
node12 weight score:9595.720837493542
node16: train data size:920
node16 epoch0:node_model train_loss:0.05853555002249777,train_acc:0.9809999465942383
node16 epoch1:node_model train_loss:0.03286853253375739,train_acc:0.9909999966621399
node16_model on test-dataset: loss:0.10190927386516706,acc:0.9684959053993225
node16 weight score:9027.637673262425
node19: train data size:5781
node19 epoch0:node_model train_loss:0.06678220169450125,train_acc:0.9794423580169678
node19 epoch1:node_model train_loss:0.03959831463751094,train_acc:0.9863795042037964
node19_model on test-dataset: loss:0.07354008966707624,acc:0.9762980341911316
node19 weight score:78610.1842705278
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0425652231607819,acc:0.9866969472169876
total cost energy:9.323062329783655 | all_enery_cp：7.9670000000000005 | all_enery_tp: 1.356062329783655
ef: 31.33952483610426
reward: 22.016462506320607
step 444:loss:7.72327995300293|running q:47.30447769165039
episode7,iteration24 selected nodes:[1, 4, 12, 10, 18],center node:12
################################################## episode7,iteration24 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.06658704367591374,train_acc:0.9797747731208801
node1 epoch1:node_model train_loss:0.041329440000508706,train_acc:0.9867531657218933
node1_model on test-dataset: loss:0.0717125202229363,acc:0.9772961139678955
node1 weight score:92828.97852850592
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04708706309239185,train_acc:0.9869768023490906
node4 epoch1:node_model train_loss:0.030258997898483866,train_acc:0.9890697598457336
node4_model on test-dataset: loss:0.07459557706635678,acc:0.9760966897010803
node4 weight score:57617.357074357074
node10: train data size:1915
node10 epoch0:node_model train_loss:0.07580815199762583,train_acc:0.9784998297691345
node10 epoch1:node_model train_loss:0.042337502929149194,train_acc:0.9854998588562012
node10_model on test-dataset: loss:0.08578131037211278,acc:0.9731967449188232
node10 weight score:22324.210153620596
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05405618439738949,train_acc:0.9806665778160095
node12 epoch1:node_model train_loss:0.032338467946586506,train_acc:0.987333357334137
node12_model on test-dataset: loss:0.06091654959891457,acc:0.9817960262298584
node12 weight score:23080.75571018639
node18: train data size:801
node18 epoch0:node_model train_loss:0.04739600673486974,train_acc:0.9822222590446472
node18 epoch1:node_model train_loss:0.03693373113249739,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.0783280436656787,acc:0.9772977232933044
node18 weight score:10226.222467892138
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04095736713032238,acc:0.9874979519844055
total cost energy:9.253798244508296 | all_enery_cp：7.5385 | all_enery_tp: 1.7152982445082952
ef: 31.94042337728915
reward: 22.686625132780854
step 445:loss:11.685335159301758|running q:49.061378479003906
episode7,iteration25 selected nodes:[3, 9, 7, 17, 15],center node:17
################################################## episode7,iteration25 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.07287574304561865,train_acc:0.9776314496994019
node3 epoch1:node_model train_loss:0.03936039179710573,train_acc:0.9861543774604797
node3_model on test-dataset: loss:0.13043047325539192,acc:0.95899498462677
node3 weight score:28842.952924304296
node7: train data size:3637
node7 epoch0:node_model train_loss:0.06215415979307648,train_acc:0.9789188504219055
node7 epoch1:node_model train_loss:0.0515982011534475,train_acc:0.9829728603363037
node7_model on test-dataset: loss:0.10945924639177974,acc:0.9656930565834045
node7 weight score:33226.97825802987
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06507631636817347,train_acc:0.9790908098220825
node9 epoch1:node_model train_loss:0.026725798030383885,train_acc:0.9909089803695679
node9_model on test-dataset: loss:0.06374162121777772,acc:0.9805958271026611
node9 weight score:33337.71497182019
node15: train data size:1376
node15 epoch0:node_model train_loss:0.07395325521273273,train_acc:0.9840600490570068
node15 epoch1:node_model train_loss:0.03778122192514794,train_acc:0.9854886531829834
node15_model on test-dataset: loss:0.0963441618649449,acc:0.9693959355354309
node15 weight score:14282.131614045018
node17: train data size:719
node17 epoch0:node_model train_loss:0.06305353937204927,train_acc:0.9824999570846558
node17 epoch1:node_model train_loss:0.015290841765818186,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.06159275393642019,acc:0.9792981743812561
node17 weight score:11673.451080661142
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04466283085697796,acc:0.9845969474315643
total cost energy:7.725943655149935 | all_enery_cp：5.8095 | all_enery_tp: 1.9164436551499349
ef: 31.45338732110901
reward: 23.727443665959072
step 446:loss:11.045164108276367|running q:50.86282730102539
episode7,iteration26 selected nodes:[7, 19, 5, 8, 6],center node:6
################################################## episode7,iteration26 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.07404185356382205,train_acc:0.9762437343597412
node5 epoch1:node_model train_loss:0.043911091051995754,train_acc:0.9867348074913025
node5_model on test-dataset: loss:0.10160843078083417,acc:0.9711962938308716
node5 weight score:47604.31750425552
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04202026652637869,train_acc:0.9858331680297852
node6 epoch1:node_model train_loss:0.03422392173928933,train_acc:0.9894443154335022
node6_model on test-dataset: loss:0.0797520704715862,acc:0.9753949642181396
node6 weight score:44249.634888881046
node7: train data size:3637
node7 epoch0:node_model train_loss:0.044589731131202064,train_acc:0.9835938215255737
node7 epoch1:node_model train_loss:0.03588811201169281,train_acc:0.9875674247741699
node7_model on test-dataset: loss:0.07929144074128999,acc:0.9761959910392761
node7 weight score:45868.75917498721
node8: train data size:2290
node8 epoch0:node_model train_loss:0.07277011709368747,train_acc:0.9769564270973206
node8 epoch1:node_model train_loss:0.051211632221289306,train_acc:0.9855071306228638
node8_model on test-dataset: loss:0.06841502606635913,acc:0.979896068572998
node8 weight score:33472.17901779085
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04916808074581083,train_acc:0.9842295050621033
node19 epoch1:node_model train_loss:0.035969934337933,train_acc:0.989229679107666
node19_model on test-dataset: loss:0.06472740188241005,acc:0.9803981781005859
node19 weight score:89313.02403427707
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040809068529633806,acc:0.9871979522705078
total cost energy:11.153227766016837 | all_enery_cp：10.036999999999999 | all_enery_tp: 1.116227766016838
ef: 32.10709718673274
reward: 20.9538694207159
step 447:loss:7.69061279296875|running q:52.59449005126953
episode7,iteration27 selected nodes:[1, 10, 17, 12, 11],center node:11
################################################## episode7,iteration27 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.05100685006021453,train_acc:0.9834330081939697
node1 epoch1:node_model train_loss:0.034685692527512114,train_acc:0.9889553189277649
node1_model on test-dataset: loss:0.08893396994038995,acc:0.9741958975791931
node1 weight score:74853.28726989258
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06325647790217773,train_acc:0.9749999046325684
node10 epoch1:node_model train_loss:0.04366477504372597,train_acc:0.9836664199829102
node10_model on test-dataset: loss:0.06374581975222099,acc:0.9807981848716736
node10 weight score:30041.185562340797
node11: train data size:1575
node11 epoch0:node_model train_loss:0.0650558410998201,train_acc:0.9770832657814026
node11 epoch1:node_model train_loss:0.04816457242122851,train_acc:0.9835415482521057
node11_model on test-dataset: loss:0.07125152804073878,acc:0.9780939221382141
node11 weight score:22104.78909448058
node12: train data size:1406
node12 epoch0:node_model train_loss:0.08060605535283685,train_acc:0.9715555310249329
node12 epoch1:node_model train_loss:0.04423343046970937,train_acc:0.9893332719802856
node12_model on test-dataset: loss:0.07887289795966353,acc:0.9767939448356628
node12 weight score:17826.148605811897
node17: train data size:719
node17 epoch0:node_model train_loss:0.06410561106167734,train_acc:0.9824999570846558
node17 epoch1:node_model train_loss:0.012898868531920016,train_acc:0.9975000023841858
node17_model on test-dataset: loss:0.058821704306756144,acc:0.9811002016067505
node17 weight score:12223.37925216181
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04807924106400605,acc:0.9846969449520111
total cost energy:7.145901951359278 | all_enery_cp：6.135999999999999 | all_enery_tp: 1.0099019513592786
ef: 32.138817319554995
reward: 24.992915368195717
step 448:loss:8.964372634887695|running q:54.309242248535156
episode7,iteration28 selected nodes:[17, 16, 2, 6, 19],center node:6
################################################## episode7,iteration28 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.06313751551738762,train_acc:0.97957444190979
node2 epoch1:node_model train_loss:0.038000801895829314,train_acc:0.9870212078094482
node2_model on test-dataset: loss:0.05618363281857455,acc:0.9818978905677795
node2 weight score:82052.36594234458
node6: train data size:3529
node6 epoch0:node_model train_loss:0.04296653813475536,train_acc:0.9855554699897766
node6 epoch1:node_model train_loss:0.023496370824028015,train_acc:0.9927778244018555
node6_model on test-dataset: loss:0.0697682822245406,acc:0.979897141456604
node6 weight score:50581.72406541915
node16: train data size:920
node16 epoch0:node_model train_loss:0.052713659149594604,train_acc:0.9839999079704285
node16 epoch1:node_model train_loss:0.05764261218719184,train_acc:0.9819999933242798
node16_model on test-dataset: loss:0.09301773685285297,acc:0.9742890000343323
node16 weight score:9890.586796960783
node17: train data size:719
node17 epoch0:node_model train_loss:0.051458575879223645,train_acc:0.9887500405311584
node17 epoch1:node_model train_loss:0.04882896941853687,train_acc:0.979671061038971
node17_model on test-dataset: loss:0.0639106181220268,acc:0.9790971279144287
node17 weight score:11250.086779432926
node19: train data size:5781
node19 epoch0:node_model train_loss:0.04780580565044335,train_acc:0.986643373966217
node19 epoch1:node_model train_loss:0.033357577962431544,train_acc:0.9899596571922302
node19_model on test-dataset: loss:0.0895623352448456,acc:0.9737973809242249
node19 weight score:64547.22271584251
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0420068296205136,acc:0.9872969448566437
total cost energy:9.252365690108165 | all_enery_cp：7.7795000000000005 | all_enery_tp: 1.472865690108165
ef: 31.840110193644563
reward: 22.5877445035364
step 449:loss:13.586024284362793|running q:55.95326232910156
episode7,iteration29 selected nodes:[0, 1, 3, 12, 5],center node:5
################################################## episode7,iteration29 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.05476906435150239,train_acc:0.9824894666671753
node0 epoch1:node_model train_loss:0.03155756833924291,train_acc:0.9897226691246033
node0_model on test-dataset: loss:0.10459304970976518,acc:0.9698908925056458
node0 weight score:68503.5957922838
node1: train data size:6657
node1 epoch0:node_model train_loss:0.036255352831542936,train_acc:0.9882091283798218
node1 epoch1:node_model train_loss:0.0321134622660535,train_acc:0.9898874759674072
node1_model on test-dataset: loss:0.07870182745606144,acc:0.9767917394638062
node1 weight score:84585.07527943423
node3: train data size:3762
node3 epoch0:node_model train_loss:0.053754023435574616,train_acc:0.9811543226242065
node3 epoch1:node_model train_loss:0.05221692668764215,train_acc:0.9834208488464355
node3_model on test-dataset: loss:0.13351790846132644,acc:0.9581969380378723
node3 weight score:28175.99559005724
node5: train data size:4837
node5 epoch0:node_model train_loss:0.06369584019542956,train_acc:0.9810203313827515
node5 epoch1:node_model train_loss:0.03967997450761649,train_acc:0.9857142567634583
node5_model on test-dataset: loss:0.06841393024425997,acc:0.9790972471237183
node5 weight score:70701.97520783177
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04749414473226352,train_acc:0.9826666116714478
node12 epoch1:node_model train_loss:0.030301088653504848,train_acc:0.9879999756813049
node12_model on test-dataset: loss:0.07092248276421742,acc:0.9782968759536743
node12 weight score:19824.46109048752
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0467149042004894,acc:0.9857949256896973
total cost energy:13.28543616859962 | all_enery_cp：11.913499999999999 | all_enery_tp: 1.371936168599622
ef: 31.689450880410824
reward: 18.404014711811204
step 450:loss:10.919820785522461|running q:57.76836395263672
episode7,iteration30 selected nodes:[6, 7, 18, 12, 14],center node:12
################################################## episode7,iteration30 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03757180806132965,train_acc:0.9845975041389465
node6 epoch1:node_model train_loss:0.03357445046357396,train_acc:0.988764226436615
node6_model on test-dataset: loss:0.07250349906437804,acc:0.9796928763389587
node6 weight score:48673.512941306384
node7: train data size:3637
node7 epoch0:node_model train_loss:0.05049321601620397,train_acc:0.9832431674003601
node7 epoch1:node_model train_loss:0.03393655114907872,train_acc:0.9884586334228516
node7_model on test-dataset: loss:0.07015002383890533,acc:0.9777909517288208
node7 weight score:51846.02657230906
node12: train data size:1406
node12 epoch0:node_model train_loss:0.04144816732344528,train_acc:0.9879999160766602
node12 epoch1:node_model train_loss:0.030974929073272504,train_acc:0.9886666536331177
node12_model on test-dataset: loss:0.10360405844316119,acc:0.972294807434082
node12 weight score:13570.896942916128
node14: train data size:1540
node14 epoch0:node_model train_loss:0.07159550458891317,train_acc:0.9790624380111694
node14 epoch1:node_model train_loss:0.03171462162572425,train_acc:0.9918749332427979
node14_model on test-dataset: loss:0.07328371676048846,acc:0.9775971174240112
node14 weight score:21014.218001976453
node18: train data size:801
node18 epoch0:node_model train_loss:0.05257412123965979,train_acc:0.9833332896232605
node18 epoch1:node_model train_loss:0.021748899352840252,train_acc:0.992222249507904
node18_model on test-dataset: loss:0.07431528239720137,acc:0.9768970012664795
node18 weight score:10778.402155815058
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04934279635075654,acc:0.9854959362745285
total cost energy:6.6728514615838765 | all_enery_cp：5.4565 | all_enery_tp: 1.2163514615838766
ef: 31.674912661461562
reward: 25.002061199877687
step 451:loss:14.475611686706543|running q:59.47123336791992
episode7,iteration31 selected nodes:[11, 0, 14, 6, 4],center node:11
################################################## episode7,iteration31 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04123897250327799,train_acc:0.987350583076477
node0 epoch1:node_model train_loss:0.042745576039629266,train_acc:0.9858976006507874
node0_model on test-dataset: loss:0.06634563287192577,acc:0.98089599609375
node0 weight score:107995.0509151278
node4: train data size:4298
node4 epoch0:node_model train_loss:0.05293028360925788,train_acc:0.9827860593795776
node4 epoch1:node_model train_loss:0.03191429732345738,train_acc:0.9906882047653198
node4_model on test-dataset: loss:0.06187518720224034,acc:0.9805968999862671
node4 weight score:69462.41610472866
node6: train data size:3529
node6 epoch0:node_model train_loss:0.034350686254684765,train_acc:0.990555465221405
node6 epoch1:node_model train_loss:0.024966318551580317,train_acc:0.9912643432617188
node6_model on test-dataset: loss:0.06170963629723701,acc:0.9826949238777161
node6 weight score:57187.178725245656
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05967566481558606,train_acc:0.9785415530204773
node11 epoch1:node_model train_loss:0.04570060991682112,train_acc:0.98499995470047
node11_model on test-dataset: loss:0.0826977270011048,acc:0.9774949550628662
node11 weight score:19045.263480808353
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06936298235086724,train_acc:0.9774999022483826
node14 epoch1:node_model train_loss:0.04446958456537686,train_acc:0.9874998927116394
node14_model on test-dataset: loss:0.07496575737663079,acc:0.9781988263130188
node14 weight score:20542.712484888027
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.043149596445837234,acc:0.9864969503879547
total cost energy:10.766450321738645 | all_enery_cp：9.0535 | all_enery_tp: 1.7129503217386457
ef: 32.26783069527895
reward: 21.501380373540307
step 452:loss:11.959169387817383|running q:61.13484573364258
episode7,iteration32 selected nodes:[9, 0, 15, 4, 5],center node:5
################################################## episode7,iteration32 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.04007826020824723,train_acc:0.9872222542762756
node0 epoch1:node_model train_loss:0.031991192505099915,train_acc:0.9892309904098511
node0_model on test-dataset: loss:0.08349314516253799,acc:0.9742950797080994
node0 weight score:85815.42815343382
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03683241031154297,train_acc:0.987441897392273
node4 epoch1:node_model train_loss:0.029484888776963532,train_acc:0.9899998903274536
node4_model on test-dataset: loss:0.06670495282196498,acc:0.9780992865562439
node4 weight score:64432.99662427361
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05902369084473395,train_acc:0.9816933870315552
node5 epoch1:node_model train_loss:0.0333540646539887,train_acc:0.9880199432373047
node5_model on test-dataset: loss:0.05916457355604507,acc:0.9818960428237915
node5 weight score:81755.00488342125
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04192890943324363,train_acc:0.988636314868927
node9 epoch1:node_model train_loss:0.02486873749744105,train_acc:0.9899998903274536
node9_model on test-dataset: loss:0.07109215921751456,acc:0.9795958995819092
node9 weight score:29890.77872143847
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0677488526063306,train_acc:0.9795489311218262
node15 epoch1:node_model train_loss:0.03515955654438585,train_acc:0.987631618976593
node15_model on test-dataset: loss:0.08730926986681879,acc:0.9738917946815491
node15 weight score:15760.067654888706
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04233229623088846,acc:0.9864979553222656
total cost energy:11.488205430228724 | all_enery_cp：9.9005 | all_enery_tp: 1.5877054302287246
ef: 32.06955273325084
reward: 20.581347303022113
step 453:loss:11.104336738586426|running q:62.807987213134766
episode7,iteration33 selected nodes:[17, 13, 14, 3, 0],center node:14
################################################## episode7,iteration33 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.0323646254126086,train_acc:0.9884614944458008
node0 epoch1:node_model train_loss:0.023596175930126466,train_acc:0.992008626461029
node0_model on test-dataset: loss:0.10224614987237146,acc:0.9746968746185303
node0 weight score:70075.9882787146
node3: train data size:3762
node3 epoch0:node_model train_loss:0.062452284803025816,train_acc:0.979575514793396
node3 epoch1:node_model train_loss:0.0363254312634174,train_acc:0.9894736409187317
node3_model on test-dataset: loss:0.07614656099649438,acc:0.9764960408210754
node3 weight score:49404.72623803975
node13: train data size:1056
node13 epoch0:node_model train_loss:0.058915724300525406,train_acc:0.9814934730529785
node13 epoch1:node_model train_loss:0.05650683147409423,train_acc:0.9845454096794128
node13_model on test-dataset: loss:0.07243664711655583,acc:0.9776949286460876
node13 weight score:14578.25620090918
node14: train data size:1540
node14 epoch0:node_model train_loss:0.05964567884802818,train_acc:0.9790624380111694
node14 epoch1:node_model train_loss:0.03906786281731911,train_acc:0.9831249117851257
node14_model on test-dataset: loss:0.08576829098237795,acc:0.9754999279975891
node14 weight score:17955.353690286425
node17: train data size:719
node17 epoch0:node_model train_loss:0.04951511445688084,train_acc:0.9837499856948853
node17 epoch1:node_model train_loss:0.02329009008826688,train_acc:0.9912499785423279
node17_model on test-dataset: loss:0.05964439300820232,acc:0.9817980527877808
node17 weight score:12054.779397304334
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.05097918702998868,acc:0.9850979542732239
total cost energy:9.384302431987063 | all_enery_cp：7.121 | all_enery_tp: 2.263302431987063
ef: 31.222253498367543
reward: 21.83795106638048
step 454:loss:9.620644569396973|running q:64.4486083984375
episode7,iteration34 selected nodes:[17, 3, 14, 15, 5],center node:5
################################################## episode7,iteration34 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.055908105559769626,train_acc:0.9824701547622681
node3 epoch1:node_model train_loss:0.02805627185826827,train_acc:0.9898386597633362
node3_model on test-dataset: loss:0.07322958583419677,acc:0.9786961078643799
node3 weight score:51372.67891310701
node5: train data size:4837
node5 epoch0:node_model train_loss:0.05742960176383126,train_acc:0.9831218719482422
node5 epoch1:node_model train_loss:0.03502068901909705,train_acc:0.989448606967926
node5_model on test-dataset: loss:0.05701372386014555,acc:0.9813961386680603
node5 weight score:84839.2224276587
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06347735683084466,train_acc:0.9812498688697815
node14 epoch1:node_model train_loss:0.025782803459151182,train_acc:0.9899999499320984
node14_model on test-dataset: loss:0.07983266205897963,acc:0.9777991771697998
node14 weight score:19290.350093327244
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05316076560744217,train_acc:0.9795489311218262
node15 epoch1:node_model train_loss:0.05070640060252377,train_acc:0.9850375652313232
node15_model on test-dataset: loss:0.06975102278353006,acc:0.9765971899032593
node15 weight score:19727.30929366254
node17: train data size:719
node17 epoch0:node_model train_loss:0.03633526148587407,train_acc:0.9887499213218689
node17 epoch1:node_model train_loss:0.026481959777811426,train_acc:0.9874999523162842
node17_model on test-dataset: loss:0.06135503495912417,acc:0.9817972183227539
node17 weight score:11718.679656510843
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04154479888828064,acc:0.9866969436407089
total cost energy:7.79014926254739 | all_enery_cp：6.117 | all_enery_tp: 1.6731492625473892
ef: 31.824114446497422
reward: 24.03396518395003
step 455:loss:10.350229263305664|running q:66.1192855834961
episode7,iteration35 selected nodes:[19, 18, 13, 6, 17],center node:18
################################################## episode7,iteration35 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.034330500019778915,train_acc:0.986944317817688
node6 epoch1:node_model train_loss:0.028240937024949946,train_acc:0.9899998903274536
node6_model on test-dataset: loss:0.09466758242797824,acc:0.9755939841270447
node6 weight score:37277.80840590086
node13: train data size:1056
node13 epoch0:node_model train_loss:0.05076650039038875,train_acc:0.9854544997215271
node13 epoch1:node_model train_loss:0.024089260238476774,train_acc:0.9927272200584412
node13_model on test-dataset: loss:0.07969752515593427,acc:0.9763960242271423
node13 weight score:13250.097765694176
node17: train data size:719
node17 epoch0:node_model train_loss:0.04089299462066265,train_acc:0.9862499833106995
node17 epoch1:node_model train_loss:0.030309770547319204,train_acc:0.9937499165534973
node17_model on test-dataset: loss:0.05876948607241502,acc:0.983296811580658
node17 weight score:12234.240046170511
node18: train data size:801
node18 epoch0:node_model train_loss:0.037206419898817934,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.020896017085760832,train_acc:0.9944445490837097
node18_model on test-dataset: loss:0.15636007186464668,acc:0.9594970345497131
node18 weight score:5122.7911988515
node19: train data size:5781
node19 epoch0:node_model train_loss:0.05361445598175813,train_acc:0.9856494069099426
node19 epoch1:node_model train_loss:0.03511827152848629,train_acc:0.9894828200340271
node19_model on test-dataset: loss:0.059155867388763,acc:0.9819982051849365
node19 weight score:97724.87929233094
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.049202750298645695,acc:0.9842979574203491
total cost energy:7.137427190999915 | all_enery_cp：5.943 | all_enery_tp: 1.194427190999916
ef: 31.53331977797421
reward: 24.395892586974295
step 456:loss:13.457749366760254|running q:67.7672119140625
episode7,iteration36 selected nodes:[4, 19, 7, 3, 1],center node:7
################################################## episode7,iteration36 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.04624657582408234,train_acc:0.9840664863586426
node1 epoch1:node_model train_loss:0.02308225473163149,train_acc:0.9926866888999939
node1_model on test-dataset: loss:0.05875153553628479,acc:0.9828969240188599
node1 weight score:113307.67679916476
node3: train data size:3762
node3 epoch0:node_model train_loss:0.04674809279018327,train_acc:0.9847366809844971
node3 epoch1:node_model train_loss:0.025441529933902387,train_acc:0.9916807413101196
node3_model on test-dataset: loss:0.08259285227279178,acc:0.9753966927528381
node3 weight score:45548.73571352977
node4: train data size:4298
node4 epoch0:node_model train_loss:0.04498585760810001,train_acc:0.9869720339775085
node4 epoch1:node_model train_loss:0.021426309040899195,train_acc:0.9920929074287415
node4_model on test-dataset: loss:0.06040338046848774,acc:0.9824950695037842
node4 weight score:71154.95799514488
node7: train data size:3637
node7 epoch0:node_model train_loss:0.04797852035561526,train_acc:0.9868370294570923
node7 epoch1:node_model train_loss:0.029746892415238795,train_acc:0.9910810589790344
node7_model on test-dataset: loss:0.07403346274819342,acc:0.9796980023384094
node7 weight score:49126.43370971798
node19: train data size:5781
node19 epoch0:node_model train_loss:0.037135229325564255,train_acc:0.988275945186615
node19 epoch1:node_model train_loss:0.025004344450406217,train_acc:0.9925864338874817
node19_model on test-dataset: loss:0.08834026939439354,acc:0.9737988710403442
node19 weight score:65440.14456409262
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.038874746486253574,acc:0.9864969426393508
total cost energy:14.300736498562665 | all_enery_cp：12.0675 | all_enery_tp: 2.2332364985626643
ef: 32.2025329436188
reward: 17.901796445056135
step 457:loss:10.209174156188965|running q:69.38733673095703
episode7,iteration37 selected nodes:[18, 19, 11, 2, 15],center node:15
################################################## episode7,iteration37 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.05821818458609917,train_acc:0.9821277260780334
node2 epoch1:node_model train_loss:0.035490809741647956,train_acc:0.9878724217414856
node2_model on test-dataset: loss:0.08199055296907318,acc:0.9750909805297852
node2 weight score:56225.989861769696
node11: train data size:1575
node11 epoch0:node_model train_loss:0.057591279852204025,train_acc:0.9841665625572205
node11 epoch1:node_model train_loss:0.047032761402078904,train_acc:0.9856249094009399
node11_model on test-dataset: loss:0.06545040191311273,acc:0.9817949533462524
node11 weight score:24064.02335146631
node15: train data size:1376
node15 epoch0:node_model train_loss:0.05891783520512815,train_acc:0.979060173034668
node15 epoch1:node_model train_loss:0.03551978577992746,train_acc:0.98906010389328
node15_model on test-dataset: loss:0.08687273402540086,acc:0.9746952056884766
node15 weight score:15839.26205888339
node18: train data size:801
node18 epoch0:node_model train_loss:0.026245708099344886,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.015466887193421522,train_acc:0.9966666102409363
node18_model on test-dataset: loss:0.09206322585661837,acc:0.9725950956344604
node18 weight score:8700.542399497253
node19: train data size:5781
node19 epoch0:node_model train_loss:0.037195366281405864,train_acc:0.9877183437347412
node19 epoch1:node_model train_loss:0.025877499467179436,train_acc:0.9918967485427856
node19_model on test-dataset: loss:0.07225553259726439,acc:0.9805967211723328
node19 weight score:80007.71418047605
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04268497661782021,acc:0.9864969426393508
total cost energy:8.170036903096545 | all_enery_cp：7.0714999999999995 | all_enery_tp: 1.0985369030965462
ef: 31.88308214349348
reward: 23.713045240396934
step 458:loss:12.555936813354492|running q:70.93184661865234
episode7,iteration38 selected nodes:[15, 0, 7, 3, 16],center node:7
################################################## episode7,iteration38 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03608381547706409,train_acc:0.9885258078575134
node0 epoch1:node_model train_loss:0.02485205530036991,train_acc:0.9925001263618469
node0_model on test-dataset: loss:0.07780193651713034,acc:0.976597249507904
node0 weight score:92092.82340192675
node3: train data size:3762
node3 epoch0:node_model train_loss:0.047780207235758244,train_acc:0.9830983877182007
node3 epoch1:node_model train_loss:0.02807618834470448,train_acc:0.9893123507499695
node3_model on test-dataset: loss:0.0747133193281661,acc:0.9772950410842896
node3 weight score:50352.467723673566
node7: train data size:3637
node7 epoch0:node_model train_loss:0.040617612047385225,train_acc:0.9856756329536438
node7 epoch1:node_model train_loss:0.03631842001718846,train_acc:0.9883784055709839
node7_model on test-dataset: loss:0.06045043558382531,acc:0.9823979139328003
node7 weight score:60164.99244172775
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04526434281641351,train_acc:0.9869171977043152
node15 epoch1:node_model train_loss:0.02728673456502812,train_acc:0.9926315546035767
node15_model on test-dataset: loss:0.10192727209807799,acc:0.9704888463020325
node15 weight score:13499.821703027277
node16: train data size:920
node16 epoch0:node_model train_loss:0.07863403181545436,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.04659159344155341,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.0737593568037846,acc:0.9776961207389832
node16 weight score:12472.993798568408
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04405311329222968,acc:0.9864969438314438
total cost energy:10.253334547203387 | all_enery_cp：8.430000000000001 | all_enery_tp: 1.8233345472033855
ef: 31.77231567935917
reward: 21.518981132155783
step 459:loss:8.328474044799805|running q:72.4216537475586
episode7,iteration39 selected nodes:[16, 15, 17, 3, 11],center node:11
################################################## episode7,iteration39 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.045556401276323745,train_acc:0.9834210276603699
node3 epoch1:node_model train_loss:0.016372443486225643,train_acc:0.9949999451637268
node3_model on test-dataset: loss:0.06195390254205449,acc:0.9820969700813293
node3 weight score:60722.56703193706
node11: train data size:1575
node11 epoch0:node_model train_loss:0.049969920262810774,train_acc:0.9837499260902405
node11 epoch1:node_model train_loss:0.03293979827139992,train_acc:0.9912499189376831
node11_model on test-dataset: loss:0.06778231752247849,acc:0.9807969927787781
node11 weight score:23236.148564523286
node15: train data size:1376
node15 epoch0:node_model train_loss:0.046243987029551396,train_acc:0.9807517528533936
node15 epoch1:node_model train_loss:0.0490262244295861,train_acc:0.9824060201644897
node15_model on test-dataset: loss:0.06910216654941906,acc:0.9795920252799988
node15 weight score:19912.544985343415
node16: train data size:920
node16 epoch0:node_model train_loss:0.049023708794265985,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.05743557061068714,train_acc:0.9860000014305115
node16_model on test-dataset: loss:0.09674906554500921,acc:0.9737960696220398
node16 weight score:9509.135771156376
node17: train data size:719
node17 epoch0:node_model train_loss:0.05170407457626425,train_acc:0.987500011920929
node17 epoch1:node_model train_loss:0.0195066830056021,train_acc:0.9937499761581421
node17_model on test-dataset: loss:0.06840484398664558,acc:0.9789978861808777
node17 weight score:10510.95153641996
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04547944297131835,acc:0.9865969491004943
total cost energy:5.498929753251347 | all_enery_cp：4.176 | all_enery_tp: 1.322929753251347
ef: 31.83832890055571
reward: 26.339399147304363
step 460:loss:8.689970970153809|running q:73.89393615722656
episode7,iteration40 selected nodes:[12, 2, 8, 14, 16],center node:12
################################################## episode7,iteration40 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.046710262675720086,train_acc:0.9870213270187378
node2 epoch1:node_model train_loss:0.027498932461194853,train_acc:0.9919149875640869
node2_model on test-dataset: loss:0.08797863898129435,acc:0.9748961925506592
node2 weight score:52399.08292943881
node8: train data size:2290
node8 epoch0:node_model train_loss:0.0665032810167126,train_acc:0.9799032807350159
node8 epoch1:node_model train_loss:0.032657852011692266,train_acc:0.9886955618858337
node8_model on test-dataset: loss:0.06007155104467529,acc:0.9821979999542236
node8 weight score:38121.20646422004
node12: train data size:1406
node12 epoch0:node_model train_loss:0.05906363294149439,train_acc:0.9819999933242798
node12 epoch1:node_model train_loss:0.028586793339733654,train_acc:0.9913333058357239
node12_model on test-dataset: loss:0.07947991736502445,acc:0.977095901966095
node12 weight score:17690.00329407385
node14: train data size:1540
node14 epoch0:node_model train_loss:0.06469781836494803,train_acc:0.9809374213218689
node14 epoch1:node_model train_loss:0.02872343291528523,train_acc:0.9918749332427979
node14_model on test-dataset: loss:0.05708149578174925,acc:0.9817991256713867
node14 weight score:26978.9706613187
node16: train data size:920
node16 epoch0:node_model train_loss:0.05116308027645573,train_acc:0.9829999208450317
node16 epoch1:node_model train_loss:0.04489048938266933,train_acc:0.9819999933242798
node16_model on test-dataset: loss:0.08002894270477555,acc:0.9798930287361145
node16 weight score:11495.84099085069
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04179571308592131,acc:0.9873969441652298
total cost energy:6.832736515126095 | all_enery_cp：5.383 | all_enery_tp: 1.4497365151260955
ef: 31.882358269737924
reward: 25.049621754611827
step 461:loss:7.251135349273682|running q:75.3727798461914
episode7,iteration41 selected nodes:[2, 17, 16, 0, 18],center node:16
################################################## episode7,iteration41 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03436567578723447,train_acc:0.98680579662323
node0 epoch1:node_model train_loss:0.023495454879594035,train_acc:0.9926391243934631
node0_model on test-dataset: loss:0.06980153185810195,acc:0.9790950417518616
node0 weight score:102648.17704238319
node2: train data size:4610
node2 epoch0:node_model train_loss:0.03241008343150285,train_acc:0.9893616437911987
node2 epoch1:node_model train_loss:0.029719754868370304,train_acc:0.9882978796958923
node2_model on test-dataset: loss:0.07260994178279362,acc:0.9793918132781982
node2 weight score:63489.92833227188
node16: train data size:920
node16 epoch0:node_model train_loss:0.05546668414026499,train_acc:0.9799999594688416
node16 epoch1:node_model train_loss:0.029606749955564737,train_acc:0.9859998822212219
node16_model on test-dataset: loss:0.07241135897933418,acc:0.9802948832511902
node16 weight score:12705.188978190054
node17: train data size:719
node17 epoch0:node_model train_loss:0.03135034136357717,train_acc:0.9862499237060547
node17 epoch1:node_model train_loss:0.023028952244203538,train_acc:0.9912499189376831
node17_model on test-dataset: loss:0.07050881159437267,acc:0.980099081993103
node17 weight score:10197.307027897541
node18: train data size:801
node18 epoch0:node_model train_loss:0.13340680839286911,train_acc:0.8733332753181458
node18 epoch1:node_model train_loss:0.04530980275012553,train_acc:0.9911110997200012
node18_model on test-dataset: loss:0.17928050491550493,acc:0.9561970829963684
node18 weight score:4467.858902882453
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04950847663727473,acc:0.9861969488859177
total cost energy:8.843269214320955 | all_enery_cp：7.107500000000001 | all_enery_tp: 1.7357692143209538
ef: 31.542483378726455
reward: 22.6992141644055
step 462:loss:6.752776622772217|running q:76.78564453125
episode7,iteration42 selected nodes:[6, 4, 11, 15, 16],center node:11
################################################## episode7,iteration42 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.038403410459110554,train_acc:0.9888371229171753
node4 epoch1:node_model train_loss:0.024074806325505814,train_acc:0.9934883713722229
node4_model on test-dataset: loss:0.05474923326110002,acc:0.9818951487541199
node4 weight score:78503.38249492491
node6: train data size:3529
node6 epoch0:node_model train_loss:0.03780485247908574,train_acc:0.9852777123451233
node6 epoch1:node_model train_loss:0.021952537363783147,train_acc:0.9934866428375244
node6_model on test-dataset: loss:0.07485636069672182,acc:0.9801950454711914
node6 weight score:47143.62236093246
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05692860164708691,train_acc:0.9808332324028015
node11 epoch1:node_model train_loss:0.025270473837736063,train_acc:0.9931249022483826
node11_model on test-dataset: loss:0.07441345043567708,acc:0.9775969982147217
node11 weight score:21165.528419642742
node15: train data size:1376
node15 epoch0:node_model train_loss:0.0406643482191222,train_acc:0.9862029552459717
node15 epoch1:node_model train_loss:0.01799768182848181,train_acc:0.9935714602470398
node15_model on test-dataset: loss:0.0714328272860439,acc:0.979296088218689
node15 weight score:19262.852280646523
node16: train data size:920
node16 epoch0:node_model train_loss:0.04421103997156024,train_acc:0.9860000014305115
node16 epoch1:node_model train_loss:0.0486631334759295,train_acc:0.9889999628067017
node16_model on test-dataset: loss:0.06456033348382334,acc:0.9810966849327087
node16 weight score:14250.236179937348
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03892465238488512,acc:0.9876969450712204
total cost energy:7.1509578422302456 | all_enery_cp：5.848999999999999 | all_enery_tp: 1.3019578422302462
ef: 32.18548479592953
reward: 25.034526953699288
step 463:loss:7.382880210876465|running q:78.23015594482422
episode7,iteration43 selected nodes:[6, 11, 4, 18, 12],center node:12
################################################## episode7,iteration43 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.029953826913274393,train_acc:0.9899953007698059
node4 epoch1:node_model train_loss:0.012917691895868196,train_acc:0.9960466623306274
node4_model on test-dataset: loss:0.08252863705198252,acc:0.9754969477653503
node4 weight score:52078.89229156672
node6: train data size:3529
node6 epoch0:node_model train_loss:0.02746556563943159,train_acc:0.9919443130493164
node6 epoch1:node_model train_loss:0.014664528774220444,train_acc:0.9944444298744202
node6_model on test-dataset: loss:0.07977552230299807,acc:0.9798990488052368
node6 weight score:44236.626701062356
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05410627432866022,train_acc:0.9872915744781494
node11 epoch1:node_model train_loss:0.025004506955156103,train_acc:0.9918748140335083
node11_model on test-dataset: loss:0.05864378020385629,acc:0.9819977879524231
node11 weight score:26857.068124275374
node12: train data size:1406
node12 epoch0:node_model train_loss:0.036167270581548415,train_acc:0.9879999756813049
node12 epoch1:node_model train_loss:0.10676270695403219,train_acc:0.9795554876327515
node12_model on test-dataset: loss:0.09123267173446947,acc:0.9751951098442078
node12 weight score:15411.145736169272
node18: train data size:801
node18 epoch0:node_model train_loss:0.030085301130182214,train_acc:0.9922221302986145
node18 epoch1:node_model train_loss:0.01805348710776242,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.06237265663772632,acc:0.9815971255302429
node18 weight score:12842.165833217248
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04676271967216963,acc:0.9865979564189911
total cost energy:7.019798244508296 | all_enery_cp：5.804500000000001 | all_enery_tp: 1.215298244508295
ef: 31.989418226589287
reward: 24.969619982080992
step 464:loss:16.262645721435547|running q:79.65735626220703
episode7,iteration44 selected nodes:[9, 18, 5, 14, 6],center node:5
################################################## episode7,iteration44 ##################################################
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04900368663710447,train_acc:0.9826530814170837
node5 epoch1:node_model train_loss:0.02550902574000006,train_acc:0.9918368458747864
node5_model on test-dataset: loss:0.07672330637171398,acc:0.9793949723243713
node5 weight score:63044.728241577504
node6: train data size:3529
node6 epoch0:node_model train_loss:0.023336119079936504,train_acc:0.991944432258606
node6 epoch1:node_model train_loss:0.01707686898427912,train_acc:0.9938889741897583
node6_model on test-dataset: loss:0.05658612525323406,acc:0.9840951561927795
node6 weight score:62365.11130965461
node9: train data size:2125
node9 epoch0:node_model train_loss:0.06458835142918608,train_acc:0.9790908694267273
node9 epoch1:node_model train_loss:0.030166671727783978,train_acc:0.988636314868927
node9_model on test-dataset: loss:0.05458548038812296,acc:0.9838969111442566
node9 weight score:38929.76639374544
node14: train data size:1540
node14 epoch0:node_model train_loss:0.0592539751669392,train_acc:0.9803124666213989
node14 epoch1:node_model train_loss:0.024976378997962456,train_acc:0.9918748736381531
node14_model on test-dataset: loss:0.06409645242456463,acc:0.9798972606658936
node14 weight score:24026.290718857366
node18: train data size:801
node18 epoch0:node_model train_loss:0.02815893694706675,train_acc:0.9911110401153564
node18 epoch1:node_model train_loss:0.016687857090598217,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.0826111818816571,acc:0.9780979752540588
node18 weight score:9696.023973455018
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040660741438696274,acc:0.9876969438791275
total cost energy:8.053163089161124 | all_enery_cp：6.416 | all_enery_tp: 1.6371630891611249
ef: 32.03291268341534
reward: 23.979749594254216
step 465:loss:11.215198516845703|running q:81.1255874633789
episode7,iteration45 selected nodes:[11, 16, 0, 8, 3],center node:8
################################################## episode7,iteration45 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.030345620681247156,train_acc:0.988333523273468
node0 epoch1:node_model train_loss:0.017991677527283575,train_acc:0.9937501549720764
node0_model on test-dataset: loss:0.0806303491763083,acc:0.978097140789032
node0 weight score:88862.3213615612
node3: train data size:3762
node3 epoch0:node_model train_loss:0.041834746499692925,train_acc:0.9860525727272034
node3 epoch1:node_model train_loss:0.02466678512466483,train_acc:0.9919439554214478
node3_model on test-dataset: loss:0.06570043854990218,acc:0.9806968569755554
node3 weight score:57259.891760731654
node8: train data size:2290
node8 epoch0:node_model train_loss:0.05688719925187204,train_acc:0.9855553507804871
node8 epoch1:node_model train_loss:0.031040515082523874,train_acc:0.9899999499320984
node8_model on test-dataset: loss:0.08823762323075243,acc:0.9740958213806152
node8 weight score:25952.648271263646
node11: train data size:1575
node11 epoch0:node_model train_loss:0.03944272560693207,train_acc:0.9879166483879089
node11 epoch1:node_model train_loss:0.027018753895390546,train_acc:0.9931249618530273
node11_model on test-dataset: loss:0.07559933489152172,acc:0.9802970886230469
node11 weight score:20833.516621012393
node16: train data size:920
node16 epoch0:node_model train_loss:0.0713077431806596,train_acc:0.979999840259552
node16 epoch1:node_model train_loss:0.05240565477870405,train_acc:0.98499995470047
node16_model on test-dataset: loss:0.10468787282512494,acc:0.9699941277503967
node16 weight score:8788.028404558443
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04631678491466119,acc:0.9874969458580017
total cost energy:9.37867840600983 | all_enery_cp：7.856 | all_enery_tp: 1.5226784060098284
ef: 31.620553952424785
reward: 22.241875546414956
step 466:loss:9.68142032623291|running q:82.46468353271484
episode7,iteration46 selected nodes:[17, 18, 9, 8, 4],center node:8
################################################## episode7,iteration46 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.03569686860062702,train_acc:0.9890698194503784
node4 epoch1:node_model train_loss:0.018023939014055093,train_acc:0.9948837161064148
node4_model on test-dataset: loss:0.06562607266237591,acc:0.9813950061798096
node4 weight score:65492.262840590294
node8: train data size:2290
node8 epoch0:node_model train_loss:0.048886422999203205,train_acc:0.9855071306228638
node8 epoch1:node_model train_loss:0.024090428148274837,train_acc:0.9925119876861572
node8_model on test-dataset: loss:0.07158565442776307,acc:0.9796979427337646
node8 weight score:31989.64957861542
node9: train data size:2125
node9 epoch0:node_model train_loss:0.04427173369648782,train_acc:0.9877271056175232
node9 epoch1:node_model train_loss:0.014743432386735962,train_acc:0.9940907955169678
node9_model on test-dataset: loss:0.061861330345855094,acc:0.9824979901313782
node9 weight score:34351.02329871543
node17: train data size:719
node17 epoch0:node_model train_loss:0.04347131530448678,train_acc:0.9874999523162842
node17 epoch1:node_model train_loss:0.041152204503305256,train_acc:0.9874999523162842
node17_model on test-dataset: loss:0.06526774992089486,acc:0.9792991280555725
node17 weight score:11016.160368197692
node18: train data size:801
node18 epoch0:node_model train_loss:0.01840754482610565,train_acc:0.9922221302986145
node18 epoch1:node_model train_loss:0.01507292553368542,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.08376483743606514,acc:0.9796982407569885
node18 weight score:9562.484981975595
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04386839902890642,acc:0.9868969470262527
total cost energy:6.692605849064606 | all_enery_cp：5.1165 | all_enery_tp: 1.576105849064605
ef: 31.760483635605016
reward: 25.06787778654041
step 467:loss:13.548150062561035|running q:83.84577941894531
episode7,iteration47 selected nodes:[6, 14, 9, 4, 17],center node:14
################################################## episode7,iteration47 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.026657044642529083,train_acc:0.9911580681800842
node4 epoch1:node_model train_loss:0.01270234018163539,train_acc:0.9953395128250122
node4_model on test-dataset: loss:0.06518247320650516,acc:0.980292022228241
node4 weight score:65937.97057045485
node6: train data size:3529
node6 epoch0:node_model train_loss:0.027324989861679368,train_acc:0.9911110401153564
node6 epoch1:node_model train_loss:0.011001525019107956,train_acc:0.9958332777023315
node6_model on test-dataset: loss:0.07268273017914907,acc:0.9786989092826843
node6 weight score:48553.48707047311
node9: train data size:2125
node9 epoch0:node_model train_loss:0.031164705150083384,train_acc:0.9899998903274536
node9 epoch1:node_model train_loss:0.021434490853359668,train_acc:0.9918180108070374
node9_model on test-dataset: loss:0.056160562236982514,acc:0.9826950430870056
node9 weight score:37837.94027974773
node14: train data size:1540
node14 epoch0:node_model train_loss:0.046000690665096045,train_acc:0.9815623760223389
node14 epoch1:node_model train_loss:0.03283063774870243,train_acc:0.9868748784065247
node14_model on test-dataset: loss:0.09673568195445113,acc:0.9730991721153259
node14 weight score:15919.668615404218
node17: train data size:719
node17 epoch0:node_model train_loss:0.029846149711374892,train_acc:0.9899999499320984
node17 epoch1:node_model train_loss:0.025184836267726496,train_acc:0.9871710538864136
node17_model on test-dataset: loss:0.07043001297861337,acc:0.9786980748176575
node17 weight score:10208.715994675878
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04002535633637308,acc:0.9876979517936707
total cost energy:7.558554874300444 | all_enery_cp：6.1055 | all_enery_tp: 1.4530548743004437
ef: 32.00483475715758
reward: 24.446279882857137
step 468:loss:9.29084300994873|running q:85.24649810791016
episode7,iteration48 selected nodes:[16, 15, 2, 9, 10],center node:10
################################################## episode7,iteration48 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.041852973032794755,train_acc:0.9868086576461792
node2 epoch1:node_model train_loss:0.022184822146107067,train_acc:0.9925533533096313
node2_model on test-dataset: loss:0.06788647293622489,acc:0.9804971218109131
node2 weight score:67907.49026438313
node9: train data size:2125
node9 epoch0:node_model train_loss:0.024463505418018693,train_acc:0.9922727346420288
node9 epoch1:node_model train_loss:0.017434251952429557,train_acc:0.9927272200584412
node9_model on test-dataset: loss:0.06561847709977883,acc:0.9824959635734558
node9 weight score:32384.171256652986
node10: train data size:1915
node10 epoch0:node_model train_loss:0.06390260382613633,train_acc:0.9825000166893005
node10 epoch1:node_model train_loss:0.027050758466066326,train_acc:0.9909998774528503
node10_model on test-dataset: loss:0.0461936196503666,acc:0.9851987957954407
node10 weight score:41455.941632077804
node15: train data size:1376
node15 epoch0:node_model train_loss:0.04175554099492729,train_acc:0.9849998950958252
node15 epoch1:node_model train_loss:0.019841191749687175,train_acc:0.9928570985794067
node15_model on test-dataset: loss:0.07037938727651635,acc:0.9793939590454102
node15 weight score:19551.179020552983
node16: train data size:920
node16 epoch0:node_model train_loss:0.05478944224305451,train_acc:0.9889999628067017
node16 epoch1:node_model train_loss:0.01922274110838771,train_acc:0.9939999580383301
node16_model on test-dataset: loss:0.050195406779748736,acc:0.9855971336364746
node16 weight score:18328.370243852125
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03734449566942203,acc:0.9874969434738159
total cost energy:7.195212513921046 | all_enery_cp：5.473000000000001 | all_enery_tp: 1.7222125139210447
ef: 32.411601741380736
reward: 25.21638922745969
step 469:loss:10.137612342834473|running q:86.57929992675781
episode7,iteration49 selected nodes:[6, 16, 8, 1, 13],center node:6
################################################## episode7,iteration49 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.042594480372569174,train_acc:0.9864180684089661
node1 epoch1:node_model train_loss:0.02486022855085668,train_acc:0.9913433194160461
node1_model on test-dataset: loss:0.07983171686450077,acc:0.9762919545173645
node1 weight score:83387.90973641463
node6: train data size:3529
node6 epoch0:node_model train_loss:0.019431938769735604,train_acc:0.9913888573646545
node6 epoch1:node_model train_loss:0.01885445529680712,train_acc:0.9941666126251221
node6_model on test-dataset: loss:0.13931710147984633,acc:0.9618909955024719
node6 weight score:25330.702135735337
node8: train data size:2290
node8 epoch0:node_model train_loss:0.053165484715820006,train_acc:0.985168993473053
node8 epoch1:node_model train_loss:0.02109638297849375,train_acc:0.9939129948616028
node8_model on test-dataset: loss:0.06760482671772479,acc:0.9803988933563232
node8 weight score:33873.32105090068
node13: train data size:1056
node13 epoch0:node_model train_loss:0.04531749672341076,train_acc:0.9863635301589966
node13 epoch1:node_model train_loss:0.03036817954853177,train_acc:0.9901948571205139
node13_model on test-dataset: loss:0.06384014808310895,acc:0.9803958535194397
node13 weight score:16541.315014264514
node16: train data size:920
node16 epoch0:node_model train_loss:0.0356897494988516,train_acc:0.9919999241828918
node16 epoch1:node_model train_loss:0.02525384316104464,train_acc:0.9920000433921814
node16_model on test-dataset: loss:0.07732854955009316,acc:0.9798958897590637
node16 weight score:11897.287681621745
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03836095730672241,acc:0.9885979568958283
total cost energy:8.563338021109637 | all_enery_cp：7.226000000000001 | all_enery_tp: 1.337338021109636
ef: 31.41544061563682
reward: 22.852102594527185
step 470:loss:9.948022842407227|running q:87.8388900756836
episode7,iteration50 selected nodes:[5, 14, 18, 2, 17],center node:5
################################################## episode7,iteration50 ##################################################
node2: train data size:4610
node2 epoch0:node_model train_loss:0.036168971557809196,train_acc:0.9876596927642822
node2 epoch1:node_model train_loss:0.017647303056288907,train_acc:0.993617057800293
node2_model on test-dataset: loss:0.0720870554485009,acc:0.979296088218689
node2 weight score:63950.45506184381
node5: train data size:4837
node5 epoch0:node_model train_loss:0.04364941939735329,train_acc:0.9861225485801697
node5 epoch1:node_model train_loss:0.024838434211548646,train_acc:0.9918367862701416
node5_model on test-dataset: loss:0.06743431785085705,acc:0.9800958037376404
node5 weight score:71729.05657172782
node14: train data size:1540
node14 epoch0:node_model train_loss:0.0523261058551725,train_acc:0.9846874475479126
node14 epoch1:node_model train_loss:0.02806540281744674,train_acc:0.989687442779541
node14_model on test-dataset: loss:0.07582697987432767,acc:0.9785958528518677
node14 weight score:20309.393866831157
node17: train data size:719
node17 epoch0:node_model train_loss:0.03929073392646387,train_acc:0.9862499833106995
node17 epoch1:node_model train_loss:0.026716927211964503,train_acc:0.9896711111068726
node17_model on test-dataset: loss:0.06451738095613109,acc:0.9796971082687378
node17 weight score:11144.283747179501
node18: train data size:801
node18 epoch0:node_model train_loss:0.03313586029172358,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.03514298706108497,train_acc:0.9944444298744202
node18_model on test-dataset: loss:0.0686207885388444,acc:0.9794919490814209
node18 weight score:11672.847500820764
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.039733075259719045,acc:0.9874969458580017
total cost energy:7.938602768697551 | all_enery_cp：6.2535 | all_enery_tp: 1.685102768697552
ef: 31.55562532334795
reward: 23.617022554650397
step 471:loss:8.557470321655273|running q:89.13259887695312
episode7,iteration51 selected nodes:[16, 9, 3, 4, 14],center node:9
################################################## episode7,iteration51 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.038515072081568895,train_acc:0.9872071146965027
node3 epoch1:node_model train_loss:0.02309137071470583,train_acc:0.9918421506881714
node3_model on test-dataset: loss:0.05419323647540295,acc:0.9836970567703247
node3 weight score:69418.25668056353
node4: train data size:4298
node4 epoch0:node_model train_loss:0.027894957787540017,train_acc:0.9913858771324158
node4 epoch1:node_model train_loss:0.015819820151718463,train_acc:0.9948791861534119
node4_model on test-dataset: loss:0.06039110156227252,acc:0.9812949299812317
node4 weight score:71169.42544205954
node9: train data size:2125
node9 epoch0:node_model train_loss:0.026099523169581185,train_acc:0.9918181300163269
node9 epoch1:node_model train_loss:0.01496454266386784,train_acc:0.9950000047683716
node9_model on test-dataset: loss:0.053078055348814816,acc:0.9842957854270935
node9 weight score:40035.37782300175
node14: train data size:1540
node14 epoch0:node_model train_loss:0.0381181681950693,train_acc:0.9837499260902405
node14 epoch1:node_model train_loss:0.022099032532423735,train_acc:0.9921874403953552
node14_model on test-dataset: loss:0.0717459543134646,acc:0.9784969687461853
node14 weight score:21464.624935806136
node16: train data size:920
node16 epoch0:node_model train_loss:0.04285987471230328,train_acc:0.9839999079704285
node16 epoch1:node_model train_loss:0.02794977677986026,train_acc:0.9879999160766602
node16_model on test-dataset: loss:0.06952597800162039,acc:0.9796959757804871
node16 weight score:13232.463986030634
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.040916490058516504,acc:0.9873949259519577
total cost energy:7.770147131067164 | all_enery_cp：6.3225 | all_enery_tp: 1.4476471310671646
ef: 32.55728169036511
reward: 24.787134559297947
step 472:loss:12.101848602294922|running q:90.44467163085938
episode7,iteration52 selected nodes:[19, 12, 13, 6, 16],center node:16
################################################## episode7,iteration52 ##################################################
node6: train data size:3529
node6 epoch0:node_model train_loss:0.0248439830288084,train_acc:0.9916664958000183
node6 epoch1:node_model train_loss:0.023195939866127446,train_acc:0.9916666150093079
node6_model on test-dataset: loss:0.05624154854012886,acc:0.9840960502624512
node6 weight score:62747.20543091068
node12: train data size:1406
node12 epoch0:node_model train_loss:0.026582217829612394,train_acc:0.9886665940284729
node12 epoch1:node_model train_loss:0.027946113247890026,train_acc:0.9919999837875366
node12_model on test-dataset: loss:0.07480932265403681,acc:0.9791909456253052
node12 weight score:18794.449008744374
node13: train data size:1056
node13 epoch0:node_model train_loss:0.055656653818335726,train_acc:0.9854545593261719
node13 epoch1:node_model train_loss:0.024106648279650308,train_acc:0.9936363101005554
node13_model on test-dataset: loss:0.05728427005964477,acc:0.9822939038276672
node13 weight score:18434.379959812453
node16: train data size:920
node16 epoch0:node_model train_loss:0.052475178861641325,train_acc:0.9869999885559082
node16 epoch1:node_model train_loss:0.013272859016433358,train_acc:0.9950000047683716
node16_model on test-dataset: loss:0.06640344908951193,acc:0.9815950393676758
node16 weight score:13854.702016454581
node19: train data size:5781
node19 epoch0:node_model train_loss:0.046481568436539764,train_acc:0.9866838455200195
node19 epoch1:node_model train_loss:0.025056674804514404,train_acc:0.9924139380455017
node19_model on test-dataset: loss:0.06465777951718338,acc:0.9810969829559326
node19 weight score:89409.19473523906
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.03827618856703339,acc:0.988495934009552
total cost energy:7.354425994008306 | all_enery_cp：6.345999999999999 | all_enery_tp: 1.0084259940083065
ef: 32.60164272747033
reward: 25.24721673346202
step 473:loss:11.633710861206055|running q:91.69587707519531
episode7,iteration53 selected nodes:[4, 19, 7, 9, 14],center node:14
################################################## episode7,iteration53 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.018508513135900503,train_acc:0.99395352602005
node4 epoch1:node_model train_loss:0.016810020077215552,train_acc:0.9944186210632324
node4_model on test-dataset: loss:0.07648236895418677,acc:0.9779930114746094
node4 weight score:56195.958085117876
node7: train data size:3637
node7 epoch0:node_model train_loss:0.047022020994886955,train_acc:0.9855659604072571
node7 epoch1:node_model train_loss:0.028354006970452296,train_acc:0.9906207919120789
node7_model on test-dataset: loss:0.05343227998762814,acc:0.9838970899581909
node7 weight score:68067.46784606836
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03470734925940633,train_acc:0.9877271056175232
node9 epoch1:node_model train_loss:0.020047761851773514,train_acc:0.9931817054748535
node9_model on test-dataset: loss:0.06731395003676881,acc:0.9792966842651367
node9 weight score:31568.493586236793
node14: train data size:1540
node14 epoch0:node_model train_loss:0.04597218053822871,train_acc:0.9843748807907104
node14 epoch1:node_model train_loss:0.014335270185256377,train_acc:0.9950000047683716
node14_model on test-dataset: loss:0.08024296929153935,acc:0.979895830154419
node14 weight score:19191.71254000909
node19: train data size:5781
node19 epoch0:node_model train_loss:0.03373608511876604,train_acc:0.9903450608253479
node19 epoch1:node_model train_loss:0.02053960414346436,train_acc:0.9924139976501465
node19_model on test-dataset: loss:0.06737600032694445,acc:0.9818941354751587
node19 weight score:85802.06560121544
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04263825837078912,acc:0.987595933675766
total cost energy:10.357485996217536 | all_enery_cp：8.6905 | all_enery_tp: 1.6669859962175348
ef: 32.3700105139781
reward: 22.012524517760568
step 474:loss:6.605464458465576|running q:92.96453857421875
episode7,iteration54 selected nodes:[7, 11, 13, 5, 4],center node:11
################################################## episode7,iteration54 ##################################################
node4: train data size:4298
node4 epoch0:node_model train_loss:0.015143651761612746,train_acc:0.9948838949203491
node4 epoch1:node_model train_loss:0.013931906331319709,train_acc:0.9960465431213379
node4_model on test-dataset: loss:0.0584974521139884,acc:0.9841947555541992
node4 weight score:73473.28549668963
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03222332337671625,train_acc:0.989183783531189
node5 epoch1:node_model train_loss:0.027558468424178167,train_acc:0.9914287328720093
node5_model on test-dataset: loss:0.06909578562979732,acc:0.9793950319290161
node5 weight score:70004.26952109305
node7: train data size:3637
node7 epoch0:node_model train_loss:0.031998846461253894,train_acc:0.990810751914978
node7 epoch1:node_model train_loss:0.01835764063454573,train_acc:0.9927829504013062
node7_model on test-dataset: loss:0.05672387135046847,acc:0.9834979772567749
node7 weight score:64117.62655494357
node11: train data size:1575
node11 epoch0:node_model train_loss:0.05058301531244069,train_acc:0.9841665029525757
node11 epoch1:node_model train_loss:0.0286811898695305,train_acc:0.9906249046325684
node11_model on test-dataset: loss:0.06650774550857022,acc:0.9806949496269226
node11 weight score:23681.45225727197
node13: train data size:1056
node13 epoch0:node_model train_loss:0.0388377479870211,train_acc:0.9872727394104004
node13 epoch1:node_model train_loss:0.025774244769391687,train_acc:0.9936363101005554
node13_model on test-dataset: loss:0.060889722633874044,acc:0.9823967814445496
node13 weight score:17342.82821995527
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.041489218869865,acc:0.9867959314584732
total cost energy:9.146465990938049 | all_enery_cp：7.7015 | all_enery_tp: 1.4449659909380483
ef: 32.54019344662089
reward: 23.393727455682843
step 475:loss:8.755927085876465|running q:94.23216247558594
episode7,iteration55 selected nodes:[16, 12, 18, 8, 14],center node:12
################################################## episode7,iteration55 ##################################################
node8: train data size:2290
node8 epoch0:node_model train_loss:0.037492039162949055,train_acc:0.9886955618858337
node8 epoch1:node_model train_loss:0.02146142240330253,train_acc:0.9934298396110535
node8_model on test-dataset: loss:0.06078134226539987,acc:0.9813950657844543
node8 weight score:37676.035353098734
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03780112030605475,train_acc:0.9866666793823242
node12 epoch1:node_model train_loss:0.02236168275849195,train_acc:0.9926666021347046
node12_model on test-dataset: loss:0.07491866657141145,acc:0.9807969331741333
node12 weight score:18767.01847943089
node14: train data size:1540
node14 epoch0:node_model train_loss:0.044583967013750225,train_acc:0.9837498664855957
node14 epoch1:node_model train_loss:0.028972008149139583,train_acc:0.989687442779541
node14_model on test-dataset: loss:0.06730655989922525,acc:0.9816990494728088
node14 weight score:22880.38494770443
node16: train data size:920
node16 epoch0:node_model train_loss:0.058205251046456394,train_acc:0.9829999208450317
node16 epoch1:node_model train_loss:0.023960971634369343,train_acc:0.9899999499320984
node16_model on test-dataset: loss:0.06216696897536167,acc:0.9829961061477661
node16 weight score:14798.855648963987
node18: train data size:801
node18 epoch0:node_model train_loss:0.018666010531079438,train_acc:0.9911110997200012
node18 epoch1:node_model train_loss:0.009627253282814429,train_acc:0.9977777600288391
node18_model on test-dataset: loss:0.0767651675385633,acc:0.9810919761657715
node18 weight score:10434.420006933671
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.0408714288417741,acc:0.9885969460010529
total cost energy:4.811079227600715 | all_enery_cp：3.4785 | all_enery_tp: 1.3325792276007147
ef: 31.84560025331696
reward: 27.034521025716245
step 476:loss:4.466967582702637|running q:95.46752166748047
episode7,iteration56 selected nodes:[9, 15, 5, 3, 17],center node:5
################################################## episode7,iteration56 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.03328431821673324,train_acc:0.9897367358207703
node3 epoch1:node_model train_loss:0.018717696366366,train_acc:0.99289470911026
node3_model on test-dataset: loss:0.06558487290861194,acc:0.9828961491584778
node3 weight score:57360.788138479606
node5: train data size:4837
node5 epoch0:node_model train_loss:0.03152932743161765,train_acc:0.9879591464996338
node5 epoch1:node_model train_loss:0.021425422086209362,train_acc:0.9923056364059448
node5_model on test-dataset: loss:0.06642137651007715,acc:0.9821939468383789
node5 weight score:72822.94125997453
node9: train data size:2125
node9 epoch0:node_model train_loss:0.03551537759432739,train_acc:0.9872726202011108
node9 epoch1:node_model train_loss:0.014624931076964871,train_acc:0.9954545497894287
node9_model on test-dataset: loss:0.05611418291629434,acc:0.9847949147224426
node9 weight score:37869.21397697027
node15: train data size:1376
node15 epoch0:node_model train_loss:0.038996608495446186,train_acc:0.9864285588264465
node15 epoch1:node_model train_loss:0.020113094131894677,train_acc:0.9928571581840515
node15_model on test-dataset: loss:0.060210905075300615,acc:0.9832950234413147
node15 weight score:22853.003094358985
node17: train data size:719
node17 epoch0:node_model train_loss:0.03987641868297942,train_acc:0.9900000095367432
node17 epoch1:node_model train_loss:0.012757298580254428,train_acc:0.9962499737739563
node17_model on test-dataset: loss:0.07157227562187472,acc:0.9781971573829651
node17 weight score:10045.789291353078
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04364350049487257,acc:0.9868949258327484
total cost energy:8.105598789335419 | all_enery_cp：6.4095 | all_enery_tp: 1.6960987893354187
ef: 32.033351815982975
reward: 23.927753026647558
step 477:loss:8.972678184509277|running q:96.62311553955078
episode7,iteration57 selected nodes:[18, 8, 3, 0, 14],center node:8
################################################## episode7,iteration57 ##################################################
node0: train data size:7165
node0 epoch0:node_model train_loss:0.03282526555833303,train_acc:0.988536536693573
node0 epoch1:node_model train_loss:0.024435691822468977,train_acc:0.9926283955574036
node0_model on test-dataset: loss:0.0730811614914046,acc:0.9811919331550598
node0 weight score:98041.68206662544
node3: train data size:3762
node3 epoch0:node_model train_loss:0.022780013956913824,train_acc:0.9915788769721985
node3 epoch1:node_model train_loss:0.014853007937122848,train_acc:0.9950000643730164
node3_model on test-dataset: loss:0.05775589557889816,acc:0.9816950559616089
node3 weight score:65136.20752120229
node8: train data size:2290
node8 epoch0:node_model train_loss:0.032060303611417665,train_acc:0.9916907548904419
node8 epoch1:node_model train_loss:0.022479331890202087,train_acc:0.9926086068153381
node8_model on test-dataset: loss:0.06036498416076938,acc:0.9830971360206604
node8 weight score:37935.89995652229
node14: train data size:1540
node14 epoch0:node_model train_loss:0.0433020333875902,train_acc:0.9862498641014099
node14 epoch1:node_model train_loss:0.021061505329271313,train_acc:0.9924999475479126
node14_model on test-dataset: loss:0.056856905065505996,acc:0.9833981990814209
node14 weight score:27085.54041458526
node18: train data size:801
node18 epoch0:node_model train_loss:0.018380181553463142,train_acc:0.9933332800865173
node18 epoch1:node_model train_loss:0.02429452682066666,train_acc:0.9922221302986145
node18_model on test-dataset: loss:0.06388233294623205,acc:0.9816990494728088
node18 weight score:12538.677957709826
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.044459111670062154,acc:0.9870949280261994
total cost energy:9.405493029505548 | all_enery_cp：7.779 | all_enery_tp: 1.6264930295055482
ef: 32.343646065605824
reward: 22.938153036100275
step 478:loss:14.497231483459473|running q:97.84396362304688
episode7,iteration58 selected nodes:[8, 3, 15, 18, 10],center node:10
################################################## episode7,iteration58 ##################################################
node3: train data size:3762
node3 epoch0:node_model train_loss:0.015242774272337556,train_acc:0.9949999451637268
node3 epoch1:node_model train_loss:0.01822799543917522,train_acc:0.9936841726303101
node3_model on test-dataset: loss:0.056387863637837654,acc:0.9830968976020813
node3 weight score:66716.48396119772
node8: train data size:2290
node8 epoch0:node_model train_loss:0.035276284385675,train_acc:0.9889854192733765
node8 epoch1:node_model train_loss:0.016384870952764606,train_acc:0.994782567024231
node8_model on test-dataset: loss:0.08533015326283021,acc:0.97759610414505
node8 weight score:26836.937617426305
node10: train data size:1915
node10 epoch0:node_model train_loss:0.0552602062234655,train_acc:0.9854999780654907
node10 epoch1:node_model train_loss:0.024204137630295008,train_acc:0.9929999709129333
node10_model on test-dataset: loss:0.05558336543598671,acc:0.9851971864700317
node10 weight score:34452.75371469606
node15: train data size:1376
node15 epoch0:node_model train_loss:0.036243987968191504,train_acc:0.98906010389328
node15 epoch1:node_model train_loss:0.022031515504100492,train_acc:0.9914284944534302
node15_model on test-dataset: loss:0.06045471475208615,acc:0.9823979735374451
node15 weight score:22760.838515949123
node18: train data size:801
node18 epoch0:node_model train_loss:0.030236032920786075,train_acc:0.9900000095367432
node18 epoch1:node_model train_loss:0.027940558222730436,train_acc:0.9933332800865173
node18_model on test-dataset: loss:0.09034304516175325,acc:0.973996102809906
node18 weight score:8866.205456832482
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.042253604210363845,acc:0.9868979543447495
total cost energy:6.547091111987144 | all_enery_cp：5.071999999999999 | all_enery_tp: 1.475091111987145
ef: 32.11236479683899
reward: 25.565273684851846
step 479:loss:12.668319702148438|running q:99.20761108398438
episode7,iteration59 selected nodes:[17, 12, 6, 1, 3],center node:6
################################################## episode7,iteration59 ##################################################
node1: train data size:6657
node1 epoch0:node_model train_loss:0.03998533113678889,train_acc:0.987200915813446
node1 epoch1:node_model train_loss:0.02045679846239874,train_acc:0.993170976638794
node1_model on test-dataset: loss:0.0716089786440898,acc:0.9780939817428589
node1 weight score:92963.20274426133
node3: train data size:3762
node3 epoch0:node_model train_loss:0.018444051183331266,train_acc:0.9923684000968933
node3 epoch1:node_model train_loss:0.012927996853250079,train_acc:0.9950000643730164
node3_model on test-dataset: loss:0.09530666976557044,acc:0.9748998880386353
node3 weight score:39472.57845913134
node6: train data size:3529
node6 epoch0:node_model train_loss:0.021382794803584047,train_acc:0.991944432258606
node6 epoch1:node_model train_loss:0.023497552963413507,train_acc:0.9913888573646545
node6_model on test-dataset: loss:0.07940070499853277,acc:0.9777951836585999
node6 weight score:44445.449194251
node12: train data size:1406
node12 epoch0:node_model train_loss:0.03435114396270365,train_acc:0.9893332719802856
node12 epoch1:node_model train_loss:0.02545158730451173,train_acc:0.9893332719802856
node12_model on test-dataset: loss:0.09555956709074963,acc:0.9744948148727417
node12 weight score:14713.335805139952
node17: train data size:719
node17 epoch0:node_model train_loss:0.03846937808339135,train_acc:0.9899999499320984
node17 epoch1:node_model train_loss:0.019138245428621303,train_acc:0.9924999475479126
node17_model on test-dataset: loss:0.06324755501005712,acc:0.9825960397720337
node17 weight score:11368.028374941456
start merge all node model param
merge model finish!
global-model on test-dataset:loss:0.04620079911957873,acc:0.9867979544401169
total cost energy:9.557086815591589 | all_enery_cp：8.0365 | all_enery_tp: 1.520586815591588
ef: 31.548922010544146
reward: 21.991835194952557
step 480:loss:19.450489044189453|running q:100.45832061767578
episode7_cost time: 14207.798815727234
